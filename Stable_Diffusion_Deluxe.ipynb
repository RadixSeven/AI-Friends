{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Skquark/AI-Friends/blob/main/Stable_Diffusion_Deluxe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49n2AC6spxQS"
      },
      "source": [
        "# üé® **Stable Diffusion Deluxe Edition** üë®‚Äçüé®Ô∏è - Full Featured Fancy Flet/Flutter Framework\n",
        "\n",
        "*...using `üß®diffusers`* and many advanced bonus features...\n",
        "\n",
        "---\n",
        "### Designed by [**Skquark**, Inc.](https://www.Skquark.com) üòã\n",
        "<p align=center>\n",
        "<a href=\"https://github.com/Skquark/AI-Friends/blob/main/Stable_Diffusion_Deluxe.ipynb\"><img src=\"https://badgen.net/badge/icon/github?icon=github&label\" alt=\"Github\"></a> <a href=\"https://github.com/Skquark/AI-Friends\"><img src=\"https://badgen.net/github/release/Skquark/AI-Friends/stable\" alt=\"Release version\"></a>\n",
        "<a href=\"https://colab.research.google.com/github/Skquark/AI-Friends/blob/main/Stable_Diffusion_Deluxe.ipynb\"><img src=\"https://img.shields.io/badge/Open-in%20Colab-brightgreen?logo=google-colab&style=flat-square\" alt=\"Open in Google Colab\"/></a>\n",
        "</p>\n",
        "\n",
        "*   Runs in a pretty WebUI using [Flet - Flutter for Python](https://flet.dev) with themes, interactivity & sound\n",
        "*   Saves all settings/parameters in your config file, don't need to Copy to Drive\n",
        "*   Run a batch list of prompts at once, so queue many and walk away\n",
        "*   Option to override any parameter per prompt in queue\n",
        "*   Use Stable Diffusion [2.1](https://huggingface.co/stabilityai/stable-diffusion-2-1), [2.0](https://huggingface.co/stabilityai/stable-diffusion-2), [1.5 ](https://huggingface.co/runwayml/stable-diffusion-v1-5), or [1.4](https://huggingface.co/CompVis/stable-diffusion-v1-4) Checkpoint Model File\n",
        "*   Supports many custom community Finetuned Model checkpoints & DreamBooth Libraries\n",
        "*   Supports Stable Diffusion image2image to use an init_image\n",
        "*   Supports Stable Diffusion [Inpaint](https://huggingface.co/stabilityai/stable-diffusion-2-inpainting) mask_image layer\n",
        "*   Supports Negative Prompts to specify what you don't want\n",
        "*   Supports Long Prompt Weighting to emphasize (positive) & [negative] word strengths\n",
        "*   Prompt tweening to combine latent space of 2 prompts in a series\n",
        "*   Can use Interpolation to walk steps between latent space of prompt list\n",
        "*   Can use CLIP Guidance with LAION & OpenAI ViT models\n",
        "*   Can use Textual Inversion Conceptualizer with 760+ Community Concepts\n",
        "*   Can Centipede prompts as init images feeding down the list\n",
        "*   Can use Composable weighted | prompt | segments for img2img\n",
        "*   Can use iMagic to edit init image with your prompt \n",
        "*   Can use Kakaobrain unCLIP Generator, which gives similar results to DALL-E 2\n",
        "*   Can save all images to your Google Drive (PyDrive support soon)\n",
        "*   Can Upscale automatically with Real-ESRGAN enlarging\n",
        "*   Option to use Stability-API tokens for more samplers, bigger size & CPU runtime\n",
        "*   Embeds exif metadata directly into png files\n",
        "*   Disabled NSFW filtering and added custom sampler options\n",
        "*   Renames image filenames to the prompt text, with options\n",
        "*   OpenAI Prompt Generator, Remixer, Brainstormer & Noodle Soup Prompt Writer included\n",
        "*   Standalone ESRGAN Upscaler for batch uploads and image splitting\n",
        "*   Train your own models with DreamBooth, LoRA or Textual-Inversion & upload to HuggingFace\n",
        "*   Replicate Material Diffusion to make Seamless Tile Textures\n",
        "*   Experimental HarmonAI Dance Diffusion audio generator\n",
        "*   Experimental DreamFusion 3D model generator with texture & video\n",
        "*   Image2Text CLIP-Interrogator to get prompt from images\n",
        "*   Bonus Image Generators DALL-E 2 and Kandinsky 2 to compare\n",
        "*   Additional features added regularly...\n",
        "\n",
        "If you have a powerful enough GPU with > 8GB VRAM, you can run Desktop app locally with [SDD-setup.exe](https://colab.research.google.com/github/Skquark/AI-Friends/blob/main/Stable-Diffusion-Deluxe/SDD-setup.exe) or [sdd-linux.sh](https://colab.research.google.com/github/Skquark/AI-Friends/blob/main/Stable-Diffusion-Deluxe/sdd-linux.sh). Still in beta...\n",
        "\n",
        "Can also use origional Colab implementation of [Enhanced Stable Diffusion](https://colab.research.google.com/github/Skquark/structured-prompt-generator/blob/main/Enhanced_Stable_Diffusion_with_diffusers.ipynb) instead..\n",
        "\n",
        "Try these other useful notebooks [Enhanced DiscoArt](https://colab.research.google.com/github/Skquark/structured-prompt-generator/blob/main/DiscoArt_%5B_w_Batch_Prompts_%26_GPT_3_Generator%5D.ipynb) and [Structured Prompt Generator](https://colab.research.google.com/github/Skquark/structured-prompt-generator/blob/main/Structured_Prompt_Generator.ipynb)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "jf90tMc3tu9q"
      },
      "outputs": [],
      "source": [
        "#@title üñ•Ô∏è Check GPU Status (A100 > G100 > V100 > P100 > T4 > K8)\n",
        "import subprocess\n",
        "simple_nvidia_smi_display = True #@param {type:\"boolean\"}\n",
        "if simple_nvidia_smi_display:\n",
        "    print(subprocess.run(['nvidia-smi', '-L'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "else:\n",
        "    print(subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "    print(subprocess.run(['nvidia-smi', '-i', '0', '-e', '0'], stdout=subprocess.PIPE).stdout.decode('utf-8'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EH3ClNnvV1S"
      },
      "source": [
        "# üß∞ **Stable Diffusion Deluxe** - Easy Web App Launcher\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "KTwZrpxEuKR0"
      },
      "outputs": [],
      "source": [
        "#@title ## ‚öôÔ∏è Install WebUI Framework & Initiallize Settings\n",
        "#markdown If you're running on Google Colab, authorize Google Drive with the popup to connect storage. If you're on a local or cloud Jupyter Notebook, you must get a OAuth json using instructions below to save images to GDrive, however the feature is not currently working and in progress. When set, continue to run the Web UI and experiment away..\n",
        "#@markdown We'll connect to your Google Drive and save all of your preferences in realtime there, as well as your created images. This is the folder location and file name we recommend in your mounted gdrive, will be created if you're new, but you can save elsewhere.  Launches webpage with localtunnel, but in case it's down you can use ngrok instead.\n",
        "storage_type = \"Colab Google Drive\" #@param [\"Colab Google Drive\", \"Local Drive\"]\n",
        "#, \"PyDrive Google Drive\"\n",
        "Google_OAuth_client_secret_json = \"/content/client_secrets.json\" #param {'type': 'string'}\n",
        "save_to_GDrive = True #param {'type': 'boolean'}\n",
        "saved_settings_json = '/content/drive/MyDrive/AI/Stable_Diffusion/sdd-settings.json' #@param {'type': 'string'}\n",
        "tunnel_type = \"localtunnel\" #@param [\"localtunnel\", \"ngrok\"] \n",
        "#, \"cloudflared\"\n",
        "auto_launch_website = True #@param {'type': 'boolean'}\n",
        "force_updates = True\n",
        "SDD_version = \"v1.8.0\"\n",
        "import os, subprocess, sys, shutil, requests\n",
        "import random as rnd\n",
        "from IPython.display import clear_output\n",
        "root_dir = '/content/'\n",
        "dist_dir = root_dir\n",
        "is_Colab = True\n",
        "try:\n",
        "  import google.colab\n",
        "  root_dir = '/content/'\n",
        "except:\n",
        "  root_dir = os.getcwd()\n",
        "  dist_dir = os.path.join(root_dir, 'dist', 'Stable-Diffusion-Deluxe')\n",
        "  if not os.path.isdir(dist_dir):\n",
        "    dist_dir = root_dir\n",
        "  print(f'Root: {root_dir} Dist:{dist_dir}')\n",
        "  is_Colab = False\n",
        "  pass\n",
        "stable_dir = root_dir\n",
        "env = os.environ.copy()\n",
        "def run_sp(cmd_str, cwd=None, realtime=True):\n",
        "  cmd_list = cmd_str if type(cmd_str) is list else cmd_str.split()\n",
        "  if realtime:\n",
        "    if cwd is None:\n",
        "      process = subprocess.Popen(cmd_str, shell = True, env=env, bufsize = 1, stdout=subprocess.PIPE, stderr = subprocess.STDOUT, encoding='utf-8', errors = 'replace' ) \n",
        "    else:\n",
        "      process = subprocess.Popen(cmd_str, shell = True, cwd=cwd, env=env, bufsize = 1, stdout=subprocess.PIPE, stderr = subprocess.STDOUT, encoding='utf-8', errors = 'replace' ) \n",
        "    while True:\n",
        "      realtime_output = process.stdout.readline()\n",
        "      if realtime_output == '' and process.poll() is not None:\n",
        "        break\n",
        "      if realtime_output:\n",
        "        print(realtime_output.strip(), flush=False)\n",
        "        sys.stdout.flush()\n",
        "  else:\n",
        "    if cwd is None:\n",
        "      return subprocess.run(cmd_list, stdout=subprocess.PIPE, env=env).stdout.decode('utf-8')\n",
        "    else:\n",
        "      return subprocess.run(cmd_list, stdout=subprocess.PIPE, env=env, cwd=cwd).stdout.decode('utf-8')\n",
        "save_to_GDrive = storage_type == \"Colab Google Drive\"\n",
        "if save_to_GDrive:\n",
        "  if not os.path.isdir(f'{root_dir}drive'):\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "elif storage_type == \"PyDrive Google Drive\":\n",
        "  \"pip install PyDrive2\"\n",
        "stable_dir = os.path.join(root_dir, 'Stable_Diffusion')\n",
        "if not os.path.exists(stable_dir):\n",
        "  os.makedirs(stable_dir)\n",
        "#if not os.path.exists(image_output):\n",
        "#  os.makedirs(image_output)\n",
        "sample_data = '/content/sample_data'\n",
        "if os.path.exists(sample_data):\n",
        "  for f in os.listdir(sample_data):\n",
        "    os.remove(os.path.join(sample_data, f))\n",
        "  os.rmdir(sample_data)\n",
        "os.chdir(stable_dir)\n",
        "#loaded_Stability_api = False\n",
        "#loaded_img2img = False\n",
        "#use_Stability_api = False\n",
        "def version_checker():\n",
        "  response = requests.get(\"https://raw.githubusercontent.com/Skquark/AI-Friends/main/DSD_version.txt\")\n",
        "  current_v = response.text.strip()\n",
        "  if current_v != SDD_version:\n",
        "    print(f'A new update is available. You are running {SDD_version} and {current_v} is up. We recommended refreshing Stable Diffusion Deluxe for the latest cool features or fixes.\\nhttps://colab.research.google.com/github/Skquark/AI-Friends/blob/main/Stable_Diffusion_Deluxe.ipynb\\nChangelog if interested: https://github.com/Skquark/AI-Friends/commits/main/Stable_Diffusion_Deluxe.ipynb')\n",
        "def ng():\n",
        "  response = requests.get(\"https://raw.githubusercontent.com/Skquark/AI-Friends/main/_ng\")\n",
        "  ng_list = response.text.strip().split('\\n')\n",
        "  _ng = rnd.choice(ng_list).partition('_')\n",
        "  return _ng[2]+_ng[1]+_ng[0]\n",
        "\n",
        "def download_file(url, to=None):\n",
        "    local_filename = url.split(slash)[-1]\n",
        "    if '?' in local_filename:\n",
        "        local_filename = local_filename.rpartition('?')[0]\n",
        "    local_filename = os.path.join(to if to != None else root_dir, local_filename)\n",
        "    with requests.get(url, stream=True) as r:\n",
        "        with open(local_filename, 'wb') as f:\n",
        "            shutil.copyfileobj(r.raw, f)\n",
        "    return local_filename\n",
        "\n",
        "try:\n",
        "  import flet\n",
        "except ImportError as e:\n",
        "  run_sp(\"pip install --upgrade flet==0.3.0\", realtime=False)\n",
        "  #run_sp(\"pip install -i https://test.pypi.org/simple/ flet\")\n",
        "  #run_sp(\"pip install --upgrade git+https://github.com/flet-dev/flet.git@controls-s3#egg=flet-dev\")\n",
        "  pass\n",
        "\n",
        "try:\n",
        "  from emoji import emojize\n",
        "except ImportError as e:\n",
        "  run_sp(\"pip install emoji --quiet\", realtime=False)\n",
        "  from emoji import emojize\n",
        "  pass\n",
        "if 'url' not in locals():\n",
        "  url=\"\"\n",
        "if tunnel_type == \"ngrok\":\n",
        "  try:\n",
        "    import pyngrok\n",
        "  except ImportError as e:\n",
        "    run_sp(\"pip install pyngrok --quiet\", realtime=False)\n",
        "    #run_sp(f\"ngrok authtoken {ng()}\", realtime=False)\n",
        "    run_sp(f\"ngrok config add-authtoken {ng()}\", realtime=False)\n",
        "    run_sp(\"ngrok config upgrade\", realtime=False)\n",
        "    import pyngrok\n",
        "    pass\n",
        "elif tunnel_type == \"localtunnel\":\n",
        "  if not bool(url):\n",
        "    import re\n",
        "    run_sp(\"npm install -g -q localtunnel\", realtime=False)\n",
        "    #localtunnel = subprocess.Popen(['lt', '--port', '80', 'http'], stdout=subprocess.PIPE)\n",
        "    #url = str(localtunnel.stdout.readline())\n",
        "    #url = (re.search(\"(?P<url>https?:\\/\\/[^\\s]+loca.lt)\", url).group(\"url\"))\n",
        "    #print(url)\n",
        "\n",
        "gdrive = None\n",
        "if storage_type == \"PyDrive Google Drive\":\n",
        "  if not os.path.isfile(Google_OAuth_client_secret_json):\n",
        "    raise ValueError(\"Couldn't locate your client_secret.json file to authenticate. Follow instructions below then copy certificate to your root dir.\")\n",
        "  try:\n",
        "    from pydrive2.auth import GoogleAuth, ServiceAccountCredentials\n",
        "    from pydrive2.drive import GoogleDrive\n",
        "    from oauth2client.contrib.gce import AppAssertionCredentials\n",
        "  except ImportError as e:\n",
        "    run_sp(\"pip install PyDrive2 -q\")\n",
        "    from pydrive2.auth import GoogleAuth, ServiceAccountCredentials\n",
        "    from pydrive2.drive import GoogleDrive\n",
        "    from oauth2client.contrib.gce import AppAssertionCredentials\n",
        "    pass\n",
        "  import httplib2\n",
        "\n",
        "  old_local_webserver_auth = GoogleAuth.LocalWebserverAuth\n",
        "  def LocalWebServerAuth(self, *args, **kwargs):\n",
        "      if isinstance(self.credentials, AppAssertionCredentials):\n",
        "          self.credentials.refresh(httplib2.Http())\n",
        "          return\n",
        "      return old_local_webserver_auth(self, *args, **kwargs)\n",
        "  GoogleAuth.LocalWebserverAuth = LocalWebServerAuth\n",
        "\n",
        "  #scope = 'https://www.googleapis.com/auth/drive'\n",
        "  #credentials = ServiceAccountCredentials.from_json_keyfile_name(Google_OAuth_client_secret_json, scope)\n",
        "  gauth = GoogleAuth()\n",
        "  gauth.LoadCredentialsFile(Google_OAuth_client_secret_json)\n",
        "  #gauth.LocalWebserverAuth()\n",
        "  if is_Colab: gauth.CommandLineAuth()\n",
        "  else:\n",
        "    gauth.LocalWebserverAuth()\n",
        "    gauth.SaveCredentialsFile(Google_OAuth_client_secret_json)\n",
        "  gdrive = GoogleDrive(gauth)\n",
        "slash = '/'\n",
        "from pathlib import Path\n",
        "if not is_Colab:\n",
        "    image_output = os.path.join(Path.home(), \"Pictures\", \"Stable_Diffusion\")\n",
        "    if \"\\\\\" in image_output:\n",
        "        slash = '\\\\'\n",
        "else:\n",
        "    image_output = '/content/drive/MyDrive/AI/Stable_Diffusion/images_out'\n",
        "\n",
        "favicon = os.path.join(root_dir, \"favicon.png\")\n",
        "assets = os.path.join(root_dir, \"assets\")\n",
        "if not os.path.isfile(favicon):\n",
        "    download_file(\"https://github.com/Skquark/AI-Friends/blob/main/assets/favicon.png?raw=true\")\n",
        "if not os.path.exists(assets):\n",
        "    os.makedirs(assets)\n",
        "    download_file(\"https://github.com/Skquark/AI-Friends/blob/main/assets/snd-alert.mp3?raw=true\", to=assets)\n",
        "    download_file(\"https://github.com/Skquark/AI-Friends/blob/main/assets/snd-delete.mp3?raw=true\", to=assets)\n",
        "    download_file(\"https://github.com/Skquark/AI-Friends/blob/main/assets/snd-error.mp3?raw=true\", to=assets)\n",
        "    download_file(\"https://github.com/Skquark/AI-Friends/blob/main/assets/snd-done.mp3?raw=true\", to=assets)\n",
        "    download_file(\"https://github.com/Skquark/AI-Friends/blob/main/assets/snd-drop.mp3?raw=true\", to=assets)\n",
        "#clear_output()\n",
        "\n",
        "import json\n",
        "prefs = {}\n",
        "def load_settings_file():\n",
        "  global prefs\n",
        "  if os.path.isfile(saved_settings_json):\n",
        "    with open(saved_settings_json) as settings:\n",
        "      prefs = json.load(settings)\n",
        "    print(\"Successfully loaded settings json...\")\n",
        "  else:\n",
        "    print(\"Settings file not found, starting with defaults...\")\n",
        "    prefs = {\n",
        "      'save_to_GDrive': True,\n",
        "      'image_output': image_output,\n",
        "      'file_prefix': 'sd-',\n",
        "      'file_suffix_seed': False,\n",
        "      'file_max_length': 220,\n",
        "      'file_allowSpace': False,\n",
        "      'save_image_metadata': True,\n",
        "      'meta_ArtistName':'',\n",
        "      'meta_Copyright': '',\n",
        "      'save_config_in_metadata': True,\n",
        "      'save_config_json': False,\n",
        "      'theme_mode': 'Dark',\n",
        "      'theme_color': 'Green',\n",
        "      'enable_sounds': True,\n",
        "      'start_in_installation': False,\n",
        "      'disable_nsfw_filter': True,\n",
        "      'retry_attempts': 3,\n",
        "      'HuggingFace_api_key': \"\",\n",
        "      'Stability_api_key': \"\",\n",
        "      'OpenAI_api_key': \"\",\n",
        "      'TextSynth_api_key': \"\",\n",
        "      'Replicate_api_key': \"\",\n",
        "      'AIHorde_api_key': \"0000000000\",\n",
        "      'HuggingFace_username': \"\",\n",
        "      'scheduler_mode': \"DDIM\",\n",
        "      'higher_vram_mode': False,\n",
        "      'enable_attention_slicing': True,\n",
        "      'memory_optimization': \"Attention Slicing\",\n",
        "      'sequential_cpu_offload': False,\n",
        "      'vae_slicing': True,\n",
        "      'vae_tiling': False,\n",
        "      'enable_torch_compile': False,\n",
        "      'enable_tome': False,\n",
        "      'tome_ratio': 0.5,\n",
        "      'cache_dir': '',\n",
        "      'install_diffusers': True,\n",
        "      'install_interpolation': False,\n",
        "      'install_text2img': True,\n",
        "      'install_img2img': False,\n",
        "      'install_megapipe': True,\n",
        "      'install_CLIP_guided': False,\n",
        "      'install_OpenAI': False,\n",
        "      'install_TextSynth': False,\n",
        "      'install_dreamfusion': False,\n",
        "      'install_repaint': False,\n",
        "      'install_imagic': False,\n",
        "      'install_composable': False,\n",
        "      'install_safe': False,\n",
        "      'install_versatile': False,\n",
        "      'install_depth2img': False,\n",
        "      'install_alt_diffusion': False,\n",
        "      'install_attend_and_excite': False,\n",
        "      'install_SAG': False,\n",
        "      'install_panorama': False,\n",
        "      'install_upscale': False,\n",
        "      'safety_config': 'Strong',\n",
        "      'use_imagic': False,\n",
        "      'use_composable': False,\n",
        "      'use_safe': False,\n",
        "      'use_versatile': False,\n",
        "      'use_alt_diffusion': False,\n",
        "      'use_attend_and_excite': False,\n",
        "      'max_iter_to_alter': 25,\n",
        "      'use_SAG': False,\n",
        "      'sag_scale': 0.75,\n",
        "      'use_panorama': False,\n",
        "      'panorama_width': 2048,\n",
        "      'use_upscale': False,\n",
        "      'upscale_noise_level': 20,\n",
        "      'install_conceptualizer': False,\n",
        "      'use_conceptualizer': False,\n",
        "      'concepts_model': 'cat-toy',\n",
        "      'model_ckpt': 'Stable Diffusion v1.5',\n",
        "      'finetuned_model': 'Midjourney v4 style',\n",
        "      'dreambooth_model': 'disco-diffusion-style',\n",
        "      'custom_model': '',\n",
        "      'custom_models': [],\n",
        "      'tortoise_custom_voices': [],\n",
        "      'custom_dance_diffusion_models': [],\n",
        "      'clip_model_id': \"laion/CLIP-ViT-B-32-laion2B-s34B-b79K\",\n",
        "      'install_Stability_api': False,\n",
        "      'use_Stability_api': False,\n",
        "      'model_checkpoint': \"stable-diffusion-768-v2-1\",\n",
        "      'generation_sampler': \"K_EULER_ANCESTRAL\",\n",
        "      'clip_guidance_preset': \"FAST_BLUE\",\n",
        "      'install_AIHorde_api': False,\n",
        "      'use_AIHorde_api': False,\n",
        "      'AIHorde_model': 'stable_diffusion',\n",
        "      'AIHorde_sampler': 'k_euler_a',\n",
        "      'AIHorde_post_processing': \"None\",\n",
        "      'install_ESRGAN': True,\n",
        "      'batch_folder_name': \"\",\n",
        "      'batch_size': 1,\n",
        "      'n_iterations': 1,\n",
        "      'steps': 50,\n",
        "      'eta': 0.4,\n",
        "      'seed': 0,\n",
        "      'guidance_scale': 8,\n",
        "      'width': 960,\n",
        "      'height': 512,\n",
        "      'init_image': \"\",\n",
        "      'mask_image': \"\",\n",
        "      'init_image_strength': 0.25,\n",
        "      'alpha_mask': False,\n",
        "      'invert_mask': False,\n",
        "      'precision': 'autocast',\n",
        "      'use_inpaint_model': False,\n",
        "      'centipede_prompts_as_init_images': False,\n",
        "      'use_depth2img': False,\n",
        "      'use_LoRA_model': False,\n",
        "      'LoRA_model': 'Von Platen LoRA',\n",
        "      'custom_LoRA_models': [],\n",
        "      'custom_LoRA_model': \"\",\n",
        "      'use_interpolation': False,\n",
        "      'num_interpolation_steps': 22,\n",
        "      'use_clip_guided_model': False,\n",
        "      'clip_guidance_scale': 571,\n",
        "      'use_cutouts': True,\n",
        "      'num_cutouts': 4,\n",
        "      'unfreeze_unet': True,\n",
        "      'unfreeze_vae': True,\n",
        "      'apply_ESRGAN_upscale': True,\n",
        "      'enlarge_scale': 1.5,\n",
        "      'face_enhance':False,\n",
        "      'display_upscaled_image': False,\n",
        "      'prompt_list': [],\n",
        "      'prompt_generator': {\n",
        "          'phrase': '',\n",
        "          'subject_detail': '',\n",
        "          'phrase_as_subject': False,\n",
        "          'amount': 10,\n",
        "          'random_artists': 2,\n",
        "          'random_styles': 1,\n",
        "          'permutate_artists': False,\n",
        "          'request_mode': 3,\n",
        "          'AI_temperature': 0.9,\n",
        "          'AI_engine': \"OpenAI GPT-3\",\n",
        "          'economy_mode': True,\n",
        "      },\n",
        "      'prompt_remixer': {\n",
        "          'seed_prompt': '',\n",
        "          'optional_about_influencer': '',\n",
        "          'amount': 10,\n",
        "          'random_artists': 2,\n",
        "          'random_styles': 1,\n",
        "          'permutate_artists': False,\n",
        "          'request_mode': 3,\n",
        "          'AI_temperature': 0.9,\n",
        "          'AI_engine': \"OpenAI GPT-3\",\n",
        "      },\n",
        "      'prompt_brainstormer': {\n",
        "          'AI_engine': 'OpenAI GPT-3',\n",
        "          'about_prompt': '',\n",
        "          'request_mode': 'Brainstorm',\n",
        "          'AI_temperature': 0.9,\n",
        "      },\n",
        "      'prompt_writer': {\n",
        "          'art_Subjects': '',\n",
        "          'negative_prompt': '',\n",
        "          'by_Artists': '',\n",
        "          'art_Styles': '',\n",
        "          'amount': 10,\n",
        "          'random_artists': 2,\n",
        "          'random_styles': 1,\n",
        "          'permutate_artists': False,\n",
        "      },\n",
        "    }\n",
        "\n",
        "load_settings_file()\n",
        "version_checker()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "SV5d8rB5uYZC"
      },
      "outputs": [],
      "source": [
        "#@title ## **‚ñ∂Ô∏è Run Stable Diffusion Deluxe** - Flet/Flutter WebUI App\n",
        "import flet as ft\n",
        "#from flet import *\n",
        "from flet import Page, View, Column, Row, ResponsiveRow, Container, Text, Stack, TextField, Checkbox, Switch, Image, ElevatedButton, FilledButton, IconButton, Markdown, Tab, Tabs, AppBar, Divider, VerticalDivider, GridView, Tooltip, SnackBar, AnimatedSwitcher, ButtonStyle, FloatingActionButton, Audio, Theme, Dropdown, Slider, ListTile, ListView, TextButton, PopupMenuButton, PopupMenuItem, AlertDialog, Banner, Icon, ProgressBar, ProgressRing, GestureDetector, KeyboardEvent, FilePicker, FilePickerResultEvent, FilePickerUploadFile, FilePickerUploadEvent, UserControl, Ref\n",
        "from flet import icons, dropdown, colors, padding, margin, alignment, border_radius, theme, animation, KeyboardType, TextThemeStyle, AnimationCurve\n",
        "from flet import TextAlign, FontWeight, ClipBehavior, MainAxisAlignment, CrossAxisAlignment, ScrollMode, ImageFit, ThemeMode\n",
        "from flet import Image as Img\n",
        "try:\n",
        "    import PIL\n",
        "except ModuleNotFoundError:\n",
        "    run_sp(\"pip install Pillow\", realtime=False)\n",
        "    run_sp(\"pip install image\", realtime=False)\n",
        "    import PIL\n",
        "    pass\n",
        "from PIL import Image as PILImage # Avoids flet conflict\n",
        "import random as rnd\n",
        "import io, shutil, traceback\n",
        "from packaging import version\n",
        "from contextlib import redirect_stdout\n",
        "try:\n",
        "  import numpy as np\n",
        "except ModuleNotFoundError:\n",
        "  run_sp(\"pip install numpy\", realtime=False)\n",
        "  import numpy as np\n",
        "  pass\n",
        "\n",
        "if 'prefs' not in locals():\n",
        "    raise ValueError(\"Setup not initialized. Run the previous code block first and authenticate your Drive storage.\")\n",
        "status = {\n",
        "    'installed_diffusers': False,\n",
        "    'installed_txt2img': False,\n",
        "    'installed_img2img': False,\n",
        "    'installed_stability': False,\n",
        "    'installed_AIHorde': False,\n",
        "    'installed_megapipe': False,\n",
        "    'installed_interpolation': False,\n",
        "    'installed_clip': False,\n",
        "    'installed_ESRGAN': False,\n",
        "    'installed_OpenAI': False,\n",
        "    'installed_TextSynth': False,\n",
        "    'installed_conceptualizer': False,\n",
        "    'installed_dreamfusion': False,\n",
        "    'installed_repaint': False,\n",
        "    'installed_imagic': False,\n",
        "    'installed_composable': False,\n",
        "    'installed_safe': False,\n",
        "    'installed_versatile': False,\n",
        "    'installed_depth2img': False,\n",
        "    'installed_attend_and_excite': False,\n",
        "    'installed_SAG': False,\n",
        "    'installed_panorama': False,\n",
        "    'installed_alt_diffusion': False,\n",
        "    'installed_upscale': False,\n",
        "    'installed_xformers': False,\n",
        "    'finetuned_model': False,\n",
        "    'loaded_scheduler': '',\n",
        "    'loaded_model': '',\n",
        "    'loaded_controlnet': '',\n",
        "    'changed_settings': False,\n",
        "    'changed_installers': False,\n",
        "    'changed_parameters': False,\n",
        "    'changed_prompts': False,\n",
        "    'changed_prompt_generator': False,\n",
        "    'changed_prompt_remixer': False,\n",
        "    'changed_prompt_brainstormer': False,\n",
        "    'changed_prompt_writer': False,\n",
        "    'initialized': False,\n",
        "}\n",
        "\n",
        "def save_settings_file(page, change_icon=True):\n",
        "  if change_icon:\n",
        "    page.app_icon_save()\n",
        "  if not os.path.isfile(saved_settings_json):\n",
        "    settings_path = saved_settings_json.rpartition(slash)[0]\n",
        "    os.makedirs(settings_path, exist_ok=True)\n",
        "  with open(saved_settings_json, \"w\") as write_file:\n",
        "    json.dump(prefs, write_file, indent=4)\n",
        "\n",
        "current_tab = 0\n",
        "def tab_on_change (e):\n",
        "    t = e.control\n",
        "    global current_tab, status\n",
        "    #print (f\"tab changed from {current_tab} to: {t.selected_index}\")\n",
        "    #print(str(t.tabs[t.selected_index].text))\n",
        "    if current_tab == 0:\n",
        "      #if not status['initialized']:\n",
        "      #  initState(e.page)\n",
        "      #  status['initialized'] = True\n",
        "      if status['changed_settings']:\n",
        "        save_settings_file(e.page)\n",
        "        status['changed_settings'] = False\n",
        "        #print(len(e.page.Settings.content.controls))\n",
        "        #save_settings(e.page.Settings.content.controls)\n",
        "    if current_tab == 1:\n",
        "      if status['changed_installers']:\n",
        "        save_settings_file(e.page)\n",
        "        status['changed_installers'] = False\n",
        "        #print(\"Saving Installers\")\n",
        "      e.page.show_install_fab(False)\n",
        "    if current_tab == 2:\n",
        "      if status['changed_parameters']:\n",
        "        update_args()\n",
        "        e.page.update_prompts()\n",
        "        save_settings_file(e.page)\n",
        "        status['changed_parameters'] = False\n",
        "      e.page.show_apply_fab(False)\n",
        "    if current_tab == 3:\n",
        "      if status['changed_prompts']:\n",
        "        e.page.save_prompts()\n",
        "        save_settings_file(e.page)\n",
        "        status['changed_prompts'] = False\n",
        "      e.page.show_run_diffusion_fab(False, p=e.page)\n",
        "    if current_tab == 5:\n",
        "      if status['changed_prompt_generator']:\n",
        "        save_settings_file(e.page)\n",
        "        status['changed_prompt_generator'] = False\n",
        "    \n",
        "    current_tab = t.selected_index\n",
        "    if current_tab == 1:\n",
        "      refresh_installers(e.page.Installers.controls[0].content.controls)\n",
        "      e.page.show_install_fab(True)\n",
        "      #page.Installers.init_boxes()\n",
        "    if current_tab == 2:\n",
        "      update_parameters(e.page)\n",
        "      #for p in e.page.Parameters.content.controls:\n",
        "      e.page.Parameters.controls[0].content.update()\n",
        "      e.page.Parameters.update()\n",
        "      e.page.show_apply_fab(len(prompts) > 0 and status['changed_parameters'])\n",
        "    if current_tab == 3:\n",
        "      e.page.show_run_diffusion_fab(len(prompts) > 0, p=e.page)\n",
        "    e.page.update()\n",
        "\n",
        "def buildTabs(page):\n",
        "    page.Settings = buildSettings(page)\n",
        "    page.Installers = buildInstallers(page)\n",
        "    page.Parameters = buildParameters(page)\n",
        "    page.PromptsList = buildPromptsList(page)\n",
        "    page.PromptHelpers = buildPromptHelpers(page)\n",
        "    page.Images = buildImages(page)\n",
        "    page.StableDiffusers = buildStableDiffusers(page)\n",
        "    page.Trainers = buildTrainers(page)\n",
        "    page.AudioAIs = buildAudioAIs(page)\n",
        "    page.Text3DAIs = build3DAIs(page)\n",
        "    page.Extras = buildExtras(page)\n",
        "    \n",
        "    t = Tabs(selected_index=0, animation_duration=300, expand=1,\n",
        "        tabs=[\n",
        "            Tab(text=\"Settings\", content=page.Settings, icon=icons.SETTINGS_OUTLINED),\n",
        "            Tab(text=\"Installation\", content=page.Installers, icon=icons.INSTALL_DESKTOP),\n",
        "            Tab(text=\"Image Parameters\", content=page.Parameters, icon=icons.DISPLAY_SETTINGS),\n",
        "            Tab(text=\"Prompts List\", content=page.PromptsList, icon=icons.FORMAT_LIST_BULLETED),\n",
        "            Tab(text=\"Generate Images\", content=page.Images, icon=icons.IMAGE_OUTLINED),\n",
        "            Tab(text=\"Prompt Helpers\", content=page.PromptHelpers, icon=icons.BUBBLE_CHART_OUTLINED),\n",
        "            Tab(text=\"Stable Diffusers\", content=page.StableDiffusers, icon=icons.PALETTE),\n",
        "            Tab(text=\"3D AIs\", content=page.Text3DAIs, icon=icons.VIEW_IN_AR),\n",
        "            Tab(text=\"Audio AIs\", content=page.AudioAIs, icon=icons.EQUALIZER),\n",
        "            Tab(text=\"AI Trainers\", content=page.Trainers, icon=icons.TSUNAMI),\n",
        "            Tab(text=\"Extras\", content=page.Extras, icon=icons.ALL_INBOX),\n",
        "        ],\n",
        "    )\n",
        "    page.tabs = t\n",
        "    return t\n",
        "\n",
        "def buildPromptHelpers(page):\n",
        "    def changed(e, pref=None):\n",
        "      if pref is not None:\n",
        "        prefs[pref] = e.control.value\n",
        "      status['changed_prompt_helpers'] = True\n",
        "    page.generator = buildPromptGenerator(page)\n",
        "    page.remixer = buildPromptRemixer(page)\n",
        "    page.brainstormer = buildPromptBrainstormer(page)\n",
        "    page.writer = buildPromptWriter(page)\n",
        "    page.Image2Text = buildImage2Text(page)\n",
        "    page.MagicPrompt = buildMagicPrompt(page)\n",
        "    page.DistilGPT2 = buildDistilGPT2(page)\n",
        "    page.RetrievePrompts = buildRetrievePrompts(page)\n",
        "    page.InitFolder = buildInitFolder(page)\n",
        "    page.InitVideo = buildInitVideo(page)\n",
        "    promptTabs = Tabs(\n",
        "        selected_index=0,\n",
        "        animation_duration=300,\n",
        "        tabs=[\n",
        "            Tab(text=\"Prompt Writer\", content=page.writer, icon=icons.CLOUD_CIRCLE),\n",
        "            Tab(text=\"Prompt Generator\", content=page.generator, icon=icons.CLOUD),\n",
        "            Tab(text=\"Prompt Remixer\", content=page.remixer, icon=icons.CLOUD_SYNC_ROUNDED),\n",
        "            Tab(text=\"Prompt Brainstormer\", content=page.brainstormer, icon=icons.CLOUDY_SNOWING),\n",
        "            Tab(text=\"Image2Text\", content=page.Image2Text, icon=icons.WRAP_TEXT),\n",
        "            Tab(text=\"Magic Prompt\", content=page.MagicPrompt, icon=icons.AUTO_FIX_HIGH),\n",
        "            Tab(text=\"Distil GPT-2\", content=page.DistilGPT2, icon=icons.FILTER_ALT),\n",
        "            Tab(text=\"Retrieve Prompt from Image\", content=page.RetrievePrompts, icon=icons.PHOTO_LIBRARY_OUTLINED),\n",
        "            Tab(text=\"Init Images from Folder\", content=page.InitFolder, icon=icons.FOLDER_SPECIAL),\n",
        "            Tab(text=\"Init Images from Video\", content=page.InitVideo, icon=icons.SWITCH_VIDEO),\n",
        "        ],\n",
        "        expand=1,\n",
        "        #on_change=tab_on_change\n",
        "    )\n",
        "    return promptTabs\n",
        "\n",
        "def buildStableDiffusers(page):\n",
        "    page.RePainter = buildRepainter(page)\n",
        "    page.unCLIP = buildUnCLIP(page)\n",
        "    page.unCLIP_Interpolation = buildUnCLIP_Interpolation(page)\n",
        "    page.unCLIP_ImageInterpolation = buildUnCLIP_ImageInterpolation(page)\n",
        "    page.UnCLIP_ImageVariation = buildUnCLIP_ImageVariation(page)\n",
        "    page.EDICT = buildEDICT(page)\n",
        "    page.DiffEdit = buildDiffEdit(page)\n",
        "    page.ImageVariation = buildImageVariation(page)\n",
        "    page.CLIPstyler = buildCLIPstyler(page)\n",
        "    page.MagicMix = buildMagicMix(page)\n",
        "    page.SemanticGuidance = buildSemanticGuidance(page)\n",
        "    page.PaintByExample = buildPaintByExample(page)\n",
        "    page.InstructPix2Pix = buildInstructPix2Pix(page)\n",
        "    page.ControlNet = buildControlNet(page)\n",
        "    page.DeepFloyd = buildDeepFloyd(page)\n",
        "    page.TextToVideo = buildTextToVideo(page)\n",
        "    page.TextToVideoZero = buildTextToVideoZero(page)\n",
        "    page.StableAnimation = buildStableAnimation(page)\n",
        "    page.MaterialDiffusion = buildMaterialDiffusion(page)\n",
        "    page.MaskMaker = buildDreamMask(page)\n",
        "    page.DiT = buildDiT(page)\n",
        "    page.DreamFusion = buildDreamFusion(page)\n",
        "    page.Point_E = buildPoint_E(page)\n",
        "    page.Shap_E = buildShap_E(page)\n",
        "    page.InstantNGP = buildInstantNGP(page)\n",
        "    diffusersTabs = Tabs(\n",
        "        selected_index=0,\n",
        "        animation_duration=300,\n",
        "        tabs=[\n",
        "            Tab(text=\"Instruct Pix2Pix\", content=page.InstructPix2Pix, icon=icons.SOLAR_POWER),\n",
        "            Tab(text=\"ControlNet\", content=page.ControlNet, icon=icons.HUB),\n",
        "            Tab(text=\"DeepFloyd\", content=page.DeepFloyd, icon=icons.LOOKS),\n",
        "            Tab(text=\"unCLIP\", content=page.unCLIP, icon=icons.ATTACHMENT_SHARP),\n",
        "            Tab(text=\"unCLIP Interpolation\", content=page.unCLIP_Interpolation, icon=icons.TRANSFORM),\n",
        "            Tab(text=\"unCLIP Image Interpolation\", content=page.unCLIP_ImageInterpolation, icon=icons.ANIMATION),\n",
        "            Tab(text=\"unCLIP Image Variation\", content=page.UnCLIP_ImageVariation, icon=icons.AIRLINE_STOPS),\n",
        "            Tab(text=\"Image Variation\", content=page.ImageVariation, icon=icons.FORMAT_COLOR_FILL),\n",
        "            Tab(text=\"EDICT Edit\", content=page.EDICT, icon=icons.AUTO_AWESOME),\n",
        "            Tab(text=\"DiffEdit\", content=page.DiffEdit, icon=icons.AUTO_GRAPH),\n",
        "            Tab(text=\"RePainter\", content=page.RePainter, icon=icons.FORMAT_PAINT),\n",
        "            Tab(text=\"MagicMix\", content=page.MagicMix, icon=icons.BLENDER),\n",
        "            Tab(text=\"Paint-by-Example\", content=page.PaintByExample, icon=icons.FORMAT_SHAPES),\n",
        "            Tab(text=\"CLIP-Styler\", content=page.CLIPstyler, icon=icons.STYLE),\n",
        "            Tab(text=\"Semantic Guidance\", content=page.SemanticGuidance, icon=icons.ROUTE),\n",
        "            Tab(text=\"Text-to-Video\", content=page.TextToVideo, icon=icons.MISSED_VIDEO_CALL),\n",
        "            Tab(text=\"Text-to-Video Zero\", content=page.TextToVideoZero, icon=icons.ONDEMAND_VIDEO),\n",
        "            Tab(text=\"Stable Animation\", content=page.StableAnimation, icon=icons.SHUTTER_SPEED),\n",
        "            Tab(text=\"Material Diffusion\", content=page.MaterialDiffusion, icon=icons.TEXTURE),\n",
        "            Tab(text=\"DiT\", content=page.DiT, icon=icons.ANALYTICS),\n",
        "            #Tab(text=\"DreamFusion 3D\", content=page.DreamFusion, icon=icons.THREED_ROTATION),\n",
        "            #Tab(text=\"Point-E 3D\", content=page.Point_E, icon=icons.SWIPE_UP),\n",
        "            #Tab(text=\"Shap-E 3D\", content=page.Shap_E, icon=icons.PRECISION_MANUFACTURING),\n",
        "            #Tab(text=\"Instant-NGP\", content=page.InstantNGP, icon=icons.STADIUM),\n",
        "            #Tab(text=\"Dream Mask Maker\", content=page.MaskMaker, icon=icons.GRADIENT),\n",
        "        ],\n",
        "        expand=1,\n",
        "        #on_change=tab_on_change\n",
        "    )\n",
        "    return diffusersTabs\n",
        "\n",
        "def build3DAIs(page):\n",
        "    page.DreamFusion = buildDreamFusion(page)\n",
        "    page.Point_E = buildPoint_E(page)\n",
        "    page.Shap_E = buildShap_E(page)\n",
        "    page.InstantNGP = buildInstantNGP(page)\n",
        "    diffusersTabs = Tabs(\n",
        "        selected_index=0,\n",
        "        animation_duration=300,\n",
        "        tabs=[\n",
        "            Tab(text=\"DreamFusion 3D\", content=page.DreamFusion, icon=icons.THREED_ROTATION),\n",
        "            Tab(text=\"Point-E 3D\", content=page.Point_E, icon=icons.SWIPE_UP),\n",
        "            Tab(text=\"Shap-E 3D\", content=page.Shap_E, icon=icons.PRECISION_MANUFACTURING),\n",
        "            Tab(text=\"Instant-NGP\", content=page.InstantNGP, icon=icons.STADIUM),\n",
        "        ],\n",
        "        expand=1,\n",
        "    )\n",
        "    return diffusersTabs\n",
        "\n",
        "def buildTrainers(page):\n",
        "    page.DreamBooth = buildDreamBooth(page)\n",
        "    page.TexualInversion = buildTextualInversion(page)\n",
        "    page.LoRA_Dreambooth = buildLoRA_Dreambooth(page)\n",
        "    page.LoRA = buildLoRA(page)\n",
        "    page.Converter = buildConverter(page)\n",
        "    page.CheckpointMerger = buildCheckpointMerger(page)\n",
        "    trainersTabs = Tabs(\n",
        "        selected_index=0,\n",
        "        animation_duration=300,\n",
        "        tabs=[\n",
        "            Tab(text=\"LoRA\", content=page.LoRA, icon=icons.SETTINGS_SYSTEM_DAYDREAM),\n",
        "            Tab(text=\"LoRA DreamBooth\", content=page.LoRA_Dreambooth, icon=icons.SETTINGS_BRIGHTNESS),\n",
        "            Tab(text=\"DreamBooth\", content=page.DreamBooth, icon=icons.PHOTO),\n",
        "            Tab(text=\"Texual-Inversion\", content=page.TexualInversion, icon=icons.PHOTO_ALBUM),\n",
        "            Tab(text=\"Model Converter\", content=page.Converter, icon=icons.PUBLISHED_WITH_CHANGES),\n",
        "            Tab(text=\"Checkpoint Merger\", content=page.CheckpointMerger, icon=icons.JOIN_FULL),\n",
        "        ],\n",
        "        expand=1,\n",
        "        #on_change=tab_on_change\n",
        "    )\n",
        "    return trainersTabs\n",
        "\n",
        "def buildAudioAIs(page):\n",
        "    page.TortoiseTTS = buildTortoiseTTS(page)\n",
        "    page.DanceDiffusion = buildDanceDiffusion(page)\n",
        "    page.AudioDiffusion = buildAudioDiffusion(page)\n",
        "    page.AudioLDM = buildAudioLDM(page)\n",
        "    page.Bark = buildBark(page)\n",
        "    page.Riffusion = buildRiffusion(page)\n",
        "    page.Mubert = buildMubert(page)\n",
        "    audioAIsTabs = Tabs(\n",
        "        selected_index=0,\n",
        "        animation_duration=300,\n",
        "        tabs=[\n",
        "            Tab(text=\"Tortoise-TTS\", content=page.TortoiseTTS, icon=icons.RECORD_VOICE_OVER),\n",
        "            Tab(text=\"AudioLDM\", content=page.AudioLDM, icon=icons.NOISE_AWARE),\n",
        "            Tab(text=\"Bark\", content=page.Bark, icon=icons.PETS),\n",
        "            Tab(text=\"Riffusion\", content=page.Riffusion, icon=icons.SPATIAL_AUDIO),\n",
        "            Tab(text=\"Audio Diffusion\", content=page.AudioDiffusion, icon=icons.GRAPHIC_EQ),\n",
        "            Tab(text=\"HarmonAI Dance Diffusion\", content=page.DanceDiffusion, icon=icons.QUEUE_MUSIC),\n",
        "            Tab(text=\"Mubert Music\", content=page.Mubert, icon=icons.MUSIC_VIDEO),\n",
        "        ],\n",
        "        expand=1,\n",
        "        #on_change=tab_on_change\n",
        "    )\n",
        "    return audioAIsTabs\n",
        "\n",
        "def buildExtras(page):\n",
        "    page.ESRGAN_upscaler = buildESRGANupscaler(page)\n",
        "    page.CachedModelManager = buildCachedModelManager(page)\n",
        "    page.CustomModelManager = buildCustomModelManager(page)\n",
        "    page.BLIP2Image2Text = buildBLIP2Image2Text(page)\n",
        "    page.DallE2 = buildDallE2(page)\n",
        "    page.Kandinsky = buildKandinsky(page)\n",
        "    page.KandinskyFuse = buildKandinskyFuse(page)\n",
        "    page.DeepDaze = buildDeepDaze(page)\n",
        "    extrasTabs = Tabs(\n",
        "        selected_index=0,\n",
        "        animation_duration=300,\n",
        "        tabs=[\n",
        "            Tab(text=\"Real-ESRGAN Batch Upscaler\", content=page.ESRGAN_upscaler, icon=icons.PHOTO_SIZE_SELECT_LARGE),\n",
        "            Tab(text=\"Cache Manager\", content=page.CachedModelManager, icon=icons.CACHED),\n",
        "            Tab(text=\"Model Manager\", content=page.CustomModelManager, icon=icons.DIFFERENCE),\n",
        "            Tab(text=\"BLIP2 Image2Text\", content=page.BLIP2Image2Text, icon=icons.BATHTUB),\n",
        "            Tab(text=\"OpenAI Dall-E 2\", content=page.DallE2, icon=icons.BLUR_CIRCULAR),\n",
        "            Tab(text=\"Kandinsky 2.1\", content=page.Kandinsky, icon=icons.AC_UNIT),\n",
        "            Tab(text=\"Kandinsky Fuse\", content=page.KandinskyFuse, icon=icons.FIREPLACE),\n",
        "            Tab(text=\"DeepDaze\", content=page.DeepDaze, icon=icons.FACE),\n",
        "        ],\n",
        "        expand=1,\n",
        "        #on_change=tab_on_change\n",
        "    )\n",
        "    return extrasTabs\n",
        "\n",
        "\n",
        "def b_style():\n",
        "    return ButtonStyle(elevation=8)\n",
        "def dict_diff(dict1, dict2):\n",
        "    return {k: v for k, v in dict1.items() if k in dict2 and v != dict2[k]}\n",
        "def arg_diffs(dict1, dict2):\n",
        "    diff = dict_diff(dict1, dict2)\n",
        "    if len(diff) > 0:\n",
        "      dif = []\n",
        "      for k, v in diff.items():\n",
        "        dif.append(f'{to_title(k)}: {v}')\n",
        "      return ', '.join(dif)\n",
        "    else: return None\n",
        "def get_color(color):\n",
        "    if color == \"green\": return colors.GREEN\n",
        "    elif color == \"blue\": return colors.BLUE\n",
        "    elif color == \"indigo\": return colors.INDIGO\n",
        "    elif color == \"red\": return colors.RED\n",
        "    elif color == \"purple\": return colors.DEEP_PURPLE\n",
        "    elif color == \"orange\": return colors.ORANGE\n",
        "    elif color == \"amber\": return colors.AMBER\n",
        "    elif color == \"brown\": return colors.BROWN\n",
        "    elif color == \"teal\": return colors.TEAL\n",
        "    elif color == \"yellow\": return colors.YELLOW\n",
        "\n",
        "\n",
        "# Delete these after everyone's updated\n",
        "if 'install_conceptualizer' not in prefs: prefs['install_conceptualizer'] = False\n",
        "if 'use_conceptualizer' not in prefs: prefs['use_conceptualizer'] = False\n",
        "if 'concepts_model' not in prefs: prefs['concepts_model'] = 'cat-toy'\n",
        "if 'memory_optimization' not in prefs: prefs['memory_optimization'] = 'Attention Slicing'\n",
        "if 'sequential_cpu_offload' not in prefs: prefs['sequential_cpu_offload'] = False\n",
        "if 'vae_slicing' not in prefs: prefs['vae_slicing'] = True\n",
        "if 'vae_tiling' not in prefs: prefs['vae_tiling'] = False\n",
        "if 'use_inpaint_model' not in prefs: prefs['use_inpaint_model'] = False\n",
        "if 'cache_dir' not in prefs: prefs['cache_dir'] = ''\n",
        "if 'Replicate_api_key' not in prefs: prefs['Replicate_api_key'] = ''\n",
        "if 'install_dreamfusion' not in prefs: prefs['install_dreamfusion'] = False\n",
        "if 'install_repaint' not in prefs: prefs['install_repaint'] = False\n",
        "if 'finetuned_model' not in prefs: prefs['finetuned_model'] = 'Midjourney v4 style'\n",
        "if 'dreambooth_model' not in prefs: prefs['dreambooth_model'] = 'disco-diffusion-style'\n",
        "if 'custom_model' not in prefs: prefs['custom_model'] = ''\n",
        "if 'custom_models' not in prefs: prefs['custom_models'] = []\n",
        "if 'start_in_installation' not in prefs: prefs['start_in_installation'] = False\n",
        "if 'install_imagic' not in prefs: prefs['install_imagic'] = False\n",
        "if 'use_imagic' not in prefs: prefs['use_imagic'] = False\n",
        "if 'install_composable' not in prefs: prefs['install_composable'] = False\n",
        "if 'use_composable' not in prefs: prefs['use_composable'] = False\n",
        "if 'install_safe' not in prefs: prefs['install_safe'] = False\n",
        "if 'use_safe' not in prefs: prefs['use_safe'] = False\n",
        "if 'safety_config' not in prefs: prefs['safety_config'] = \"Strong\"\n",
        "if 'install_versatile' not in prefs: prefs['install_versatile'] = False\n",
        "if 'use_versatile' not in prefs: prefs['use_versatile'] = False\n",
        "if 'install_depth2img' not in prefs: prefs['install_depth2img'] = False\n",
        "if 'use_depth2img' not in prefs: prefs['use_depth2img'] = False\n",
        "if 'install_alt_diffusion' not in prefs: prefs['install_alt_diffusion'] = False\n",
        "if 'use_alt_diffusion' not in prefs: prefs['use_alt_diffusion'] = False\n",
        "if 'install_upscale' not in prefs: prefs['install_upscale'] = False\n",
        "if 'use_upscale' not in prefs: prefs['use_upscale'] = False\n",
        "if 'upscale_noise_level' not in prefs: prefs['upscale_noise_level'] = 20\n",
        "if 'alpha_mask' not in prefs: prefs['alpha_mask'] = False\n",
        "if 'invert_mask' not in prefs: prefs['invert_mask'] = False\n",
        "if 'clip_guidance_preset' not in prefs: prefs['clip_guidance_preset'] = \"FAST_BLUE\"\n",
        "if 'tortoise_custom_voices' not in prefs: prefs['tortoise_custom_voices'] = []\n",
        "if 'use_LoRA_model' not in prefs: prefs['use_LoRA_model'] = False\n",
        "if 'LoRA_model' not in prefs: prefs['LoRA_model'] = \"Von Platen LoRA\"\n",
        "if 'custom_LoRA_models' not in prefs: prefs['custom_LoRA_models'] = []\n",
        "if 'custom_LoRA_model' not in prefs: prefs['custom_LoRA_model'] = ''\n",
        "if 'custom_dance_diffusion_models' not in prefs: prefs['custom_dance_diffusion_models'] = []\n",
        "if 'negative_prompt' not in prefs['prompt_writer']: prefs['prompt_writer']['negative_prompt'] = ''\n",
        "if 'install_attend_and_excite' not in prefs: prefs['install_attend_and_excite'] = False\n",
        "if 'use_attend_and_excite' not in prefs: prefs['use_attend_and_excite'] = False\n",
        "if 'max_iter_to_alter' not in prefs: prefs['max_iter_to_alter'] = 25\n",
        "if 'install_SAG' not in prefs: prefs['install_SAG'] = False\n",
        "if 'use_SAG' not in prefs: prefs['use_SAG'] = False\n",
        "if 'sag_scale' not in prefs: prefs['sag_scale'] = 0.75\n",
        "if 'install_panorama' not in prefs: prefs['install_panorama'] = False\n",
        "if 'use_panorama' not in prefs: prefs['use_panorama'] = False\n",
        "if 'panorama_width' not in prefs: prefs['panorama_width'] = 2048\n",
        "if 'AI_engine' not in prefs['prompt_generator']: prefs['prompt_generator']['AI_engine'] = 'OpenAI GPT-3'\n",
        "if 'AI_engine' not in prefs['prompt_remixer']: prefs['prompt_remixer']['AI_engine'] = 'OpenAI GPT-3'\n",
        "if 'AIHorde_api_key' not in prefs: prefs['AIHorde_api_key'] = '0000000000'\n",
        "if 'install_AIHorde_api' not in prefs: prefs['install_AIHorde_api'] = False\n",
        "if 'use_AIHorde_api' not in prefs: prefs['use_AIHorde_api'] = False\n",
        "if 'AIHorde_model' not in prefs: prefs['AIHorde_model'] = 'stable_diffusion'\n",
        "if 'AIHorde_sampler' not in prefs: prefs['AIHorde_sampler'] = 'k_euler_a'\n",
        "if 'AIHorde_post_processing' not in prefs: prefs['AIHorde_post_processing'] = \"None\"\n",
        "if 'enable_torch_compile' not in prefs: prefs['enable_torch_compile'] = False\n",
        "if 'enable_tome' not in prefs: prefs['enable_tome'] = False\n",
        "if 'tome_ratio' not in prefs: prefs['tome_ratio'] = 0.5\n",
        "\n",
        "def initState(page):\n",
        "    global status, current_tab\n",
        "    if os.path.isdir(os.path.join(root_dir, 'Real-ESRGAN')):\n",
        "      status['installed_ESRGAN'] = True\n",
        "    page.load_prompts()\n",
        "    \n",
        "    # TODO: Try to load from assets folder\n",
        "    page.snd_alert = Audio(src=os.path.join(assets, \"snd-alert.mp3\"), autoplay=False)\n",
        "    page.snd_delete = Audio(src=os.path.join(assets, \"snd-delete.mp3\"), autoplay=False)\n",
        "    page.snd_error = Audio(src=os.path.join(assets, \"snd-error.mp3\"), autoplay=False)\n",
        "    page.snd_done = Audio(src=os.path.join(assets, \"snd-done.mp3\"), autoplay=False)\n",
        "    page.snd_drop = Audio(src=os.path.join(assets, \"snd-drop.mp3\"), autoplay=False)\n",
        "    #page.snd_notification = Audio(src=\"https://github.com/Skquark/AI-Friends/blob/main/assets/snd-notification.mp3?raw=true\", autoplay=False)\n",
        "    page.overlay.append(page.snd_alert)\n",
        "    page.overlay.append(page.snd_delete)\n",
        "    page.overlay.append(page.snd_error)\n",
        "    page.overlay.append(page.snd_done)\n",
        "    page.overlay.append(page.snd_drop)\n",
        "    #page.overlay.append(page.snd_notification)\n",
        "    #print(\"Running Init State\")\n",
        "    import importlib\n",
        "    if '_PYIBoot_SPLASH' in os.environ and importlib.util.find_spec(\"pyi_splash\"):\n",
        "      import pyi_splash\n",
        "      pyi_splash.update_text('Ready to get creative...')\n",
        "      pyi_splash.close()\n",
        "      #log.info('Splash screen closed.')\n",
        "    if prefs['scheduler_mode'] != \"DDIM\":\n",
        "      for eta in page.etas:\n",
        "        if isinstance(eta, SliderRow):\n",
        "          eta.show = False\n",
        "        else:\n",
        "          eta.visible = False\n",
        "          eta.update()\n",
        "    if prefs['start_in_installation'] and current_tab == 0:\n",
        "      page.tabs.selected_index = 1\n",
        "      page.tabs.update()\n",
        "      page.show_install_fab(True)\n",
        "      page.update()\n",
        "      current_tab = 1\n",
        "\n",
        "def buildSettings(page):\n",
        "  global prefs, status\n",
        "  def open_url(e):\n",
        "    page.launch_url(e.data)\n",
        "  def save_settings(e):\n",
        "    save_settings_file(e.page)\n",
        "    page.snack_bar = SnackBar(content=Text(f\"Saving all settings to {saved_settings_json.rpartition(slash)[2]}\"))\n",
        "    page.snack_bar.open = True\n",
        "    page.tabs.selected_index = 1\n",
        "    page.tabs.update()\n",
        "    page.update()\n",
        "  def changed(e, pref=None):\n",
        "      if pref is not None:\n",
        "        prefs[pref] = e.control.value\n",
        "      has_changed = True\n",
        "      page.update()\n",
        "      status['changed_settings'] = True\n",
        "  def change_theme_mode(e):\n",
        "    prefs['theme_mode'] = e.control.value\n",
        "    if prefs['theme_mode'].lower() == \"dark\":\n",
        "      page.dark_theme = Theme(color_scheme_seed=get_color(prefs['theme_color'].lower()))\n",
        "    else:\n",
        "      page.theme = theme.Theme(color_scheme_seed=get_color(prefs['theme_color'].lower()))\n",
        "    page.theme_mode = prefs['theme_mode'].lower()\n",
        "    page.update()\n",
        "    status['changed_settings'] = True\n",
        "  def change_theme_color(e):\n",
        "    prefs['theme_color'] = e.control.value\n",
        "    if prefs['theme_mode'].lower() == \"dark\":\n",
        "      page.dark_theme = Theme(color_scheme_seed=get_color(prefs['theme_color'].lower()))\n",
        "    else:\n",
        "      page.theme = theme.Theme(color_scheme_seed=get_color(prefs['theme_color'].lower()))\n",
        "    page.update()\n",
        "    status['changed_settings'] = True\n",
        "  def toggle_nsfw(e):\n",
        "    retry_attempts.width = 0 if e.control.value else None\n",
        "    retry_attempts.update()\n",
        "    changed(e, 'disable_nsfw_filter')\n",
        "  #haschanged = False\n",
        "  #save_to_GDrive = Checkbox(label=\"Save to Google Drive\", value=prefs['save_to_GDrive'])\n",
        "  def default_cache_dir(e):\n",
        "    default_dir = prefs['image_output'].strip()\n",
        "    if default_dir.endswith(slash):\n",
        "      default_dir = default_dir[:-1]\n",
        "    default_dir = default_dir.rpartition(slash)[0]\n",
        "    default_dir = os.path.join(default_dir, 'models')\n",
        "    prefs['cache_dir'] = default_dir\n",
        "    optional_cache_dir.value = default_dir\n",
        "    optional_cache_dir.update()\n",
        "  def folder_picker_result(e):\n",
        "    folder = folder_picker.result\n",
        "    if folder != None and folder.path != None:\n",
        "      image_output.value = folder.path\n",
        "      image_output.update()\n",
        "  folder_picker = FilePicker(on_result=folder_picker_result)\n",
        "  page.overlay.append(folder_picker)\n",
        "  def pick_output_dir(e):\n",
        "    if not is_Colab:\n",
        "      folder_picker.get_directory_path(dialog_title=\"Pick Directory to Save Outputs\")\n",
        "  image_output = TextField(label=\"Image Output Path\", value=prefs['image_output'], on_change=lambda e:changed(e, 'image_output'), col={\"md\":12, \"lg\":6}, suffix=IconButton(icon=icons.FOLDER_OUTLINED, on_click=pick_output_dir))\n",
        "  optional_cache_dir = TextField(label=\"Optional Cache Directory (saves large models to drive)\", hint_text=\"(button on right inserts recommended folder)\", value=prefs['cache_dir'], on_change=lambda e:changed(e, 'cache_dir'), suffix=IconButton(icon=icons.ARCHIVE, tooltip=\"Insert recommended models cache path\", on_click=default_cache_dir), col={\"md\":12, \"lg\":6})\n",
        "  file_prefix = TextField(label=\"Filename Prefix\",  value=prefs['file_prefix'], width=150, height=60, on_change=lambda e:changed(e, 'file_prefix'))\n",
        "  file_suffix_seed = Checkbox(label=\"Filename Suffix Seed   \", tooltip=\"Appends -seed# to the end of the image name\", value=prefs['file_suffix_seed'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e, 'file_suffix_seed'))\n",
        "  file_allowSpace = Checkbox(label=\"Filename Allow Space\", tooltip=\"Otherwise will replace spaces with _ underscores\", value=prefs['file_allowSpace'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e, 'file_allowSpace'))\n",
        "  file_max_length = TextField(label=\"Filename Max Length\", tooltip=\"How long can the name taken from prompt text be? Max 250\", value=prefs['file_max_length'], keyboard_type=KeyboardType.NUMBER, width=150, height=60, on_change=lambda e:changed(e, 'file_max_length'))\n",
        "  save_image_metadata = Checkbox(label=\"Save Image Metadata in png\", tooltip=\"Embeds your Artist Name & Copyright in the file's EXIF\", value=prefs['save_image_metadata'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e, 'save_image_metadata'))\n",
        "  meta_ArtistName = TextField(label=\"Artist Name Metadata\", value=prefs['meta_ArtistName'], keyboard_type=KeyboardType.NAME, on_change=lambda e:changed(e, 'meta_ArtistName'))\n",
        "  meta_Copyright = TextField(label=\"Copyright Metadata\", value=prefs['meta_Copyright'], keyboard_type=KeyboardType.NAME, on_change=lambda e:changed(e, 'meta_Copyright'))\n",
        "  save_config_in_metadata = Checkbox(label=\"Save Config in Metadata    \", tooltip=\"Embeds all prompt parameters in the file's EXIF to recreate\", value=prefs['save_config_in_metadata'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e, 'save_config_in_metadata'))\n",
        "  save_config_json = Checkbox(label=\"Save Config JSON files\", tooltip=\"Creates a json text file with all prompt parameters with each image\", value=prefs['save_config_json'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e, 'save_config_json'))\n",
        "  theme_mode = Dropdown(label=\"Theme Mode\", width=200, options=[dropdown.Option(\"Dark\"), dropdown.Option(\"Light\")], value=prefs['theme_mode'], on_change=change_theme_mode)\n",
        "  theme_color = Dropdown(label=\"Accent Color\", width=200, options=[dropdown.Option(\"Green\"), dropdown.Option(\"Blue\"), dropdown.Option(\"Red\"), dropdown.Option(\"Indigo\"), dropdown.Option(\"Purple\"), dropdown.Option(\"Orange\"), dropdown.Option(\"Amber\"), dropdown.Option(\"Brown\"), dropdown.Option(\"Teal\"), dropdown.Option(\"Yellow\")], value=prefs['theme_color'], on_change=change_theme_color)\n",
        "  enable_sounds = Checkbox(label=\"Enable UI Sound Effects    \", tooltip=\"Turn on for audible errors, deletes and generation done notifications\", value=prefs['enable_sounds'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e, 'enable_sounds'))\n",
        "  start_in_installation = Checkbox(label=\"Start in Installation Page\", tooltip=\"When launching app, switch to Installer tab. Saves time..\", value=prefs['start_in_installation'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e, 'start_in_installation'))\n",
        "  disable_nsfw_filter = Checkbox(label=\"Disable NSFW Filters\", value=prefs['disable_nsfw_filter'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=toggle_nsfw)\n",
        "  retry_attempts = Container(NumberPicker(label=\"Retry Attempts if Not Safe\", min=0, max=8, value=prefs['retry_attempts'], on_change=lambda e:changed(e, 'retry_attempts')), padding=padding.only(left=20), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "  retry_attempts.width = 0 if prefs['disable_nsfw_filter'] else None\n",
        "  api_instructions = Container(height=170, content=Markdown(\"Get **HuggingFace API key** from https://huggingface.co/settings/tokens, preferably the WRITE access key.\\n\\nGet **Stability-API key** from https://beta.dreamstudio.ai/membership?tab=apiKeys then API key\\n\\nGet **OpenAI GPT-3 API key** from https://beta.openai.com, user menu, View API Keys\\n\\nGet **TextSynth GPT-J key** from https://TextSynth.com, login, Setup\\n\\nGet **Replicate API Token** from https://replicate.com/account, for Material Diffusion\\n\\nGet **AIHorde API Token** from https://aihorde.net/register, for Stable Horde cloud\", extension_set=\"gitHubWeb\", on_tap_link=open_url))\n",
        "  HuggingFace_api = TextField(label=\"HuggingFace API Key\", value=prefs['HuggingFace_api_key'], password=True, can_reveal_password=True, on_change=lambda e:changed(e, 'HuggingFace_api_key'))\n",
        "  Stability_api = TextField(label=\"Stability.ai API Key (optional)\", value=prefs['Stability_api_key'], password=True, can_reveal_password=True, on_change=lambda e:changed(e, 'Stability_api_key'))\n",
        "  OpenAI_api = TextField(label=\"OpenAI API Key (optional)\", value=prefs['OpenAI_api_key'], password=True, can_reveal_password=True, on_change=lambda e:changed(e, 'OpenAI_api_key'))\n",
        "  TextSynth_api = TextField(label=\"TextSynth API Key (optional)\", value=prefs['TextSynth_api_key'], password=True, can_reveal_password=True, on_change=lambda e:changed(e, 'TextSynth_api_key'))\n",
        "  Replicate_api = TextField(label=\"Replicate API Key (optional)\", value=prefs['Replicate_api_key'], password=True, can_reveal_password=True, on_change=lambda e:changed(e, 'Replicate_api_key'))\n",
        "  AIHorde_api = TextField(label=\"AIHorde API Key (optional)\", value=prefs['AIHorde_api_key'], password=True, can_reveal_password=True, on_change=lambda e:changed(e, 'AIHorde_api_key'))\n",
        "  #save_button = ElevatedButton(content=Text(value=\"üíæ  Save Settings\", size=20), on_click=save_settings, style=b_style())\n",
        "  \n",
        "  c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"‚öôÔ∏è   Deluxe Stable Diffusion Settings & Preferences\"),\n",
        "        #save_to_GDrive,\n",
        "        ResponsiveRow([image_output, optional_cache_dir], run_spacing=2),\n",
        "        #VerticalDivider(thickness=2),\n",
        "        Row([file_prefix, file_suffix_seed]) if (page.window_width or page.width) > 500 else Column([file_prefix, file_suffix_seed]),\n",
        "        Row([file_max_length, file_allowSpace]),\n",
        "        #file_allowSpace,\n",
        "        #file_max_length,\n",
        "        #Row([disable_nsfw_filter, retry_attempts]),\n",
        "        #VerticalDivider(thickness=2, width=1),\n",
        "        save_image_metadata,\n",
        "        Row([meta_ArtistName, meta_Copyright]) if (page.window_width or page.width) > 712 else Column([meta_ArtistName, meta_Copyright]),\n",
        "        Row([save_config_in_metadata, save_config_json,]),\n",
        "        Row([theme_mode, theme_color]),\n",
        "        Row([enable_sounds, start_in_installation]),\n",
        "        #VerticalDivider(thickness=2, width=1),\n",
        "        HuggingFace_api,\n",
        "        Stability_api,\n",
        "        OpenAI_api,\n",
        "        TextSynth_api,\n",
        "        Replicate_api,\n",
        "        AIHorde_api,\n",
        "        api_instructions,\n",
        "        #save_button,\n",
        "        Container(content=None, height=32),\n",
        "      ],  \n",
        "  ))], scroll=ScrollMode.AUTO,)\n",
        "  return c\n",
        "\n",
        "def run_process(cmd_str, cwd=None, realtime=True, page=None, close_at_end=False, show=False, print=False): # show when debugging\n",
        "  cmd_list = cmd_str if type(cmd_str) is list else cmd_str.split()\n",
        "  if realtime:\n",
        "    if cwd is None:\n",
        "      process = subprocess.Popen(cmd_str, shell = True, env=env, bufsize = 1, stdout=subprocess.PIPE, stderr = subprocess.STDOUT, encoding='utf-8', errors = 'replace' ) \n",
        "    else:\n",
        "      process = subprocess.Popen(cmd_str, shell = True, cwd=cwd, env=env, bufsize = 1, stdout=subprocess.PIPE, stderr = subprocess.STDOUT, encoding='utf-8', errors = 'replace' ) \n",
        "    while True:\n",
        "      realtime_output = process.stdout.readline()\n",
        "      if realtime_output == '' and process.poll() is not None:\n",
        "        break\n",
        "      if realtime_output and show and page != None:\n",
        "        #print(realtime_output.strip(), flush=False)\n",
        "        page.banner.content.controls.append(Text(realtime_output.strip()))\n",
        "        page.update()\n",
        "        sys.stdout.flush()\n",
        "      if print:\n",
        "        print(realtime_output.strip())\n",
        "    if close_at_end:\n",
        "      page.banner.open = False\n",
        "      page.update()\n",
        "  else:\n",
        "    if cwd is None:\n",
        "      #return subprocess.run(cmd_list, stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "      output = subprocess.run(cmd_list, stdout=subprocess.PIPE, env=env).stdout.decode('utf-8')\n",
        "    else:\n",
        "      output = subprocess.run(cmd_list, stdout=subprocess.PIPE, env=env, cwd=cwd).stdout.decode('utf-8')\n",
        "    if print:\n",
        "      print(output)\n",
        "    return output\n",
        "\n",
        "def close_alert_dlg(e):\n",
        "      e.page.alert_dlg.open = False\n",
        "      e.page.update()\n",
        "def alert_msg(page, msg, content=None, okay=\"\", sound=True, width=None, wide=False):\n",
        "      try:\n",
        "        if page.alert_dlg.open == True: return\n",
        "      except Exception: pass\n",
        "      try:\n",
        "        if prefs['enable_sounds'] and sound: page.snd_error.play()\n",
        "      except Exception:\n",
        "        msg += \" May have to restart runtime.\"\n",
        "        pass\n",
        "      okay = ElevatedButton(content=Text(\"üëå  OKAY \" if okay == \"\" else okay, size=18), on_click=close_alert_dlg)\n",
        "      if content == None: content = Container(content=None)\n",
        "      page.alert_dlg = AlertDialog(title=Text(msg), content=Column([content], scroll=ScrollMode.AUTO), actions=[okay], actions_alignment=MainAxisAlignment.END)#, width=None if not wide else (page.window_width or page.width) - 200)\n",
        "      page.dialog = page.alert_dlg\n",
        "      page.alert_dlg.open = True\n",
        "      try:\n",
        "        page.update()\n",
        "      except Exception: pass\n",
        "\n",
        "def save_installers(controls):\n",
        "  for c in controls:\n",
        "    if isinstance(c, Switch):\n",
        "      #print(f\"elif c.value == '{c.label}': prefs[''] = c.value\")\n",
        "      if c.value == 'Install HuggingFace Diffusers Pipeline': prefs['install_diffusers'] = c.value\n",
        "      elif c.value == 'Install Stability-API DreamStudio Pipeline': prefs['install_Stability_api'] = c.value\n",
        "      elif c.value == 'Install Real-ESRGAN AI Upscaler': prefs['install_ESRGAN'] = c.value\n",
        "      elif c.value == 'Install OpenAI GPT-3 Text Engine': prefs['install_OpenAI'] = c.value\n",
        "      elif c.value == 'Install TextSynth GPT-J Text Engine': prefs['install_TextSynth'] = c.value\n",
        "    elif isinstance(c, Container):\n",
        "      '''try:\n",
        "        for i in c.content.controls:\n",
        "          if isinstance(i, Switch):\n",
        "            print(f\"elif i.value == '{c.label}': prefs[''] = i.value\")\n",
        "      except: continue'''\n",
        "def refresh_installers(controls):\n",
        "  for c in controls:\n",
        "    if isinstance(c, Switch):\n",
        "      c.update()\n",
        "\n",
        "def buildInstallers(page):\n",
        "  global prefs, status, model_path\n",
        "  def changed(e, pref=None):\n",
        "      if pref is not None:\n",
        "        prefs[pref] = e.control.value\n",
        "      page.update()\n",
        "      status['changed_installers'] = True\n",
        "    #has_changed = True\n",
        "    #page.update()\n",
        "  def changed_status(e, stat=None):\n",
        "      if stat is not None:\n",
        "        status[stat] = e.control.value\n",
        "  #has_changed = False\n",
        "  #save_to_GDrive = Checkbox(label=\"Save to Google Drive\", value=prefs['save_to_GDrive'])\n",
        "\n",
        "  def toggle_diffusers(e):\n",
        "      prefs['install_diffusers'] = install_diffusers.content.value\n",
        "      diffusers_settings.height=None if prefs['install_diffusers'] else 0\n",
        "      diffusers_settings.update()\n",
        "      status['changed_installers'] = True\n",
        "  install_diffusers = Tooltip(message=\"Required Libraries for most Image Generation functionality\", content=Switch(label=\"Install HuggingFace Diffusers Pipeline\", value=prefs['install_diffusers'], disabled=status['installed_diffusers'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_diffusers))\n",
        "  def change_scheduler(e):\n",
        "      show = e.control.value == \"DDIM\"\n",
        "      update = prefs['scheduler_mode'] == \"DDIM\" or show\n",
        "      changed(e, 'scheduler_mode')\n",
        "      if update:\n",
        "          for eta in page.etas:\n",
        "            if isinstance(eta, SliderRow):\n",
        "              eta.show = show\n",
        "            else:\n",
        "              eta.visible = show\n",
        "              eta.update()\n",
        "  scheduler_mode = Dropdown(label=\"Scheduler/Sampler Mode\", hint_text=\"They're very similar, with minor differences in the noise\", width=200,\n",
        "            options=[\n",
        "                dropdown.Option(\"DDIM\"),\n",
        "                dropdown.Option(\"LMS Discrete\"),\n",
        "                dropdown.Option(\"PNDM\"),\n",
        "                #dropdown.Option(\"DDPM\"),\n",
        "                dropdown.Option(\"DPM Solver\"),\n",
        "                dropdown.Option(\"DPM Solver++\"),\n",
        "                dropdown.Option(\"SDE-DPM Solver++\"),\n",
        "                #dropdown.Option(\"DPM Stochastic\"),\n",
        "                dropdown.Option(\"K-Euler Discrete\"),\n",
        "                dropdown.Option(\"K-Euler Ancestral\"),\n",
        "                dropdown.Option(\"DEIS Multistep\"),\n",
        "                dropdown.Option(\"UniPC Multistep\"),\n",
        "                dropdown.Option(\"Heun Discrete\"),\n",
        "                dropdown.Option(\"Karras Heun Discrete\"),\n",
        "                dropdown.Option(\"K-DPM2 Ancestral\"),\n",
        "                dropdown.Option(\"K-DPM2 Discrete\"),\n",
        "                dropdown.Option(\"Karras-LMS\"),\n",
        "            ], value=prefs['scheduler_mode'], autofocus=False, on_change=change_scheduler,\n",
        "        )\n",
        "  def changed_model_ckpt(e):\n",
        "      changed(e, 'model_ckpt')\n",
        "      model = get_model(e.control.value)\n",
        "      model_card.value = f\"  [**Model Card**](https://huggingface.co/{model['path']})\"\n",
        "      model_card.update()\n",
        "      if e.control.value.startswith(\"Stable\"):\n",
        "        custom_area.content = model_card\n",
        "      elif e.control.value == \"Community Finetuned Model\":\n",
        "        custom_area.content = Row([finetuned_model, model_card])\n",
        "      elif e.control.value == \"DreamBooth Library Model\":\n",
        "        custom_area.content = Row([dreambooth_library, model_card])\n",
        "      elif e.control.value == \"Custom Model Path\":\n",
        "        custom_area.content = Row([custom_model, model_card])\n",
        "      custom_area.update()\n",
        "  def changed_finetuned_model(e):\n",
        "      changed(e, 'finetuned_model')\n",
        "      model = get_finetuned_model(e.control.value)\n",
        "      model_card.value = f\"  [**Model Card**](https://huggingface.co/{model['path']})\"\n",
        "      model_card.update()\n",
        "  def changed_dreambooth_library(e):\n",
        "      changed(e, 'dreambooth_model')\n",
        "      model = get_dreambooth_model(e.control.value)\n",
        "      model_card.value = f\"  [**Model Card**](https://huggingface.co/{model['path']})\"\n",
        "      model_card.update()\n",
        "  def changed_custom_model(e):\n",
        "      changed(e, 'custom_model')\n",
        "      model = {'name': 'Custom Model', 'path': e.control.value, 'prefix': ''}\n",
        "      model_card.value = f\"  [**Model Card**](https://huggingface.co/{model['path']})\"\n",
        "      model_card.update()\n",
        "  def toggle_safe(e):\n",
        "      changed(e, 'install_safe')\n",
        "      safety_config.visible = e.control.value\n",
        "      safety_config.update()\n",
        "  model = get_model(prefs['model_ckpt'])\n",
        "  model_path = model['path']\n",
        "  model_ckpt = Container(Dropdown(label=\"Model Checkpoint\", width=262, options=[\n",
        "      dropdown.Option(\"Stable Diffusion v2.1 x768\"), dropdown.Option(\"Stable Diffusion v2.1 x512\"), \n",
        "      dropdown.Option(\"Stable Diffusion v2.0 x768\"), dropdown.Option(\"Stable Diffusion v2.0 x512\"), dropdown.Option(\"Stable Diffusion v1.5\"), dropdown.Option(\"Stable Diffusion v1.4\"), \n",
        "      dropdown.Option(\"Community Finetuned Model\"), dropdown.Option(\"DreamBooth Library Model\"), dropdown.Option(\"Custom Model Path\")], value=prefs['model_ckpt'], tooltip=\"Make sure you accepted the HuggingFace Model Cards first\", autofocus=False, on_change=changed_model_ckpt), col={'xs':9, 'lg':4}, width=262)\n",
        "  finetuned_model = Dropdown(label=\"Finetuned Model\", tooltip=\"Make sure you accepted the HuggingFace Model Cards first\", width=370, options=[], value=prefs['finetuned_model'], autofocus=False, on_change=changed_finetuned_model, col={'xs':11, 'lg':6})\n",
        "  model_card = Markdown(f\"  [**Model Card**](https://huggingface.co/{model['path']})\", on_tap_link=lambda e: e.page.launch_url(e.data))\n",
        "  for mod in finetuned_models:\n",
        "      finetuned_model.options.append(dropdown.Option(mod[\"name\"]))\n",
        "  page.finetuned_model = finetuned_model\n",
        "  dreambooth_library = Dropdown(label=\"DreamBooth Library\", hint_text=\"\", width=370, options=[], value=prefs['dreambooth_model'], autofocus=False, on_change=changed_dreambooth_library, col={'xs':10, 'md':4})\n",
        "  for db in dreambooth_models:\n",
        "      dreambooth_library.options.append(dropdown.Option(db[\"name\"]))\n",
        "  custom_model = TextField(label=\"Custom Model Path\", value=prefs['custom_model'], width=370, on_change=changed_custom_model)\n",
        "  page.custom_model = custom_model\n",
        "  #custom_area = AnimatedSwitcher(model_card, transition=\"scale\", duration=500, reverse_duration=200, switch_in_curve=AnimationCurve.EASE_OUT, switch_out_curve=\"easeIn\")\n",
        "  custom_area = Container(model_card, col={'xs':12, 'lg':6})\n",
        "  if prefs['model_ckpt'].startswith(\"Stable\"):\n",
        "      custom_area.content = model_card\n",
        "  elif prefs['model_ckpt'] == \"Community Finetuned Model\":\n",
        "      custom_area.content = Row([finetuned_model, model_card], col={'xs':9, 'lg':4})\n",
        "  elif prefs['model_ckpt'] == \"DreamBooth Library Model\":\n",
        "      custom_area.content = Row([dreambooth_library, model_card], col={'xs':9, 'lg':4})\n",
        "  elif prefs['model_ckpt'] == \"Custom Model Path\":\n",
        "      custom_area.content = Row([custom_model, model_card], col={'xs':9, 'lg':4})\n",
        "  model_row = ResponsiveRow([model_ckpt, custom_area], run_spacing=8)\n",
        "  memory_optimization = Dropdown(label=\"Enable Memory Optimization\", width=320, options=[dropdown.Option(\"None\"), dropdown.Option(\"Attention Slicing\")], value=prefs['memory_optimization'], on_change=lambda e:changed(e, 'memory_optimization'))\n",
        "  if version.parse(torch.__version__) < version.parse(\"2.0.0\"):\n",
        "      memory_optimization.options.append(dropdown.Option(\"Xformers Mem Efficient Attention\"))\n",
        "  higher_vram_mode = Checkbox(label=\"Higher VRAM Mode\", tooltip=\"Adds a bit more precision but takes longer & uses much more GPU memory. Not recommended.\", value=prefs['higher_vram_mode'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e, 'higher_vram_mode'))\n",
        "  sequential_cpu_offload = Checkbox(label=\"Enable Sequential CPU Offload\", tooltip=\"Offloads all models to CPU using accelerate, significantly reducing memory usage.\", value=prefs['sequential_cpu_offload'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e, 'sequential_cpu_offload'))\n",
        "  enable_attention_slicing = Checkbox(label=\"Enable Attention Slicing\", tooltip=\"Saves VRAM while creating images so you can go bigger without running out of mem.\", value=prefs['enable_attention_slicing'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e, 'enable_attention_slicing'))\n",
        "  enable_vae_tiling = Checkbox(label=\"Enable VAE Tiling\", tooltip=\"The VAE will split the input tensor into tiles to compute decoding and encoding in several steps. This is useful to save a large amount of memory and to allow the processing of larger images.\", value=prefs['vae_tiling'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e, 'vae_tiling'))\n",
        "  enable_vae_slicing = Checkbox(label=\"Enable VAE Slicing\", tooltip=\"Sliced VAE decode latents for larger batches of images with limited VRAM. Splits the input tensor in slices to compute decoding in several steps\", value=prefs['vae_slicing'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e, 'vae_slicing'))\n",
        "  enable_tome = Checkbox(label=\"Enable Token Merging\", tooltip=\"ToMe optimizes the Pipelines to create images faster, at the expense of some quality. Works by merging the redundant tokens / patches progressively in the forward pass of a Transformer-based network.\", value=prefs['enable_tome'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e, 'enable_tome'))\n",
        "  enable_torch_compile = Checkbox(label=\"Enable Torch Compiling\", tooltip=\"Speeds up Torch 2.0 Processing, but takes a bit longer to initialize.\", value=prefs['enable_torch_compile'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e, 'enable_torch_compile'))\n",
        "  #install_megapipe = Switch(label=\"Install Stable Diffusion txt2image, img2img & Inpaint Mega Pipeline\", value=prefs['install_megapipe'], disabled=status['installed_megapipe'], on_change=lambda e:changed(e, 'install_megapipe'))\n",
        "  install_text2img = Tooltip(message=\"The best general purpose component. Create images with long prompts, weights & models\", content=Switch(label=\"Install Stable Diffusion text2image, image2image & Inpaint Pipeline (/w Long Prompt Weighting)\", value=prefs['install_text2img'], disabled=status['installed_txt2img'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e, 'install_text2img')))\n",
        "  install_img2img = Tooltip(message=\"Gets more coherant results modifying Inpaint init & mask images\", content=Switch(label=\"Install Stable Diffusion Specialized Inpainting Model for image2image & Inpaint Pipeline\", value=prefs['install_img2img'], disabled=status['installed_img2img'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e, 'install_img2img')))\n",
        "  #install_repaint = Tooltip(message=\"Without using prompts, redraw masked areas to remove and repaint.\", content=Switch(label=\"Install Stable Diffusion RePaint Pipeline\", value=prefs['install_repaint'], disabled=status['installed_repaint'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e, 'install_repaint')))\n",
        "  install_interpolation = Tooltip(message=\"Create multiple tween images between prompts latent space. Almost animation.\", content=Switch(label=\"Install Stable Diffusion Prompt Walk Interpolation Pipeline\", value=prefs['install_interpolation'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, disabled=status['installed_interpolation'], on_change=lambda e:changed(e, 'install_interpolation')))\n",
        "  #install_dreamfusion = Tooltip(message=\"Generate interesting mesh .obj, texture and preview video from a prompt.\", content=Switch(label=\"Install Stable Diffusion DreamFusion 3D Pipeline\", value=prefs['install_dreamfusion'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, disabled=status['installed_dreamfusion'], on_change=lambda e:changed(e, 'install_dreamfusion')))\n",
        "  install_alt_diffusion = Tooltip(message=\"Multilingual Stable Diffusion supporting English, Chinese, Spanish, French, Russian, Japanese, Korean, Arabic and Italian.\", content=Switch(label=\"Install AltDiffusion text2image & image2image Multilingual Pipeline\", value=prefs['install_alt_diffusion'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, disabled=status['installed_alt_diffusion'], on_change=lambda e:changed(e, 'install_alt_diffusion')))\n",
        "  install_attend_and_excite = Tooltip(message=\"Provides textual Attention-Based Semantic Guidance control over the image generation.\", content=Switch(label=\"Install Attend and Excite text2image Pipeline\", value=prefs['install_attend_and_excite'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, disabled=status['installed_attend_and_excite'], on_change=lambda e:changed(e, 'install_attend_and_excite')))\n",
        "  install_SAG = Tooltip(message=\"Intelligent guidance that can plugged into any diffusion model using their self-attention map, improving sample quality.\", content=Switch(label=\"Install Self-Attention Guidance (SAG) text2image Pipeline\", value=prefs['install_SAG'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, disabled=status['installed_SAG'], on_change=lambda e:changed(e, 'install_SAG')))\n",
        "  install_panorama = Tooltip(message=\"Generate panorama-like wide images, Fusing Diffusion Paths for Controlled Image Generation\", content=Switch(label=\"Install MultiDiffusion Panorama text2image Pipeline\", value=prefs['install_panorama'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, disabled=status['installed_panorama'], on_change=lambda e:changed(e, 'install_panorama')))\n",
        "  install_imagic = Tooltip(message=\"Edit your image according to the prompted instructions like magic.\", content=Switch(label=\"Install Stable Diffusion iMagic image2image Pipeline\", value=prefs['install_imagic'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, disabled=status['installed_imagic'], on_change=lambda e:changed(e, 'install_imagic')))\n",
        "  install_depth2img = Tooltip(message=\"Uses Depth-map of init image for text-guided image to image generation.\", content=Switch(label=\"Install Stable Diffusion Depth2Image Pipeline\", value=prefs['install_depth2img'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, disabled=status['installed_depth2img'], on_change=lambda e:changed(e, 'install_depth2img')))\n",
        "  install_composable = Tooltip(message=\"Craft your prompts with | precise | weights AND composed together components | with AND NOT negatives.\", content=Switch(label=\"Install Stable Diffusion Composable text2image Pipeline\", value=prefs['install_composable'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, disabled=status['installed_composable'], on_change=lambda e:changed(e, 'install_composable')))\n",
        "  install_safe = Tooltip(message=\"Use a content quality tuned safety model, providing levels of NSFW protection.\", content=Switch(label=\"Install Stable Diffusion Safe text2image Pipeline\", value=prefs['install_safe'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, disabled=status['installed_safe'], on_change=toggle_safe))\n",
        "  safety_config = Container(Dropdown(label=\"Model Safety Level\", width=350, options=[dropdown.Option(\"Weak\"), dropdown.Option(\"Medium\"), dropdown.Option(\"Strong\"), dropdown.Option(\"Max\")], value=prefs['safety_config'], on_change=lambda e:changed(e, 'safety_config')), padding=padding.only(left=32))\n",
        "  safety_config.visible = prefs['install_safe']\n",
        "  install_versatile = Tooltip(message=\"Multi-flow model that provides both image and text data streams and conditioned on both text and image.\", content=Switch(label=\"Install Versatile Diffusion text2image, Dual Guided & Image Variation Pipeline\", value=prefs['install_versatile'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, disabled=status['installed_versatile'], on_change=lambda e:changed(e, 'install_versatile')))\n",
        "  \n",
        "  def toggle_clip(e):\n",
        "      prefs['install_CLIP_guided'] = install_CLIP_guided.content.value\n",
        "      status['changed_installers'] = True\n",
        "      clip_settings.height=None if prefs['install_CLIP_guided'] else 0\n",
        "      clip_settings.update()\n",
        "  install_CLIP_guided = Tooltip(message=\"Uses alternative LAION & OpenAI ViT diffusion. Takes more VRAM, so may need to make images smaller\", content=Switch(label=\"Install Stable Diffusion CLIP-Guided Pipeline\", value=prefs['install_CLIP_guided'], disabled=status['installed_clip'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_clip))\n",
        "  clip_model_id = Dropdown(label=\"CLIP Model ID\", width=350,\n",
        "            options=[\n",
        "                dropdown.Option(\"laion/CLIP-ViT-B-32-laion2B-s34B-b79K\"),\n",
        "                dropdown.Option(\"laion/CLIP-ViT-L-14-laion2B-s32B-b82K\"),\n",
        "                dropdown.Option(\"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\"),\n",
        "                dropdown.Option(\"laion/CLIP-ViT-g-14-laion2B-s12B-b42K\"),\n",
        "                dropdown.Option(\"openai/clip-vit-base-patch32\"),\n",
        "                dropdown.Option(\"openai/clip-vit-base-patch16\"),\n",
        "                dropdown.Option(\"openai/clip-vit-large-patch14\"),\n",
        "            ], value=prefs['clip_model_id'], autofocus=False, on_change=lambda e:changed(e, 'clip_model_id'),\n",
        "        )\n",
        "  clip_settings = Container(animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE, padding=padding.only(left=32, top=4), content=Column([clip_model_id]))\n",
        "  \n",
        "  def toggle_conceptualizer(e):\n",
        "      changed(e, 'install_conceptualizer')\n",
        "      conceptualizer_settings.height = None if e.control.value else 0\n",
        "      conceptualizer_settings.update()\n",
        "  def change_concepts_model(e):\n",
        "      nonlocal concept\n",
        "      changed(e, 'concepts_model')\n",
        "      concept = get_concept(e.control.value)\n",
        "      concepts_info.value = f\"To use the concept, include keyword token **<{concept['token']}>** in your Prompts. Info at [https://huggingface.co/sd-concepts-library/{concept['name']}](https://huggingface.co/sd-concepts-library/{concept['name']})\"\n",
        "      concepts_info.update()\n",
        "  def open_url(e):\n",
        "      page.launch_url(e.data)\n",
        "  def copy_token(e):\n",
        "      nonlocal concept\n",
        "      page.set_clipboard(f\"<{concept['token']}>\")\n",
        "      page.snack_bar = SnackBar(content=Text(f\"üìã  Token <{concept['token']}> copied to clipboard... Paste as word in your Prompt Text.\"))\n",
        "      page.snack_bar.open = True\n",
        "      page.update()\n",
        "  install_conceptualizer = Tooltip(message=\"Loads specially trained concept models to include in prompt with token\", content=Switch(label=\"Install Stable Diffusion Textual-Inversion Conceptualizer Pipeline\", value=prefs['install_conceptualizer'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_conceptualizer))\n",
        "  concept = get_concept(prefs['concepts_model'])\n",
        "  concepts_model = Dropdown(label=\"SD-Concepts Library Model\", hint_text=\"Specially trained community models made with Textual-Inversion\", width=451, options=[], value=prefs['concepts_model'], on_change=change_concepts_model)\n",
        "  copy_token_btn = IconButton(icon=icons.CONTENT_COPY, tooltip=\"Copy Token to Clipboard\", on_click=copy_token)\n",
        "  concepts_row = Row([concepts_model, copy_token_btn])\n",
        "  concepts_info = Markdown(f\"To use the concept, include keyword token **<{concept['token']}>** in your Prompts. Info at [https://huggingface.co/sd-concepts-library/{concept['name']}](https://huggingface.co/sd-concepts-library/{concept['name']})\", selectable=True, on_tap_link=open_url)\n",
        "  conceptualizer_settings = Container(animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE, padding=padding.only(left=32, top=5), content=Column([concepts_row, concepts_info]))\n",
        "  conceptualizer_settings.height = None if prefs['install_conceptualizer'] else 0\n",
        "  for c in concepts: concepts_model.options.append(dropdown.Option(c['name']))\n",
        "  install_upscale = Tooltip(message=\"Allows you to enlarge images with prompts. Note: Will run out of mem for images larger than 512px, start small.\", content=Switch(label=\"Install Stable Diffusion v2 Upscale 4X Pipeline\", value=prefs['install_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, disabled=status['installed_upscale'], on_change=lambda e:changed(e, 'install_upscale')))\n",
        "\n",
        "  diffusers_settings = Container(animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE, content=\n",
        "                                 Column([Container(Column([Container(None, height=3), model_row, Container(content=None, height=4), scheduler_mode,\n",
        "                                 Row([memory_optimization,\n",
        "                                 higher_vram_mode]), \n",
        "                                 #  enable_vae_slicing\n",
        "                                 #enable_attention_slicing,\n",
        "                                 #Row([sequential_cpu_offload, enable_vae_tiling]),\n",
        "                                 Row([enable_tome, enable_torch_compile]),\n",
        "                                 ]), padding=padding.only(left=32, top=4)),\n",
        "                                         install_text2img, install_img2img, #install_repaint, #install_megapipe, install_alt_diffusion, \n",
        "                                         install_interpolation, install_CLIP_guided, clip_settings, install_conceptualizer, conceptualizer_settings, install_safe, safety_config, \n",
        "                                         install_versatile, install_SAG, install_attend_and_excite, install_panorama, install_imagic, install_depth2img, install_composable, install_upscale]))\n",
        "  def toggle_stability(e):\n",
        "      prefs['install_Stability_api'] = install_Stability_api.content.value\n",
        "      has_changed=True\n",
        "      #print(f\"Toggle Stability {prefs['install_Stability_api']}\")\n",
        "      stability_settings.height=None if prefs['install_Stability_api'] else 0\n",
        "      stability_settings.update()\n",
        "      page.update()\n",
        "      #stability_box.content = stability_settings if prefs['install_stability'] else Container(content=None)\n",
        "      #stability_box.update()\n",
        "  install_Stability_api = Tooltip(message=\"Use DreamStudio.com servers without your GPU to create images on CPU.\", content=Switch(label=\"Install Stability-API DreamStudio Pipeline\", value=prefs['install_Stability_api'], disabled=status['installed_stability'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_stability))\n",
        "  use_Stability_api = Checkbox(label=\"Use Stability-ai API by default\", tooltip=\"Instead of using Diffusers, generate images in their cloud. Can toggle to compare batches..\", value=prefs['use_Stability_api'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e, 'use_Stability_api'))\n",
        "  model_checkpoint = Dropdown(label=\"Model Checkpoint\", hint_text=\"\", width=350, options=[dropdown.Option(\"stable-diffusion-xl-beta-v2-2-2\"), dropdown.Option(\"stable-diffusion-768-v2-1\"), dropdown.Option(\"stable-diffusion-512-v2-1\"), dropdown.Option(\"stable-diffusion-768-v2-0\"), dropdown.Option(\"stable-diffusion-512-v2-0\"), dropdown.Option(\"stable-diffusion-v1-5\"), dropdown.Option(\"stable-diffusion-v1\"), dropdown.Option(\"stable-inpainting-512-v2-0\"), dropdown.Option(\"stable-inpainting-v1-0\")], value=prefs['model_checkpoint'], autofocus=False, on_change=lambda e:changed(e, 'model_checkpoint'))\n",
        "  clip_guidance_preset = Dropdown(label=\"Clip Guidance Preset\", width=350, options=[dropdown.Option(\"SIMPLE\"), dropdown.Option(\"FAST_BLUE\"), dropdown.Option(\"FAST_GREEN\"), dropdown.Option(\"SLOW\"), dropdown.Option(\"SLOWER\"), dropdown.Option(\"SLOWEST\"), dropdown.Option(\"NONE\")], value=prefs['clip_guidance_preset'], autofocus=False, on_change=lambda e:changed(e, 'clip_guidance_preset'))\n",
        "  #generation_sampler = Dropdown(label=\"Generation Sampler\", hint_text=\"\", width=350, options=[dropdown.Option(\"ddim\"), dropdown.Option(\"plms\"), dropdown.Option(\"k_euler\"), dropdown.Option(\"k_euler_ancestral\"), dropdown.Option(\"k_heun\"), dropdown.Option(\"k_dpm_2\"), dropdown.Option(\"k_dpm_2_ancestral\"), dropdown.Option(\"k_lms\")], value=prefs['generation_sampler'], autofocus=False, on_change=lambda e:changed(e, 'generation_sampler'))\n",
        "  generation_sampler = Dropdown(label=\"Generation Sampler\", hint_text=\"\", width=350, options=[dropdown.Option(\"DDIM\"), dropdown.Option(\"DDPM\"), dropdown.Option(\"K_EULER\"), dropdown.Option(\"K_EULER_ANCESTRAL\"), dropdown.Option(\"K_HEUN\"), dropdown.Option(\"K_DPMPP_2M\"), dropdown.Option(\"K_DPM_2_ANCESTRAL\"), dropdown.Option(\"K_LMS\"), dropdown.Option(\"K_DPMPP_2S_ANCESTRAL\"), dropdown.Option(\"K_DPM_2\")], value=prefs['generation_sampler'], autofocus=False, on_change=lambda e:changed(e, 'generation_sampler'))\n",
        "  #\"K_EULER\" \"K_DPM_2\" \"K_LMS\" \"K_DPMPP_2S_ANCESTRAL\" \"K_DPMPP_2M\" \"DDIM\" \"DDPM\" \"K_EULER_ANCESTRAL\" \"K_HEUN\" \"K_DPM_2_ANCESTRAL\"\n",
        "  stability_settings = Container(animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE, padding=padding.only(left=32), content=Column([use_Stability_api, model_checkpoint, generation_sampler, clip_guidance_preset]))\n",
        "  \n",
        "  def toggle_AIHorde(e):\n",
        "      prefs['install_AIHorde_api'] = e.control.value\n",
        "      AIHorde_settings.height=None if prefs['install_AIHorde_api'] else 0\n",
        "      AIHorde_settings.update()\n",
        "      page.update()\n",
        "  install_AIHorde = Tooltip(message=\"Use AIHorde.net Crowdsourced cloud without your GPU to create images on CPU.\", content=Switch(label=\"Install AIHorde Crowdsorced Pipeline\", value=prefs['install_AIHorde_api'],active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_AIHorde))\n",
        "  use_AIHorde = Checkbox(label=\"Use Stable Horde API by default\", tooltip=\"Instead of using Diffusers, generate images in their cloud. Can toggle to compare batches..\", value=prefs['use_AIHorde_api'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e, 'use_AIHorde_api'))\n",
        "  AIHorde_model = Dropdown(label=\"Model Checkpoint\", hint_text=\"\", width=350, options=[dropdown.Option(\"3DKX\"), dropdown.Option(\"Abyss OrangeMix\"), dropdown.Option(\"AbyssOrangeMix-AfterDark\"), dropdown.Option(\"ACertainThing\"), dropdown.Option(\"AIO Pixel Art\"), dropdown.Option(\"Analog Diffusion\"), dropdown.Option(\"Anime Pencil Diffusion\"), dropdown.Option(\"Anygen\"), dropdown.Option(\"Anything Diffusion\"), dropdown.Option(\"Anything v3\"), dropdown.Option(\"App Icon Diffusion\"), dropdown.Option(\"Arcane Diffusion\"), dropdown.Option(\"Archer Diffusion\"), dropdown.Option(\"Asim Simpsons\"), dropdown.Option(\"A to Zovya RPG\"), dropdown.Option(\"Balloon Art\"), dropdown.Option(\"Borderlands\"), dropdown.Option(\"BPModel\"), dropdown.Option(\"BubblyDubbly\"), dropdown.Option(\"Char\"), dropdown.Option(\"CharHelper\"), dropdown.Option(\"Cheese Daddys Landscape Mix\"), dropdown.Option(\"ChilloutMix\"), dropdown.Option(\"ChromaV5\"), dropdown.Option(\"Classic Animation Diffusion\"), dropdown.Option(\"Clazy\"), dropdown.Option(\"Colorful\"), dropdown.Option(\"Coloring Book\"), dropdown.Option(\"Comic-Diffusion\"), dropdown.Option(\"Concept Sheet\"), dropdown.Option(\"Counterfeit\"), dropdown.Option(\"Cyberpunk Anime Diffusion\"), dropdown.Option(\"CyriousMix\"), dropdown.Option(\"Dan Mumford Style\"), dropdown.Option(\"Darkest Diffusion\"), dropdown.Option(\"Dark Victorian Diffusion\"), dropdown.Option(\"Deliberate\"), dropdown.Option(\"DGSpitzer Art Diffusion\"), dropdown.Option(\"Disco Elysium\"), dropdown.Option(\"DnD Item\"), dropdown.Option(\"Double Exposure Diffusion\"), dropdown.Option(\"Dreamlike Diffusion\"), dropdown.Option(\"Dreamlike Photoreal\"), dropdown.Option(\"DreamLikeSamKuvshinov\"), dropdown.Option(\"Dreamshaper\"), dropdown.Option(\"DucHaiten\"), dropdown.Option(\"DucHaiten Classic Anime\"), dropdown.Option(\"Dungeons and Diffusion\"), dropdown.Option(\"Dungeons n Waifus\"), dropdown.Option(\"Eimis Anime Diffusion\"), dropdown.Option(\"Elden Ring Diffusion\"), dropdown.Option(\"Elldreth's Lucid Mix\"), dropdown.Option(\"Elldreths Retro Mix\"), dropdown.Option(\"Epic Diffusion\"), dropdown.Option(\"Eternos\"), dropdown.Option(\"Experience\"), dropdown.Option(\"ExpMix Line\"), dropdown.Option(\"FaeTastic\"), dropdown.Option(\"Fantasy Card Diffusion\"), dropdown.Option(\"FKing SciFi\"), dropdown.Option(\"Funko Diffusion\"), dropdown.Option(\"Furry Epoch\"), dropdown.Option(\"Future Diffusion\"), dropdown.Option(\"Ghibli Diffusion\"), dropdown.Option(\"GorynichMix\"), dropdown.Option(\"Grapefruit Hentai\"), dropdown.Option(\"Graphic-Art\"), dropdown.Option(\"GTA5 Artwork Diffusion\"), dropdown.Option(\"GuoFeng\"), dropdown.Option(\"Guohua Diffusion\"), dropdown.Option(\"HASDX\"), dropdown.Option(\"Hassanblend\"), dropdown.Option(\"Healy's Anime Blend\"), dropdown.Option(\"Hentai Diffusion\"), dropdown.Option(\"HRL\"), dropdown.Option(\"iCoMix\"), dropdown.Option(\"Illuminati Diffusion\"), dropdown.Option(\"Inkpunk Diffusion\"), dropdown.Option(\"Jim Eidomode\"), dropdown.Option(\"JWST Deep Space Diffusion\"), dropdown.Option(\"Kenshi\"), dropdown.Option(\"Knollingcase\"), dropdown.Option(\"Korestyle\"), dropdown.Option(\"kurzgesagt\"), dropdown.Option(\"Laolei New Berry Protogen Mix\"), dropdown.Option(\"Lawlas's yiff mix\"), dropdown.Option(\"Liberty\"), dropdown.Option(\"Marvel Diffusion\"), dropdown.Option(\"Mega Merge Diffusion\"), dropdown.Option(\"Microcasing\"), dropdown.Option(\"Microchars\"), dropdown.Option(\"Microcritters\"), dropdown.Option(\"Microscopic\"), dropdown.Option(\"Microworlds\"), dropdown.Option(\"Midjourney Diffusion\"), dropdown.Option(\"Midjourney PaintArt\"), dropdown.Option(\"Min Illust Background\"), dropdown.Option(\"ModernArt Diffusion\"), dropdown.Option(\"mo-di-diffusion\"), dropdown.Option(\"Moedel\"), dropdown.Option(\"MoistMix\"), dropdown.Option(\"Movie Diffusion\"), dropdown.Option(\"NeverEnding Dream\"), dropdown.Option(\"Nitro Diffusion\"), dropdown.Option(\"Openniji\"), dropdown.Option(\"OrbAI\"), dropdown.Option(\"Papercutcraft\"), dropdown.Option(\"Papercut Diffusion\"), dropdown.Option(\"Pastel Mix\"), dropdown.Option(\"Perfect World\"), dropdown.Option(\"PFG\"), dropdown.Option(\"pix2pix\"), dropdown.Option(\"PIXHELL\"), dropdown.Option(\"Poison\"), dropdown.Option(\"Pokemon3D\"), dropdown.Option(\"PortraitPlus\"), dropdown.Option(\"PPP\"), dropdown.Option(\"Pretty 2.5D\"), dropdown.Option(\"PRMJ\"), dropdown.Option(\"Project Unreal Engine 5\"), dropdown.Option(\"ProtoGen\"), dropdown.Option(\"Protogen Anime\"), dropdown.Option(\"Protogen Infinity\"), dropdown.Option(\"Pulp Vector Art\"), dropdown.Option(\"PVC\"), dropdown.Option(\"Rachel Walker Watercolors\"), dropdown.Option(\"Rainbowpatch\"), dropdown.Option(\"Ranma Diffusion\"), dropdown.Option(\"RCNZ Dumb Monkey\"), dropdown.Option(\"RCNZ Gorilla With A Brick\"), dropdown.Option(\"RealBiter\"), dropdown.Option(\"Realism Engine\"), dropdown.Option(\"Realistic Vision\"), dropdown.Option(\"Redshift Diffusion\"), dropdown.Option(\"Rev Animated\"), dropdown.Option(\"Robo-Diffusion\"), dropdown.Option(\"Rodent Diffusion\"), dropdown.Option(\"RPG\"), dropdown.Option(\"Samdoesarts Ultmerge\"), dropdown.Option(\"Sci-Fi Diffusion\"), dropdown.Option(\"SD-Silicon\"), dropdown.Option(\"Seek.art MEGA\"), dropdown.Option(\"Smoke Diffusion\"), dropdown.Option(\"Something\"), dropdown.Option(\"Sonic Diffusion\"), dropdown.Option(\"Spider-Verse Diffusion\"), dropdown.Option(\"Squishmallow Diffusion\"), dropdown.Option(\"stable_diffusion\"), dropdown.Option(\"stable_diffusion_2.1\"), dropdown.Option(\"stable_diffusion_inpainting\"), dropdown.Option(\"Supermarionation\"), dropdown.Option(\"Sygil-Dev Diffusion\"), dropdown.Option(\"Synthwave\"), dropdown.Option(\"SynthwavePunk\"), dropdown.Option(\"TrexMix\"), dropdown.Option(\"trinart\"), dropdown.Option(\"Trinart Characters\"), dropdown.Option(\"Tron Legacy Diffusion\"), dropdown.Option(\"T-Shirt Diffusion\"), dropdown.Option(\"T-Shirt Print Designs\"), dropdown.Option(\"Uhmami\"), dropdown.Option(\"Ultraskin\"), dropdown.Option(\"UMI Olympus\"), dropdown.Option(\"Unstable Ink Dream\"), dropdown.Option(\"URPM\"), dropdown.Option(\"Valorant Diffusion\"), dropdown.Option(\"Van Gogh Diffusion\"), dropdown.Option(\"Vector Art\"), dropdown.Option(\"vectorartz\"), dropdown.Option(\"Vintedois Diffusion\"), dropdown.Option(\"VinteProtogenMix\"), dropdown.Option(\"Vivid Watercolors\"), dropdown.Option(\"Voxel Art Diffusion\"), dropdown.Option(\"waifu_diffusion\"), dropdown.Option(\"Wavyfusion\"), dropdown.Option(\"Woop-Woop Photo\"), dropdown.Option(\"Xynthii-Diffusion\"), dropdown.Option(\"Yiffy\"), dropdown.Option(\"Zack3D\"), dropdown.Option(\"Zeipher Female Model\"), dropdown.Option(\"Zelda BOTW\")], value=prefs['AIHorde_model'], autofocus=False, on_change=lambda e:changed(e, 'AIHorde_model'))\n",
        "  AIHorde_sampler = Dropdown(label=\"Generation Sampler\", hint_text=\"\", width=350, options=[dropdown.Option(\"k_lms\"), dropdown.Option(\"k_heun\"), dropdown.Option(\"k_euler\"), dropdown.Option(\"k_euler_a\"), dropdown.Option(\"k_dpm_2\"), dropdown.Option(\"k_dpm_2_a\"), dropdown.Option(\"k_dpm_fast\"), dropdown.Option(\"k_dpm_adaptive\"), dropdown.Option(\"k_dpmpp_2s_a\"), dropdown.Option(\"k_dpmpp_2m\"), dropdown.Option(\"dpmsolver\"), dropdown.Option(\"k_dpmpp_sde\"), dropdown.Option(\"DDIM\")], value=prefs['AIHorde_sampler'], autofocus=False, on_change=lambda e:changed(e, 'AIHorde_sampler'))\n",
        "  AIHorde_post_processing = Dropdown(label=\"Post-Processing\", hint_text=\"\", width=350, options=[dropdown.Option(\"None\"), dropdown.Option(\"GFPGAN\"), dropdown.Option(\"RealESRGAN_x4plus\"), dropdown.Option(\"RealESRGAN_x2plus\"), dropdown.Option(\"RealESRGAN_x4plus_anime_6B\"), dropdown.Option(\"NMKD_Siax\"), dropdown.Option(\"4x_AnimeSharp\"), dropdown.Option(\"CodeFormers\"), dropdown.Option(\"strip_background\")], value=prefs['AIHorde_post_processing'], autofocus=False, on_change=lambda e:changed(e, 'AIHorde_post_processing'))\n",
        "  AIHorde_settings = Container(animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE, padding=padding.only(left=32), content=Column([use_AIHorde, AIHorde_model, AIHorde_sampler, AIHorde_post_processing]))\n",
        "  AIHorde_settings.height = None if prefs['install_AIHorde_api'] else 0\n",
        "  \n",
        "  install_ESRGAN = Tooltip(message=\"Recommended to enlarge & sharpen all images as they're made.\", content=Switch(label=\"Install Real-ESRGAN AI Upscaler\", value=prefs['install_ESRGAN'], disabled=status['installed_ESRGAN'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e, 'install_ESRGAN')))\n",
        "  install_OpenAI = Tooltip(message=\"Use advanced AI to help make creative prompts. Also enables DALL-E 2 generation.\", content=Switch(label=\"Install OpenAI GPT-3 Text Engine\", value=prefs['install_OpenAI'], disabled=status['installed_OpenAI'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e, 'install_OpenAI')))\n",
        "  install_TextSynth = Tooltip(message=\"Alternative Text AI for brainstorming & rewriting your prompts. Pretty smart..\", content=Switch(label=\"Install TextSynth GPT-J Text Engine\", value=prefs['install_TextSynth'], disabled=status['installed_TextSynth'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e, 'install_TextSynth')))\n",
        "  diffusers_settings.height = None if prefs['install_diffusers'] else 0\n",
        "  stability_settings.height = None if prefs['install_Stability_api'] else 0\n",
        "  clip_settings.height = None if prefs['install_CLIP_guided'] else 0\n",
        "  \n",
        "  \n",
        "  def run_installers(e):\n",
        "      global force_updates\n",
        "      def console_clear():\n",
        "        page.banner.content.controls = []\n",
        "        page.update()\n",
        "      def console_msg(msg, clear=True, show_progress=True):\n",
        "        if not page.banner.open:\n",
        "          page.banner.open = True\n",
        "        if clear:\n",
        "          page.banner.content.controls = []\n",
        "        if show_progress:\n",
        "          page.banner.content.controls.append(Row([Stack([Icon(icons.DOWNLOADING, color=colors.AMBER, size=48), Container(content=ProgressRing(), padding=padding.only(top=6, left=6), alignment=alignment.center)]), Container(content=Text(\"  \" + msg.strip() , weight=FontWeight.BOLD, color=colors.ON_SECONDARY_CONTAINER, size=18), alignment=alignment.bottom_left, padding=padding.only(top=6)) ]))\n",
        "          #page.banner.content.controls.append(Stack([Container(content=Text(msg.strip() + \"  \", weight=FontWeight.BOLD, color=colors.ON_SECONDARY_CONTAINER, size=18), alignment=alignment.bottom_left, padding=padding.only(top=6)), Container(content=ProgressRing(), alignment=alignment.center if (page.window_width or page.width) > 768 else alignment.center_right)]))\n",
        "          #page.banner.content.controls.append(Stack([Container(content=Text(msg.strip() + \"  \", weight=FontWeight.BOLD, color=colors.ON_SECONDARY_CONTAINER, size=18), alignment=alignment.bottom_left, padding=padding.only(top=6)), Container(content=ProgressRing(), alignment=alignment.center)]))\n",
        "          #page.banner.content.controls.append(Row([Text(msg.strip() + \"  \", weight=FontWeight.BOLD, color=colors.GREEN_600), ProgressRing()]))\n",
        "        else:\n",
        "          page.banner.content.controls.append(Text(msg.strip(), weight=FontWeight.BOLD, color=colors.GREEN_600))\n",
        "        page.update()\n",
        "      page.console_msg = console_msg\n",
        "      if status['changed_installers']:\n",
        "        save_settings_file(page, change_icon=False)\n",
        "        status['changed_installers'] = False\n",
        "      # Temporary until I get Xformers to work\n",
        "      #prefs['memory_optimization'] = 'Attention Slicing' if prefs['enable_attention_slicing'] else 'None'\n",
        "      if prefs['install_diffusers'] and not bool(prefs['HuggingFace_api_key']):\n",
        "        alert_msg(e.page, \"You must provide your HuggingFace API Key to use Diffusers.\")\n",
        "        return\n",
        "      if prefs['install_Stability_api'] and not bool(prefs['Stability_api_key']):\n",
        "        alert_msg(e.page, \"You must have your DreamStudio.ai Stability-API Key to use Stability.  Note that it will use your tokens.\")\n",
        "        return\n",
        "      if prefs['install_OpenAI'] and not bool(prefs['OpenAI_api_key']):\n",
        "        alert_msg(e.page, \"You must have your OpenAI API Key to use GPT-3 Text AI.\")\n",
        "        return\n",
        "      if prefs['install_TextSynth'] and not bool(prefs['TextSynth_api_key']):\n",
        "        alert_msg(e.page, \"You must have your TextSynth API Key to use GPT-J Text AI.\")\n",
        "        return\n",
        "      if prefs['install_AIHorde_api'] and not bool(prefs['AIHorde_api_key']):\n",
        "        alert_msg(e.page, \"You must have your AIHorde.net API Key to use Stable Horde.  Note that it will use your Kudos.\")\n",
        "        return\n",
        "      page.banner.content = Column([], scroll=ScrollMode.AUTO, auto_scroll=True, tight=True, spacing=0, alignment=MainAxisAlignment.END)\n",
        "      page.banner.open = True\n",
        "      page.update()\n",
        "      if prefs['install_diffusers']:\n",
        "        console_msg(\"Installing Hugging Face Diffusers Pipeline...\")\n",
        "        get_diffusers(page)\n",
        "        status['installed_diffusers'] = True\n",
        "      if prefs['install_text2img'] and prefs['install_diffusers']:\n",
        "        console_msg(\"Downloading Stable Diffusion Text2Image, Image2Image & Inpaint Pipeline...\")\n",
        "        #with io.StringIO() as buf, redirect_stdout(buf):\n",
        "        #print('redirected')\n",
        "        get_text2image(page)\n",
        "        #output = buf.getvalue()\n",
        "        #page.banner.content.controls.append(Text(output.strip()))\n",
        "        status['installed_txt2img'] = True\n",
        "        page.img_block.height = None\n",
        "        page.img_block.update()\n",
        "        page.update()\n",
        "      if prefs['install_img2img'] and prefs['install_diffusers']:\n",
        "        console_msg(\"Downloading Stable Diffusion Inpaint Model & Image2Image Pipeline...\")\n",
        "        get_image2image(page)\n",
        "        status['installed_img2img'] = True\n",
        "        page.img_block.height = None\n",
        "        page.img_block.update()\n",
        "        page.use_inpaint_model.visible = True\n",
        "        page.use_inpaint_model.update()\n",
        "        if not status['installed_txt2img']:\n",
        "          prefs['use_inpaint_model'] = True\n",
        "      '''if prefs['install_megapipe'] and prefs['install_diffusers']:\n",
        "        console_msg(\"Downloading Stable Diffusion Unified Mega Pipeline...\")\n",
        "        get_text2image(page)\n",
        "        status['installed_megapipe'] = True\n",
        "        page.img_block.height = None\n",
        "        page.img_block.update()'''\n",
        "      if prefs['install_alt_diffusion'] and prefs['install_diffusers']:\n",
        "        console_msg(\"Installing AltDiffusion text2image & image2image Pipeline...\")\n",
        "        get_alt_diffusion(page)\n",
        "        status['installed_alt_diffusion'] = True\n",
        "        page.use_alt_diffusion.visible = True\n",
        "        page.use_alt_diffusion.update()\n",
        "      if prefs['install_interpolation'] and prefs['install_diffusers']:\n",
        "        console_msg(\"Downloading Stable Diffusion Walk Interpolation Pipeline...\")\n",
        "        get_interpolation(page)\n",
        "        status['installed_interpolation'] = True\n",
        "        page.interpolation_block.visible = True\n",
        "        page.interpolation_block.update()\n",
        "      if prefs['install_CLIP_guided'] and prefs['install_diffusers']:\n",
        "        console_msg(\"Downloading Stable Diffusion CLIP-Guided Pipeline...\")\n",
        "        get_clip(page)\n",
        "        status['installed_clip'] = True\n",
        "        page.use_clip_guided_model.visible = True\n",
        "        page.use_clip_guided_model.update()\n",
        "        page.clip_block.height = None if prefs['use_clip_guided_model'] else 0\n",
        "        page.clip_block.update()\n",
        "        if prefs['use_clip_guided_model']:\n",
        "          page.img_block.height = 0\n",
        "          page.img_block.update()\n",
        "      if prefs['install_conceptualizer'] and prefs['install_diffusers']:\n",
        "        console_msg(\"Installing SD Concepts Library Textual Inversion Pipeline...\")\n",
        "        get_conceptualizer(page)\n",
        "        page.use_conceptualizer_model.visible = True\n",
        "        page.use_conceptualizer_model.update()\n",
        "        if prefs['use_conceptualizer']:\n",
        "          page.img_block.height = 0\n",
        "          page.img_block.update()\n",
        "        status['installed_conceptualizer'] = True\n",
        "      if prefs['install_repaint'] and not status['installed_repaint'] and prefs['install_diffusers']:\n",
        "        console_msg(\"Installing Stable Diffusion RePaint Pipeline...\")\n",
        "        get_repaint(page)\n",
        "        status['installed_repaint'] = True\n",
        "      if prefs['install_depth2img'] and prefs['install_diffusers']:\n",
        "        console_msg(\"Installing Stable Diffusion 2 Depth2Image Pipeline...\")\n",
        "        get_depth2img(page)\n",
        "        status['installed_depth2img'] = True\n",
        "        if not status['installed_txt2img']:\n",
        "          page.img_block.height = None\n",
        "          page.img_block.update()\n",
        "        page.use_depth2img.visible = True\n",
        "        page.use_depth2img.update()\n",
        "      if prefs['install_SAG'] and prefs['install_diffusers']:\n",
        "        console_msg(\"Installing Stable Diffusion Self-Attention Guidance text2image Pipeline...\")\n",
        "        get_SAG(page)\n",
        "        status['installed_SAG'] = True\n",
        "        page.use_SAG.visible = True\n",
        "        page.use_SAG.update()\n",
        "      if prefs['install_attend_and_excite'] and prefs['install_diffusers']:\n",
        "        console_msg(\"Installing Stable Diffusion Attend and Excite text2image Pipeline...\")\n",
        "        get_attend_and_excite(page)\n",
        "        status['installed_attend_and_excite'] = True\n",
        "        page.use_attend_and_excite.visible = True\n",
        "        page.use_attend_and_excite.update()\n",
        "      if prefs['install_imagic'] and prefs['install_diffusers']:\n",
        "        console_msg(\"Installing Stable Diffusion iMagic image2image Pipeline...\")\n",
        "        get_imagic(page)\n",
        "        status['installed_imagic'] = True\n",
        "        if not status['installed_txt2img']:\n",
        "          page.img_block.height = None\n",
        "          page.img_block.update()\n",
        "        page.use_imagic.visible = True\n",
        "        page.use_imagic.update()\n",
        "      if prefs['install_composable'] and prefs['install_diffusers']:\n",
        "        console_msg(\"Installing Stable Diffusion Composable text2image Pipeline...\")\n",
        "        get_composable(page)\n",
        "        status['installed_composable'] = True\n",
        "        page.use_composable.visible = True\n",
        "        page.use_composable.update()\n",
        "      if prefs['install_versatile'] and prefs['install_diffusers']:\n",
        "        console_msg(\"Installing Stable Diffusion Versatile text2image, Variation & Inpaint Pipeline...\")\n",
        "        get_versatile(page)\n",
        "        status['installed_versatile'] = True\n",
        "        if not status['installed_txt2img']:\n",
        "          page.img_block.height = None\n",
        "          page.img_block.update()\n",
        "        page.use_versatile.visible = True\n",
        "        page.use_versatile.update()\n",
        "      if prefs['install_safe'] and prefs['install_diffusers']:\n",
        "        console_msg(\"Installing Stable Diffusion Safe text2image Pipeline...\")\n",
        "        get_safe(page)\n",
        "        status['installed_safe'] = True\n",
        "        page.use_safe.visible = True\n",
        "        page.use_safe.update()\n",
        "      if prefs['install_panorama'] and prefs['install_diffusers']:\n",
        "        console_msg(\"Installing MultiDiffusion Panorama text2image Pipeline...\")\n",
        "        get_panorama(page)\n",
        "        status['installed_panorama'] = True\n",
        "        page.use_panorama.visible = True\n",
        "        page.use_panorama.update()\n",
        "      if prefs['install_upscale'] and prefs['install_diffusers']:\n",
        "        console_msg(\"Installing Stable Diffusion 4X Upscale Pipeline...\")\n",
        "        get_upscale(page)\n",
        "        status['installed_upscale'] = True\n",
        "        page.use_upscale.visible = True\n",
        "        page.use_upscale.update()\n",
        "      if prefs['install_dreamfusion'] and not status['installed_dreamfusion'] and prefs['install_diffusers']:\n",
        "        console_msg(\"Installing Stable Diffusion DreamFusion 3D Pipeline...\")\n",
        "        get_dreamfusion(page) # No longer installing from here\n",
        "        status['installed_dreamfusion'] = True\n",
        "      if prefs['install_Stability_api']:\n",
        "        console_msg(\"Installing Stability-API DreamStudio.ai Pipeline...\")\n",
        "        get_stability(page)\n",
        "        status['installed_stability'] = True\n",
        "      if prefs['install_AIHorde_api']:\n",
        "        console_msg(\"Installing Stable Horde AIHorde.net Pipeline...\")\n",
        "        get_AIHorde(page)\n",
        "      if prefs['install_ESRGAN'] and not status['installed_ESRGAN']:\n",
        "        if not os.path.isdir(os.path.join(dist_dir, 'Real-ESRGAN')):\n",
        "          get_ESRGAN(page)\n",
        "          console_msg(\"Installing Real-ESRGAN Upscaler...\")\n",
        "        status['installed_ESRGAN'] = True\n",
        "      if prefs['install_ESRGAN']:\n",
        "        page.ESRGAN_block.height = None\n",
        "        page.ESRGAN_block_material.height = None\n",
        "        page.ESRGAN_block_dalle.height = None\n",
        "        page.ESRGAN_block_kandinsky.height = None\n",
        "        page.ESRGAN_block_kandinsky_fuse.height = None\n",
        "        page.ESRGAN_block_unCLIP.height = None\n",
        "        page.ESRGAN_block_unCLIP_image_variation.height = None\n",
        "        page.ESRGAN_block_unCLIP_interpolation.height = None\n",
        "        page.ESRGAN_block_unCLIP_image_interpolation.height = None\n",
        "        page.ESRGAN_block_semantic.height = None\n",
        "        page.ESRGAN_block_EDICT.height = None\n",
        "        page.ESRGAN_block_DiffEdit.height = None\n",
        "        page.ESRGAN_block_magic_mix.height = None\n",
        "        page.ESRGAN_block_paint_by_example.height = None\n",
        "        page.ESRGAN_block_instruct_pix2pix.height = None\n",
        "        page.ESRGAN_block_controlnet.height = None\n",
        "        page.ESRGAN_block_styler.height = None\n",
        "        page.ESRGAN_block_deep_daze.height = None\n",
        "        page.ESRGAN_block_DiT.height = None\n",
        "        page.ESRGAN_block_text_to_video.height = None\n",
        "        page.ESRGAN_block_text_to_video_zero.height = None\n",
        "        page.ESRGAN_block.update()\n",
        "        page.ESRGAN_block_material.update()\n",
        "        page.ESRGAN_block_dalle.update()\n",
        "        page.ESRGAN_block_kandinsky.update()\n",
        "        page.ESRGAN_block_kandinsky_fuse.update()\n",
        "        page.ESRGAN_block_unCLIP.update()\n",
        "        page.ESRGAN_block_unCLIP_image_variation.update()\n",
        "        page.ESRGAN_block_unCLIP_interpolation.update()\n",
        "        page.ESRGAN_block_unCLIP_image_interpolation.update()\n",
        "        page.ESRGAN_block_semantic.update()\n",
        "        page.ESRGAN_block_EDICT.update()\n",
        "        page.ESRGAN_block_DiffEdit.update()\n",
        "        page.ESRGAN_block_magic_mix.update()\n",
        "        page.ESRGAN_block_paint_by_example.update()\n",
        "        page.ESRGAN_block_instruct_pix2pix.update()\n",
        "        page.ESRGAN_block_controlnet.update()\n",
        "        page.ESRGAN_block_styler.update()\n",
        "        page.ESRGAN_block_deep_daze.update()\n",
        "        page.ESRGAN_block_DiT.update()\n",
        "        page.ESRGAN_block_text_to_video.update()\n",
        "        page.ESRGAN_block_text_to_video_zero.update()\n",
        "      if prefs['install_OpenAI'] and not status['installed_OpenAI']:\n",
        "        try:\n",
        "          import openai\n",
        "        except ModuleNotFoundError as e:\n",
        "          console_msg(\"Installing OpenAI GPT-3 Libraries...\")\n",
        "          run_process(\"pip install openai -qq\", page=page)\n",
        "          pass\n",
        "        status['installed_OpenAI'] = True\n",
        "      if prefs['install_TextSynth'] and not status['installed_TextSynth']:\n",
        "        try:\n",
        "          from textsynthpy import TextSynth, Complete\n",
        "        except ModuleNotFoundError as e:\n",
        "          console_msg(\"Installing TextSynth GPT-J Libraries...\")\n",
        "          run_process(\"pip install textsynthpy -qq\", page=page)\n",
        "          pass\n",
        "        status['installed_TextSynth'] = True\n",
        "      #print('Done Installing...')\n",
        "      if prefs['enable_sounds']: page.snd_done.play()\n",
        "      console_clear()\n",
        "      page.banner.open = False\n",
        "      page.banner.update()\n",
        "      page.update()\n",
        "      install_diffusers.update()\n",
        "      #install_text2img.update()\n",
        "      #install_img2img.update()\n",
        "      install_Stability_api.update()\n",
        "      install_CLIP_guided.update()\n",
        "      install_ESRGAN.update()\n",
        "      install_OpenAI.update()\n",
        "      install_TextSynth.update()\n",
        "      update_parameters(page)\n",
        "      page.Parameters.controls[0].content.update()\n",
        "      #page.Parameters.updater()\n",
        "      force_updates = False\n",
        "      if current_tab==1:\n",
        "        page.Installers.controls[0].content.update()\n",
        "        page.Installers.update()\n",
        "        page.show_install_fab(False)\n",
        "        page.tabs.selected_index = 2\n",
        "        page.tabs.update()\n",
        "        page.update()\n",
        "  def show_install_fab(show = True):\n",
        "    if show:\n",
        "      page.floating_action_button = FloatingActionButton(content=Row([Icon(icons.FILE_DOWNLOAD), Text(\"Run Installations\", size=18)], alignment=\"center\", spacing=5), width=205, shape=ft.RoundedRectangleBorder(radius=22), on_click=run_installers)\n",
        "      #page.floating_action_button = FloatingActionButton(icon=icons.FILE_DOWNLOAD, text=\"Run Installations\", on_click=run_installers)\n",
        "      page.update()\n",
        "    else:\n",
        "      if page.floating_action_button is not None:\n",
        "        page.floating_action_button = None\n",
        "        page.update()\n",
        "  page.show_install_fab = show_install_fab\n",
        "  install_button = ElevatedButton(content=Text(value=\"‚è¨   Run Installations \", size=20), on_click=run_installers)\n",
        "  \n",
        "  #image_output = TextField(label=\"Image Output Path\", value=prefs['image_output'], on_change=changed)\n",
        "  c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "                content=Column([\n",
        "        Header(\"üì•  Stable Diffusion Required & Optional Installers\"),\n",
        "        install_diffusers,\n",
        "        diffusers_settings,\n",
        "        #install_text2img,\n",
        "        #install_img2img,\n",
        "        install_Stability_api,\n",
        "        stability_settings,\n",
        "        install_AIHorde,\n",
        "        AIHorde_settings,\n",
        "        #install_CLIP_guided,\n",
        "        #clip_settings,\n",
        "        install_ESRGAN,\n",
        "        install_OpenAI,\n",
        "        install_TextSynth,\n",
        "        #install_button,\n",
        "        Container(content=None, height=32),\n",
        "      ],\n",
        "  ))], scroll=ScrollMode.AUTO)\n",
        "  def init_boxes():\n",
        "    diffusers_settings.height = None if prefs['install_diffusers'] else 0\n",
        "    stability_settings.height = None if prefs['install_Stability_api'] else 0\n",
        "    clip_settings.height = None if prefs['install_CLIP_guided'] else 0\n",
        "    diffusers_settings.update()\n",
        "    stability_settings.update()\n",
        "    clip_settings.update()\n",
        "    page.update()\n",
        "  #init_boxes()\n",
        "  return c\n",
        "\n",
        "def update_parameters(page):\n",
        "  #page.img_block.height = None if status['installed_img2img'] or status['installed_megapipe'] or status['installed_stability'] else 0\n",
        "  page.img_block.height = None if (status['installed_txt2img'] or status['installed_stability']) and not (status['installed_clip'] and prefs['use_clip_guided_model']) else 0\n",
        "  page.clip_block.height = None if status['installed_clip']  and prefs['use_clip_guided_model'] else 0\n",
        "  page.ESRGAN_block.height = None if status['installed_ESRGAN'] else 0\n",
        "  page.img_block.update()\n",
        "  page.clip_block.update()\n",
        "  page.ESRGAN_block.update()\n",
        "  page.Parameters.update()\n",
        "  #print(\"Updated Parameters\")\n",
        "\n",
        "if is_Colab:\n",
        "    from google.colab import files\n",
        "\n",
        "#LoRA_models = [{'name': 'Von Platen LoRA', 'path': 'patrickvonplaten/lora'}, {'name': 'Dog Example', 'path':'patrickvonplaten/lora_dreambooth_dog_example'}, {'name': 'Trauter LoRAs', 'path': 'YoungMasterFromSect/Trauter_LoRAs'}, {'name': 'Capitalize T5', 'path': 'ShengdingHu/Capitalize_T5-LoRA'}, {'name': 'SayakPaul LoRA-T4', 'path': 'sayakpaul/sd-model-finetuned-lora-t4'}]\n",
        "#[{'name': 'sample-dog', 'path': 'lora-library/lora-dreambooth-sample-dog', 'prefix': 'sksdog'}, {'name': 'kdekuni', 'path': 'lora-library/kdekuni', 'prefix': 'a kdekuni golden funkopop'}, {'name': 'yarosnnv', 'path': 'lora-library/yarosnnv', 'prefix': 'yarosnnv'}, {'name': '', 'path': '', 'prefix': ''}, {'name': '', 'path': '', 'prefix': ''}, {'name': '', 'path': '', 'prefix': ''}, {'name': '', 'path': '', 'prefix': ''}, {'name': '', 'path': '', 'prefix': ''}, {'name': '', 'path': '', 'prefix': ''}, {'name': '', 'path': '', 'prefix': ''}, ]\n",
        "LoRA_models = [{'name': 'Dog Example', 'path':'patrickvonplaten/lora_dreambooth_dog_example'}, {'name': 'SayakPaul LoRA-T4', 'path': 'sayakpaul/sd-model-finetuned-lora-t4'}, {'name':'Openjourney LoRA', 'path':'prompthero/openjourney-lora', 'prefix': ''}, {'name':'Analog Diffusion', 'path':'https://replicate.delivery/pbxt/IzbeguwVsW3PcC1gbiLy5SeALwk4sGgWroHagcYIn9I960bQA/tmpjlodd7vazekezip.safetensors', 'prefix':'<1> '}]\n",
        "\n",
        "def buildParameters(page):\n",
        "  global prefs, status, args\n",
        "  def changed(e, pref=None, asInt=False, apply=True):\n",
        "      if pref is not None:\n",
        "        prefs[pref] = e.control.value if not asInt else int(e.control.value)\n",
        "      if page.floating_action_button is None and apply:\n",
        "        show_apply_fab(len(prompts) > 0)\n",
        "      #if apply_changes_button.visible != (len(prompts) > 0): #status['changed_parameters']:\n",
        "      #  apply_changes_button.visible = len(prompts) > 0\n",
        "      #  apply_changes_button.update()\n",
        "      status['changed_parameters'] = True\n",
        "      #page.update()\n",
        "  def change(e):\n",
        "      if page.floating_action_button is None:\n",
        "        show_apply_fab(len(prompts) > 0)\n",
        "      status['changed_parameters'] = True\n",
        "  def run_parameters(e):\n",
        "      save_parameters()\n",
        "      #page.tabs.current_tab = 3\n",
        "      page.show_apply_fab(False)\n",
        "      page.tabs.selected_index = 3\n",
        "      page.tabs.update()\n",
        "      page.update()\n",
        "  def save_parameters():\n",
        "      update_args()\n",
        "      page.update_prompts()\n",
        "      save_settings_file(page)\n",
        "      status['changed_parameters'] = False\n",
        "  def apply_to_prompts(e):\n",
        "      update_args()\n",
        "      page.apply_changes(e)\n",
        "      save_settings_file(page)\n",
        "      show_apply_fab(False)\n",
        "      #apply_changes_button.visible = False\n",
        "      #apply_changes_button.update()\n",
        "  def pick_files_result(e: FilePickerResultEvent):\n",
        "      # TODO: This is not working on Colab, maybe it can get_upload_url on other platform?\n",
        "      if e.files:\n",
        "        img = e.files\n",
        "        uf = []\n",
        "        fname = img[0]\n",
        "        print(\", \".join(map(lambda f: f.name, e.files)))\n",
        "        #print(os.path.join(fname.path, fname.name))\n",
        "        #src_path = os.path.join(fname.path, fname.name)\n",
        "        #for f in pick_files_dialog.result.files:\n",
        "        src_path = page.get_upload_url(fname.name, 600)\n",
        "        uf.append(FilePickerUploadFile(fname.name, upload_url=src_path))\n",
        "        pick_files_dialog.upload(uf)\n",
        "        print(str(src_path))\n",
        "        #src_path = ''.join(src_path)\n",
        "        print(str(uf[0]))\n",
        "        dst_path = os.path.join(root_dir, fname.name)\n",
        "        print(f'Copy {src_path} to {dst_path}')\n",
        "        #shutil.copy(src_path, dst_path)\n",
        "        # TODO: is init or mask?\n",
        "        init_image.value = dst_path\n",
        "      #selected_files.value = (\", \".join(map(lambda f: f.name, e.files)) if e.files else \"Cancelled!\")\n",
        "      #selected_files.update()\n",
        "\n",
        "  pick_files_dialog = FilePicker(on_result=pick_files_result)\n",
        "  page.overlay.append(pick_files_dialog)\n",
        "  #selected_files = Text()\n",
        "\n",
        "  def file_picker_result(e: FilePickerResultEvent):\n",
        "      if e.files != None:\n",
        "        upload_files(e)\n",
        "  def on_upload_progress(e: FilePickerUploadEvent):\n",
        "    nonlocal pick_type\n",
        "    if e.progress == 1:\n",
        "      if not slash in e.file_name:\n",
        "        fname = os.path.join(root_dir, e.file_name)\n",
        "      else:\n",
        "        fname = e.file_name\n",
        "      if pick_type == \"init\":\n",
        "        init_image.value = fname\n",
        "        init_image.update()\n",
        "        prefs['init_image'] = fname\n",
        "      elif pick_type == \"mask\":\n",
        "        mask_image.value = fname\n",
        "        mask_image.update()\n",
        "        prefs['mask_image'] = fname\n",
        "      page.update()\n",
        "  file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)\n",
        "  def upload_files(e):\n",
        "      uf = []\n",
        "      if file_picker.result != None and file_picker.result.files != None:\n",
        "          for f in file_picker.result.files:\n",
        "              if page.web:\n",
        "                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))\n",
        "              else:\n",
        "                on_upload_progress(FilePickerUploadEvent(f.path, 1, \"\"))\n",
        "          file_picker.upload(uf)\n",
        "  page.overlay.append(file_picker)\n",
        "  pick_type = \"\"\n",
        "  #page.overlay.append(pick_files_dialog)\n",
        "  def pick_init(e):\n",
        "      nonlocal pick_type\n",
        "      pick_type = \"init\"\n",
        "      file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"png\", \"PNG\"], dialog_title=\"Pick Init Image File\")\n",
        "  def pick_mask(e):\n",
        "      nonlocal pick_type\n",
        "      pick_type = \"mask\"\n",
        "      file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"png\", \"PNG\"], dialog_title=\"Pick Black & White Mask Image\")\n",
        "  def toggle_ESRGAN(e):\n",
        "      ESRGAN_settings.height = None if e.control.value else 0\n",
        "      prefs['apply_ESRGAN_upscale'] = e.control.value\n",
        "      ESRGAN_settings.update()\n",
        "      has_changed = True\n",
        "  def toggle_clip(e):\n",
        "      if e.control.value:\n",
        "        page.img_block.height = 0\n",
        "        page.clip_block.height = None if status['installed_clip'] else 0\n",
        "      else:\n",
        "        page.img_block.height = None if status['installed_txt2img'] or status['installed_stability'] else 0\n",
        "        page.clip_block.height = 0\n",
        "      page.img_block.update()\n",
        "      page.clip_block.update()\n",
        "      changed(e, 'use_clip_guided_model')\n",
        "  def change_use_cutouts(e):\n",
        "      num_cutouts.visible = e.control.value\n",
        "      num_cutouts.update()\n",
        "      changed(e, 'use_cutouts')\n",
        "  def toggle_interpolation(e):\n",
        "      interpolation_steps_slider.height = None if e.control.value else 0\n",
        "      interpolation_steps_slider.update()\n",
        "      if e.control.value: page.img_block.height = 0\n",
        "      else: page.img_block.height = None if status['installed_txt2img'] or status['installed_stability'] else 0\n",
        "      page.img_block.update()\n",
        "      changed(e, 'use_interpolation', apply=False)\n",
        "  def change_interpolation_steps(e):\n",
        "      interpolation_steps_value.value = f\" {int(e.control.value)} steps\"\n",
        "      interpolation_steps_value.update()\n",
        "      changed(e, 'num_interpolation_steps', asInt=True, apply=False)\n",
        "  def toggle_SAG(e):\n",
        "      sag_scale_slider.height = None if e.control.value else 0\n",
        "      sag_scale_slider.update()\n",
        "      changed(e, 'use_SAG', apply=False)\n",
        "  def change_sag_scale(e):\n",
        "      sag_scale_value.value = f\" {float(e.control.value)}\"\n",
        "      sag_scale_value.update()\n",
        "      changed(e, 'sag_scale', apply=False)\n",
        "  def toggle_attend_and_excite(e):\n",
        "      max_iter_to_alter_slider.height = None if e.control.value else 0\n",
        "      max_iter_to_alter_slider.update()\n",
        "      changed(e, 'use_attend_and_excite', apply=False)\n",
        "  def change_max_iter_to_alter(e):\n",
        "      max_iter_to_alter_value.value = f\" {int(e.control.value)} Iterations\"\n",
        "      max_iter_to_alter_value.update()\n",
        "      changed(e, 'max_iter_to_alter', asInt=True, apply=False)\n",
        "  def change_enlarge_scale(e):\n",
        "      enlarge_scale_slider.controls[1].value = f\" {float(e.control.value)}x\"\n",
        "      enlarge_scale_slider.update()\n",
        "      changed(e, 'enlarge_scale', apply=False)\n",
        "  def change_strength(e):\n",
        "      strength_value.value = f\" {int(e.control.value * 100)}\"\n",
        "      strength_value.update()\n",
        "      guidance.update()\n",
        "      changed(e, 'init_image_strength')\n",
        "  def toggle_conceptualizer(e):\n",
        "      if e.control.value:\n",
        "        page.img_block.height = 0\n",
        "      else:\n",
        "        page.img_block.height = None if status['installed_txt2img'] or status['installed_stability'] else 0\n",
        "      page.img_block.update()\n",
        "      changed(e, 'use_conceptualizer')\n",
        "  def toggle_centipede(e):\n",
        "      changed(e,'centipede_prompts_as_init_images', apply=False)\n",
        "      image_pickers.height = None if not e.control.value else 0\n",
        "      image_pickers.update()\n",
        "  def toggle_LoRA(e):\n",
        "      changed(e,'use_LoRA_model', apply=False)\n",
        "      LoRA_block.width = None if e.control.value else 0\n",
        "      LoRA_block.update()\n",
        "  def changed_LoRA(e):\n",
        "      changed(e, 'LoRA_model', apply=False)\n",
        "      custom_LoRA_model.visible = True if prefs['LoRA_model'] == \"Custom LoRA Path\" else False\n",
        "      custom_LoRA_model.update()\n",
        "      \n",
        "  batch_folder_name = TextField(label=\"Batch Folder Name\", value=prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name', apply=False))\n",
        "  #batch_size = TextField(label=\"Batch Size\", value=prefs['batch_size'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'batch_size'))\n",
        "  #n_iterations = TextField(label=\"Number of Iterations\", value=prefs['n_iterations'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'n_iterations'))\n",
        "  batch_size = NumberPicker(label=\"Batch Size: \", min=1, max=10, value=prefs['batch_size'], tooltip=\"Generates multiple images at the same time. Uses more memory...\", on_change=lambda e: changed(e, 'batch_size'))\n",
        "  n_iterations = NumberPicker(label=\"Number of Iterations: \", min=1, max=30, value=prefs['n_iterations'], tooltip=\"Creates multiple images in batch seperately\", on_change=lambda e: changed(e, 'n_iterations'))\n",
        "  #steps = TextField(label=\"Steps\", value=prefs['steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'steps', asInt=True))\n",
        "  steps = SliderRow(label=\"Steps\", min=0, max=200, divisions=200, pref=prefs, key='steps', on_change=change)\n",
        "  #eta = TextField(label=\"DDIM ETA\", value=prefs['eta'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'eta'))\n",
        "  eta = SliderRow(label=\"DDIM ETA\", min=0, max=1, divisions=20, round=1, pref=prefs, key='eta', tooltip=\"\", visible=False, on_change=change)\n",
        "  page.etas.append(eta)\n",
        "  seed = TextField(label=\"Seed\", value=prefs['seed'], keyboard_type=KeyboardType.NUMBER, width = 160, on_change=lambda e:changed(e,'seed'))\n",
        "  param_rows = Row([Column([batch_folder_name, seed, batch_size]), Column([steps, eta, n_iterations])])\n",
        "  batch_row = Row([batch_folder_name, seed])\n",
        "  number_row = Row([batch_size, n_iterations])\n",
        "  guidance = SliderRow(label=\"Guidance Scale\", min=0, max=50, divisions=100, round=1, pref=prefs, key='guidance_scale', on_change=change)\n",
        "  width_slider = SliderRow(label=\"Width\", min=256, max=1280, divisions=64, multiple=16, suffix=\"px\", pref=prefs, key='width', on_change=change)\n",
        "  height_slider = SliderRow(label=\"Height\", min=256, max=1280, divisions=64, multiple=16, suffix=\"px\", pref=prefs, key='height', on_change=change)\n",
        "\n",
        "  init_image = TextField(label=\"Init Image\", value=prefs['init_image'], on_change=lambda e:changed(e,'init_image'), expand=True, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_init))\n",
        "  mask_image = TextField(label=\"Mask Image\", value=prefs['mask_image'], on_change=lambda e:changed(e,'mask_image'), expand=True, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD_OUTLINED, on_click=pick_mask))\n",
        "  alpha_mask = Checkbox(label=\"Alpha Mask\", value=prefs['alpha_mask'], tooltip=\"Use Transparent Alpha Channel of Init as Mask\", fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'alpha_mask'))\n",
        "  invert_mask = Checkbox(label=\"Invert Mask\", value=prefs['invert_mask'], tooltip=\"Reverse Black & White of Image Mask\", fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'invert_mask'))\n",
        "  image_pickers = Container(content=ResponsiveRow([Row([init_image, alpha_mask], col={\"lg\":6}), Row([mask_image, invert_mask], col={\"lg\":6})]), padding=padding.only(top=5), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "  image_pickers.height = None if not prefs['centipede_prompts_as_init_images'] else 0\n",
        "  init_image_strength = Slider(min=0.1, max=0.9, divisions=16, label=\"{value}%\", value=prefs['init_image_strength'], on_change=change_strength, expand=True)\n",
        "  strength_value = Text(f\" {int(prefs['init_image_strength'] * 100)}%\", weight=FontWeight.BOLD)\n",
        "  strength_slider = Row([Text(\"Init Image Strength: \"), strength_value, init_image_strength])\n",
        "  page.use_inpaint_model = Tooltip(message=\"When using init_image and/or mask, use the newer pipeline for potentially better results\", content=Switch(label=\"Use Specialized Inpaint Model Instead\", tooltip=\"When using init_image and/or mask, use the newer pipeline for potentially better results\", value=prefs['use_inpaint_model'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'use_inpaint_model', apply=False)))\n",
        "  page.use_inpaint_model.visible = status['installed_img2img']\n",
        "  page.use_alt_diffusion = Tooltip(message=\"Supports 9 different languages for text2image & image2image\", content=Switch(label=\"Use AltDiffusion Pipeline Model Instead\", value=prefs['use_versatile'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'use_versatile', apply=False)))\n",
        "  page.use_alt_diffusion.visible = status['installed_alt_diffusion']\n",
        "  page.use_versatile = Tooltip(message=\"Dual Guided between prompt & image, or create Image Variation\", content=Switch(label=\"Use Versatile Pipeline Model Instead\", value=prefs['use_versatile'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'use_versatile', apply=False)))\n",
        "  page.use_versatile.visible = status['installed_versatile']\n",
        "  use_LoRA_model = Tooltip(message=\"Applies custom trained weighted attention model on top of loaded model\", content=Switch(label=\"Use LoRA Model Adapter Layer \", value=prefs['use_LoRA_model'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_LoRA))\n",
        "  page.LoRA_model = Dropdown(label=\"LoRA Model Weights\", width=200, options=[], value=prefs['LoRA_model'], on_change=changed_LoRA)\n",
        "  if len(prefs['custom_LoRA_models']) > 0:\n",
        "    for l in prefs['custom_LoRA_models']:\n",
        "      page.LoRA_model.options.append(dropdown.Option(l['name']))\n",
        "  for m in LoRA_models:\n",
        "      page.LoRA_model.options.append(dropdown.Option(m['name']))\n",
        "  page.LoRA_model.options.append(dropdown.Option(\"Custom LoRA Path\"))\n",
        "  \n",
        "  custom_LoRA_model = TextField(label=\"Custom LoRA Model Path\", value=prefs['custom_LoRA_model'], width=370, on_change=lambda e:changed(e, 'custom_LoRA_model', apply=False))\n",
        "  custom_LoRA_model.visible = True if prefs['LoRA_model'] == \"Custom LoRA Path\" else False\n",
        "  LoRA_block = Container(Row([page.LoRA_model, custom_LoRA_model]), padding=padding.only(top=3), animate_size=animation.Animation(800, AnimationCurve.EASE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "  LoRA_block.width = None if prefs['use_LoRA_model'] else 0\n",
        "  centipede_prompts_as_init_images = Tooltip(message=\"Feeds each image to the next prompt sequentially down the line\", content=Switch(label=\"Centipede Prompts as Init Images\", tooltip=\"Feeds each image to the next prompt sequentially down the line\", value=prefs['centipede_prompts_as_init_images'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_centipede))\n",
        "  use_interpolation = Tooltip(message=\"Creates animation frames transitioning, but it's not always perfect.\", content=Switch(label=\"Use Interpolation to Walk Latent Space between Prompts\", tooltip=\"Creates animation frames transitioning, but it's not always perfect.\", value=prefs['use_interpolation'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_interpolation))\n",
        "  interpolation_steps = Slider(min=1, max=100, divisions=99, label=\"{value}\", value=prefs['num_interpolation_steps'], on_change=change_interpolation_steps, expand=True)\n",
        "  interpolation_steps_value = Text(f\" {int(prefs['num_interpolation_steps'])} steps\", weight=FontWeight.BOLD)\n",
        "  interpolation_steps_slider = Container(Row([Text(f\"Number of Interpolation Steps between Prompts: \"), interpolation_steps_value, interpolation_steps]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "  Row([Text(f\"Number of Interpolation Steps between Prompts: \"), interpolation_steps_value, interpolation_steps])\n",
        "  if not bool(prefs['use_interpolation']):\n",
        "    interpolation_steps_slider.height = 0\n",
        "  page.interpolation_block = Column([use_interpolation, interpolation_steps_slider])\n",
        "  page.img_block = Container(Column([image_pickers, strength_slider, page.use_inpaint_model, centipede_prompts_as_init_images, Divider(height=9, thickness=2)]), padding=padding.only(top=5), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "  if not status['installed_interpolation']:\n",
        "    page.interpolation_block.visible = False\n",
        "  elif bool(prefs['use_interpolation']):\n",
        "    page.img_block.height = 0\n",
        "  use_SAG = Tooltip(message=\"Can drastically boost the performance and quality. Extracts the intermediate attention map from a diffusion model at every iteration and selects tokens above a certain attention score for masking and blurring to obtain a partially blurred input.\", content=Switch(label=\"Use Self-Attention Guidance (SAG) Text-to-Image\", value=prefs['use_SAG'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_SAG))\n",
        "  sag_scale = Slider(min=0, max=1, divisions=20, label=\"{value}\", value=prefs['sag_scale'], tooltip=\"How much Self-Attention Guidance to apply.\", on_change=change_sag_scale, expand=True)\n",
        "  sag_scale_value = Text(f\" {float(prefs['sag_scale'])}\", weight=FontWeight.BOLD)\n",
        "  sag_scale_slider = Container(Row([Text(f\"SAG Scale: \"),sag_scale_value, sag_scale]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "  page.use_SAG = Column([use_SAG, sag_scale_slider])\n",
        "  if not bool(prefs['use_SAG']):\n",
        "    sag_scale_slider.height = 0\n",
        "  if not status['installed_SAG']:\n",
        "    page.use_SAG.visible = False\n",
        "  use_attend_and_excite = Tooltip(message=\"To use, include plus signs before subject words in prompt to indicate token indices, like 'a +cat and a +frog'.\", content=Switch(label=\"Use Attend and Excite\", value=prefs['use_attend_and_excite'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_attend_and_excite))\n",
        "  max_iter_to_alter = Slider(min=1, max=100, divisions=99, label=\"{value}\", value=prefs['max_iter_to_alter'], tooltip=\"The first denoising steps are where the attend-and-excite is applied. I.e. if `max_iter_to_alter` is 25 and there are a total of `30` denoising steps, the first 25 denoising steps will apply attend-and-excite and the last 5 will not apply attend-and-excite.\", on_change=change_max_iter_to_alter, expand=True)\n",
        "  max_iter_to_alter_value = Text(f\" {int(prefs['max_iter_to_alter'])} iterations\", weight=FontWeight.BOLD)\n",
        "  max_iter_to_alter_slider = Container(Row([Text(f\"Max Iterations to Alter: \"), max_iter_to_alter_value, max_iter_to_alter]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "  page.use_attend_and_excite = Column([use_attend_and_excite, max_iter_to_alter_slider])\n",
        "  if not bool(prefs['use_attend_and_excite']):\n",
        "    max_iter_to_alter_slider.height = 0\n",
        "  if not status['installed_attend_and_excite']:\n",
        "    page.use_attend_and_excite.visible = False\n",
        "  page.use_clip_guided_model = Tooltip(message=\"Uses more VRAM, so you'll probably need to make image size smaller\", content=Switch(label=\"Use CLIP-Guided Model\", tooltip=\"Uses more VRAM, so you'll probably need to make image size smaller\", value=prefs['use_clip_guided_model'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_clip))\n",
        "  clip_guidance_scale = Slider(min=1, max=5000, divisions=4999, label=\"{value}\", value=prefs['clip_guidance_scale'], on_change=lambda e:changed(e,'clip_guidance_scale'), expand=True)\n",
        "  clip_guidance_scale_slider = Row([Text(\"CLIP Guidance Scale: \"), clip_guidance_scale])\n",
        "  use_cutouts = Checkbox(label=\"Use Cutouts\", value=bool(prefs['use_cutouts']), fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=change_use_cutouts)\n",
        "  num_cutouts = NumberPicker(label=\"    Number of Cutouts: \", min=1, max=10, value=prefs['num_cutouts'], on_change=lambda e: changed(e, 'num_cutouts', asInt=True))\n",
        "  num_cutouts.visible = bool(prefs['use_cutouts'])\n",
        "  #num_cutouts = TextField(label=\"Number of Cutouts\", value=prefs['num_cutouts'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'num_cutouts', asInt=True))\n",
        "  unfreeze_unet = Checkbox(label=\"Unfreeze UNET\", value=prefs['unfreeze_unet'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'unfreeze_unet', apply=False))\n",
        "  unfreeze_vae = Checkbox(label=\"Unfreeze VAE\", value=prefs['unfreeze_vae'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'unfreeze_vae', apply=False))\n",
        "  page.clip_block = Container(Column([clip_guidance_scale_slider, Row([use_cutouts, num_cutouts], expand=False), unfreeze_unet, unfreeze_vae, Divider(height=9, thickness=2)]), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "  page.use_conceptualizer_model = Tooltip(message=\"Use Textual-Inversion Community Model Concepts\", content=Switch(label=\"Use Custom Conceptualizer Model\", tooltip=\"Use Textual-Inversion Community Model\", value=prefs['use_conceptualizer'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_conceptualizer))\n",
        "  page.use_conceptualizer_model.visible = bool(status['installed_conceptualizer'])\n",
        "  page.use_depth2img = Tooltip(message=\"To use, provide init_image with a good composition and prompts to approximate same depth.\", content=Switch(label=\"Use Depth2Image Pipeline for img2img init image generation\", value=prefs['use_depth2img'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'use_depth2img', apply=False)))\n",
        "  page.use_depth2img.visible = bool(status['installed_depth2img'])\n",
        "  page.use_imagic = Tooltip(message=\"Allows you to edit an image with prompt text.\", content=Switch(label=\"Use iMagic for img2img init image editing\", value=prefs['use_imagic'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'use_imagic', apply=False)))\n",
        "  page.use_imagic.visible = bool(status['installed_imagic'])\n",
        "  page.use_composable = Tooltip(message=\"Allows conjunction and negation operators for compositional generation with conditional diffusion models\", content=Switch(label=\"Use Composable Prompts for txt2img Weight | Segments\", value=prefs['use_composable'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'use_composable', apply=False)))\n",
        "  page.use_composable.visible = bool(status['installed_composable'])\n",
        "  page.use_panorama = Column([Tooltip(message=\"Fuses together images to make extra-wide 2048x512\", content=Switch(label=\"Use Panorama text2image Pipeline Instead\", value=prefs['use_panorama'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'use_panorama', apply=False))),\n",
        "                              Row([Text(\"Panoramic Width x 512:\"), Slider(min=1024, max=4608, divisions=28, label=\"{value}px\", expand=True, value=prefs['panorama_width'], on_change=lambda e:changed(e, 'panorama_width', asInt=True, apply=False))])])\n",
        "  page.use_panorama.visible = status['installed_panorama']\n",
        "  page.use_safe = Tooltip(message=\"Models trained only on Safe images\", content=Switch(label=\"Use Safe Diffusion Pipeline instead\", value=prefs['use_safe'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'use_safe', apply=False)))\n",
        "  page.use_safe.visible = bool(status['installed_safe'])\n",
        "  page.use_upscale = Tooltip(message=\"Enlarges your Image Generations guided by the same Prompt.\", content=Switch(label=\"Upscale 4X with Stable Diffusion 2\", value=prefs['use_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'use_upscale', apply=False)))\n",
        "  page.use_upscale.visible = bool(status['installed_upscale'])\n",
        "  apply_ESRGAN_upscale = Switch(label=\"Apply ESRGAN Upscale\", value=prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)\n",
        "  enlarge_scale_value = Text(f\" {float(prefs['enlarge_scale'])}x\", weight=FontWeight.BOLD)\n",
        "  enlarge_scale = Slider(min=1, max=4, divisions=6, label=\"{value}x\", value=prefs['enlarge_scale'], on_change=change_enlarge_scale, expand=True)\n",
        "  enlarge_scale_slider = Row([Text(\"Enlarge Scale: \"), enlarge_scale_value, enlarge_scale])\n",
        "  face_enhance = Checkbox(label=\"Use Face Enhance GPFGAN\", value=prefs['face_enhance'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'face_enhance', apply=False))\n",
        "  display_upscaled_image = Checkbox(label=\"Display Upscaled Image\", value=prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image', apply=False))\n",
        "  ESRGAN_settings = Container(Column([enlarge_scale_slider, face_enhance, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "  page.ESRGAN_block = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "  page.img_block.height = None if status['installed_txt2img'] or status['installed_stability'] else 0\n",
        "  page.use_clip_guided_model.visible = status['installed_clip']\n",
        "  page.clip_block.height = None if status['installed_clip'] and prefs['use_clip_guided_model'] else 0\n",
        "  page.ESRGAN_block.height = None if status['installed_ESRGAN'] else 0\n",
        "  if not prefs['apply_ESRGAN_upscale']:\n",
        "    ESRGAN_settings.height = 0\n",
        "  parameters_button = ElevatedButton(content=Text(value=\"üìú   Continue to Image Prompts\", size=20), on_click=run_parameters)\n",
        "  parameters_row = Row([parameters_button], alignment=MainAxisAlignment.SPACE_BETWEEN)\n",
        "  #apply_changes_button = ElevatedButton(content=Text(value=\"üîÄ   Apply Changes to Current Prompts\", size=20), on_click=apply_to_prompts)\n",
        "  #apply_changes_button.visible = len(prompts) > 0 and status['changed_parameters']\n",
        "  def show_apply_fab(show = True):\n",
        "    if show:\n",
        "      page.floating_action_button = FloatingActionButton(content=Row([Icon(icons.TRANSFORM), Text(\"Apply Changes to Current Prompts\", size=18)], alignment=\"center\", spacing=5), width=333, shape=ft.RoundedRectangleBorder(radius=22), on_click=apply_to_prompts)\n",
        "      #page.floating_action_button = FloatingActionButton(icon=icons.TRANSFORM, text=\"Apply Changes to Current Prompts\", on_click=apply_to_prompts)\n",
        "      page.update()\n",
        "    else:\n",
        "      if page.floating_action_button is not None:\n",
        "        page.floating_action_button = None\n",
        "        page.update()\n",
        "  show_apply_fab(len(prompts) > 0 and status['changed_parameters'])\n",
        "  page.show_apply_fab = show_apply_fab\n",
        "  def updater():\n",
        "      #parameters.update()\n",
        "      c.update()\n",
        "      page.update()\n",
        "      #print(\"Updated Parameters Page\")\n",
        "\n",
        "  c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10), content=Column([\n",
        "        Header(\"üìù  Stable Diffusion Image Parameters\"),\n",
        "        #param_rows,\n",
        "        batch_row,\n",
        "        number_row,\n",
        "        steps,\n",
        "        guidance,\n",
        "        eta,\n",
        "        width_slider, height_slider, #Divider(height=9, thickness=2), \n",
        "        page.interpolation_block, page.use_safe, page.img_block, page.use_alt_diffusion, page.use_clip_guided_model, page.clip_block, page.use_versatile, page.use_SAG, page.use_attend_and_excite, page.use_panorama, page.use_conceptualizer_model,\n",
        "        Row([use_LoRA_model, LoRA_block]), page.use_imagic, page.use_depth2img, page.use_composable, page.use_upscale, page.ESRGAN_block,\n",
        "        #(img_block if status['installed_img2img'] or status['installed_stability'] else Container(content=None)), (clip_block if prefs['install_CLIP_guided'] else Container(content=None)), (ESRGAN_block if prefs['install_ESRGAN'] else Container(content=None)), \n",
        "        #parameters_row,\n",
        "      ],\n",
        "  ))], scroll=ScrollMode.AUTO)#batch_folder_name, batch_size, n_iterations, steps, eta, seed, \n",
        "  return c\n",
        "\n",
        "prompts = []\n",
        "args = {}\n",
        "\n",
        "def update_args():\n",
        "    global args\n",
        "    args = {\n",
        "        \"batch_size\":int(prefs['batch_size']),\n",
        "        \"n_iterations\":int(prefs['n_iterations']),\n",
        "        \"steps\":int(prefs['steps']),\n",
        "        \"eta\":float(prefs['eta']), \n",
        "        \"width\":int(prefs['width']),\n",
        "        \"height\":int(prefs['height']),\n",
        "        \"guidance_scale\":float(prefs['guidance_scale']),\n",
        "        \"seed\":int(prefs['seed']),\n",
        "        \"precision\":prefs['precision'],\n",
        "        \"init_image\": prefs['init_image'],\n",
        "        \"init_image_strength\": prefs['init_image_strength'],\n",
        "        \"mask_image\": prefs['mask_image'],\n",
        "        \"alpha_mask\": prefs['alpha_mask'],\n",
        "        \"invert_mask\": prefs['invert_mask'],\n",
        "        \"prompt2\": None, \"tweens\": 10,\n",
        "        \"negative_prompt\": None,\n",
        "        \"use_clip_guided_model\": prefs['use_clip_guided_model'],\n",
        "        \"clip_prompt\": \"\",\n",
        "        \"clip_guidance_scale\": float(prefs['clip_guidance_scale']),\n",
        "        \"use_cutouts\": bool(prefs['use_cutouts']),\n",
        "        \"num_cutouts\": int(prefs['num_cutouts']),\n",
        "        \"unfreeze_unet\": prefs['unfreeze_unet'],\n",
        "        \"unfreeze_vae\": prefs['unfreeze_vae'],\n",
        "        \"use_Stability\": False,\n",
        "        \"use_conceptualizer\": False} \n",
        "\n",
        "update_args()\n",
        "\n",
        "class Dream: \n",
        "    def __init__(self, prompt, **kwargs):\n",
        "        self.prompt = prompt\n",
        "        self.arg = args.copy()\n",
        "        for key, value in kwargs.items():\n",
        "          if key=='arg': self.arg = value\n",
        "          elif key==\"batch_size\": self.arg[key] = int(value)\n",
        "          elif key==\"n_iterations\": self.arg[key] = int(value)\n",
        "          elif key==\"steps\": self.arg[key] = int(value)\n",
        "          elif key==\"eta\": self.arg[key] = float(value)\n",
        "          elif key==\"width\": self.arg[key] = int(value)\n",
        "          elif key==\"height\": self.arg[key] = int(value)\n",
        "          elif key==\"guidance_scale\": self.arg[key] = float(value)\n",
        "          elif key==\"seed\": self.arg[key] = int(value)\n",
        "          elif key==\"precision\": self.arg[key] = value\n",
        "          elif key==\"init_image\": self.arg[key] = value\n",
        "          elif key==\"init_image_strength\": self.arg[key] = value\n",
        "          elif key==\"mask_image\": self.arg[key] = value\n",
        "          elif key==\"alpha_mask\": self.arg[key] = value\n",
        "          elif key==\"invert_mask\": self.arg[key] = value\n",
        "          elif key==\"prompt2\": self.arg[key] = value\n",
        "          elif key==\"tweens\": self.arg[key] = int(value)\n",
        "          elif key==\"negative_prompt\": self.arg[key] = value\n",
        "          elif key==\"clip_prompt\": self.arg[key] = value\n",
        "          elif key==\"use_clip_guided_model\": self.arg[key] = value\n",
        "          elif key==\"clip_guidance_scale\": self.arg[key] = float(value)\n",
        "          elif key==\"use_cutouts\": self.arg[key] = value\n",
        "          elif key==\"num_cutouts\": self.arg[key] = int(value)\n",
        "          elif key==\"unfreeze_unet\": self.arg[key] = value\n",
        "          elif key==\"unfreeze_vae\": self.arg[key] = value\n",
        "          elif key==\"use_Stability\": self.arg[key] = value\n",
        "          elif key==\"use_conceptualizer\": self.arg[key] = value\n",
        "          elif key==\"prompt\": self.prompt = value\n",
        "          else: print(f\"Unknown argument: {key} = {value}\")\n",
        "        #self.arg = arg\n",
        "    #arg = args\n",
        "#print(str(args))\n",
        "import string\n",
        "\n",
        "def format_filename(s, force_underscore=False, use_dash=False, max_length=None):\n",
        "    file_max_length = int(prefs['file_max_length']) if max_length == None else int(max_length)\n",
        "    valid_chars = \"-_.() %s%s\" % (string.ascii_letters, string.digits)\n",
        "    filename = ''.join(c for c in s if c in valid_chars)\n",
        "    if use_dash:\n",
        "      filename = filename.replace(' ','-')\n",
        "    else:\n",
        "      if not prefs['file_allowSpace'] or force_underscore: filename = filename.replace(' ','_')\n",
        "    return filename[:file_max_length]\n",
        "\n",
        "def to_title(s, sentence=False):\n",
        "    s = s.replace('_',' ')\n",
        "    s = s.replace('-',' ')\n",
        "    if sentence:\n",
        "        sentences = s.split(\". \")\n",
        "        sentences2 = [sentence[0].capitalize() + sentence[1:] for sentence in sentences]\n",
        "        s2 = '. '.join(sentences2)\n",
        "        return s2\n",
        "    else:\n",
        "        return string.capwords(s)\n",
        "    \n",
        "'''from collections import ChainMap\n",
        "def merge_dict(*dicts):\n",
        "    all_keys  = set(k for d in dicts for k in d.keys())\n",
        "    chain_map = ChainMap(*reversed(dicts))\n",
        "    return {k: chain_map[k] for k in all_keys}\n",
        "\n",
        "def merge_dict(dict1, dict2): \n",
        "    merged_dict = {**dict1, **dict2}\n",
        "    return merged_dict\n",
        "'''\n",
        "def merge_dict(dict1, dict2):\n",
        "    new_dict = {}\n",
        "    for key in dict1:\n",
        "        new_dict[key] = dict1[key]\n",
        "    for key in dict2:\n",
        "        new_dict[key] = dict2[key]\n",
        "    return new_dict\n",
        "\n",
        "import copy\n",
        "\n",
        "def editPrompt(e):\n",
        "    global prompts, prefs, status\n",
        "    open_dream = e.control.data\n",
        "    idx = prompts.index(open_dream)\n",
        "    def changed_tweening(e):\n",
        "        status['changed_prompts'] = True\n",
        "        tweening_params.height = None if e.control.value else 0\n",
        "        tweening_params.update()\n",
        "        #prompt2.visible = e.control.value\n",
        "        #tweens.visible = e.control.value\n",
        "        prompt_tweening = e.control.value\n",
        "        e.page.update()\n",
        "    def changed_tweens(e):\n",
        "        prefs['tweens'] = int(e.control.value)\n",
        "    def close_dlg(e):\n",
        "        nonlocal edit_dlg\n",
        "        edit_dlg.open = False\n",
        "        #e.page.dialog.open = False\n",
        "        e.page.update()\n",
        "        #del edit_dlg\n",
        "        #e.page.dialog = None\n",
        "    def open_dlg():\n",
        "        nonlocal edit_dlg\n",
        "        e.page.dialog = edit_dlg\n",
        "        edit_dlg.open = True\n",
        "        e.page.update()\n",
        "    def save_dlg(e):\n",
        "        nonlocal arg, open_dream\n",
        "        dream = open_dream #e.control.data\n",
        "        dream.prompt = edit_text.value\n",
        "        arg['batch_size'] = int(batch_size.value)\n",
        "        arg['n_iterations'] = int(n_iterations.value)\n",
        "        arg['steps'] = int(steps.value)\n",
        "        arg['eta'] = float(eta.value)\n",
        "        arg['seed'] = int(seed.value)\n",
        "        arg['guidance_scale'] = float(guidance_scale.value)\n",
        "        arg['width'] = int(width_slider.value)\n",
        "        arg['height'] = int(height_slider.value)\n",
        "        arg['init_image'] = init_image.value\n",
        "        arg['mask_image'] = mask_image.value\n",
        "        arg['init_image_strength'] = float(init_image_strength.value)\n",
        "        arg['alpha_mask'] = alpha_mask.value\n",
        "        arg['invert_mask'] = invert_mask.value\n",
        "        arg['prompt2'] = prompt2.value if bool(use_prompt_tweening.value) else None\n",
        "        arg['tweens'] = int(tweens.value)\n",
        "        arg['negative_prompt'] = negative_prompt.value if bool(negative_prompt.value) else None\n",
        "        arg['use_clip_guided_model'] = use_clip_guided_model.content.value\n",
        "        arg['clip_guidance_scale'] = float(clip_guidance_scale.value)\n",
        "        arg['use_cutouts'] = use_cutouts.value\n",
        "        arg['num_cutouts'] = int(num_cutouts.value)\n",
        "        arg['unfreeze_unet'] = unfreeze_unet.value\n",
        "        arg['unfreeze_vae'] = unfreeze_vae.value\n",
        "        dream.arg = arg\n",
        "        diffs = arg_diffs(arg, args)\n",
        "        if bool(diffs):\n",
        "          e.page.prompts_list.controls[idx].subtitle = Text(\"    \" + diffs)\n",
        "        else:\n",
        "          e.page.prompts_list.controls[idx].subtitle = None\n",
        "        e.page.prompts_list.controls[idx].title.value = dream.prompt # = Text(edit_text.value)\n",
        "        status['changed_prompts'] = True\n",
        "        edit_dlg.open = False\n",
        "        e.page.update()\n",
        "    def file_picker_result(e: FilePickerResultEvent):\n",
        "        if e.files != None:\n",
        "          upload_files(e)\n",
        "    def on_upload_progress(e: FilePickerUploadEvent):\n",
        "      nonlocal pick_type\n",
        "      if e.progress == 1:\n",
        "        if not slash in e.file_name:\n",
        "          fname = os.path.join(root_dir, e.file_name)\n",
        "        else:\n",
        "          fname = e.file_name\n",
        "        if pick_type == \"init\":\n",
        "          init_image.value = fname\n",
        "          init_image.update()\n",
        "          prefs['init_image'] = fname\n",
        "        elif pick_type == \"mask\":\n",
        "          mask_image.value = fname\n",
        "          mask_image.update()\n",
        "          prefs['mask_image'] = fname\n",
        "        e.page.update()\n",
        "    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)\n",
        "    def upload_files(e):\n",
        "        uf = []\n",
        "        if file_picker.result != None and file_picker.result.files != None:\n",
        "            for f in file_picker.result.files:\n",
        "              if e.page.web:\n",
        "                uf.append(FilePickerUploadFile(f.name, upload_url=e.page.get_upload_url(f.name, 600)))\n",
        "              else:\n",
        "                on_upload_progress(FilePickerUploadEvent(f.path, 1, \"\"))\n",
        "            file_picker.upload(uf)\n",
        "    e.page.overlay.append(file_picker)\n",
        "    pick_type = \"\"\n",
        "    #page.overlay.append(pick_files_dialog)\n",
        "    def pick_init(e):\n",
        "        nonlocal pick_type\n",
        "        pick_type = \"init\"\n",
        "        file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"png\", \"PNG\"], dialog_title=\"Pick Init Image File\")\n",
        "    def pick_mask(e):\n",
        "        nonlocal pick_type\n",
        "        pick_type = \"mask\"\n",
        "        file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"png\", \"PNG\"], dialog_title=\"Pick Black & White Mask Image\")\n",
        "    def toggle_clip(e):\n",
        "        if e.control.value:\n",
        "          img_block.height = 0\n",
        "          clip_block.height = None if status['installed_clip'] else 0\n",
        "        else:\n",
        "          img_block.height = None if status['installed_txt2img'] or status['installed_stability'] else 0\n",
        "          clip_block.height = 0\n",
        "        img_block.update()\n",
        "        clip_block.update()\n",
        "        #changed(e)\n",
        "    arg = open_dream.arg #e.control.data.arg\n",
        "    edit_text = TextField(label=\"Composable | Prompt | Text\" if prefs['use_composable'] and status['installed_composable'] else \"Prompt Text\", col={'lg':9}, value=open_dream.prompt, multiline=True)\n",
        "    negative_prompt = TextField(label=\"Segmented Weights 1 | -0.7 | 1.2\" if prefs['use_composable'] and status['installed_composable'] else \"Negative Prompt Text\", col={'lg':3}, value=str((arg['negative_prompt'] or '') if 'negative_prompt' in arg else ''))\n",
        "    #batch_folder_name = TextField(label=\"Batch Folder Name\", value=arg['batch_folder_name'], on_change=changed)\n",
        "    #print(str(arg))\n",
        "    prompt_tweening = bool(arg['prompt2']) if 'prompt2' in arg else False\n",
        "    use_prompt_tweening = Switch(label=\"Prompt Tweening\", value=prompt_tweening, active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=changed_tweening)\n",
        "    use_prompt_tweening.visible = True if status['installed_txt2img'] and prefs['higher_vram_mode'] else False\n",
        "#TODO: Fix tweening code for float16 lpw pipeline to reactivate tweening\n",
        "    prompt2 = TextField(label=\"Prompt 2 Transition Text\", expand=True, value=arg['prompt2'] if 'prompt2' in arg else '')\n",
        "    tweens = TextField(label=\"# of Tweens\", value=str(arg['tweens'] if 'tweens' in arg else 8), keyboard_type=KeyboardType.NUMBER, width = 90)\n",
        "    #tweens =  NumberPicker(label=\"# of Tweens: \", min=2, max=300, value=int(arg['tweens'] if 'tweens' in arg else 8), on_change=changed_tweens),\n",
        "    #prompt2.visible = prompt_tweening\n",
        "    #tweens.visible = prompt_tweening\n",
        "    tweening_params = Container(Row([Container(content=None, width=8), prompt2, tweens]), padding=padding.only(top=4, bottom=3), animate_size=animation.Animation(1000, AnimationCurve.EASE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    tweening_params.height = None if prompt_tweening else 0\n",
        "    #tweening_row = Row([use_prompt_tweening, ])#tweening_params\n",
        "    batch_size = NumberPicker(label=\"Batch Size: \", min=1, max=10, value=arg['batch_size'])\n",
        "    n_iterations = NumberPicker(label=\"Number of Iterations: \", min=1, max=30, value=arg['n_iterations'])\n",
        "    #batch_size = TextField(label=\"Batch Size\", value=str(arg['batch_size']), keyboard_type=KeyboardType.NUMBER)\n",
        "    #n_iterations = TextField(label=\"Number of Iterations\", value=str(arg['n_iterations']), keyboard_type=KeyboardType.NUMBER)\n",
        "    steps = TextField(label=\"Steps\", value=str(arg['steps']), keyboard_type=KeyboardType.NUMBER)\n",
        "    eta = TextField(label=\"DDIM ETA\", value=str(arg['eta']), keyboard_type=KeyboardType.NUMBER, hint_text=\"Amount of Noise (only with DDIM sampler)\")\n",
        "    seed = TextField(label=\"Seed\", value=str(arg['seed']), keyboard_type=KeyboardType.NUMBER, hint_text=\"0 or -1 picks a Random seed\")\n",
        "    guidance_scale = TextField(label=\"Guidance Scale\", value=str(arg['guidance_scale']), keyboard_type=KeyboardType.NUMBER)\n",
        "    param_columns = Row([Column([steps, seed, batch_size]), Column([guidance_scale, eta, n_iterations])])\n",
        "    #guidance_scale = Slider(min=0, max=50, divisions=100, label=\"{value}\", value=arg['guidance_scale'], expand=True)\n",
        "    #guidance = Row([Text(\"Guidance Scale: \"), guidance_scale])\n",
        "    width_slider = SliderRow(label=\"Width\", min=256, max=1280, divisions=64, multiple=16, suffix=\"px\", pref=arg, key='width')\n",
        "    height_slider = SliderRow(label=\"Height\", min=256, max=1280, divisions=64, multiple=16, suffix=\"px\", pref=arg, key='height')\n",
        "    init_image = TextField(label=\"Init Image\", value=arg['init_image'], expand=1, height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_init))\n",
        "    mask_image = TextField(label=\"Mask Image\", value=arg['mask_image'], expand=1, height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD_OUTLINED, on_click=pick_mask))\n",
        "    alpha_mask = Checkbox(label=\"Alpha Mask\", value=arg['alpha_mask'], tooltip=\"Use Transparent Alpha Channel of Init as Mask\", fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER)\n",
        "    invert_mask = Checkbox(label=\"Invert Mask\", value=arg['invert_mask'], tooltip=\"Reverse Black & White of Image Mask\", fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER)\n",
        "    image_row = ResponsiveRow([Row([init_image, alpha_mask], col={\"lg\":6}), Row([mask_image, invert_mask], col={\"lg\":6})])\n",
        "    init_image_strength = Slider(min=0.1, max=0.9, divisions=16, label=\"{value}%\", value=float(arg['init_image_strength']), expand=True)\n",
        "    strength_slider = Row([Text(\"Init Image Strength: \"), init_image_strength])\n",
        "    img_block = Container(content=Column([image_row, strength_slider]), padding=padding.only(top=4, bottom=3), animate_size=animation.Animation(1000, AnimationCurve.EASE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    img_block.height = None if (status['installed_txt2img'] or status['installed_stability']) else 0\n",
        "    use_clip_guided_model = Tooltip(message=\"Uses more VRAM, so you'll probably need to make image size smaller\", content=Switch(label=\"Use CLIP-Guided Model\", tooltip=\"Uses more VRAM, so you'll probably need to make image size smaller\", value=arg['use_clip_guided_model'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_clip))\n",
        "    clip_guidance_scale = Slider(min=1, max=5000, divisions=4999, label=\"{value}\", value=arg['clip_guidance_scale'], expand=True)\n",
        "    clip_guidance_scale_slider = Row([Text(\"CLIP Guidance Scale: \"), clip_guidance_scale])\n",
        "    use_cutouts = Checkbox(label=\"Use Cutouts\", value=bool(arg['use_cutouts']), fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER)\n",
        "    num_cutouts = NumberPicker(label=\"    Number of Cutouts: \", min=1, max=10, value=arg['num_cutouts'])\n",
        "    #num_cutouts.visible = bool(prefs['use_cutouts'])\n",
        "    #num_cutouts = TextField(label=\"Number of Cutouts\", value=prefs['num_cutouts'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'num_cutouts', asInt=True))\n",
        "    unfreeze_unet = Checkbox(label=\"Unfreeze UNET\", value=arg['unfreeze_unet'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER)\n",
        "    unfreeze_vae = Checkbox(label=\"Unfreeze VAE\", value=arg['unfreeze_vae'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER)\n",
        "    clip_block = Container(Column([clip_guidance_scale_slider, Row([use_cutouts, num_cutouts], expand=False), unfreeze_unet, unfreeze_vae, Divider(height=9, thickness=2)]), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    if not status['installed_clip']:\n",
        "      use_clip_guided_model.visible = False\n",
        "      clip_block.height = 0\n",
        "    elif not arg['use_clip_guided_model']:\n",
        "      clip_block.height = 0\n",
        "    edit_dlg = AlertDialog(title=Text(\"üìù  Edit Prompt Dream Parameters\"), content=Container(Column([\n",
        "          Container(content=None, height=7),\n",
        "          ResponsiveRow([\n",
        "            edit_text,\n",
        "            negative_prompt,\n",
        "          ]),\n",
        "          #Text(\"Override any Default Parameters\"),\n",
        "          #use_prompt_tweening,\n",
        "          #tweening_params,\n",
        "          #batch_size, n_iterations, steps, eta, seed, guidance, \n",
        "          param_columns, \n",
        "          width_slider, height_slider, img_block,\n",
        "          use_clip_guided_model, clip_block,\n",
        "          #Row([Column([batch_size, n_iterations, steps, eta, seed,]), Column([guidance, width_slider, height_slider, Divider(height=9, thickness=2), (img_block if prefs['install_img2img'] else Container(content=None))])],),\n",
        "        ], alignment=MainAxisAlignment.START, width=(e.page.window_width or e.page.width) - 200, height=(e.page.window_height or e.page.height) - 100, scroll=ScrollMode.AUTO), width=(e.page.window_width or e.page.width) - 200, height=(e.page.window_height or e.page.height) - 100), \n",
        "        actions=[TextButton(content=Text(\"Cancel\", size=18), on_click=close_dlg), ElevatedButton(content=Text(value=emojize(\":floppy_disk:\") + \"  Save Prompt \", size=19, weight=FontWeight.BOLD), on_click=save_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "    open_dlg()\n",
        "    #e.page.dialog = edit_dlg\n",
        "    #edit_dlg.open = True\n",
        "    #e.page.update()\n",
        "\n",
        "def buildPromptsList(page):\n",
        "  parameter = Ref[ListTile]()\n",
        "  global prompts, args, prefs\n",
        "  def changed(e):\n",
        "      status['changed_prompts'] = True\n",
        "      page.update()\n",
        "  def prompt_help(e):\n",
        "      def close_help_dlg(e):\n",
        "        nonlocal prompt_help_dlg\n",
        "        prompt_help_dlg.open = False\n",
        "        page.update()\n",
        "      prompt_help_dlg = AlertDialog(title=Text(\"üíÅ   Help with Prompt Creations\"), content=Column([\n",
        "          Text(\"You can keep your text prompts simple, or get really complex with it. Just describe the image you want it to dream up with as many details as you can think of. Add artists, styles, colors, adjectives and get creative...\"),\n",
        "          Text('Now you can add prompt weighting, so you can emphasize the strength of certain words between parentheses, and de-emphasize words between brackets. For example: \"A (hyper realistic) painting of (magical:1.8) owl with the (((face of a cat))), without [[tail]], in a [twisted:0.6] tree, by Thomas Kinkade\"'),\n",
        "          Text('After adding your prompts, click on a prompt line to edit all the parameters of it. There you can add negative prompts like \"lowres, bad_anatomy, error_body, bad_fingers, missing_fingers, error_lighting, jpeg_artifacts, signature, watermark, username, blurry\" or anything else you don\\'t want.'),\n",
        "          Text('Then you can override all the parameters for each individual prompt, playing with variations of sizes, steps, guidance scale, init & mask image, seeds, etc.  In the prompts list, you can press the ... options button to duplicate, delete and move prompts in the batch queue.  When ready, Run Diffusion on Prompts...')\n",
        "        ], scroll=ScrollMode.AUTO), actions=[TextButton(\"üòÄ  Very nice... \", on_click=close_help_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = prompt_help_dlg\n",
        "      prompt_help_dlg.open = True\n",
        "      page.update()\n",
        "  def paste_prompts(e):\n",
        "      def save_prompts_list(e):\n",
        "        plist = enter_text.value.strip()\n",
        "        prompts_list = plist.split('\\n')\n",
        "        negative_prompt = negative_prompt_text.value\n",
        "        negative = None\n",
        "        if bool(negative_prompt):\n",
        "          if '_' in negative_prompt:\n",
        "            negative_prompt = nsp_parse(negative_prompt)\n",
        "          negative = {'negative_prompt': negative_prompt}\n",
        "        for pr in prompts_list:\n",
        "          if bool(pr.strip()):\n",
        "            if '_' in pr:\n",
        "              pr = nsp_parse(pr)\n",
        "            add_to_prompts(pr.strip(), negative)\n",
        "        status['changed_prompts'] = True\n",
        "        close_dlg(e)\n",
        "      def close_dlg(e):\n",
        "          dlg_paste.open = False\n",
        "          page.update()\n",
        "      enter_text = TextField(label=\"Enter Prompts List with multiple lines\", expand=True, filled=True, min_lines=30, multiline=True, autofocus=True)\n",
        "      dlg_paste = AlertDialog(modal=False, title=Text(\"üìù  Paste or Write Prompts List from Simple Text\"), content=Container(Column([enter_text], alignment=MainAxisAlignment.START, tight=True, width=(page.window_width or page.width) - 180, height=(page.window_height or page.height) - 100, scroll=\"none\"), width=(page.window_width or page.width) - 180, height=(page.window_height or page.height) - 100), actions=[TextButton(content=Text(\"Cancel\", size=18), on_click=close_dlg), ElevatedButton(content=Text(value=emojize(\":floppy_disk:\") + \"  Add to Prompts List \", size=19, weight=FontWeight.BOLD), on_click=save_prompts_list)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = dlg_paste\n",
        "      dlg_paste.open = True\n",
        "      page.update()\n",
        "  def copy_prompts(e):\n",
        "      def copy_prompts_list(pl):\n",
        "        nonlocal text_list, enter_text\n",
        "        page.set_clipboard(enter_text.value)\n",
        "        page.snack_bar = SnackBar(content=Text(f\"üìã   Prompt Text copied to clipboard...\"))\n",
        "        page.snack_bar.open = True\n",
        "        close_dlg(e)\n",
        "      def close_dlg(e):\n",
        "          dlg_copy.open = False\n",
        "          page.update()\n",
        "      text_list = \"\"\n",
        "      for d in prompts:\n",
        "          p = d.prompt[0] if type(d.prompt) == list else d.prompt if bool(d.prompt) else d['prompt'] if 'prompt' in d else d.arg['prompt'] if 'prompt' in d.arg else ''\n",
        "          text_list += f\"{p}\\n\"\n",
        "      enter_text = TextField(label=\"Prompts on multiple lines\", value=text_list.strip(), expand=True, filled=True, multiline=True, autofocus=True)\n",
        "      dlg_copy = AlertDialog(modal=False, title=Text(\"üìù  Prompts List as Plain Text\"), content=Container(Column([enter_text], alignment=MainAxisAlignment.START, tight=True, width=(page.window_width or page.width) - 180, height=(page.window_height or page.height) - 100, scroll=\"none\"), width=(page.window_width or page.width) - 180, height=(page.window_height or page.height) - 100), actions=[TextButton(content=Text(\"Cancel\", size=18), on_click=close_dlg), ElevatedButton(content=Text(value=emojize(\":floppy_disk:\") + \"  Copy Prompts List to Clipboard\", size=19, weight=FontWeight.BOLD), data=text_list, on_click=lambda ev: copy_prompts_list(text_list))], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = dlg_copy\n",
        "      dlg_copy.open = True\n",
        "      page.update()\n",
        "  def delete_prompt(e):\n",
        "      idx = prompts.index(e.control.data)\n",
        "      prompts.pop(idx)\n",
        "      prompts_list.controls.pop(idx)\n",
        "      prompts_list.update()\n",
        "      status['changed_prompts'] = True\n",
        "      if prefs['enable_sounds']: page.snd_delete.play()\n",
        "  def copy_prompt(e):\n",
        "      open_dream = e.control.data\n",
        "      page.set_clipboard(open_dream.prompt)\n",
        "      page.snack_bar = SnackBar(content=Text(f\"üìã   Prompt Text copied to clipboard...\"))\n",
        "      page.snack_bar.open = True\n",
        "      page.update()\n",
        "  def duplicate_prompt(e):\n",
        "      open_dream = e.control.data\n",
        "      add_to_prompts(open_dream.prompt, open_dream.arg)\n",
        "  def duplicate_multiple(e):\n",
        "      open_dream = e.control.data\n",
        "      num_times = 2\n",
        "      def close_dlg(e):\n",
        "          duplicate_modal.open = False\n",
        "          page.update()\n",
        "      def save_dlg(e):\n",
        "          for i in range(num_times):\n",
        "            add_to_prompts(open_dream.prompt, open_dream.arg)\n",
        "          duplicate_modal.open = False\n",
        "          page.update()\n",
        "      def change_num(e):\n",
        "          nonlocal num_times\n",
        "          num_times = int(e.control.value)\n",
        "      duplicate_modal = AlertDialog(modal=False, title=Text(\"üåÄ  Duplicate Prompt Multiple Times\"), content=Container(Column([\n",
        "            Container(content=None, height=7),\n",
        "            NumberPicker(label=\"Number of Copies: \", min=1, max=99, value=num_times, on_change=change_num),\n",
        "          ], alignment=MainAxisAlignment.START, tight=True, scroll=ScrollMode.AUTO)), actions=[TextButton(content=Text(\"Cancel\", size=18), on_click=close_dlg), ElevatedButton(content=Text(value=emojize(\":bowling:\") + \"  Duplicate Prompt \", size=19, weight=FontWeight.BOLD), on_click=save_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "      e.page.dialog = duplicate_modal\n",
        "      duplicate_modal.open = True\n",
        "      e.page.update()\n",
        "  def move_down(e):\n",
        "      idx = prompts.index(e.control.data)\n",
        "      if idx < (len(prompts) - 1):\n",
        "        d = prompts.pop(idx)\n",
        "        prompts.insert(idx+1, d)\n",
        "        dr = prompts_list.controls.pop(idx)\n",
        "        prompts_list.controls.insert(idx+1, dr)\n",
        "        prompts_list.update()\n",
        "  def move_up(e):\n",
        "      idx = prompts.index(e.control.data)\n",
        "      if idx > 0:\n",
        "        d = prompts.pop(idx)\n",
        "        prompts.insert(idx-1, d)\n",
        "        dr = prompts_list.controls.pop(idx)\n",
        "        prompts_list.controls.insert(idx-1, dr)\n",
        "        prompts_list.update()\n",
        "  def add_prompt(e):\n",
        "      positive_prompt = prompt_text.value\n",
        "      negative_prompt = negative_prompt_text.value\n",
        "      if '_' in positive_prompt:\n",
        "        positive_prompt = nsp_parse(positive_prompt)\n",
        "      if bool(negative_prompt):\n",
        "        if '_' in negative_prompt:\n",
        "          negative_prompt = nsp_parse(negative_prompt)\n",
        "        add_to_prompts(positive_prompt, {'negative_prompt': negative_prompt})\n",
        "      else:\n",
        "        add_to_prompts(positive_prompt)\n",
        "  def add_to_prompts(p, arg=None):\n",
        "      global prompts\n",
        "      dream = Dream(p)\n",
        "      if arg is not None:\n",
        "        if 'prompt' in arg: del arg['prompt']\n",
        "        arg = merge_dict(args, arg)\n",
        "        dream.arg = arg\n",
        "      prompts.append(dream)\n",
        "      prompts_list.controls.append(ListTile(title=Text(p, max_lines=6, style=TextThemeStyle.BODY_LARGE), dense=True, data=dream, on_click=editPrompt, trailing=PopupMenuButton(icon=icons.MORE_VERT,\n",
        "          items=[\n",
        "              PopupMenuItem(icon=icons.EDIT, text=\"Edit Prompt\", on_click=editPrompt, data=dream),\n",
        "              PopupMenuItem(icon=icons.DELETE, text=\"Delete Prompt\", on_click=delete_prompt, data=dream),\n",
        "              PopupMenuItem(icon=icons.CONTROL_POINT_DUPLICATE, text=\"Duplicate Prompt\", on_click=duplicate_prompt, data=dream),\n",
        "              PopupMenuItem(icon=icons.CONTROL_POINT_DUPLICATE_SHARP, text=\"Duplicate Multiple\", on_click=duplicate_multiple, data=dream),\n",
        "              PopupMenuItem(icon=icons.CONTENT_COPY, text=\"Copy to Clipboard\", on_click=copy_prompt, data=dream),\n",
        "              PopupMenuItem(icon=icons.ARROW_UPWARD, text=\"Move Up\", on_click=move_up, data=dream),\n",
        "              PopupMenuItem(icon=icons.ARROW_DOWNWARD, text=\"Move Down\", on_click=move_down, data=dream),\n",
        "          ],\n",
        "      )))\n",
        "      #prompts_list.controls.append(Text(\"Prompt 1 added to the list of prompts\"))\n",
        "      prompts_list.update()\n",
        "      if prompts_buttons.visible==False:\n",
        "          prompts_buttons.visible=True\n",
        "          prompts_buttons.update()\n",
        "          if current_tab == 3:\n",
        "            show_run_diffusion_fab(True)\n",
        "      if arg is not None:\n",
        "        update_prompts()\n",
        "      else:\n",
        "        prompt_text.focus()\n",
        "      page.update()\n",
        "      status['changed_prompts'] = True\n",
        "  page.add_to_prompts = add_to_prompts\n",
        "\n",
        "  def save_prompts():\n",
        "      if len(prompts) > 0:\n",
        "          #print(\"Saving your Prompts List\")\n",
        "          prompts_prefs = []\n",
        "          for d in prompts:\n",
        "            a = d.arg.copy()\n",
        "            #a['prompt'] = d.prompt if bool(d.prompt) else d['prompt']\n",
        "            a['prompt'] = d.prompt[0] if type(d.prompt) == list else d.prompt if bool(d.prompt) else d.arg['prompt'] if 'prompt' in d.arg else ''\n",
        "            if 'batch_size' in a: del a['batch_size']\n",
        "            if 'n_iterations' in a: del a['n_iterations']\n",
        "            if 'precision' in a: del a['precision']\n",
        "            #a['sampler'] = prefs['generation_sampler'] if prefs['use_Stability_api'] else prefs['scheduler_mode']\n",
        "            if prefs['use_Stability_api']: del a['eta']\n",
        "            if 'use_Stability' in a: del a['use_Stability']\n",
        "            if 'negative_prompt' in a:\n",
        "              if not bool(a['negative_prompt']): del a['negative_prompt']\n",
        "            if 'prompt2' in a:\n",
        "              if not bool(a['prompt2']):\n",
        "                del a['prompt2']\n",
        "                del a['tweens']\n",
        "            if 'init_image' in a:\n",
        "              if not bool(a['init_image']):\n",
        "                del a['init_image']\n",
        "                del a['init_image_strength']\n",
        "                del a['invert_mask']\n",
        "                if 'alpha_image' in a:\n",
        "                  del a['alpha_mask']\n",
        "              elif bool(a['mask_image']) and 'alpha_image' in a:\n",
        "                del a['alpha_mask']\n",
        "            if 'mask_image' in a:\n",
        "              if not bool(a['mask_image']):\n",
        "                del a['mask_image']\n",
        "                if 'alpha_image' in a:\n",
        "                  del a['alpha_mask']\n",
        "            if 'use_clip_guided_model' in a:\n",
        "              if not bool(a['use_clip_guided_model']):\n",
        "                del a[\"use_clip_guided_model\"]\n",
        "                del a[\"clip_prompt\"]\n",
        "                del a[\"clip_guidance_scale\"]\n",
        "                del a[\"num_cutouts\"]\n",
        "                del a[\"use_cutouts\"]\n",
        "                del a[\"unfreeze_unet\"]\n",
        "                del a[\"unfreeze_vae\"]\n",
        "              else:\n",
        "                a[\"clip_model_id\"] = prefs['clip_model_id']\n",
        "            if 'use_conceptualizer' in a:\n",
        "              if not bool(a['use_conceptualizer']):\n",
        "                del a['use_conceptualizer']\n",
        "            prompts_prefs.append(a)\n",
        "            #j = json.dumps(a)\n",
        "          prefs['prompt_list'] = prompts_prefs\n",
        "  page.save_prompts = save_prompts\n",
        "  def load_prompts():\n",
        "      saved_prompts = prefs['prompt_list']\n",
        "      if len(saved_prompts) > 0:\n",
        "          for d in saved_prompts:\n",
        "            #print(f'Loading {d}')\n",
        "            if 'prompt' not in d: continue\n",
        "            p = d['prompt']\n",
        "            page.add_to_prompts(p, d)\n",
        "          page.update()\n",
        "\n",
        "  page.load_prompts = load_prompts\n",
        "\n",
        "  def update_prompts():\n",
        "      if len(prompts_list.controls) > 0:\n",
        "        for p in prompts_list.controls:\n",
        "          diffs = arg_diffs(p.data.arg, args)\n",
        "          if bool(diffs):\n",
        "            subtitle = Text(\"    \" + diffs)\n",
        "          else: subtitle = None\n",
        "          p.subtitle = subtitle\n",
        "          p.update()\n",
        "        prompts_list.update()\n",
        "  page.update_prompts = update_prompts\n",
        "\n",
        "  def apply_changes(e):\n",
        "      global prompts\n",
        "      if len(prompts_list.controls) > 0:\n",
        "        i = 0\n",
        "        for p in prompts_list.controls:\n",
        "          negative = prompts[i].arg['negative_prompt']\n",
        "          init = prompts[i].arg['init_image']\n",
        "          mask = prompts[i].arg['mask_image']\n",
        "          prompts[i].arg = merge_dict(prompts[i].arg, args)\n",
        "          prompts[i].arg['negative_prompt'] = negative\n",
        "          if not bool(args['init_image']):\n",
        "            prompts[i].arg['init_image'] = init\n",
        "          if not bool(args['mask_image']):\n",
        "            prompts[i].arg['mask_image'] = mask\n",
        "          p.data = prompts[i]\n",
        "          i += 1\n",
        "        update_prompts()\n",
        "\n",
        "  page.apply_changes = apply_changes\n",
        "  def clear_prompt(e):\n",
        "      prompt_text.value = \"\"\n",
        "      prompt_text.update()\n",
        "  def clear_negative_prompt(e):\n",
        "      negative_prompt_text.value = \"\"\n",
        "      negative_prompt_text.update()\n",
        "  def clear_list(e):\n",
        "      global prompts\n",
        "      prompts_list.controls = []\n",
        "      prompts_list.update()\n",
        "      prompts = []\n",
        "      prefs['prompt_list'] = []\n",
        "      prompts_buttons.visible=False\n",
        "      prompts_buttons.update()\n",
        "      show_run_diffusion_fab(False)\n",
        "      e.page.save_prompts()\n",
        "      save_settings_file(e.page)\n",
        "      #status['changed_prompts'] = True\n",
        "      if prefs['enable_sounds']: page.snd_delete.play()\n",
        "  page.clear_prompts_list = clear_list\n",
        "  def on_keyboard (e: KeyboardEvent):\n",
        "      if e.key == \"Escape\":\n",
        "        if current_tab == 3:\n",
        "          clear_prompt(None)\n",
        "  page.on_keyboard_event = on_keyboard\n",
        "  def run_diffusion(e):\n",
        "      if not status['installed_diffusers'] and not status['installed_stability'] and not status['installed_AIHorde']:\n",
        "        alert_msg(e.page, \"You must Install the required Diffusers or Stability api first...\")\n",
        "        return\n",
        "      if prefs['use_interpolation'] and prefs['install_interpolation'] and not status['installed_interpolation']:\n",
        "        alert_msg(e.page, \"You must Install Walk Interpolation Pipeline first...\")\n",
        "        return\n",
        "      page.tabs.selected_index = 4\n",
        "      page.tabs.update()\n",
        "      show_run_diffusion_fab(False)\n",
        "      if status['changed_prompts']:\n",
        "        page.save_prompts()\n",
        "        save_settings_file(page)\n",
        "        status['changed_prompts'] = False\n",
        "      page.update()\n",
        "      start_diffusion(page)\n",
        "  has_changed = False\n",
        "  prompts_list = Column([],spacing=1)\n",
        "  page.prompts_list = prompts_list\n",
        "  prompt_text = TextField(label=\"Prompt Text\", suffix=IconButton(icons.CLEAR, on_click=clear_prompt), autofocus=True, filled=True, multiline=True, max_lines=6, on_submit=add_prompt, col={'lg':9})\n",
        "  negative_prompt_text = TextField(label=\"Segmented Weights 1 | -0.7 | 1.2\" if prefs['use_composable'] and status['installed_composable'] else \"Negative Prompt Text\", filled=True, multiline=True, max_lines=4, suffix=IconButton(icons.CLEAR, on_click=clear_negative_prompt), col={'lg':3})\n",
        "  add_prompt_button = ElevatedButton(content=Text(value=\"‚ûï  Add\" + (\" Prompt\" if (page.window_width or page.width) > 720 else \"\"), size=17, weight=FontWeight.BOLD), height=52, on_click=add_prompt)\n",
        "  prompt_help_button = IconButton(icons.HELP_OUTLINE, tooltip=\"Help with Prompt Creation\", on_click=prompt_help)\n",
        "  copy_prompts_button = IconButton(icons.COPY_ALL, tooltip=\"Save Prompts as Plain-Text List\", on_click=copy_prompts)\n",
        "  paste_prompts_button = IconButton(icons.CONTENT_PASTE, tooltip=\"Load Prompts from Plain-Text List\", on_click=paste_prompts)\n",
        "  prompt_row = Row([ResponsiveRow([prompt_text, negative_prompt_text], expand=True), add_prompt_button])\n",
        "  #diffuse_prompts_button = ElevatedButton(content=Text(value=\"‚ñ∂Ô∏è    Run Diffusion on Prompts \", size=20), on_click=run_diffusion)\n",
        "  clear_prompts_button = ElevatedButton(content=Text(\"‚ùå   Clear Prompts List\", size=18), on_click=clear_list)\n",
        "  prompts_buttons = Row([clear_prompts_button], alignment=MainAxisAlignment.SPACE_BETWEEN)\n",
        "  def show_run_diffusion_fab(show = True, p = None):\n",
        "    if p is None:\n",
        "      p = page\n",
        "    if show:\n",
        "      p.floating_action_button = FloatingActionButton(content=Row([Icon(icons.PLAY_ARROW), Text(\"Run Diffusion on Prompts\", size=18)], alignment=\"center\", spacing=5), width=270, shape=ft.RoundedRectangleBorder(radius=22), on_click=run_diffusion)\n",
        "      #page.floating_action_button = FloatingActionButton(icon=icons.PLAY_ARROW, text=\"Run Diffusion on Prompts\", on_click=run_diffusion)\n",
        "      p.update()\n",
        "    else:\n",
        "      if p.floating_action_button is not None:\n",
        "        p.floating_action_button = None\n",
        "        try:\n",
        "          p.update()\n",
        "        except Exception:\n",
        "          print(\"Problem updating page while showing Run Diffusion FAB\")\n",
        "          pass\n",
        "        \n",
        "  page.show_run_diffusion_fab = show_run_diffusion_fab\n",
        "  show_run_diffusion_fab(len(prompts_list.controls) > 0)\n",
        "  #page.load_prompts()\n",
        "  if len(prompts_list.controls) < 1:\n",
        "    prompts_buttons.visible=False\n",
        "  c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10), content=Column([\n",
        "        Header(\"üóíÔ∏è   List of Prompts to Diffuse\", actions=[prompt_help_button, copy_prompts_button, paste_prompts_button]),\n",
        "        #add_prompt_button,\n",
        "        prompt_row,\n",
        "        prompts_list,\n",
        "        prompts_buttons,\n",
        "      ],\n",
        "  ))], scroll=ScrollMode.AUTO)\n",
        "  return c\n",
        "\n",
        "def buildImages(page):\n",
        "    auto_scroll = True\n",
        "    def auto_scrolling(auto):\n",
        "      page.imageColumn.auto_scroll = auto\n",
        "      page.imageColumn.update()\n",
        "      c.update()\n",
        "    page.auto_scrolling = auto_scrolling\n",
        "    page.imageColumn = Column([Text(\"‚ñ∂Ô∏è   Start Run from Prompts List.  Get ready...\", style=TextThemeStyle.TITLE_LARGE, color=colors.SECONDARY, weight=FontWeight.BOLD), Divider(thickness=3, height=5, color=colors.SURFACE_VARIANT)], scroll=ScrollMode.AUTO, auto_scroll=True)\n",
        "    c = Container(padding=padding.only(18, 12, 0, 0), content=page.imageColumn)\n",
        "    return c\n",
        "\n",
        "def buildPromptGenerator(page):\n",
        "    def changed(e, pref=None):\n",
        "      if pref is not None:\n",
        "        prefs['prompt_generator'][pref] = e.control.value\n",
        "      status['changed_prompt_generator'] = True\n",
        "    page.prompt_generator_list = Column([], spacing=0)\n",
        "    def add_to_prompt_list(p):\n",
        "      page.add_to_prompts(p)\n",
        "      if prefs['enable_sounds']: page.snd_drop.play()\n",
        "    def add_to_prompt_generator(p):\n",
        "      page.prompt_generator_list.controls.append(ListTile(title=Text(p, max_lines=3, style=TextThemeStyle.BODY_LARGE), dense=True, on_click=lambda _: add_to_prompt_list(p)))\n",
        "      page.prompt_generator_list.update()\n",
        "      generator_list_buttons.visible = True\n",
        "      generator_list_buttons.update()\n",
        "    page.add_to_prompt_generator = add_to_prompt_generator\n",
        "    def click_prompt_generator(e):\n",
        "      if status['installed_OpenAI']:\n",
        "        run_prompt_generator(page)\n",
        "      else:\n",
        "        alert_msg(page, \"You must Install OpenAI GPT-3 Library first before using...\")\n",
        "    def add_to_list(e):\n",
        "      for p in page.prompt_generator_list.controls:\n",
        "        page.add_to_prompts(p.title.value)\n",
        "      if prefs['enable_sounds']: page.snd_drop.play()\n",
        "    def clear_prompts(e):\n",
        "      page.prompt_generator_list.controls = []\n",
        "      page.prompt_generator_list.update()\n",
        "      #prompts = []\n",
        "      generator_list_buttons.visible = False\n",
        "      generator_list_buttons.update()\n",
        "      if prefs['enable_sounds']: page.snd_delete.play()\n",
        "    def changed_request(e):\n",
        "      request_slider.label = generator_request_modes[int(request_slider.value)]\n",
        "      request_slider.update()\n",
        "      changed(e, 'request_mode')\n",
        "    request_slider = Slider(label=\"{value}\", min=0, max=7, divisions=7, expand=True, value=prefs['prompt_generator']['request_mode'], on_change=changed_request)\n",
        "    AI_engine = Dropdown(label=\"AI Engine\", width=250, options=[dropdown.Option(\"OpenAI GPT-3\"), dropdown.Option(\"ChatGPT-3.5 Turbo\")], value=prefs['prompt_generator']['AI_engine'], on_change=lambda e: changed(e, 'AI_engine'))\n",
        "    generator_list_buttons = Row([\n",
        "        ElevatedButton(content=Text(\"‚ùå   Clear Prompts\", size=18), on_click=clear_prompts),\n",
        "        FilledButton(content=Text(\"‚ûï  Add All Prompts to List\", size=20), on_click=add_to_list)\n",
        "    ], alignment=MainAxisAlignment.SPACE_BETWEEN)\n",
        "    if len(page.prompt_generator_list.controls) < 1:\n",
        "      generator_list_buttons.visible = False\n",
        "      #generator_list_buttons.update()\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üß†  OpenAI GPT-3 Prompt Genenerator\", \"Enter a phrase each prompt should start with and the amount of prompts to generate. 'Subject Details' is optional to influence the output. 'Phase as subject' makes it about phrase and subject detail. 'Request mode' is the way it asks for the visual description. Just experiment, AI will continue to surprise.\"),\n",
        "        Row([TextField(label=\"Subject Phrase\", expand=True, value=prefs['prompt_generator']['phrase'], multiline=True, on_change=lambda e: changed(e, 'phrase')), TextField(label=\"Subject Detail\", expand=True, hint_text=\"Optional about detail\", value=prefs['prompt_generator']['subject_detail'], multiline=True, on_change=lambda e: changed(e, 'subject_detail')), Checkbox(label=\"Phrase as Subject\", value=prefs['prompt_generator']['phrase_as_subject'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e: changed(e, 'phrase_as_subject'))]),\n",
        "        ResponsiveRow([\n",
        "          Row([NumberPicker(label=\"Amount: \", min=1, max=20, value=prefs['prompt_generator']['amount'], on_change=lambda e: changed(e, 'amount')),\n",
        "              NumberPicker(label=\"Random Artists: \", min=0, max=10, value=prefs['prompt_generator']['random_artists'], on_change=lambda e: changed(e, 'random_artists')),], col={'lg':6}, alignment=MainAxisAlignment.SPACE_BETWEEN),\n",
        "          Row([NumberPicker(label=\"Random Styles: \", min=0, max=10, value=prefs['prompt_generator']['random_styles'], on_change=lambda e: changed(e, 'random_styles')),\n",
        "              Checkbox(label=\"Permutate Artists\", value=prefs['prompt_generator']['permutate_artists'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e: changed(e, 'permutate_artists'))], col={'lg':6}, alignment=MainAxisAlignment.SPACE_BETWEEN),\n",
        "        ]),\n",
        "        AI_engine,\n",
        "        ResponsiveRow([\n",
        "          Row([Text(\"Request Mode:\"), request_slider,], col={'lg':6}),\n",
        "          Row([Text(\" AI Temperature:\"), Slider(label=\"{value}\", min=0, max=1, divisions=10, expand=True, value=prefs['prompt_generator']['AI_temperature'], on_change=lambda e: changed(e, 'AI_temperature'))], col={'lg':6}),\n",
        "        ]),\n",
        "        ElevatedButton(content=Text(\"üí≠   Generate Prompts\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda e: run_prompt_generator(page)),\n",
        "        page.prompt_generator_list,\n",
        "        generator_list_buttons,\n",
        "      ],\n",
        "    ))], scroll=ScrollMode.AUTO)\n",
        "    return c\n",
        "\n",
        "def buildPromptRemixer(page):\n",
        "    def changed(e, pref=None):\n",
        "      if pref is not None:\n",
        "        prefs['prompt_remixer'][pref] = e.control.value\n",
        "      status['changed_prompt_remixer'] = True\n",
        "    page.prompt_remixer_list = Column([], spacing=0)\n",
        "    def click_prompt_remixer(e):\n",
        "      if status['installed_OpenAI']:\n",
        "        run_prompt_remixer(page)\n",
        "      else:\n",
        "        alert_msg(page, \"You must Install OpenAI GPT-3 Library first before using...\")\n",
        "    def add_to_prompt_list(p):\n",
        "      page.add_to_prompts(p)\n",
        "      if prefs['enable_sounds']: page.snd_drop.play()\n",
        "    def add_to_prompt_remixer(p):\n",
        "      page.prompt_remixer_list.controls.append(ListTile(title=Text(p, max_lines=4, style=TextThemeStyle.BODY_LARGE), dense=True, data=p, on_click=lambda _: add_to_prompt_list(p)))\n",
        "      page.prompt_remixer_list.update()\n",
        "      remixer_list_buttons.visible = True\n",
        "      remixer_list_buttons.update()\n",
        "    page.add_to_prompt_remixer = add_to_prompt_remixer\n",
        "    def add_to_list(e):\n",
        "      if prefs['enable_sounds']: page.snd_drop.play()\n",
        "      for p in page.prompt_remixer_list.controls:\n",
        "        page.add_to_prompts(p.data)#(p.title.value)\n",
        "    def clear_prompts(e):\n",
        "      if prefs['enable_sounds']: page.snd_delete.play()\n",
        "      page.prompt_remixer_list.controls = []\n",
        "      page.prompt_remixer_list.update()\n",
        "      remixer_list_buttons.visible = False\n",
        "      remixer_list_buttons.update()\n",
        "    def changed_request(e):\n",
        "      request_slider.label = remixer_request_modes[int(request_slider.value)]\n",
        "      request_slider.update()\n",
        "      changed(e, 'request_mode')\n",
        "    request_slider = Slider(label=\"{value}\", min=0, max=8, divisions=8, expand=True, value=prefs['prompt_remixer']['request_mode'], on_change=changed_request)\n",
        "    AI_engine = Dropdown(label=\"AI Engine\", width=250, options=[dropdown.Option(\"OpenAI GPT-3\"), dropdown.Option(\"ChatGPT-3.5 Turbo\")], value=prefs['prompt_remixer']['AI_engine'], on_change=lambda e: changed(e, 'AI_engine'))\n",
        "    remixer_list_buttons = Row([\n",
        "        ElevatedButton(content=Text(\"‚ùå   Clear Prompts\", size=18), on_click=clear_prompts),\n",
        "        FilledButton(content=Text(\"Add All Prompts to List\", size=20), height=45, on_click=add_to_list),\n",
        "    ], alignment=MainAxisAlignment.SPACE_BETWEEN)\n",
        "    if len(page.prompt_remixer_list.controls) < 1:\n",
        "      remixer_list_buttons.visible = False\n",
        "    \n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üîÑ  Prompt Remixer - GPT-3 AI Helper\", \"Enter a complete prompt you've written that is well worded and descriptive, and get variations of it with our AI Friend. Experiment.\", actions=[ElevatedButton(content=Text(\"üçú  NSP Instructions\", size=18), on_click=lambda _: NSP_instructions(page))]),\n",
        "        Row([TextField(label=\"Seed Prompt\", expand=True, value=prefs['prompt_remixer']['seed_prompt'], multiline=True, on_change=lambda e: changed(e, 'seed_prompt')), TextField(label=\"Optional About Detail\", expand=True, hint_text=\"Optional about detail\", value=prefs['prompt_remixer']['optional_about_influencer'], multiline=True, on_change=lambda e: changed(e, 'optional_about_influencer'))]),\n",
        "        ResponsiveRow([\n",
        "          Row([NumberPicker(label=\"Amount: \", min=1, max=20, value=prefs['prompt_remixer']['amount'], on_change=lambda e: changed(e, 'amount')),\n",
        "              NumberPicker(label=\"Random Artists: \", min=0, max=10, value=prefs['prompt_remixer']['random_artists'], on_change=lambda e: changed(e, 'random_artists')),], col={'lg':6}, alignment=MainAxisAlignment.SPACE_BETWEEN),\n",
        "          Row([NumberPicker(label=\"Random Styles: \", min=0, max=10, value=prefs['prompt_remixer']['random_styles'], on_change=lambda e: changed(e, 'random_styles')),\n",
        "              Checkbox(label=\"Permutate Artists\", value=prefs['prompt_remixer']['permutate_artists'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e: changed(e, 'permutate_artists'))], col={'lg':6}, alignment=MainAxisAlignment.SPACE_BETWEEN),\n",
        "        ]),\n",
        "        AI_engine,\n",
        "        ResponsiveRow([\n",
        "          Row([Text(\"Request Mode:\"), request_slider,], col={'lg':6}),\n",
        "          Row([Text(\" AI Temperature:\"), Slider(label=\"{value}\", min=0, max=1, divisions=10, expand=True, value=prefs['prompt_remixer']['AI_temperature'], on_change=lambda e: changed(e, 'AI_temperature'))], col={'lg':6}),\n",
        "        ]),\n",
        "        ElevatedButton(content=Text(\"üçπ   Remix Prompts\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda e: run_prompt_remixer(page)),\n",
        "        page.prompt_remixer_list,\n",
        "        remixer_list_buttons,\n",
        "      ],\n",
        "    ))], scroll=ScrollMode.AUTO)\n",
        "    return c\n",
        "\n",
        "def buildPromptBrainstormer(page):\n",
        "    def changed(e, pref=None):\n",
        "      if pref is not None:\n",
        "        prefs['prompt_brainstormer'][pref] = e.control.value\n",
        "      status['changed_prompt_brainstormer'] = True\n",
        "    def click_prompt_brainstormer(e):\n",
        "      if prefs['prompt_brainstormer']['AI_engine'] == \"OpenAI GPT-3\":\n",
        "        if status['installed_OpenAI']:\n",
        "          run_prompt_brainstormer(page)\n",
        "        else: alert_msg(page, \"You must Install OpenAI GPT-3 Library first before using this Request Mode...\")\n",
        "      elif prefs['prompt_brainstormer']['AI_engine'] == \"TextSynth GPT-J\":\n",
        "        if status['installed_TextSynth']:\n",
        "          run_prompt_brainstormer(page)\n",
        "        else: alert_msg(page, \"You must Install TextSynth GPT-J Library first before using this Request Mode...\")\n",
        "      elif prefs['prompt_brainstormer']['AI_engine'] == \"HuggingFace Bloom 176B\":\n",
        "        if bool(prefs['HuggingFace_api_key']):\n",
        "          run_prompt_brainstormer(page)\n",
        "        else: alert_msg(page, \"You must provide your HuggingFace API Key in settings first before using this Request Mode...\")\n",
        "      elif prefs['prompt_brainstormer']['AI_engine'] == \"HuggingFace Flan-T5 XXL\":\n",
        "        if bool(prefs['HuggingFace_api_key']):\n",
        "          run_prompt_brainstormer(page)\n",
        "        else: alert_msg(page, \"You must provide your HuggingFace API Key in settings first before using this Request Mode...\")\n",
        "    page.prompt_brainstormer_list = Column([], spacing=0)\n",
        "    def add_to_prompt_brainstormer(p):\n",
        "      page.prompt_brainstormer_list.controls.append(Text(p, max_lines=3, style=TextThemeStyle.BODY_LARGE, selectable=True))\n",
        "      page.prompt_brainstormer_list.update()\n",
        "      brainstormer_list_buttons.visible = True\n",
        "      brainstormer_list_buttons.update()\n",
        "    page.add_to_prompt_brainstormer = add_to_prompt_brainstormer\n",
        "    def add_to_prompts(e):\n",
        "      page.add_to_prompts(new_prompt_text.value)\n",
        "    def clear_prompts(e):\n",
        "      page.prompt_brainstormer_list.controls = []\n",
        "      page.prompt_brainstormer_list.update()\n",
        "      brainstormer_list_buttons.visible = False\n",
        "      brainstormer_list_buttons.update()\n",
        "    def clear_prompt_text(e):\n",
        "      new_prompt_text.value = \"\"\n",
        "      new_prompt_text.update()\n",
        "\n",
        "    new_prompt_text = TextField(label=\"New Prompt Text\", expand=True, suffix=IconButton(icons.CLEAR, on_click=clear_prompt_text), autofocus=True, on_submit=add_to_prompts)\n",
        "    add_to_prompts_button = ElevatedButton(\"‚ûï  Add to Prompts\", on_click=add_to_prompts)#, icon=icons.ADD_ROUNDED\n",
        "    brainstormer_list_buttons = Row([\n",
        "        new_prompt_text, add_to_prompts_button,\n",
        "        ElevatedButton(content=Text(\"‚ùå   Clear Brainstorms\"), on_click=clear_prompts),\n",
        "    ], alignment=MainAxisAlignment.END)\n",
        "    \n",
        "    if len(page.prompt_brainstormer_list.controls) < 1:\n",
        "      brainstormer_list_buttons.visible = False\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"ü§î  Prompt Brainstormer - TextSynth GPT-J-6B, OpenAI GPT-3 & HuggingFace Bloom AI\", \n",
        "               \"Enter a complete prompt you've written that is well worded and descriptive, and get variations of it with our AI Friends. Experiment, each has different personalities.\", actions=[ElevatedButton(content=Text(\"üçú  NSP Instructions\", size=18), on_click=lambda _: NSP_instructions(page))]),\n",
        "        Row([Dropdown(label=\"AI Engine\", width=250, options=[dropdown.Option(\"TextSynth GPT-J\"), dropdown.Option(\"OpenAI GPT-3\"), dropdown.Option(\"ChatGPT-3.5 Turbo\"), dropdown.Option(\"HuggingFace Bloom 176B\"), dropdown.Option(\"HuggingFace Flan-T5 XXL\"), dropdown.Option(\"StableLM 7b\"), dropdown.Option(\"StableLM 3b\")], value=prefs['prompt_brainstormer']['AI_engine'], on_change=lambda e: changed(e, 'AI_engine')),\n",
        "          Dropdown(label=\"Request Mode\", width=250, options=[dropdown.Option(\"Brainstorm\"), dropdown.Option(\"Write\"), dropdown.Option(\"Rewrite\"), dropdown.Option(\"Edit\"), dropdown.Option(\"Story\"), dropdown.Option(\"Description\"), dropdown.Option(\"Picture\"), dropdown.Option(\"Raw Request\")], value=prefs['prompt_brainstormer']['request_mode'], on_change=lambda e: changed(e, 'request_mode')),\n",
        "        ], alignment=MainAxisAlignment.START),\n",
        "        Row([TextField(label=\"About Prompt\", expand=True, value=prefs['prompt_brainstormer']['about_prompt'], multiline=True, on_change=lambda e: changed(e, 'about_prompt')),]),\n",
        "        ElevatedButton(content=Text(\"‚õàÔ∏è    Brainstorm Prompt\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_prompt_brainstormer(page)),\n",
        "        page.prompt_brainstormer_list,\n",
        "        brainstormer_list_buttons,\n",
        "      ],\n",
        "    ))], scroll=ScrollMode.AUTO)\n",
        "    return c\n",
        "\n",
        "def buildPromptWriter(page):\n",
        "    def changed(e, pref=None):\n",
        "      if pref is not None:\n",
        "        prefs['prompt_writer'][pref] = e.control.value\n",
        "      status['changed_prompt_writer'] = True\n",
        "    page.prompt_writer_list = Column([], spacing=0)\n",
        "    def add_to_prompt_list(p):\n",
        "      negative_prompt = prefs['prompt_writer']['negative_prompt']\n",
        "      if bool(negative_prompt):\n",
        "        if '_' in negative_prompt:\n",
        "          negative_prompt = nsp_parse(negative_prompt)\n",
        "        page.add_to_prompts(p, {'negative_prompt':negative_prompt})\n",
        "      else:\n",
        "        page.add_to_prompts(p)\n",
        "      if prefs['enable_sounds']: page.snd_drop.play()\n",
        "    def add_to_prompt_writer(p):\n",
        "      page.prompt_writer_list.controls.append(ListTile(title=Text(p, max_lines=3, style=TextThemeStyle.BODY_LARGE), dense=True, on_click=lambda _: add_to_prompt_list(p)))\n",
        "      page.prompt_writer_list.update()\n",
        "      writer_list_buttons.visible = True\n",
        "      writer_list_buttons.update()\n",
        "    page.add_to_prompt_writer = add_to_prompt_writer\n",
        "\n",
        "    def add_to_list(e):\n",
        "      if prefs['enable_sounds']: page.snd_drop.play()\n",
        "      negative_prompt = prefs['prompt_writer']['negative_prompt']\n",
        "      if bool(negative_prompt):\n",
        "        if '_' in negative_prompt:\n",
        "          negative_prompt = nsp_parse(negative_prompt)\n",
        "      for p in page.prompt_writer_list.controls:\n",
        "        if bool(negative_prompt):\n",
        "          page.add_to_prompts(p.title.value, {'negative_prompt':negative_prompt})\n",
        "        else:\n",
        "          page.add_to_prompts(p.title.value)\n",
        "    def clear_prompts(e):\n",
        "      if prefs['enable_sounds']: page.snd_delete.play()\n",
        "      page.prompt_writer_list.controls = []\n",
        "      page.prompt_writer_list.update()\n",
        "      writer_list_buttons.visible = False\n",
        "      writer_list_buttons.update()\n",
        "    writer_list_buttons = Row([\n",
        "        ElevatedButton(content=Text(\"‚ùå   Clear Prompts\", size=18), on_click=clear_prompts),\n",
        "        FilledButton(content=Text(\"‚ûï  Add All Prompts to List\", size=20), on_click=add_to_list),\n",
        "    ], alignment=MainAxisAlignment.SPACE_BETWEEN)\n",
        "    if len(page.prompt_writer_list.controls) < 1:\n",
        "      writer_list_buttons.visible = False\n",
        "\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üìú Advanced Prompt Writer with Noodle Soup Prompt random variables \", \"Construct your Art descriptions easier, with all the extras you need to engineer perfect prompts faster. Note, you don't have to use any randoms if you rather do all custom.\", actions=[ElevatedButton(content=Text(\"üçú  NSP Instructions\", size=18), on_click=lambda _: NSP_instructions(page))]),\n",
        "        ResponsiveRow([\n",
        "          TextField(label=\"Prompt Art Subjects\", value=prefs['prompt_writer']['art_Subjects'], on_change=lambda e: changed(e, 'art_Subjects'), multiline=True, max_lines=4, col={'lg':9}),\n",
        "          TextField(label=\"Negative Prompt (optional)\", value=prefs['prompt_writer']['negative_prompt'], on_change=lambda e: changed(e, 'negative_prompt'), multiline=True, max_lines=4, col={'lg':3}),\n",
        "        ]),\n",
        "        Row([TextField(label=\"by Artists\", value=prefs['prompt_writer']['by_Artists'], on_change=lambda e: changed(e, 'by_Artists')),\n",
        "             TextField(label=\"Art Styles\", value=prefs['prompt_writer']['art_Styles'], on_change=lambda e: changed(e, 'art_Styles')),]),\n",
        "        ResponsiveRow([\n",
        "          Row([NumberPicker(label=\"Amount: \", min=1, max=20, value=prefs['prompt_writer']['amount'], on_change=lambda e: changed(e, 'amount')),\n",
        "              NumberPicker(label=\"Random Artists: \", min=0, max=10, value=prefs['prompt_writer']['random_artists'], on_change=lambda e: changed(e, 'random_artists')),], col={'lg':6}, alignment=MainAxisAlignment.SPACE_BETWEEN),\n",
        "          Row([NumberPicker(label=\"Random Styles: \", min=0, max=10, value=prefs['prompt_writer']['random_styles'], on_change=lambda e: changed(e, 'random_styles')),\n",
        "              Checkbox(label=\"Permutate Artists\", value=prefs['prompt_writer']['permutate_artists'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e: changed(e, 'permutate_artists'))], col={'lg':6}, alignment=MainAxisAlignment.SPACE_BETWEEN),\n",
        "        ]),\n",
        "        ElevatedButton(content=Text(\"‚úçÔ∏è   Write Prompts\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_prompt_writer(page)),\n",
        "        page.prompt_writer_list,\n",
        "        writer_list_buttons,\n",
        "      ],\n",
        "    ))], scroll=ScrollMode.AUTO)\n",
        "    return c\n",
        "\n",
        "magic_prompt_prefs = {\n",
        "    'seed_prompt': '',\n",
        "    'seed': 0,\n",
        "    'amount': 8,\n",
        "    'random_artists': 0,\n",
        "    'random_styles': 0,\n",
        "    'permutate_artists': True,\n",
        "}\n",
        "\n",
        "def buildMagicPrompt(page):\n",
        "    global magic_prompt_prefs, prefs, status\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            magic_prompt_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            magic_prompt_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            magic_prompt_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    page.magic_prompt_list = Column([], spacing=0)\n",
        "    page.magic_prompt_output = Column([])\n",
        "    def add_to_prompt_list(p):\n",
        "      page.add_to_prompts(p)\n",
        "      if prefs['enable_sounds']: page.snd_drop.play()\n",
        "    def add_to_magic_prompt(p):\n",
        "      page.magic_prompt_list.controls.append(ListTile(title=Text(p, max_lines=3, style=TextThemeStyle.BODY_LARGE), dense=True, on_click=lambda _: add_to_prompt_list(p)))\n",
        "      page.magic_prompt_list.update()\n",
        "      magic_list_buttons.visible = True\n",
        "      magic_list_buttons.update()\n",
        "    page.add_to_magic_prompt = add_to_magic_prompt\n",
        "    def add_to_list(e):\n",
        "      if prefs['enable_sounds']: page.snd_drop.play()\n",
        "      for p in page.magic_prompt_list.controls:\n",
        "        page.add_to_prompts(p.title.value)\n",
        "    def clear_prompts(e):\n",
        "      if prefs['enable_sounds']: page.snd_delete.play()\n",
        "      page.magic_prompt_list.controls = []\n",
        "      page.magic_prompt_list.update()\n",
        "      magic_list_buttons.visible = False\n",
        "      magic_list_buttons.update()\n",
        "    seed = TextField(label=\"Seed\", value=magic_prompt_prefs['seed'], keyboard_type=KeyboardType.NUMBER, width = 90, on_change=lambda e:changed(e,'seed', ptype=\"int\"))\n",
        "    magic_list_buttons = Row([\n",
        "        ElevatedButton(content=Text(\"‚ùå   Clear Prompts\", size=18), on_click=clear_prompts),\n",
        "        FilledButton(content=Text(\"Add All Prompts to List\", size=20), height=45, on_click=add_to_list),\n",
        "    ], alignment=MainAxisAlignment.SPACE_BETWEEN)\n",
        "    if len(page.magic_prompt_list.controls) < 1:\n",
        "      magic_list_buttons.visible = False\n",
        "    \n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üé©  Magic Prompt Generator - GPT-2 AI Helper\", \"Generates new Image Prompts made for Stable Diffusion with a specially trained GPT-2 Text AI by Gustavosta...\", actions=[ElevatedButton(content=Text(\"üçú  NSP Instructions\", size=18), on_click=lambda _: NSP_instructions(page))]),\n",
        "        Row([TextField(label=\"Starter Prompt Text\", expand=True, value=magic_prompt_prefs['seed_prompt'], multiline=True, on_change=lambda e: changed(e, 'seed_prompt'))]),\n",
        "        ResponsiveRow([\n",
        "          Row([NumberPicker(label=\"Amount: \", min=1, max=40, value=magic_prompt_prefs['amount'], on_change=lambda e: changed(e, 'amount')), seed,\n",
        "              NumberPicker(label=\"Random Artists: \", min=0, max=10, value=magic_prompt_prefs['random_artists'], on_change=lambda e: changed(e, 'random_artists')),], col={'xl':6}, alignment=MainAxisAlignment.SPACE_BETWEEN),\n",
        "          Row([NumberPicker(label=\"Random Styles: \", min=0, max=10, value=magic_prompt_prefs['random_styles'], on_change=lambda e: changed(e, 'random_styles')),\n",
        "              Checkbox(label=\"Permutate Artists\", value=magic_prompt_prefs['permutate_artists'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e: changed(e, 'permutate_artists'))], col={'xl':6}, alignment=MainAxisAlignment.SPACE_BETWEEN),\n",
        "        ]),\n",
        "        ElevatedButton(content=Text(\"üßô   Make Magic Prompts\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_magic_prompt(page)),\n",
        "        page.magic_prompt_output,\n",
        "        page.magic_prompt_list,\n",
        "        magic_list_buttons,\n",
        "      ],\n",
        "    ))], scroll=ScrollMode.AUTO)\n",
        "    return c\n",
        "\n",
        "distil_gpt2_prefs = {\n",
        "    'seed_prompt': '',\n",
        "    'seed': 0,\n",
        "    'amount': 8,\n",
        "    'AI_temperature': 0.9,\n",
        "    'top_k': 8,\n",
        "    'max_length': 80,\n",
        "    'repetition_penalty': 1.2,\n",
        "    'penalty_alpha': 0.6,\n",
        "    'no_repeat_ngram_size': 1,\n",
        "    'random_artists': 0,\n",
        "    'random_styles': 0,\n",
        "    'permutate_artists': True,\n",
        "}\n",
        "\n",
        "def buildDistilGPT2(page):\n",
        "    global distil_gpt2_prefs, prefs, status\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            distil_gpt2_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            distil_gpt2_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            distil_gpt2_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    page.distil_gpt2_list = Column([], spacing=0)\n",
        "    page.distil_gpt2_output = Column([])\n",
        "    def add_to_prompt_list(p):\n",
        "      page.add_to_prompts(p)\n",
        "      if prefs['enable_sounds']: page.snd_drop.play()\n",
        "    def add_to_distil_gpt2(p):\n",
        "      page.distil_gpt2_list.controls.append(ListTile(title=Text(p, max_lines=3, style=TextThemeStyle.BODY_LARGE), dense=True, on_click=lambda _: add_to_prompt_list(p)))\n",
        "      page.distil_gpt2_list.update()\n",
        "      distil_list_buttons.visible = True\n",
        "      distil_list_buttons.update()\n",
        "    page.add_to_distil_gpt2 = add_to_distil_gpt2\n",
        "    def add_to_list(e):\n",
        "      if prefs['enable_sounds']: page.snd_drop.play()\n",
        "      for p in page.distil_gpt2_list.controls:\n",
        "        page.add_to_prompts(p.title.value)\n",
        "    def clear_prompts(e):\n",
        "      if prefs['enable_sounds']: page.snd_delete.play()\n",
        "      page.distil_gpt2_list.controls = []\n",
        "      page.distil_gpt2_list.update()\n",
        "      distil_list_buttons.visible = False\n",
        "      distil_list_buttons.update()\n",
        "    AI_temperature = Row([Text(\"AI Temperature:\"), Slider(label=\"{value}\", min=0, max=1, divisions=10, expand=True, tooltip=\"The value used to module the next token probabilities\", value=distil_gpt2_prefs['AI_temperature'], on_change=lambda e: changed(e, 'AI_temperature'))], col={'lg':6})\n",
        "    top_k = Row([Text(\"Top-K Samples:\"), Slider(label=\"{value}\", min=0, max=50, divisions=50, expand=True, tooltip=\"Number of highest probability vocabulary tokens to keep for top-k-filtering\", value=distil_gpt2_prefs['top_k'], on_change=lambda e: changed(e, 'top_k'))], col={'lg':6})\n",
        "    #max_length = Row([Text(\"Max Length:\"), Slider(label=\"{value}\", min=0, max=1024, divisions=1024, expand=True, tooltip=\"The maximum length the generated tokens can have. Corresponds to the length of the input prompt + max_new_tokens.\", value=distil_gpt2_prefs['max_length'], on_change=lambda e: changed(e, 'max_length', ptype=\"int\"))], col={'lg':6})\n",
        "    max_length = SliderRow(label=\"Max Length\", min=0, max=1024, divisions=1024, pref=distil_gpt2_prefs, key='max_length')\n",
        "    repetition_penalty = Row([Text(\"Repetition Penalty:\"), Slider(label=\"{value}\", min=1.0, max=3.0, divisions=20, expand=True, tooltip=\"Penalizes repetition by discounting the scores of previously generated tokens\", value=distil_gpt2_prefs['repetition_penalty'], on_change=lambda e: changed(e, 'repetition_penalty'))], col={'lg':6})\n",
        "    penalty_alpha = Row([Text(\"Penalty Alpha:\"), Slider(label=\"{value}\", min=0, max=1, divisions=10, expand=True, tooltip=\"The degeneration penalty for contrastive search; activate when it is larger than 0\", value=distil_gpt2_prefs['penalty_alpha'], on_change=lambda e: changed(e, 'penalty_alpha', ptype=\"float\"))], col={'lg':6})\n",
        "    no_repeat_ngram_size = Row([Text(\"No Repeat NGRAM Size:\"), Slider(label=\"{value}\", min=0, max=50, expand=True, divisions=50, tooltip=\"If set > 0, all ngrams of that size can only occur once. 0 adds more commas.\", value=distil_gpt2_prefs['no_repeat_ngram_size'], on_change=lambda e: changed(e, 'no_repeat_ngram_size', ptype=\"int\"))], col={'lg':6})\n",
        "    seed = TextField(label=\"Seed\", value=distil_gpt2_prefs['seed'], keyboard_type=KeyboardType.NUMBER, width = 90, on_change=lambda e:changed(e,'seed', ptype=\"int\"))\n",
        "    distil_list_buttons = Row([\n",
        "        ElevatedButton(content=Text(\"‚ùå   Clear Prompts\", size=18), on_click=clear_prompts),\n",
        "        FilledButton(content=Text(\"Add All Prompts to List\", size=20), height=45, on_click=add_to_list),\n",
        "    ], alignment=MainAxisAlignment.SPACE_BETWEEN)\n",
        "    if len(page.distil_gpt2_list.controls) < 1:\n",
        "      distil_list_buttons.visible = False\n",
        "    \n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"‚öóÔ∏è  Distilled GPT-2 Generator - GPT-2 AI Helper\", \"Generates new Image Prompts with a model trained on 2,470,000 descriptive Stable Diffusion prompts...\", actions=[ElevatedButton(content=Text(\"üçú  NSP Instructions\", size=18), on_click=lambda _: NSP_instructions(page))]),\n",
        "        Row([TextField(label=\"Starter Prompt Text\", expand=True, value=distil_gpt2_prefs['seed_prompt'], multiline=True, on_change=lambda e: changed(e, 'seed_prompt'))]),\n",
        "        AI_temperature,\n",
        "        ResponsiveRow([top_k, no_repeat_ngram_size]),\n",
        "        ResponsiveRow([repetition_penalty, penalty_alpha]),\n",
        "        max_length,\n",
        "        ResponsiveRow([\n",
        "          Row([NumberPicker(label=\"Amount: \", min=1, max=40, value=distil_gpt2_prefs['amount'], on_change=lambda e: changed(e, 'amount')), seed,\n",
        "              NumberPicker(label=\"Random Artists: \", min=0, max=10, value=distil_gpt2_prefs['random_artists'], on_change=lambda e: changed(e, 'random_artists')),], col={'xl':6}, alignment=MainAxisAlignment.SPACE_BETWEEN),\n",
        "          Row([NumberPicker(label=\"Random Styles: \", min=0, max=10, value=distil_gpt2_prefs['random_styles'], on_change=lambda e: changed(e, 'random_styles')),\n",
        "              Checkbox(label=\"Permutate Artists\", value=distil_gpt2_prefs['permutate_artists'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e: changed(e, 'permutate_artists'))], col={'xl':6}, alignment=MainAxisAlignment.SPACE_BETWEEN),\n",
        "        ]),\n",
        "        ElevatedButton(content=Text(\"üìù   Make Distil GPT-2 Prompts\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_distil_gpt2(page)),\n",
        "        page.distil_gpt2_output,\n",
        "        page.distil_gpt2_list,\n",
        "        distil_list_buttons,\n",
        "      ],\n",
        "    ))], scroll=ScrollMode.AUTO)\n",
        "    return c\n",
        "\n",
        "\n",
        "def NSP_instructions(page):\n",
        "    def open_url(e):\n",
        "        if e.data.startswith('http'):\n",
        "          page.launch_url(e.data)\n",
        "        else:\n",
        "          page.set_clipboard(e.data)\n",
        "          page.snack_bar = SnackBar(content=Text(f\"üìã   NSP variable {e.data} copied to clipboard...\"))\n",
        "          page.snack_bar.open = True\n",
        "          page.update()\n",
        "    NSP_markdown = '''To use a term database, simply use any of the keys below in sentence. Copy to Clipboard with click. \n",
        "\n",
        "For example if you wanted beauty adjective, you would write `_adj-beauty_` in your prompt. \n",
        "\n",
        "## Terminology Keys (by [@WAS](https://rebrand.ly/easy-diffusion))\n",
        "\n",
        "### Adjective Types\n",
        "   - [\\_adj-architecture\\_](_adj-architecture_) - A list of architectural adjectives and styles\n",
        "   - [\\_adj-beauty\\_](_adj-beauty_) - A list of beauty adjectives for people (maybe things?)\n",
        "   - [\\_adj-general\\_](_adj-general_) - A list of general adjectives for people/things.\n",
        "   - [\\_adj-horror\\_](_adj-horror_) - A list of horror adjectives\n",
        "### Art Types\n",
        "   - [\\_artist\\_](_artist_) - A comprehensive list of artists by [**MisterRuffian**](https://docs.google.com/spreadsheets/d/1_jgQ9SyvUaBNP1mHHEzZ6HhL_Es1KwBKQtnpnmWW82I/edit) (Discord _Misterruffian#2891_)\n",
        "   - [\\_color\\_](_color_) - A comprehensive list of colors\n",
        "   - [\\_portrait-type\\_](_portrait-type_) - A list of common portrait types/poses\n",
        "   - [\\_style\\_](_style_) - A list of art styles and mediums\n",
        "### Computer Graphics Types\n",
        "   - [\\_3d-terms\\_](_3d-terms_) - A list of 3D graphics terminology\n",
        "   - [\\_color-palette\\_](_color-palette_) - A list of computer and video game console color palettes\n",
        "   - [\\_hd\\_](_hd_) - A list of high definition resolution terms\n",
        "### Miscellaneous Types\n",
        "   - [\\_details\\_](_details_) - A list of detail descriptors\n",
        "   - [\\_site\\_](_site_) - A list of websites to query\n",
        "   - [\\_gen-modififer\\_](_gen-modififer_) - A list of general modifiers adopted from [Weird Wonderful AI Art](https://weirdwonderfulai.art/)\n",
        "   - [\\_neg-weight\\_](_neg-weight_) - A lsit of negative weight ideas\n",
        "   - [\\_punk\\_](_punk_) - A list of punk modifier (eg. cyberpunk)\n",
        "   - [ _pop-culture\\_](_pop-culture_) - A list of popular culture movies, shows, etc\n",
        "   - [\\_pop-location\\_](_pop-location_) - A list of popular tourist locations\n",
        "   - [\\_fantasy-setting\\_](_fantasy-setting_) - A list of fantasy location settings\n",
        "   - [\\_fantasy-creature\\_](_fantasy-creature_) - A list of fantasy creatures\n",
        "   - [\\_animals\\_](_animals_) - A list of modern animals\n",
        "### Noun Types\n",
        "   - [\\_noun-beauty\\_](_noun-beauty_) - A list of beauty related nouns\n",
        "   - [\\_noun-emote\\_](_noun-emote_) - A list of emotions and expressions\n",
        "   - [\\_noun-fantasy\\_](_noun-fantasy_) - A list of fantasy nouns\n",
        "   - [\\_noun-general\\_](_noun-general_) - A list of general nouns\n",
        "   - [\\_noun-horror\\_](_noun-horror_) - A list of horror nouns\n",
        "### People Types\n",
        "   - [\\_bodyshape\\_](_bodyshape_) - A list of body shapes\n",
        "   - [\\_celeb\\_](_celeb_) - A list of celebrities\n",
        "   - [\\_eyecolor\\_](_eyecolor_) - A list of eye colors\n",
        "   - [\\_hair\\_](_hair_) - A list of hair types\n",
        "   - [\\_nationality\\_](_nationality_) - A list of nationalities\n",
        "   - [\\_occputation\\_](_occputation_) A list of occupation types\n",
        "   - [\\_skin-color\\_](_skin-color_) - A list of skin tones\n",
        "   - [\\_identity-young\\_](_identity-young_) A list of young identifiers\n",
        "   - [\\_identity-adult\\_](_identity-adult_) A list of adult identifiers\n",
        "   - [\\_identity\\_](_identity_) A list of general identifiers\n",
        "### Photography / Image / Film Types\n",
        "   - [\\_aspect-ratio\\_](_aspect-ratio_) - A list of common aspect ratios\n",
        "   - [\\_cameras\\_](_cameras_) - A list of camera models *(including manufactuerer)*\n",
        "   - [\\_camera-manu\\_](_camera-manu_) - A list of camera manufacturers\n",
        "   - [\\_f-stop\\_](_f-stop_) - A list of camera aperture f-stop\n",
        "   - [\\_focal-length\\_](_focal-length_) - A list of focal length ranges\n",
        "   - [\\_photo-term\\_](_photo-term_) - A list of photography terms relating to photos\n",
        "\n",
        "So in Subject try something like: `A _color_ _noun-general_ that is _adj-beauty_ and _adj-general_ with a _noun-emote_ _noun-fantasy_`\n",
        "'''\n",
        "    def close_NSP_dlg(e):\n",
        "      instruction_alert.open = False\n",
        "      page.update()\n",
        "    instruction_alert = AlertDialog(title=Text(\"üçú  Noodle Soup Prompt Variables Instructions\"), content=Column([Markdown(NSP_markdown, extension_set=\"gitHubWeb\", on_tap_link=open_url)], scroll=ScrollMode.AUTO), actions=[TextButton(\"üç≤  Good Soup! \", on_click=close_NSP_dlg)], actions_alignment=MainAxisAlignment.END,)\n",
        "    page.dialog = instruction_alert\n",
        "    instruction_alert.open = True\n",
        "    page.update()\n",
        "\n",
        "\n",
        "ESRGAN_prefs = {\n",
        "    'enlarge_scale': 1.5,\n",
        "    'face_enhance': False,\n",
        "    'image_path': '',\n",
        "    'save_to_GDrive': True,\n",
        "    'upload_file': False,\n",
        "    'download_locally': False,\n",
        "    'display_image': False,\n",
        "    'dst_image_path': '',\n",
        "    'filename_suffix': '',\n",
        "    'split_image_grid': False,\n",
        "    'rows': 3,\n",
        "    'cols': 3,\n",
        "}\n",
        "def buildESRGANupscaler(page):\n",
        "    def changed(e, pref=None):\n",
        "      if pref is not None:\n",
        "        ESRGAN_prefs[pref] = e.control.value\n",
        "    def add_to_ESRGAN_output(o):\n",
        "      ESRGAN_output.controls.append(o)\n",
        "      ESRGAN_output.update()\n",
        "      if clear_button.visible == False:\n",
        "        clear_button.visible = True\n",
        "        clear_button.update()\n",
        "      #generator_list_buttons.visible = True\n",
        "      #generator_list_buttons.update()\n",
        "    page.add_to_ESRGAN_output = add_to_ESRGAN_output\n",
        "    def toggle_split(e):\n",
        "      split_container.height = None if e.control.value else 0\n",
        "      changed(e, 'split_image_grid')\n",
        "      split_container.update()\n",
        "    def clear_output(e):\n",
        "      if prefs['enable_sounds']: page.snd_delete.play()\n",
        "      ESRGAN_output.controls = []\n",
        "      ESRGAN_output.update()\n",
        "      clear_button.visible = False\n",
        "      clear_button.update()\n",
        "    page.clear_ESRGAN_output = clear_output\n",
        "    def file_picker_result(e: FilePickerResultEvent):\n",
        "        if e.files != None:\n",
        "          upload_files(e)\n",
        "    def on_upload_progress(e: FilePickerUploadEvent):\n",
        "      if e.progress == 1:\n",
        "        if not slash in e.file_name:\n",
        "          fname = os.path.join(root_dir, e.file_name)\n",
        "        else:\n",
        "          fname = e.file_name\n",
        "        image_path.value = fname\n",
        "        image_path.update()\n",
        "        ESRGAN_prefs['image_path'] = fname\n",
        "        page.update()\n",
        "    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)\n",
        "    def pick_path(e):\n",
        "        file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"png\", \"PNG\", \"jpg\", \"jpeg\"], dialog_title=\"Pick Image File to Enlarge\")\n",
        "    def upload_files(e):\n",
        "        uf = []\n",
        "        if file_picker.result != None and file_picker.result.files != None:\n",
        "            for f in file_picker.result.files:\n",
        "              if page.web:\n",
        "                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))\n",
        "              else:\n",
        "                on_upload_progress(FilePickerUploadEvent(f.path, 1, \"\"))\n",
        "            file_picker.upload(uf)\n",
        "    def pick_destination(e):\n",
        "        alert_msg(page, \"Switch to Colab tab and press Files button on the Left & Find the Path you want to Save Images into, Right Click and Copy Path, then Paste here\")\n",
        "    page.overlay.append(file_picker)\n",
        "    enlarge_scale_slider = SliderRow(label=\"Enlarge Scale\", min=1, max=4, divisions=6, round=1, suffix=\"x\", pref=ESRGAN_prefs, key='enlarge_scale')\n",
        "    face_enhance = Checkbox(label=\"Use Face Enhance GPFGAN\", value=ESRGAN_prefs['face_enhance'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'face_enhance'))\n",
        "    image_path = TextField(label=\"Image File or Folder Path\", value=ESRGAN_prefs['image_path'], col={'md':6}, on_change=lambda e:changed(e,'image_path'), suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_path))\n",
        "    dst_image_path = TextField(label=\"Destination Image Path\", value=ESRGAN_prefs['dst_image_path'], col={'md':6}, on_change=lambda e:changed(e,'dst_image_path'), suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD_OUTLINED, on_click=pick_destination))\n",
        "    filename_suffix = TextField(label=\"Optional Filename Suffix\", hint_text=\"-big\", value=ESRGAN_prefs['filename_suffix'], on_change=lambda e:changed(e,'filename_suffix'), width=260)\n",
        "    download_locally = Checkbox(label=\"Download Images Locally\", value=ESRGAN_prefs['download_locally'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'download_locally'))\n",
        "    display_image = Checkbox(label=\"Display Upscaled Image\", value=ESRGAN_prefs['display_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_image'))\n",
        "    split_image_grid = Switch(label=\"Split Image Grid\", value=ESRGAN_prefs['split_image_grid'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_split)\n",
        "    rows = NumberPicker(label=\"Rows: \", min=1, max=8, value=ESRGAN_prefs['rows'], on_change=lambda e: changed(e, 'rows'))\n",
        "    cols = NumberPicker(label=\"Columns: \", min=1, max=8, value=ESRGAN_prefs['cols'], on_change=lambda e: changed(e, 'cols'))\n",
        "    split_container = Container(Row([rows, Container(content=None, width=25), cols]), animate_size=animation.Animation(800, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE, padding=padding.only(left=28), height=0)\n",
        "    ESRGAN_output = Column([])\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(ESRGAN_output.controls) > 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"‚ÜïÔ∏è   Real-ESRGAN AI Upscale Enlarging\", \"Select one or more files, or give path to image or folder. Save to your Google Drive and/or Download.\"),\n",
        "        enlarge_scale_slider,\n",
        "        face_enhance,\n",
        "        ResponsiveRow([image_path, dst_image_path]),\n",
        "        filename_suffix,\n",
        "        download_locally,\n",
        "        display_image,\n",
        "        #Divider(thickness=2, height=4),\n",
        "        split_image_grid,\n",
        "        split_container,\n",
        "        ElevatedButton(content=Text(\"üêò  Run AI Upscaling\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_upscaling(page)),\n",
        "        ESRGAN_output,\n",
        "        clear_button,\n",
        "      ],\n",
        "    ))], scroll=ScrollMode.AUTO)\n",
        "    return c\n",
        "\n",
        "retrieve_prefs = {\n",
        "    'image_path': '',\n",
        "    'add_to_prompts': True,\n",
        "    'display_full_metadata': False,\n",
        "    'display_image': False,\n",
        "    'upload_file': False,\n",
        "}\n",
        "def buildRetrievePrompts(page):\n",
        "    def changed(e, pref=None):\n",
        "        if pref is not None:\n",
        "          retrieve_prefs[pref] = e.control.value\n",
        "    def add_to_retrieve_output(o):\n",
        "      retrieve_output.controls.append(o)\n",
        "      retrieve_output.update()\n",
        "    def clear_output(e):\n",
        "      if prefs['enable_sounds']: page.snd_delete.play()\n",
        "      retrieve_output.controls = []\n",
        "      retrieve_output.update()\n",
        "      clear_button.visible = False\n",
        "      clear_button.update()\n",
        "    def pick_image(e):\n",
        "        alert_msg(page, \"Switch to Colab tab and press Files button on the Left & Find the Path you want to Retrieve, Right Click and Copy Path, then Paste here\")\n",
        "    page.add_to_retrieve_output = add_to_retrieve_output\n",
        "    image_path = TextField(label=\"Image File or Folder Path\", value=retrieve_prefs['image_path'], on_change=lambda e:changed(e,'image_path'), suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_image))\n",
        "    add_to_prompts = Checkbox(label=\"Add to Prompts\", value=retrieve_prefs['add_to_prompts'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'add_to_prompts'))\n",
        "    display_full_metadata = Checkbox(label=\"Display Full Metadata\", value=retrieve_prefs['display_full_metadata'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_full_metadata'))\n",
        "    display_image = Checkbox(label=\"Display Image\", value=retrieve_prefs['display_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_image'))\n",
        "    retrieve_output = Column([])\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(retrieve_output.controls) > 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üì∞  Retrieve Dream Prompts from Image Metadata\", \"Give it images made here and gives you all parameters used to recreate it. Either upload png file(s) or paste path to image or folder or config.json to revive your dreams..\"),\n",
        "        image_path,\n",
        "        add_to_prompts,\n",
        "        display_full_metadata,\n",
        "        display_image,\n",
        "        ElevatedButton(content=Text(\"üò¥  Retrieve Dream\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_retrieve(page)),\n",
        "        retrieve_output,\n",
        "        clear_button,\n",
        "      ],\n",
        "    ))], scroll=ScrollMode.AUTO)\n",
        "    return c\n",
        "\n",
        "initfolder_prefs = {\n",
        "    'prompt_string': '',\n",
        "    'negative_prompt': '',\n",
        "    'init_folder': '',\n",
        "    'include_strength': True,\n",
        "    'image_strength': 0.5,\n",
        "}\n",
        "def buildInitFolder(page):\n",
        "    def changed(e, pref=None):\n",
        "        if pref is not None:\n",
        "          initfolder_prefs[pref] = e.control.value\n",
        "    def add_to_initfolder_output(o):\n",
        "      initfolder_output.controls.append(o)\n",
        "      initfolder_output.update()\n",
        "    def clear_output(e):\n",
        "      if prefs['enable_sounds']: page.snd_delete.play()\n",
        "      initfolder_output.controls = []\n",
        "      initfolder_output.update()\n",
        "      clear_button.visible = False\n",
        "      clear_button.update()\n",
        "    def pick_init(e):\n",
        "        alert_msg(page, \"Switch to Colab tab and press Files button on the Left & Find the Path you want to use as Init Folder, Right Click and Copy Path, then Paste here\")\n",
        "    def toggle_strength(e):\n",
        "      changed(e,'include_strength')\n",
        "      strength_row.visible = e.control.value\n",
        "      strength_row.update()\n",
        "    page.add_to_initfolder_output = add_to_initfolder_output\n",
        "    prompt_string = TextField(label=\"Prompt Text\", value=initfolder_prefs['prompt_string'], col={'md': 9}, on_change=lambda e:changed(e,'prompt_string'))\n",
        "    negative_prompt  = TextField(label=\"Negative Prompt Text\", value=initfolder_prefs['negative_prompt'], col={'md':3}, on_change=lambda e:changed(e,'negative_prompt'))\n",
        "\n",
        "    init_folder = TextField(label=\"Init Image Folder Path\", value=initfolder_prefs['init_folder'], on_change=lambda e:changed(e,'init_folder'), suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_init))\n",
        "    include_strength = Checkbox(label=\"Include Strength\", value=initfolder_prefs['include_strength'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=toggle_strength)\n",
        "    image_strength = Slider(min=0.1, max=0.9, divisions=16, label=\"{value}%\", value=float(initfolder_prefs['image_strength']), expand=True)\n",
        "    strength_row = Row([Text(\"Image Strength:\"), image_strength])\n",
        "    strength_row.visible = initfolder_prefs['include_strength']\n",
        "    initfolder_output = Column([])\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(initfolder_output.controls) > 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üìÇ Generate Prompts from Folder as Init Images\", \"Provide a Folder with a collection of images that you want to automatically add to prompts list with init_image overides...\"),\n",
        "        init_folder,\n",
        "        ResponsiveRow([prompt_string, negative_prompt]),\n",
        "        include_strength,\n",
        "        strength_row,\n",
        "        ElevatedButton(content=Text(\"‚ûï  Add to Prompts\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_initfolder(page)),\n",
        "        initfolder_output,\n",
        "        clear_button,\n",
        "      ]\n",
        "    ))], scroll=ScrollMode.AUTO)\n",
        "    return c\n",
        "\n",
        "init_video_prefs = {\n",
        "    'prompt': '',\n",
        "    'negative_prompt': '',\n",
        "    'init_folder': '',\n",
        "    'include_strength': False,\n",
        "    'image_strength': 0.5,\n",
        "    'max_size': 960,\n",
        "    'file_prefix': 'frame-',\n",
        "    'video_file': '',\n",
        "    'fps': 15,\n",
        "    'start_time': 0.0,\n",
        "    'end_time': 0.0,\n",
        "    'batch_folder_name': '',\n",
        "    'show_images': False,\n",
        "}\n",
        "def buildInitVideo(page):\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            init_video_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            init_video_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            init_video_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def add_to_init_video_output(o):\n",
        "      init_video_output.controls.append(o)\n",
        "      init_video_output.update()\n",
        "      if clear_button.visible == False:\n",
        "          clear_button.visible = True\n",
        "          clear_button.update()\n",
        "    def clear_output(e):\n",
        "      if prefs['enable_sounds']: page.snd_delete.play()\n",
        "      init_video_output.controls = []\n",
        "      init_video_output.update()\n",
        "      clear_button.visible = False\n",
        "      clear_button.update()\n",
        "    def file_picker_result(e: FilePickerResultEvent):\n",
        "        if e.files != None:\n",
        "          upload_files(e)\n",
        "    def on_upload_progress(e: FilePickerUploadEvent):\n",
        "      if e.progress == 1:\n",
        "        if not slash in e.file_name:\n",
        "          fname = os.path.join(root_dir, e.file_name)\n",
        "          init_video_prefs['file_name'] = e.file_name.rpartition('.')[0]\n",
        "        else:\n",
        "          fname = e.file_name\n",
        "          init_video_prefs['file_name'] = e.file_name.rpartition(slash)[2].rpartition('.')[0]\n",
        "        video_file.value = fname\n",
        "        video_file.update()\n",
        "        init_video_prefs['video_file'] = fname\n",
        "        page.update()\n",
        "    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)\n",
        "    def upload_files(e):\n",
        "        uf = []\n",
        "        if file_picker.result != None and file_picker.result.files != None:\n",
        "            for f in file_picker.result.files:\n",
        "              if page.web:\n",
        "                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))\n",
        "              else:\n",
        "                on_upload_progress(FilePickerUploadEvent(f.path, 1, \"\"))\n",
        "            file_picker.upload(uf)\n",
        "    page.overlay.append(file_picker)\n",
        "    #page.overlay.append(pick_files_dialog)\n",
        "    def pick_video(e):\n",
        "        file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"avi\", \"mp4\", \"mov\"], dialog_title=\"Pick Video File\")\n",
        "    def toggle_strength(e):\n",
        "      changed(e,'include_strength')\n",
        "      strength_row.visible = e.control.value\n",
        "      strength_row.update()\n",
        "    page.add_to_init_video_output = add_to_init_video_output\n",
        "    prompt = TextField(label=\"Prompt Text\", value=init_video_prefs['prompt'], col={'md': 9}, on_change=lambda e:changed(e,'prompt'))\n",
        "    negative_prompt  = TextField(label=\"Negative Prompt Text\", value=init_video_prefs['negative_prompt'], col={'md':3}, on_change=lambda e:changed(e,'negative_prompt'))\n",
        "    batch_folder_name = TextField(label=\"Batch Folder Name\", value=init_video_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    file_prefix = TextField(label=\"Image Filename Prefix\", value=init_video_prefs['file_prefix'], width=150, on_change=lambda e:changed(e,'file_prefix'))\n",
        "    video_file = TextField(label=\"Video File\", value=init_video_prefs['video_file'], on_change=lambda e:changed(e,'video_file'), height=60, suffix=IconButton(icon=icons.VIDEO_CALL, on_click=pick_video))\n",
        "    #init_folder = TextField(label=\"Init Image Folder Path\", value=init_video_prefs['init_folder'], on_change=lambda e:changed(e,'init_folder'), suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_init))\n",
        "    fps = TextField(label=\"Frames Per Seccond\", value=init_video_prefs['fps'], width=145, keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'fps', ptype=\"int\"))\n",
        "    start_time = TextField(label=\"Start Time (s)\", value=init_video_prefs['start_time'], width=145, keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'start_time', ptype=\"float\"))\n",
        "    end_time = TextField(label=\"End Time (0 for all)\", value=init_video_prefs['end_time'], width=145, keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'end_time', ptype=\"float\"))\n",
        "    max_row = SliderRow(label=\"Max Resolution Size\", min=256, max=1280, divisions=64, multiple=16, suffix=\"px\", pref=init_video_prefs, key='max_size')\n",
        "    show_images = Checkbox(label=\"Show Extracted Images\", value=init_video_prefs['show_images'], tooltip=\"Fills up screen with all frames, you probably don't need to.\", fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'show_images'))\n",
        "    include_strength = Checkbox(label=\"Include Strength   \", value=init_video_prefs['include_strength'], tooltip=\"Otherwise defaults to setting in Image Parameters\", fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=toggle_strength)\n",
        "    image_strength = Slider(min=0.1, max=0.9, divisions=16, label=\"{value}%\", value=float(init_video_prefs['image_strength']), expand=True)\n",
        "    strength_row = Row([Text(\"Image Strength:\"), image_strength])\n",
        "    strength_row.visible = init_video_prefs['include_strength']\n",
        "    init_video_output = Column([])\n",
        "    page.init_video_output = init_video_output\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Prompts List\", size=18), on_click=page.clear_prompts_list), ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(init_video_output.controls) > 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üé• Generate Prompts from Video File Frames\", \"Provide a short video clip to automatically add sequence to prompts list with init_image overides...\"),\n",
        "        video_file, #init_folder,\n",
        "        Row([fps, start_time, end_time]),\n",
        "        Row([batch_folder_name, file_prefix]),\n",
        "        ResponsiveRow([prompt, negative_prompt]),\n",
        "        Row([include_strength, show_images]),\n",
        "        strength_row,\n",
        "        max_row,\n",
        "        ElevatedButton(content=Text(\"‚è©  Frames to Prompts\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_init_video(page)),\n",
        "        init_video_output,\n",
        "        clear_button,\n",
        "      ]\n",
        "    ))], scroll=ScrollMode.AUTO)\n",
        "    return c\n",
        "\n",
        "image2text_prefs = {\n",
        "    'mode': 'best',\n",
        "    'request_mode': 'Caption',\n",
        "    'slow_workers': True,\n",
        "    'trusted_workers': False,\n",
        "    'folder_path': '',\n",
        "    'image_path': '',\n",
        "    'max_size': 768,\n",
        "    'save_csv': False,\n",
        "    'images': [],\n",
        "    'use_AIHorde': False,\n",
        "}\n",
        "\n",
        "def buildImage2Text(page):\n",
        "    global prefs, image2text_prefs\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            image2text_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            image2text_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            image2text_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def add_to_image2text_output(o):\n",
        "      page.image2text_output.controls.append(o)\n",
        "      page.image2text_output.update()\n",
        "    def clear_output(e):\n",
        "      if prefs['enable_sounds']: page.snd_delete.play()\n",
        "      page.image2text_output.controls = []\n",
        "      page.image2text_output.update()\n",
        "      save_dir = os.path.join(root_dir, 'image2text')\n",
        "      if os.path.exists(save_dir):\n",
        "        for f in os.listdir(save_dir):\n",
        "            os.remove(os.path.join(save_dir, f))\n",
        "        os.rmdir(save_dir)\n",
        "      page.image2text_file_list.controls = []\n",
        "      page.image2text_file_list.update()\n",
        "      image2text_list_buttons.visible = False\n",
        "      image2text_list_buttons.update()\n",
        "    def i2t_help(e):\n",
        "      def close_i2t_dlg(e):\n",
        "        nonlocal i2t_help_dlg\n",
        "        i2t_help_dlg.open = False\n",
        "        page.update()\n",
        "      i2t_help_dlg = AlertDialog(title=Text(\"üíÅ   Help with Image2Text CLIP Interrogator\"), content=Column([\n",
        "          Text(\"\"),\n",
        "        ], scroll=ScrollMode.AUTO), actions=[TextButton(\"üò™  Okay then... \", on_click=close_i2t_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = i2t_help_dlg\n",
        "      i2t_help_dlg.open = True\n",
        "      page.update()\n",
        "    def file_picker_result(e: FilePickerResultEvent):\n",
        "        if e.files != None:\n",
        "          upload_files(e)\n",
        "    def on_upload_progress(e: FilePickerUploadEvent):\n",
        "      if e.progress == 1:\n",
        "        save_dir = os.path.join(root_dir, 'image2text')\n",
        "        if not os.path.exists(save_dir):\n",
        "          os.mkdir(save_dir)\n",
        "        image2text_prefs['folder_path'] = save_dir\n",
        "        if not slash in e.file_name:\n",
        "          fname = os.path.join(root_dir, e.file_name)\n",
        "          fpath = os.path.join(save_dir, e.file_name)\n",
        "        else:\n",
        "          fname = e.file_name\n",
        "          fpath = os.path.join(save_dir, e.file_name.rpartition(slash)[2])\n",
        "        original_img = PILImage.open(fname)\n",
        "        width, height = original_img.size\n",
        "        width, height = scale_dimensions(width, height, image2text_prefs['max_size'])\n",
        "        original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert(\"RGB\")\n",
        "        original_img.save(fpath)\n",
        "        #shutil.move(fname, fpath)\n",
        "        page.image2text_file_list.controls.append(ListTile(title=Text(fpath), dense=True))\n",
        "        page.image2text_file_list.update()\n",
        "        image2text_prefs['images'].append(fpath)\n",
        "    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)\n",
        "    def pick_path(e):\n",
        "        file_picker.pick_files(allow_multiple=True, allowed_extensions=[\"png\", \"PNG\", \"jpg\", \"jpeg\"], dialog_title=\"Pick Image File to Enlarge\")\n",
        "    def upload_files(e):\n",
        "        uf = []\n",
        "        if file_picker.result != None and file_picker.result.files != None:\n",
        "            for f in file_picker.result.files:\n",
        "              if page.web:\n",
        "                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))\n",
        "              else:\n",
        "                on_upload_progress(FilePickerUploadEvent(f.path, 1, \"\"))\n",
        "                #uf.append(FilePickerUploadFile(f.name, upload_url=f.path))\n",
        "            file_picker.upload(uf)\n",
        "    page.overlay.append(file_picker)\n",
        "    def add_image(e):\n",
        "        save_dir = os.path.join(root_dir, 'image2text')\n",
        "        if not os.path.exists(save_dir):\n",
        "          os.mkdir(save_dir)\n",
        "        image2text_prefs['folder_path'] = save_dir\n",
        "        if image_path.value.startswith('http'):\n",
        "          import requests\n",
        "          from io import BytesIO\n",
        "          response = requests.get(image_path.value)\n",
        "          fpath = os.path.join(save_dir, image_path.value.rpartition(slash)[2])\n",
        "          original_img = PILImage.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "          width, height = original_img.size\n",
        "          width, height = scale_dimensions(width, height, image2text_prefs['max_size'])\n",
        "          original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert(\"RGB\")\n",
        "          original_img.save(fpath)\n",
        "          page.image2text_file_list.controls.append(ListTile(title=Text(fpath), dense=True))\n",
        "          page.image2text_file_list.update()\n",
        "          image2text_prefs['images'].append(fpath)\n",
        "        elif os.path.isfile(image_path.value):\n",
        "          fpath = os.path.join(save_dir, image_path.value.rpartition(slash)[2])\n",
        "          original_img = PILImage.open(image_path.value)\n",
        "          width, height = original_img.size\n",
        "          width, height = scale_dimensions(width, height, image2text_prefs['max_size'])\n",
        "          original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert(\"RGB\")\n",
        "          original_img.save(fpath)\n",
        "          #shutil.copy(image_path.value, fpath)\n",
        "          page.image2text_file_list.controls.append(ListTile(title=Text(fpath), dense=True))\n",
        "          page.image2text_file_list.update()\n",
        "          image2text_prefs['images'].append(fpath)\n",
        "        elif os.path.isdir(image_path.value):\n",
        "          for f in os.listdir(image_path.value):\n",
        "            file_path = os.path.join(image_path.value, f)\n",
        "            if os.path.isdir(file_path): continue\n",
        "            if f.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "              fpath = os.path.join(save_dir, f)\n",
        "              original_img = PILImage.open(file_path)\n",
        "              width, height = original_img.size\n",
        "              width, height = scale_dimensions(width, height, image2text_prefs['max_size'])\n",
        "              original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert(\"RGB\")\n",
        "              original_img.save(fpath)\n",
        "              #shutil.copy(file_path, fpath)\n",
        "              page.image2text_file_list.controls.append(ListTile(title=Text(fpath), dense=True))\n",
        "              page.image2text_file_list.update()\n",
        "              image2text_prefs['images'].append(fpath)\n",
        "        else:\n",
        "          if bool(image_path.value):\n",
        "            alert_msg(page, \"Couldn't find a valid File, Path or URL...\")\n",
        "          else:\n",
        "            pick_path(e)\n",
        "          return\n",
        "        image_path.value = \"\"\n",
        "        image_path.update()\n",
        "    page.image2text_list = Column([], spacing=0)\n",
        "    def add_to_prompt_list(p):\n",
        "      page.add_to_prompts(p)\n",
        "      if prefs['enable_sounds']: page.snd_drop.play()\n",
        "    def add_to_image2text(p):\n",
        "      page.image2text_list.controls.append(ListTile(title=Text(p, max_lines=3, style=TextThemeStyle.BODY_LARGE), dense=True, on_click=lambda _: add_to_prompt_list(p)))\n",
        "      page.image2text_list.update()\n",
        "      image2text_list_buttons.visible = True\n",
        "      image2text_list_buttons.update()\n",
        "    page.add_to_image2text = add_to_image2text\n",
        "    def add_to_list(e):\n",
        "      if prefs['enable_sounds']: page.snd_drop.play()\n",
        "      for p in page.image2text_list.controls:\n",
        "        page.add_to_prompts(p.title.value)\n",
        "    def toggle_AIHorde(e):\n",
        "      use = e.control.value\n",
        "      changed(e,'use_AIHorde')\n",
        "      AIHorde_row.height=None if use else 0\n",
        "      AIHorde_row.update()\n",
        "      mode.visible = not use\n",
        "      mode.update()\n",
        "      request_mode.visible = use\n",
        "      request_mode.update()\n",
        "    def clear_prompts(e):\n",
        "      if prefs['enable_sounds']: page.snd_delete.play()\n",
        "      page.image2text_list.controls = []\n",
        "      page.image2text_list.update()\n",
        "      prompts = []\n",
        "      image2text_list_buttons.visible = False\n",
        "      image2text_list_buttons.update()\n",
        "    image2text_list_buttons = Row([\n",
        "        ElevatedButton(content=Text(\"‚ùå   Clear Prompts\"), on_click=clear_prompts),\n",
        "        FilledButton(content=Text(\"‚ûï  Add All Prompts to List\", size=20), height=45, on_click=add_to_list),\n",
        "    ], alignment=MainAxisAlignment.SPACE_BETWEEN)\n",
        "    if len(page.image2text_list.controls) < 1:\n",
        "      image2text_list_buttons.visible = False\n",
        "\n",
        "    mode = Dropdown(label=\"Interrogation Mode\", width=250, options=[dropdown.Option(\"best\"), dropdown.Option(\"classic\"), dropdown.Option(\"fast\")], value=image2text_prefs['mode'], on_change=lambda e: changed(e, 'mode'))\n",
        "    use_AIHorde = Switch(label=\"Use AIHorde Crowdsourced Interrogator\", value=image2text_prefs['use_AIHorde'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_AIHorde)\n",
        "    request_mode = Dropdown(label=\"Request Mode\", width=250, options=[dropdown.Option(\"Caption\"), dropdown.Option(\"Interrogation\"), dropdown.Option(\"Full Prompt\")], value=image2text_prefs['request_mode'], visible=False, on_change=lambda e: changed(e, 'request_mode'))\n",
        "    slow_workers = Checkbox(label=\"Allow Slow Workers\", tooltip=\"\", value=image2text_prefs['slow_workers'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'slow_workers'))\n",
        "    trusted_workers = Checkbox(label=\"Only Trusted Workers\", tooltip=\"\", value=image2text_prefs['trusted_workers'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'trusted_workers'))\n",
        "    AIHorde_row = Container(content=Row([slow_workers, trusted_workers]), height=None if image2text_prefs['use_AIHorde'] else 0, animate_size=animation.Animation(800, AnimationCurve.EASE_OUT_CIRC), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    save_csv = Checkbox(label=\"Save CSV file of Prompts\", tooltip=\"\", value=image2text_prefs['save_csv'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'save_csv'))\n",
        "    max_row = SliderRow(label=\"Max Resolution Size\", min=256, max=1024, divisions=32, multiple=16, suffix=\"px\", pref=image2text_prefs, key='max_size')\n",
        "    image_path = TextField(label=\"Image File or Folder Path or URL to Train\", value=image2text_prefs['image_path'], on_change=lambda e:changed(e,'image_path'), suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_path), expand=1)\n",
        "    add_image_button = ElevatedButton(content=Text(\"Add File or Folder\"), on_click=add_image)\n",
        "    page.image2text_file_list = Column([], tight=True, spacing=0)\n",
        "    page.image2text_output = Column([])\n",
        "    #clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    #clear_button.visible = len(page.image2text_output.controls) > 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üò∂‚Äçüå´Ô∏è  Image2Text CLIP-Interrogator\", subtitle=\"Create text prompts by describing input images...\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with Image2Text Interrogator\", on_click=i2t_help)]),\n",
        "        #mode,\n",
        "        Row([mode, request_mode, use_AIHorde]),\n",
        "        AIHorde_row,\n",
        "        max_row,\n",
        "        Row([image_path, add_image_button]),\n",
        "        page.image2text_file_list,\n",
        "        ElevatedButton(content=Text(\"üë®‚Äçüé®Ô∏è  Get Prompts from Images\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_image2text(page)),\n",
        "        page.image2text_list,\n",
        "        image2text_list_buttons,\n",
        "        page.image2text_output,\n",
        "      ],\n",
        "    ))], scroll=ScrollMode.AUTO)\n",
        "    return c\n",
        "\n",
        "BLIP2_image2text_prefs = {\n",
        "    'folder_path': '',\n",
        "    'image_path': '',\n",
        "    'max_size': 768,\n",
        "    'num_captions': 1,\n",
        "    'question_prompt': '',\n",
        "    #'model_name': 'blip2_t5',\n",
        "    'model_type': 'pretrain_flant5xxl', #pretrain_opt2.7b, pretrain_opt6.7b, caption_coco_opt2.7b, caption_coco_opt6.7b, pretrain_flant5xl, caption_coco_flant5xl\n",
        "    'images': [],\n",
        "}\n",
        "\n",
        "def buildBLIP2Image2Text(page):\n",
        "    global prefs, BLIP2_image2text_prefs\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            BLIP2_image2text_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            BLIP2_image2text_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            BLIP2_image2text_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def add_to_BLIP2_image2text_output(o):\n",
        "      page.BLIP2_image2text_output.controls.append(o)\n",
        "      page.BLIP2_image2text_output.update()\n",
        "    def clear_output(e):\n",
        "      if prefs['enable_sounds']: page.snd_delete.play()\n",
        "      page.BLIP2_image2text_output.controls = []\n",
        "      page.BLIP2_image2text_output.update()\n",
        "      save_dir = os.path.join(root_dir, 'BLIP2_image2text')\n",
        "      if os.path.exists(save_dir):\n",
        "        for f in os.listdir(save_dir):\n",
        "            os.remove(os.path.join(save_dir, f))\n",
        "        os.rmdir(save_dir)\n",
        "      page.BLIP2_image2text_file_list.controls = []\n",
        "      page.BLIP2_image2text_file_list.update()\n",
        "      BLIP2_image2text_list_buttons.visible = False\n",
        "      BLIP2_image2text_list_buttons.update()\n",
        "    def BLIP2_i2t_help(e):\n",
        "      def close_BLIP2_i2t_dlg(e):\n",
        "        nonlocal BLIP2_i2t_help_dlg\n",
        "        BLIP2_i2t_help_dlg.open = False\n",
        "        page.update()\n",
        "      BLIP2_i2t_help_dlg = AlertDialog(title=Text(\"üíÅ   Help with BLIP2 Image2Text Interrogator\"), content=Column([\n",
        "          Text(\"Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Model\", weight=FontWeight.BOLD),\n",
        "          Text(\"BLIP-2 is a generic and efficient pre-training strategy that bootstraps vision-language pre-training from off-the-shelf frozen pre-trained image encoders and frozen large language models. BLIP-2 bridges the modality gap with a lightweight Querying Transformer, which is pre-trained in two stages. The first stage bootstraps vision-language representation learning from a frozen image encoder. The second stage bootstraps vision-to-language generative learning from a frozen language model. BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods. For example, our model outperforms Flamingo80B by 8.7% on zero-shot VQAv2 with 54x fewer trainable parameters. We also demonstrate the model's emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions.\"),\n",
        "        ], scroll=ScrollMode.AUTO), actions=[TextButton(\"üòë  Alright, got it... \", on_click=close_BLIP2_i2t_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = BLIP2_i2t_help_dlg\n",
        "      BLIP2_i2t_help_dlg.open = True\n",
        "      page.update()\n",
        "    def file_picker_result(e: FilePickerResultEvent):\n",
        "        if e.files != None:\n",
        "          upload_files(e)\n",
        "    def on_upload_progress(e: FilePickerUploadEvent):\n",
        "      if e.progress == 1:\n",
        "        save_dir = os.path.join(root_dir, 'BLIP2_image2text')\n",
        "        if not os.path.exists(save_dir):\n",
        "          os.mkdir(save_dir)\n",
        "        BLIP2_image2text_prefs['folder_path'] = save_dir\n",
        "        if not slash in e.file_name:\n",
        "          fname = os.path.join(root_dir, e.file_name)\n",
        "          fpath = os.path.join(save_dir, e.file_name)\n",
        "        else:\n",
        "          fname = e.file_name\n",
        "          fpath = os.path.join(save_dir, e.file_name.rpartition(slash)[2])\n",
        "        original_img = PILImage.open(fname)\n",
        "        width, height = original_img.size\n",
        "        width, height = scale_dimensions(width, height, BLIP2_image2text_prefs['max_size'])\n",
        "        original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert(\"RGB\")\n",
        "        original_img.save(fpath)\n",
        "        #shutil.move(fname, fpath)\n",
        "        page.BLIP2_image2text_file_list.controls.append(ListTile(title=Text(fpath), dense=True))\n",
        "        page.BLIP2_image2text_file_list.update()\n",
        "    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)\n",
        "    def pick_path(e):\n",
        "        file_picker.pick_files(allow_multiple=True, allowed_extensions=[\"png\", \"PNG\", \"jpg\", \"jpeg\"], dialog_title=\"Pick Image File to Enlarge\")\n",
        "    def upload_files(e):\n",
        "        uf = []\n",
        "        if file_picker.result != None and file_picker.result.files != None:\n",
        "            for f in file_picker.result.files:\n",
        "              if page.web:\n",
        "                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))\n",
        "              else:\n",
        "                on_upload_progress(FilePickerUploadEvent(f.path, 1, \"\"))\n",
        "            file_picker.upload(uf)\n",
        "    page.overlay.append(file_picker)\n",
        "    def add_image(e):\n",
        "        save_dir = os.path.join(root_dir, 'BLIP2_image2text')\n",
        "        if not os.path.exists(save_dir):\n",
        "          os.mkdir(save_dir)\n",
        "        BLIP2_image2text_prefs['folder_path'] = save_dir\n",
        "        if image_path.value.startswith('http'):\n",
        "          import requests\n",
        "          from io import BytesIO\n",
        "          response = requests.get(image_path.value)\n",
        "          fpath = os.path.join(save_dir, image_path.value.rpartition(slash)[2])\n",
        "          original_img = PILImage.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "          width, height = original_img.size\n",
        "          width, height = scale_dimensions(width, height, BLIP2_image2text_prefs['max_size'])\n",
        "          original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert(\"RGB\")\n",
        "          original_img.save(fpath)\n",
        "          page.BLIP2_image2text_file_list.controls.append(ListTile(title=Text(fpath), dense=True))\n",
        "          page.BLIP2_image2text_file_list.update()\n",
        "        elif os.path.isfile(image_path.value):\n",
        "          fpath = os.path.join(save_dir, image_path.value.rpartition(slash)[2])\n",
        "          original_img = PILImage.open(image_path.value)\n",
        "          width, height = original_img.size\n",
        "          width, height = scale_dimensions(width, height, BLIP2_image2text_prefs['max_size'])\n",
        "          original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert(\"RGB\")\n",
        "          original_img.save(fpath)\n",
        "          #shutil.copy(image_path.value, fpath)\n",
        "          page.BLIP2_image2text_file_list.controls.append(ListTile(title=Text(fpath), dense=True))\n",
        "          page.BLIP2_image2text_file_list.update()\n",
        "        elif os.path.isdir(image_path.value):\n",
        "          for f in os.listdir(image_path.value):\n",
        "            file_path = os.path.join(image_path.value, f)\n",
        "            if os.path.isdir(file_path): continue\n",
        "            if f.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "              fpath = os.path.join(save_dir, f)\n",
        "              original_img = PILImage.open(file_path)\n",
        "              width, height = original_img.size\n",
        "              width, height = scale_dimensions(width, height, BLIP2_image2text_prefs['max_size'])\n",
        "              original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert(\"RGB\")\n",
        "              original_img.save(fpath)\n",
        "              #shutil.copy(file_path, fpath)\n",
        "              page.BLIP2_image2text_file_list.controls.append(ListTile(title=Text(fpath), dense=True))\n",
        "              page.BLIP2_image2text_file_list.update()\n",
        "        else:\n",
        "          if bool(image_path.value):\n",
        "            alert_msg(page, \"Couldn't find a valid File, Path or URL...\")\n",
        "          else:\n",
        "            pick_path(e)\n",
        "          return\n",
        "        image_path.value = \"\"\n",
        "        image_path.update()\n",
        "    page.BLIP2_image2text_list = Column([], spacing=0)\n",
        "    def add_to_prompt_list(p):\n",
        "      page.add_to_prompts(p)\n",
        "      if prefs['enable_sounds']: page.snd_drop.play()\n",
        "    def add_to_BLIP2_image2text(p):\n",
        "      page.BLIP2_image2text_list.controls.append(ListTile(title=Text(p, max_lines=3, style=TextThemeStyle.BODY_LARGE), dense=True, on_click=lambda _: add_to_prompt_list(p)))\n",
        "      page.BLIP2_image2text_list.update()\n",
        "      BLIP2_image2text_list_buttons.visible = True\n",
        "      BLIP2_image2text_list_buttons.update()\n",
        "    page.add_to_BLIP2_image2text = add_to_BLIP2_image2text\n",
        "    def add_to_list(e):\n",
        "      if prefs['enable_sounds']: page.snd_drop.play()\n",
        "      for p in page.BLIP2_image2text_list.controls:\n",
        "        page.add_to_prompts(p.title.value)\n",
        "    def clear_prompts(e):\n",
        "      if prefs['enable_sounds']: page.snd_delete.play()\n",
        "      page.BLIP2_image2text_list.controls = []\n",
        "      page.BLIP2_image2text_list.update()\n",
        "      prompts = []\n",
        "      BLIP2_image2text_list_buttons.visible = False\n",
        "      BLIP2_image2text_list_buttons.update()\n",
        "    BLIP2_image2text_list_buttons = Row([\n",
        "        ElevatedButton(content=Text(\"‚ùå   Clear Prompts\"), on_click=clear_prompts),\n",
        "        FilledButton(content=Text(\"‚ûï  Add All Prompts to List\", size=20), height=45, on_click=add_to_list),\n",
        "    ], alignment=MainAxisAlignment.SPACE_BETWEEN)\n",
        "    if len(page.BLIP2_image2text_list.controls) < 1:\n",
        "      BLIP2_image2text_list_buttons.visible = False\n",
        "    #'pretrain_flant5xxl', pretrain_opt2.7b, pretrain_opt6.7b, caption_coco_opt2.7b, caption_coco_opt6.7b, pretrain_flant5xl, caption_coco_flant5xl\n",
        "    #model_name = Dropdown(label=\"Model Nane\", width=250, options=[dropdown.Option(\"blip2_t5\"), dropdown.Option(\"blip2_opt\"), dropdown.Option(\"img2prompt_vqa\")], value=BLIP2_image2text_prefs['model_name'], on_change=lambda e: changed(e, 'model_name'))\n",
        "    model_type = Dropdown(label=\"Model Type\", width=250, options=[dropdown.Option(\"pretrain_flant5xxl\"), dropdown.Option(\"pretrain_opt2.7b\"), dropdown.Option(\"pretrain_opt6.7b\"), dropdown.Option(\"caption_coco_opt6.7b\"), dropdown.Option(\"caption_coco_flant5xl\"), dropdown.Option(\"base\")], value=BLIP2_image2text_prefs['model_type'], on_change=lambda e: changed(e, 'model_type'))\n",
        "    num_captions = Container(NumberPicker(label=\"Number of Captions: \", min=1, max=10, value=BLIP2_image2text_prefs['num_captions'], on_change=lambda e: changed(e, 'num_captions')))\n",
        "    max_row = SliderRow(label=\"Max Resolution Size\", min=256, max=1280, divisions=64, multiple=16, suffix=\"px\", pref=BLIP2_image2text_prefs, key='max_size')\n",
        "    question_prompt = TextField(label=\"Ask Question about Image (optional)\", value=BLIP2_image2text_prefs['question_prompt'], on_change=lambda e:changed(e,'question_prompt'))\n",
        "    image_path = TextField(label=\"Image File or Folder Path or URL to Examine\", value=BLIP2_image2text_prefs['image_path'], on_change=lambda e:changed(e,'image_path'), suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_path), expand=True)\n",
        "    add_image_button = ElevatedButton(content=Text(\"Add File or Folder\"), on_click=add_image)\n",
        "    page.BLIP2_image2text_file_list = Column([], tight=True, spacing=0)\n",
        "    page.BLIP2_image2text_output = Column([])\n",
        "    #clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    #clear_button.visible = len(page.BLIP2_image2text_output.controls) > 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"ü§≥  BLIP2 Image2Text Examiner\", subtitle=\"Create prompts by describing input images... Warning: Uses A LOT of VRAM, may crash session.\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with Image2Text Interrogator\", on_click=BLIP2_i2t_help)]),\n",
        "        Row([model_type, num_captions]),\n",
        "        max_row,\n",
        "        question_prompt,\n",
        "        Row([image_path, add_image_button]),\n",
        "        page.BLIP2_image2text_file_list,\n",
        "        page.BLIP2_image2text_list,\n",
        "        BLIP2_image2text_list_buttons,\n",
        "        ElevatedButton(content=Text(\"üì∏  Get Prompts from Images\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_BLIP2_image2text(page)),\n",
        "        page.BLIP2_image2text_output,\n",
        "      ],\n",
        "    ))], scroll=ScrollMode.AUTO)\n",
        "    return c\n",
        "\n",
        "\n",
        "dance_prefs = {\n",
        "    'dance_model': 'maestro-150k',\n",
        "    'installed_model': None,\n",
        "    'inference_steps': 50,\n",
        "    'batch_size': 1,\n",
        "    'seed': 0,\n",
        "    'audio_length_in_s': 4.5,\n",
        "    'community_model': 'Daft Punk',\n",
        "    'custom_model': '',\n",
        "    'train_custom': False,#https://colab.research.google.com/github/Harmonai-org/sample-generator/blob/main/Finetune_Dance_Diffusion.ipynb\n",
        "    'custom_name': '',\n",
        "    'wav_path': '',\n",
        "    'demo_every': 250,#Number of training steps between demos\n",
        "    'checkpoint_every': 500,#Number of training steps between saving model checkpoints\n",
        "    'sample_rate': 48000,#Sample rate to train at\n",
        "    'sample_size': 65536,#Number of audio samples per training sample\n",
        "    'random_crop': True,#If true, the audio samples provided will be randomly cropped to SAMPLE_SIZE samples. Turn off if you want to ensure the training data always starts at the beginning of the audio files (good for things like drum one-shots)\n",
        "    'finetune_batch_size': 2,#Batch size to fine-tune (make it as high as it can go for your GPU)\n",
        "    'accumulate_batches': 4,#Accumulate gradients over n batches, useful for training on one GPU. Effective batch size is BATCH_SIZE * ACCUM_BATCHES. Also increases the time between demos and saved checkpoints\n",
        "    'save_model': False,\n",
        "    'where_to_save_model': \"Public Library\",\n",
        "    'readme_description': '',\n",
        "}\n",
        "community_dance_diffusion_models = [\n",
        "    #{'name': 'LCD Soundsystem', 'download': 'https://drive.google.com/uc?id=1WX8nL4_x49h0OJE5iGrjXJnIJ0yvsTxI', 'ckpt':'lcd-soundsystem-200k.ckpt'}, #https://huggingface.co/Gecktendo/lcd-soundsystem/\n",
        "    {'name': 'Daft Punk', 'download': 'https://drive.google.com/uc?id=1CZjWIcL528zbZa6GrS_triob0hUy6KEs', 'ckpt':'daft-punk-241.5k.ckpt'}, #https://huggingface.co/Gecktendo/daft-punk\n",
        "    {'name': 'Vague Phrases', 'download': 'https://drive.google.com/uc?id=1nUn2qydqU7hlDUT-Skq_Ionte_8-Vdjr', 'ckpt': 'SingingInFepoch=1028-step=195500-pruned.ckpt'}, \n",
        "    {'name': 'Gesaffelstein', 'download': 'https://drive.google.com/uc?id=1-BuDzz4ajX-ufVByEX_fCkOtB00DVygB', 'ckpt':'Gesaffelstein_epoch=2537-step=445000.ckpt'},\n",
        "    {'name': 'Smash Mouth Vocals', 'download': 'https://drive.google.com/uc?id=1h3fkJnByw3mKpXUiNPWKoYtzmpeg1QEt', 'ckpt':'epoch=773-step=191500.ckpt'},\n",
        "    {'name': 'Dubstep Bass Growls', 'download': 'https://drive.google.com/file/d/104Ni-suQ0-tt2Xe9SbWjTnWFOSkT6O47', 'ckpt':'epoch=1266-step=195000.ckpt'},\n",
        "    {'name': 'Jumango Ambient', 'download': 'https://drive.google.com/file/d/1-gpOee-v7ZGFJtzr76sYTKuIKTQKTCmN', 'ckpt':'jumango-ambient-v1.ckpt'},\n",
        "    {'name': 'Serum Wavetables', 'download': 'https://drive.google.com/file/d/1l0JhA2qTaXtt5pdyv7rW1Ls4wmAFWVD1', 'ckpt':'serumwavetables-49k.ckpt'},\n",
        "    {'name': 'Textured Grooves Club', 'download': 'https://drive.google.com/file/d/13VAGMSPaIGo7FGDAtedaykhcasrFk2Z_', 'ckpt':'TexturedGroovesLargeClub_step_592200.ckpt'},\n",
        "    {'name': 'Abstract Vocal', 'download': 'https://drive.google.com/file/d/1izVPIYgPhpIT8lZtaWIO8gbTnfEL9g_J', 'ckpt':'Singing-step277000-pruned.ckpt'},\n",
        "    {'name': 'Gesaffelstein', 'download': 'https://drive.google.com/file/d/1-BuDzz4ajX-ufVByEX_fCkOtB00DVygB', 'ckpt':'Gesaffelstein_epoch=2537-step=445000.ckpt'},\n",
        "    {'name': 'Paul McCartney Vocals', 'download': 'https://drive.google.com/file/d/1-_FtUwLMnMUGLMpvtnE0EDAcUjDlXSeS', 'ckpt':'epoch=1148-step=193000.ckpt'},\n",
        "    {'name': 'Techno Kicks', 'download': 'https://drive.google.com/file/d/1-gR9QFq7ZYHn2ep0gw5IyQ6WzRZJW_tG', 'ckpt':'epoch=1296-step=441500.ckpt'},\n",
        "    {'name': 'Electronic Snare Drums', 'download': 'https://drive.google.com/file/d/1-T-PFtfyc_JUan71Px_FWsxiiuBuNN_1', 'ckpt':'epoch=1078-step=195000.ckpt'},\n",
        "    {'name': 'Electronic Snare Drums+', 'download': 'https://drive.google.com/file/d/1-50R5wwyhNrQSlvaxEqQ8CiRJPYgzGsr', 'ckpt':'epoch=1110-step=195500.ckpt'},\n",
        "    {'name': 'Electronic Kick Drums', 'download': 'https://drive.google.com/file/d/1-46jYgYfz_Jbnu-dNvepWqa7rcSP__zo', 'ckpt':'epoch=1234-step=197500.ckpt'},\n",
        "]\n",
        "dance_pipe = None\n",
        "def buildDanceDiffusion(page):\n",
        "    global dance_pipe, dance_prefs\n",
        "    def changed(e, pref=None, isInt=False):\n",
        "        if pref is not None:\n",
        "          if isInt:\n",
        "            dance_prefs[pref] = int(e.control.value)\n",
        "          else:\n",
        "            dance_prefs[pref] = e.control.value\n",
        "    def dance_help(e):\n",
        "        def close_dance_dlg(e):\n",
        "          nonlocal dance_help_dlg\n",
        "          dance_help_dlg.open = False\n",
        "          page.update()\n",
        "        dance_help_dlg = AlertDialog(title=Text(\"üíÅ   Help with Dance Diffusion\"), content=Column([\n",
        "            Text(\"HarmonAI Dance Diffusion\"),\n",
        "          ], scroll=ScrollMode.AUTO), actions=[TextButton(\"üëÑ  Sounds Good...I hope \", on_click=close_dance_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "        page.dialog = dance_help_dlg\n",
        "        dance_help_dlg.open = True\n",
        "        page.update()\n",
        "    def delete_audio(e):\n",
        "        f = e.control.data\n",
        "        if os.path.isfile(f):\n",
        "          os.remove(f)\n",
        "          for i, fl in enumerate(page.dance_file_list.controls):\n",
        "            if fl.title.value == f:\n",
        "              del page.dance_file_list.controls[i]\n",
        "              page.dance_file_list.update()\n",
        "              if f in dance_prefs['custom_wavs']:\n",
        "                dance_prefs['custom_wavs'].remove(f)\n",
        "              continue\n",
        "    def delete_all_audios(e):\n",
        "        for fl in page.dance_file_list.controls:\n",
        "          f = fl.title.value\n",
        "          if os.path.isfile(f):\n",
        "            os.remove(f)\n",
        "        page.dance_file_list.controls.clear()\n",
        "        page.dance_file_list.update()\n",
        "        dance_prefs['custom_wavs'].clear()\n",
        "    def add_file(fpath, update=True):\n",
        "        page.dance_file_list.controls.append(ListTile(title=Text(fpath), dense=True, trailing=PopupMenuButton(icon=icons.MORE_VERT,\n",
        "          items=[#TODO: View Image\n",
        "              PopupMenuItem(icon=icons.DELETE, text=\"Delete Audio\", on_click=delete_audio, data=fpath),\n",
        "              PopupMenuItem(icon=icons.DELETE_SWEEP, text=\"Delete All\", on_click=delete_all_audios, data=fpath),\n",
        "          ])))\n",
        "        dance_prefs['custom_wavs'].append(fpath)\n",
        "        if update: page.dance_file_list.update()\n",
        "    def file_picker_result(e: FilePickerResultEvent):\n",
        "        if e.files != None:\n",
        "          upload_files(e)\n",
        "    save_dir = os.path.join(root_dir, 'dance-audio')\n",
        "    def on_upload_progress(e: FilePickerUploadEvent):\n",
        "        if e.progress == 1:\n",
        "          if not os.path.exists(save_dir):\n",
        "            os.mkdir(save_dir)\n",
        "          if not slash in e.file_name:\n",
        "            fname = os.path.join(root_dir, e.file_name)\n",
        "            fpath = os.path.join(save_dir, e.file_name)\n",
        "            shutil.move(fname, fpath)\n",
        "          else:\n",
        "            fname = e.file_name\n",
        "            fpath = os.path.join(save_dir, e.file_name.rpartition(slash)[2])\n",
        "          add_file(fpath)\n",
        "    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)\n",
        "    def pick_path(e):\n",
        "        file_picker.pick_files(allow_multiple=True, allowed_extensions=[\"wav\", \"WAV\", \"mp3\", \"MP3\"], dialog_title=\"Pick Audio WAV or MP3 Files to Train\")\n",
        "    def upload_files(e):\n",
        "        uf = []\n",
        "        if file_picker.result != None and file_picker.result.files != None:\n",
        "            for f in file_picker.result.files:\n",
        "              if page.web:\n",
        "                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))\n",
        "              else:\n",
        "                on_upload_progress(FilePickerUploadEvent(f.path, 1, \"\"))\n",
        "            file_picker.upload(uf)\n",
        "    page.overlay.append(file_picker)\n",
        "    def add_wav(e):\n",
        "        if not os.path.exists(save_dir):\n",
        "            os.mkdir(save_dir)\n",
        "        if wav_path.value.startswith('http'):\n",
        "            #import requests\n",
        "            from io import BytesIO\n",
        "            #response = requests.get(wav_path.value)\n",
        "            fpath = download_file(wav_path.value)\n",
        "            #fpath = os.path.join(save_dir, wav_path.value.rpartition(slash)[2])\n",
        "            add_file(fpath)\n",
        "        elif os.path.isfile(wav_path.value):\n",
        "          fpath = os.path.join(save_dir, wav_path.value.rpartition(slash)[2])\n",
        "          shutil.copy(wav_path.value, fpath)\n",
        "          add_file(fpath)\n",
        "        elif os.path.isdir(wav_path.value):\n",
        "          for f in os.listdir(wav_path.value):\n",
        "            file_path = os.path.join(wav_path.value, f)\n",
        "            if os.path.isdir(file_path): continue\n",
        "            if f.lower().endswith(('.wav', '.WAV', '.mp3', '.MP3')):\n",
        "              fpath = os.path.join(save_dir, f)\n",
        "              shutil.copy(file_path, fpath)\n",
        "              add_file(fpath)\n",
        "        else:\n",
        "          if bool(wav_path.value):\n",
        "            alert_msg(page, \"Couldn't find a valid File, Path or URL...\")\n",
        "          else:\n",
        "            pick_path(e)\n",
        "          return\n",
        "        wav_path.value = \"\"\n",
        "        wav_path.update()\n",
        "    def load_wavs():\n",
        "        if os.path.exists(save_dir):\n",
        "          for f in os.listdir(save_dir):\n",
        "            existing = os.path.join(save_dir, f)\n",
        "            if os.path.isdir(existing): continue\n",
        "            if f.lower().endswith(('.wav', '.WAV', '.mp3', '.MP3')):\n",
        "              add_file(existing, update=False)\n",
        "    def toggle_custom(e):\n",
        "        changed(e, 'train_custom')\n",
        "        custom_box.height = None if dance_prefs['train_custom'] else 0\n",
        "        custom_box.update()\n",
        "        custom_audio_name.visible = dance_prefs['train_custom']\n",
        "        custom_audio_name.update()\n",
        "    def toggle_save(e):\n",
        "        changed(e, 'save_model')\n",
        "        where_to_save_model.visible = dance_prefs['save_model']\n",
        "        where_to_save_model.update()\n",
        "        readme_description.visible = dance_prefs['save_model']\n",
        "        readme_description.update()\n",
        "    def changed_model(e):\n",
        "      dance_prefs['dance_model'] = e.control.value\n",
        "      if e.control.value == 'Community':\n",
        "        community_dance_diffusion_model.visible = True\n",
        "        community_dance_diffusion_model.update()\n",
        "      else:\n",
        "        if community_dance_diffusion_model.visible:\n",
        "          community_dance_diffusion_model.visible = False\n",
        "          community_dance_diffusion_model.update()\n",
        "      if e.control.value == 'Custom':\n",
        "        custom_model.visible = True\n",
        "        custom_model.update()\n",
        "      else:\n",
        "        if custom_model.visible:\n",
        "          custom_model.visible = False\n",
        "          custom_model.update()\n",
        "    dance_model = Dropdown(label=\"Dance Diffusion Model\", width=250, options=[dropdown.Option(\"maestro-150k\"), dropdown.Option(\"glitch-440k\"), dropdown.Option(\"jmann-small-190k\"), dropdown.Option(\"jmann-large-580k\"), dropdown.Option(\"unlocked-250k\"), dropdown.Option(\"honk-140k\"), dropdown.Option(\"gwf-440k\"), dropdown.Option(\"Community\"), dropdown.Option(\"Custom\")], value=dance_prefs['dance_model'], on_change=changed_model)\n",
        "    community_dance_diffusion_model = Dropdown(label=\"Community Model\", width=250, options=[], value=dance_prefs['community_model'], on_change=lambda e: changed(e, 'community_model'))\n",
        "    page.community_dance_diffusion_model = community_dance_diffusion_model\n",
        "    custom_model = TextField(label=\"Custom Model Path\", value=dance_prefs['custom_model'], on_change=lambda e:changed(e,'custom_model'))\n",
        "    for c in prefs['custom_dance_diffusion_models']:\n",
        "      community_dance_diffusion_model.options.append(dropdown.Option(c['name']))\n",
        "    for c in community_dance_diffusion_models:\n",
        "      community_dance_diffusion_model.options.append(dropdown.Option(c['name']))\n",
        "    if not dance_prefs['dance_model'] == 'Community':\n",
        "      community_dance_diffusion_model.visible = False\n",
        "    if not dance_prefs['dance_model'] == 'Custom':\n",
        "      custom_model.visible = False\n",
        "    inference_row = SliderRow(label=\"Number of Inference Steps\", min=10, max=200, divisions=190, pref=dance_prefs, key='inference_steps')   \n",
        "    batch_size = TextField(label=\"Batch Size\", value=dance_prefs['batch_size'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'batch_size', isInt=True), width = 90)\n",
        "    seed = TextField(label=\"Random Seed\", value=dance_prefs['seed'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'seed', isInt=True), width = 110)\n",
        "    audio_length_in_s = TextField(label=\"Audio Length in Seconds\", value=dance_prefs['audio_length_in_s'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'audio_length_in_s'), width = 190)\n",
        "    number_row = Row([batch_size, seed, audio_length_in_s])\n",
        "\n",
        "    train_custom = Switch(label=\"Train Custom Audio   \", value=dance_prefs['train_custom'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_custom)\n",
        "    custom_audio_name = TextField(label=\"Custom Audio Name\", value=dance_prefs['custom_name'], on_change=lambda e:changed(e,'custom_name'))\n",
        "    wav_path = TextField(label=\"Audio Files or Folder Path or URL to Train\", value=dance_prefs['wav_path'], on_change=lambda e:changed(e,'wav_path'), suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_path), expand=1)\n",
        "    add_wav_button = ElevatedButton(content=Text(\"Add Audio Files\"), on_click=add_wav)\n",
        "    page.dance_file_list = Column([], tight=True, spacing=0)\n",
        "    sample_rate = Tooltip(message=\"Sample rate to train at\", content=TextField(label=\"Sample Rate\", value=dance_prefs['sample_rate'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'sample_rate', ptype='int'), width = 160))\n",
        "    sample_size = Tooltip(message=\"Number of audio samples per training sample\", content=TextField(label=\"Sample Size\", value=dance_prefs['sample_size'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'sample_size', ptype='int'), width = 160))\n",
        "    finetune_batch_size = Tooltip(message=\"Batch size to fine-tune (make it as high as it can go for your GPU)\", content=TextField(label=\"Finetune Batch Size\", value=dance_prefs['finetune_batch_size'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'finetune_batch_size', ptype='int'), width = 160))\n",
        "    accumulate_batches = Tooltip(message=\"Accumulate gradients over n batches, useful for training on one GPU. Effective batch size is BATCH_SIZE * ACCUM_BATCHES. Also increases the time between demos and saved checkpoints\", content=TextField(label=\"Accumulate Batches\", value=dance_prefs['accumulate_batches'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'accumulate_batches', ptype='int'), width = 160))\n",
        "    demo_every = Tooltip(message=\"Number of training steps between demos\", content=TextField(label=\"Demo Every\", value=dance_prefs['demo_every'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'demo_every', ptype='int'), width = 160))\n",
        "    checkpoint_every = Tooltip(message=\"Number of training steps between saving model checkpoints\", content=TextField(label=\"Checkpoint Every\", value=dance_prefs['checkpoint_every'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'checkpoint_every', ptype='int'), width = 160))\n",
        "    save_model = Tooltip(message=\"Requires WRITE access on API Key to Upload Checkpoint\", content=Switch(label=\"Save Model to HuggingFace    \", value=dance_prefs['save_model'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_save))\n",
        "    where_to_save_model = Dropdown(label=\"Where to Save Model\", width=250, options=[dropdown.Option(\"Public Library\"), dropdown.Option(\"Privately to my Profile\")], value=dance_prefs['where_to_save_model'], on_change=lambda e: changed(e, 'where_to_save_model'))\n",
        "    readme_description = TextField(label=\"Extra README Description\", value=dance_prefs['readme_description'], on_change=lambda e:changed(e,'readme_description'))\n",
        "    where_to_save_model.visible = dance_prefs['save_model']\n",
        "    readme_description.visible = dance_prefs['save_model']\n",
        "    custom_box = Container(Column([\n",
        "        Container(content=None, height=3),\n",
        "        Row([sample_rate, sample_size]),\n",
        "        Row([finetune_batch_size, accumulate_batches]),\n",
        "        Row([demo_every, checkpoint_every]),\n",
        "        Row([save_model, where_to_save_model]),\n",
        "        readme_description,\n",
        "        Text(\"Provide 3 or more ~10 second clips of Music as mp3 or wav files:\", weight=FontWeight.BOLD),\n",
        "        Row([wav_path, add_wav_button]),\n",
        "        page.dance_file_list,]), padding=padding.only(left=11), animate_size=animation.Animation(800, AnimationCurve.EASE_OUT_CIRC), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    custom_box.height = None if dance_prefs['train_custom'] else 0\n",
        "    custom_audio_name.visible = dance_prefs['train_custom']\n",
        "    load_wavs()\n",
        "    #seed = TextField(label=\"Seed\", value=dance_prefs['seed'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'seed', ptype='int'), width = 160)\n",
        "    #lambda_entropy = TextField(label=\"Lambda Entropy\", value=dreamfusdance_prefsion_prefs['lambda_entropy'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'lambda_entropy', ptype='float'), width = 160)\n",
        "    #max_steps = TextField(label=\"Max Steps\", value=dance_prefs['max_steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'max_steps', ptype='int'), width = 160)\n",
        "    page.dance_output = Column([])\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(page.dance_output.controls) > 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üëØ Create experimental music or sounds with HarmonAI trained audio models\", \"Tools to train a generative model on arbitrary audio samples...\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with DanceDiffusion Settings\", on_click=dance_help)]),\n",
        "        Row([dance_model, community_dance_diffusion_model, custom_model]),\n",
        "        inference_row,\n",
        "        number_row,\n",
        "        Row([train_custom, custom_audio_name], vertical_alignment=CrossAxisAlignment.START),\n",
        "        custom_box,\n",
        "        ElevatedButton(content=Text(\"üéµ  Run Dance Diffusion\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_dance_diffusion(page)),\n",
        "        page.dance_output,\n",
        "      ]\n",
        "    ))], scroll=ScrollMode.AUTO)\n",
        "    return c\n",
        "\n",
        "audio_diffusion_prefs = {\n",
        "    'audio_file': '',\n",
        "    'file_name': '',\n",
        "    'audio_model': 'teticio/audio-diffusion-ddim-256',\n",
        "    'scheduler': 'DDIM', #DDPM\n",
        "    'steps': 50, #number of de-noising steps (defaults to 50 for DDIM, 1000 for DDPM)\n",
        "    'start_step': 0, #step to start from\n",
        "    'slice': 0, #slice number of audio to convert\n",
        "    'eta': 0.2, #parameter between 0 and 1 used with DDIM scheduler\n",
        "    'mask_start_secs': 0.0, #number of seconds of audio to mask (not generate) at start\n",
        "    'mask_end_secs': 0.0, #number of seconds of audio to mask (not generate) at end\n",
        "    'seed': 0,\n",
        "    'audio_name': '',\n",
        "    'wav_path': '',\n",
        "    'batch_size': 1,\n",
        "    'batch_folder_name': '',\n",
        "    'file_prefix': 'ad-',\n",
        "    'loaded_model': '',\n",
        "}\n",
        "\n",
        "def buildAudioDiffusion(page):\n",
        "    global prefs, audio_diffusion_prefs\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "        if pref is not None:\n",
        "          try:\n",
        "            if ptype == \"int\":\n",
        "              audio_diffusion_prefs[pref] = int(e.control.value)\n",
        "            elif ptype == \"float\":\n",
        "              audio_diffusion_prefs[pref] = float(e.control.value)\n",
        "            else:\n",
        "              audio_diffusion_prefs[pref] = e.control.value\n",
        "          except Exception:\n",
        "            alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "            pass\n",
        "    def add_to_audio_diffusion_output(o):\n",
        "        page.audio_diffusion_output.controls.append(o)\n",
        "        page.audio_diffusion_output.update()\n",
        "    def clear_output(e):\n",
        "        if prefs['enable_sounds']: page.snd_delete.play()\n",
        "        page.audio_diffusion_output.controls = []\n",
        "        page.audio_diffusion_output.update()\n",
        "        clear_button.visible = False\n",
        "        clear_button.update()\n",
        "    def audio_diffusion_help(e):\n",
        "        def close_audio_diffusion_dlg(e):\n",
        "          nonlocal audio_diffusion_help_dlg\n",
        "          audio_diffusion_help_dlg.open = False\n",
        "          page.update()\n",
        "        audio_diffusion_help_dlg = AlertDialog(title=Text(\"üíÅ   Help with Audio-Diffusion\"), content=Column([\n",
        "            Text(\"Audio Diffusion leverages the recent advances in image generation using diffusion models by converting audio samples to and from mel spectrogram images.\"),\n",
        "            Markdown(\"The original codebase of this implementation can be found [here](https://github.com/teticio/audio-diffusion), including training scripts and example notebooks.\\n[Audio Diffusion](https://github.com/teticio/audio-diffusion) by Robert Dargavel Smith.\", on_tap_link=lambda e: e.page.launch_url(e.data)),\n",
        "          ], scroll=ScrollMode.AUTO), actions=[TextButton(\"üîä  Sounds Groovie... \", on_click=close_audio_diffusion_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "        page.dialog = audio_diffusion_help_dlg\n",
        "        audio_diffusion_help_dlg.open = True\n",
        "        page.update()\n",
        "    def file_picker_result(e: FilePickerResultEvent):\n",
        "        if e.files != None:\n",
        "          upload_files(e)\n",
        "    def on_upload_progress(e: FilePickerUploadEvent):\n",
        "        if e.progress == 1:\n",
        "            if not slash in e.file_name:\n",
        "              fname = os.path.join(root_dir, e.file_name)\n",
        "              audio_diffusion_prefs['file_name'] = e.file_name.rpartition('.')[0]\n",
        "            else:\n",
        "              fname = e.file_name\n",
        "              fpath = os.path.join(root_dir, e.file_name.rpartition(slash)[2])\n",
        "              audio_diffusion_prefs['file_name'] = e.file_name.rparition(slash)[2].rpartition('.')[0]\n",
        "            audio_file.value = fname\n",
        "            audio_file.update()\n",
        "            audio_diffusion_prefs['audio_file'] = fname\n",
        "            page.update()\n",
        "    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)\n",
        "    def upload_files(e):\n",
        "        uf = []\n",
        "        if file_picker.result != None and file_picker.result.files != None:\n",
        "            for f in file_picker.result.files:\n",
        "              if page.web:\n",
        "                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))\n",
        "              else:\n",
        "                on_upload_progress(FilePickerUploadEvent(f.path, 1, \"\"))\n",
        "            file_picker.upload(uf)\n",
        "    page.overlay.append(file_picker)\n",
        "    def pick_audio(e):\n",
        "        file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"mp3\", \"wav\"], dialog_title=\"Pick Init Audio File\")\n",
        "    audio_file = TextField(label=\"Input Audio File (optional)\", value=audio_diffusion_prefs['audio_file'], on_change=lambda e:changed(e,'audio_file'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_audio))\n",
        "    def change_eta(e):\n",
        "        changed(e, 'eta', ptype=\"float\")\n",
        "        eta_value.value = f\" {audio_diffusion_prefs['eta']}\"\n",
        "        eta_value.update()\n",
        "        eta_row.update()\n",
        "    steps_row = SliderRow(label=\"Number of Inference Steps\", min=1, max=100, divisions=99, pref=audio_diffusion_prefs, key='steps', tooltip=\"The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.\")   \n",
        "    eta = Slider(min=0.0, max=1.0, divisions=20, label=\"{value}\", value=float(audio_diffusion_prefs['eta']), tooltip=\"The weight of noise for added noise in a diffusion step. Its value is between 0.0 and 1.0 - 0.0 is DDIM and 1.0 is DDPM scheduler respectively.\", expand=True, on_change=change_eta)\n",
        "    page.etas.append(eta)\n",
        "    eta_value = Text(f\" {audio_diffusion_prefs['eta']}\", weight=FontWeight.BOLD)\n",
        "    eta_row = Row([Text(\"DDIM ETA:\"), eta_value, eta,])\n",
        "    audio_model = Dropdown(label=\"Audio Model\", width=400, options=[dropdown.Option(\"teticio/audio-diffusion-ddim-256\"), dropdown.Option(\"teticio/audio-diffusion-breaks-256\"), dropdown.Option(\"teticio/audio-diffusion-instrumental-hiphop-256\"), dropdown.Option(\"teticio/latent-audio-diffusion-256\"), dropdown.Option(\"teticio/latent-audio-diffusion-ddim-256\"), dropdown.Option(\"teticio/conditional-latent-audio-diffusion-512\")], value=audio_diffusion_prefs['audio_model'], on_change=lambda e: changed(e, 'audio_model'))\n",
        "    scheduler = Dropdown(label=\"De-noise Scheduler\", width=250, options=[dropdown.Option(\"DDIM\"), dropdown.Option(\"DDPM\")], value=audio_diffusion_prefs['scheduler'], on_change=lambda e: changed(e, 'scheduler'))\n",
        "    slice = TextField(label=\"Slice of Audio\", value=audio_diffusion_prefs['slice'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'slice', ptype='int'), width = 130)\n",
        "    start_step = TextField(label=\"Starting Step\", value=audio_diffusion_prefs['start_step'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'start_step', ptype='int'), width = 130)\n",
        "    mask_start_secs = TextField(label=\"Mask Start (s)\", value=audio_diffusion_prefs['mask_start_secs'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'mask_start_secs', ptype='float'), width = 130)\n",
        "    mask_end_secs = TextField(label=\"Mask End (s)\", value=audio_diffusion_prefs['mask_end_secs'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'mask_end_secs', ptype='float'), width = 130)\n",
        "    audio_name = TextField(label=\"Audio File Name\", value=audio_diffusion_prefs['audio_name'], on_change=lambda e:changed(e,'audio_name'))\n",
        "    batch_folder_name = TextField(label=\"Batch Folder Name\", value=audio_diffusion_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    file_prefix = TextField(label=\"Filename Prefix\", value=audio_diffusion_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))\n",
        "    batch_size = NumberPicker(label=\"Batch Size:  \", min=1, max=5, value=audio_diffusion_prefs['batch_size'], on_change=lambda e: changed(e, 'batch_size'))\n",
        "    seed = TextField(label=\"Seed\", value=audio_diffusion_prefs['seed'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'seed', ptype='int'), width = 120)\n",
        "    page.audio_diffusion_output = Column([])\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(page.audio_diffusion_output.controls) > 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üé∂  Audio Diffusion Modeling\", \"Converts Audio Samples to and from Mel Spectrogram Images...\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with Audio Diffusion-TTS Settings\", on_click=audio_diffusion_help)]),\n",
        "        audio_file,\n",
        "        audio_model,\n",
        "        #scheduler,\n",
        "        steps_row,\n",
        "        eta_row,\n",
        "        Row([slice, start_step, mask_start_secs, mask_end_secs]),\n",
        "        Row([batch_size, seed, file_prefix]),\n",
        "        Row([audio_name, batch_folder_name]),\n",
        "        ElevatedButton(content=Text(\"ü™ó  Run Audio Diffusion\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_audio_diffusion(page)),\n",
        "        page.audio_diffusion_output,\n",
        "        clear_button,\n",
        "      ]\n",
        "    ))], scroll=ScrollMode.AUTO)\n",
        "    return c\n",
        "\n",
        "\n",
        "dreamfusion_prefs = {\n",
        "    'prompt_text': '', \n",
        "    'training_iters': 5000,\n",
        "    'learning_rate': 0.001,\n",
        "    'training_nerf_resolution': 64,\n",
        "    'seed': 0,\n",
        "    'lambda_entropy': 0.0001,\n",
        "    'max_steps': 512,\n",
        "    'checkpoint': 'latest',\n",
        "    'workspace': 'trial',\n",
        "}\n",
        "\n",
        "def buildDreamFusion(page):\n",
        "    global prefs, dreamfusion_prefs\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            dreamfusion_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            dreamfusion_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            dreamfusion_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def add_to_dreamfusion_output(o):\n",
        "      page.dreamfusion_output.controls.append(o)\n",
        "      page.dreamfusion_output.update()\n",
        "    def clear_output(e):\n",
        "      if prefs['enable_sounds']: page.snd_delete.play()\n",
        "      page.dreamfusion_output.controls = []\n",
        "      page.dreamfusion_output.update()\n",
        "      clear_button.visible = False\n",
        "      clear_button.update()\n",
        "    def df_help(e):\n",
        "      def close_df_dlg(e):\n",
        "        nonlocal df_help_dlg\n",
        "        df_help_dlg.open = False\n",
        "        page.update()\n",
        "      df_help_dlg = AlertDialog(title=Text(\"üíÅ   Help with DreamFusion\"), content=Column([\n",
        "          Text(\"It's difficult to explain exactly what all these parameters do, but keep it close to defaults, keep prompt simple, or experiment to see what's what, we don't know.\"),\n",
        "          Text('It takes about 0.7s per training step, so the default 5000 training steps take around 1 hour to finish. A larger Training_iters usually leads to better results.'),\n",
        "          Text('If CUDA OOM, try to decrease Max_steps and Training_nerf_resolution.'),\n",
        "          Text('If the NeRF fails to learn anything (empty scene, only background), try to decrease Lambda_entropy which regularizes the learned opacity.')\n",
        "        ], scroll=ScrollMode.AUTO), actions=[TextButton(\"üòä  So Exciting... \", on_click=close_df_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = df_help_dlg\n",
        "      df_help_dlg.open = True\n",
        "      page.update()\n",
        "    prompt_text = TextField(label=\"Prompt Text\", value=dreamfusion_prefs['prompt_text'], on_change=lambda e:changed(e,'prompt_text'))\n",
        "    training_iters = TextField(label=\"Training Iterations\", value=dreamfusion_prefs['training_iters'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'training_iters', ptype='int'), width = 160)\n",
        "    learning_rate = TextField(label=\"Learning Rate\", value=dreamfusion_prefs['learning_rate'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'learning_rate', ptype='float'), width = 160)\n",
        "    training_nerf_resolution = TextField(label=\"Training NERF Res\", value=dreamfusion_prefs['training_nerf_resolution'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'training_nerf_resolution', ptype='int'), width = 160)\n",
        "    seed = TextField(label=\"Seed\", value=dreamfusion_prefs['seed'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'seed', ptype='int'), width = 160)\n",
        "    lambda_entropy = TextField(label=\"Lambda Entropy\", value=dreamfusion_prefs['lambda_entropy'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'lambda_entropy', ptype='float'), width = 160)\n",
        "    max_steps = TextField(label=\"Max Steps\", value=dreamfusion_prefs['max_steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'max_steps', ptype='int'), width = 160)\n",
        "    workspace = TextField(label=\"Workspace Folder\", value=dreamfusion_prefs['workspace'], on_change=lambda e:changed(e,'workspace'))\n",
        "    page.dreamfusion_output = Column([])\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(page.dreamfusion_output.controls) > 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üóø  Create experimental DreamFusion 3D Model and Video\", \"Provide a prompt to render a model. Warning: May take over an hour to run the training...\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with DreamFusion Settings\", on_click=df_help)]),\n",
        "        prompt_text,\n",
        "        Row([training_iters,learning_rate, lambda_entropy]),\n",
        "        Row([seed, training_nerf_resolution, max_steps]),\n",
        "        Row([workspace]),\n",
        "        ElevatedButton(content=Text(\"üî®  Run DreamFusion\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_dreamfusion(page)),\n",
        "        page.dreamfusion_output,\n",
        "        clear_button,\n",
        "      ]\n",
        "    ))], scroll=ScrollMode.AUTO)\n",
        "    return c\n",
        "\n",
        "point_e_prefs = {\n",
        "    'prompt_text': '',\n",
        "    'init_image': '',\n",
        "    'guidance_scale': 3.0,\n",
        "    'base_model': 'base40M-textvec', #'base40M', 'base300M' or 'base1B'\n",
        "    'upsample': False,\n",
        "    'batch_size': 1,\n",
        "    'batch_folder_name': '',\n",
        "    'seed': 0,\n",
        "    'max_steps': 512,\n",
        "}\n",
        "\n",
        "def buildPoint_E(page):\n",
        "    global prefs, point_e_prefs\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            point_e_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            point_e_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            point_e_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def add_to_point_e_output(o):\n",
        "      page.point_e_output.controls.append(o)\n",
        "      page.point_e_output.update()\n",
        "    def clear_output(e):\n",
        "      if prefs['enable_sounds']: page.snd_delete.play()\n",
        "      page.point_e_output.controls = []\n",
        "      page.point_e_output.update()\n",
        "      clear_button.visible = False\n",
        "      clear_button.update()\n",
        "    def df_help(e):\n",
        "      def close_df_dlg(e):\n",
        "        nonlocal df_help_dlg\n",
        "        df_help_dlg.open = False\n",
        "        page.update()\n",
        "      df_help_dlg = AlertDialog(title=Text(\"üíÅ   Help with OpenAI Point-E\"), content=Column([\n",
        "          Markdown(\"This is an interface for running the [official codebase](https://github.com/openai/point-e) for point cloud diffusion models and SDF regression models described in [Point-E: A System for Generating 3D Point Clouds from Complex Prompts](https://arxiv.org/abs/2212.08751). These models were trained and released by OpenAI. Following [Model Cards for Model Reporting (Mitchell et al.)](https://arxiv.org/abs/1810.03993), we're providing some information about how the models were trained and evaluated.\", on_tap_link=lambda e: e.page.launch_url(e.data)),\n",
        "          Text(\"The Point-E models are trained for use as point cloud diffusion models and SDF regression models. Our image-conditional models are often capable of producing coherent 3D point clouds, given a single rendering of a 3D object. However, the models sometimes fail to do so, either producing incorrect geometry where the rendering is occluded, or producing geometry that is inconsistent with visible parts of the rendering. The resulting point clouds are relatively low-resolution, and are often noisy and contain defects such as outliers or cracks. Our text-conditional model is sometimes capable of producing 3D point clouds which can be recognized as the provided text description, especially when the text description is simple. However, we find that this model fails to generalize to complex prompts or unusual objects.\"),\n",
        "          Text(\"While recent work on text-conditional 3D object generation has shown promising results, the state-of-the-art methods typically require multiple GPU-hours to produce a single sample. This is in stark contrast to state-of-the-art generative image models, which produce samples in a number of seconds or minutes. In this paper, we explore an alternative method for 3D object generation which produces 3D models in only 1-2 minutes on a single GPU. Our method first generates a single synthetic view using a text-to-image diffusion model, and then produces a 3D point cloud using a second diffusion model which conditions on the generated image. While our method still falls short of the state-of-the-art in terms of sample quality, it is one to two orders of magnitude faster to sample from, offering a practical trade-off for some use cases. We release our pre-trained point cloud diffusion models, as well as evaluation code and models for you to use.\"),\n",
        "        ], scroll=ScrollMode.AUTO), actions=[TextButton(\"‚òùÔ∏è  Good Points... \", on_click=close_df_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = df_help_dlg\n",
        "      df_help_dlg.open = True\n",
        "      page.update()\n",
        "    def file_picker_result(e: FilePickerResultEvent):\n",
        "        if e.files != None:\n",
        "          upload_files(e)\n",
        "    def on_upload_progress(e: FilePickerUploadEvent):\n",
        "      if e.progress == 1:\n",
        "        if not slash in e.file_name:\n",
        "          fname = os.path.join(root_dir, e.file_name)\n",
        "          point_e_prefs['file_name'] = e.file_name.rpartition('.')[0]\n",
        "        else:\n",
        "          fname = e.file_name\n",
        "          point_e_prefs['file_name'] = e.file_name.rparition(slash)[2].rpartition('.')[0]\n",
        "        init_image.value = fname\n",
        "        init_image.update()\n",
        "        point_e_prefs['init_image'] = fname\n",
        "        page.update()\n",
        "    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)\n",
        "    def upload_files(e):\n",
        "        uf = []\n",
        "        if file_picker.result != None and file_picker.result.files != None:\n",
        "            for f in file_picker.result.files:\n",
        "              if page.web:\n",
        "                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))\n",
        "              else:\n",
        "                on_upload_progress(FilePickerUploadEvent(f.path, 1, \"\"))\n",
        "            file_picker.upload(uf)\n",
        "    page.overlay.append(file_picker)\n",
        "    #page.overlay.append(pick_files_dialog)\n",
        "    def pick_original(e):\n",
        "        file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"png\", \"PNG\", \"jpg\", \"jpeg\"], dialog_title=\"Pick Original Image File\")\n",
        "    prompt_text = TextField(label=\"Prompt Text\", value=point_e_prefs['prompt_text'], on_change=lambda e:changed(e,'prompt_text'))\n",
        "    init_image = TextField(label=\"Sample Image (instead of prompt)\", value=point_e_prefs['init_image'], on_change=lambda e:changed(e,'init_image'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_original))\n",
        "    base_model = Dropdown(label=\"Base Model\", width=250, options=[dropdown.Option(\"base40M-imagevec\"), dropdown.Option(\"base40M-textvec\"), dropdown.Option(\"base40M\"), dropdown.Option(\"base300M\"), dropdown.Option(\"base1B\")], value=point_e_prefs['base_model'], on_change=lambda e: changed(e, 'base_model'))\n",
        "    batch_folder_name = TextField(label=\"3D Model Folder Name\", value=point_e_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    #batch_size = TextField(label=\"Batch Size\", value=point_e_prefs['batch_size'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'batch_size', isInt=True), width = 90)\n",
        "    batch_size = NumberPicker(label=\"Batch Size: \", min=1, max=5, value=point_e_prefs['batch_size'], on_change=lambda e: changed(e, 'batch_size'))\n",
        "    guidance = SliderRow(label=\"Guidance Scale\", min=0, max=10, divisions=20, round=1, pref=point_e_prefs, key='guidance_scale')\n",
        "    #seed = TextField(label=\"Seed\", value=point_e_prefs['seed'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'seed', ptype='int'), width = 160)\n",
        "    page.point_e_output = Column([])\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(page.point_e_output.controls) > 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üëÜ  Point-E 3D Point Clouds\", \"Provide a Prompt or Image to render from a CLIP ViT-L/14 diffusion model...\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with Point-E Settings\", on_click=df_help)]),\n",
        "        prompt_text,\n",
        "        init_image,\n",
        "        base_model,\n",
        "        Row([batch_folder_name, batch_size]),\n",
        "        guidance,\n",
        "        ElevatedButton(content=Text(\"üêû  Run Point-E\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_point_e(page)),\n",
        "        page.point_e_output,\n",
        "        clear_button,\n",
        "      ]\n",
        "    ))], scroll=ScrollMode.AUTO)\n",
        "    return c\n",
        "\n",
        "shap_e_prefs = {\n",
        "    'prompt_text': '',\n",
        "    'init_image': '',\n",
        "    'guidance_scale': 15.0,\n",
        "    'base_model': 'base40M-textvec', #'base40M', 'base300M' or 'base1B'\n",
        "    'render_mode': 'NeRF', #STF\n",
        "    'use_karras': True,\n",
        "    'karras_steps': 64,\n",
        "    'size': 64,\n",
        "    'save_frames': False,\n",
        "    'batch_size': 1,\n",
        "    'batch_folder_name': '',\n",
        "    #'seed': 0,\n",
        "    #'max_steps': 512,\n",
        "}\n",
        "\n",
        "def buildShap_E(page):\n",
        "    global prefs, shap_e_prefs\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            shap_e_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            shap_e_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            shap_e_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def clear_output(e):\n",
        "      if prefs['enable_sounds']: page.snd_delete.play()\n",
        "      page.shap_e_output.controls = []\n",
        "      page.shap_e_output.update()\n",
        "      clear_button.visible = False\n",
        "      clear_button.update()\n",
        "    def df_help(e):\n",
        "      def close_df_dlg(e):\n",
        "        nonlocal df_help_dlg\n",
        "        df_help_dlg.open = False\n",
        "        page.update()\n",
        "      df_help_dlg = AlertDialog(title=Text(\"üíÅ   Help with OpenAI Shap-E\"), content=Column([\n",
        "          Text(\"We present Shap-E, a conditional generative model for 3D assets. Unlike recent work on 3D generative models which produce a single output representation, Shap-E directly generates the parameters of implicit functions that can be rendered as both textured meshes and neural radiance fields. We train Shap-E in two stages: first, we train an encoder that deterministically maps 3D assets into the parameters of an implicit function; second, we train a conditional diffusion model on outputs of the encoder. When trained on a large dataset of paired 3D and text data, our resulting models are capable of generating complex and diverse 3D assets in a matter of seconds. When compared to Point-E, an explicit generative model over point clouds, Shap-E converges faster and reaches comparable or better sample quality despite modeling a higher-dimensional, multi-representation output space.\"),\n",
        "          Markdown(\"[GitHub Page](https://github.com/openai/shap-e) - [Read the Paper](https://arxiv.org/pdf/2305.02463.pdf)\", on_tap_link=lambda e: e.page.launch_url(e.data)),\n",
        "        ], scroll=ScrollMode.AUTO), actions=[TextButton(\"üö¥  Shaping up... \", on_click=close_df_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = df_help_dlg\n",
        "      df_help_dlg.open = True\n",
        "      page.update()\n",
        "    def file_picker_result(e: FilePickerResultEvent):\n",
        "        if e.files != None:\n",
        "          upload_files(e)\n",
        "    def on_upload_progress(e: FilePickerUploadEvent):\n",
        "      if e.progress == 1:\n",
        "        if not slash in e.file_name:\n",
        "          fname = os.path.join(root_dir, e.file_name)\n",
        "          shap_e_prefs['file_name'] = e.file_name.rpartition('.')[0]\n",
        "        else:\n",
        "          fname = e.file_name\n",
        "          shap_e_prefs['file_name'] = e.file_name.rparition(slash)[2].rpartition('.')[0]\n",
        "        init_image.value = fname\n",
        "        init_image.update()\n",
        "        shap_e_prefs['init_image'] = fname\n",
        "        page.update()\n",
        "    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)\n",
        "    def upload_files(e):\n",
        "        uf = []\n",
        "        if file_picker.result != None and file_picker.result.files != None:\n",
        "            for f in file_picker.result.files:\n",
        "              if page.web:\n",
        "                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))\n",
        "              else:\n",
        "                on_upload_progress(FilePickerUploadEvent(f.path, 1, \"\"))\n",
        "            file_picker.upload(uf)\n",
        "    page.overlay.append(file_picker)\n",
        "    #page.overlay.append(pick_files_dialog)\n",
        "    def pick_original(e):\n",
        "        file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"png\", \"PNG\", \"jpg\", \"jpeg\"], dialog_title=\"Pick Original Image File\")\n",
        "    prompt_text = TextField(label=\"Prompt Text\", value=shap_e_prefs['prompt_text'], on_change=lambda e:changed(e,'prompt_text'))\n",
        "    init_image = TextField(label=\"Sample Image (optional, instead of prompt)\", value=shap_e_prefs['init_image'], on_change=lambda e:changed(e,'init_image'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_original))\n",
        "    #base_model = Dropdown(label=\"Base Model\", width=250, options=[dropdown.Option(\"base40M-imagevec\"), dropdown.Option(\"base40M-textvec\"), dropdown.Option(\"base40M\"), dropdown.Option(\"base300M\"), dropdown.Option(\"base1B\")], value=shap_e_prefs['base_model'], on_change=lambda e: changed(e, 'base_model'))\n",
        "    render_mode = Dropdown(label=\"Render Mode\", width=250, options=[dropdown.Option(\"NeRF\"), dropdown.Option(\"STF\")], value=shap_e_prefs['render_mode'], on_change=lambda e: changed(e, 'render_mode'))\n",
        "    size = SliderRow(label=\"Size of Render\", min=32, max=512, divisions=15, multiple=32, tooltip=\"Higher values take longer to render.\", suffix=\"px\", pref=shap_e_prefs, key='size')\n",
        "    karras_steps = SliderRow(label=\"Karras Steps\", min=1, max=100, divisions=99, round=0, pref=shap_e_prefs, key='karras_steps')\n",
        "    batch_folder_name = TextField(label=\"3D Model Folder Name\", value=shap_e_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    #batch_size = TextField(label=\"Batch Size\", value=shap_e_prefs['batch_size'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'batch_size', isInt=True), width = 90)\n",
        "    batch_size = NumberPicker(label=\"Batch Size: \", min=1, max=5, value=shap_e_prefs['batch_size'], on_change=lambda e: changed(e, 'batch_size'))\n",
        "    guidance = SliderRow(label=\"Guidance Scale\", min=0, max=10, divisions=20, round=1, pref=shap_e_prefs, key='guidance_scale')\n",
        "    save_frames = Checkbox(label=\"Save Preview Frames\", tooltip=\"Saves PNG Sequence of camera rotation, same as animated gif preview.\", value=shap_e_prefs['save_frames'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'save_frames'))\n",
        "    #seed = TextField(label=\"Seed\", value=shap_e_prefs['seed'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'seed', ptype='int'), width = 160)\n",
        "    page.shap_e_output = Column([])\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(page.shap_e_output.controls) > 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üßä  Shap-E 3D Mesh\", \"Provide a Prompt or Image to Generate Conditional 3D PLY Models...\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with Shap-E Settings\", on_click=df_help)]),\n",
        "        prompt_text,\n",
        "        init_image,\n",
        "        Row([render_mode, save_frames]),\n",
        "        guidance,\n",
        "        karras_steps,\n",
        "        size,\n",
        "        Row([batch_folder_name, batch_size]),\n",
        "        ElevatedButton(content=Text(\"ü™Ä  Run Shap-E\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_shap_e(page)),\n",
        "        page.shap_e_output,\n",
        "        clear_button,\n",
        "      ]\n",
        "    ))], scroll=ScrollMode.AUTO)\n",
        "    return c\n",
        "\n",
        "\n",
        "instant_ngp_prefs = {\n",
        "    'train_steps': 2000, #Total number of training steps to perform.  If provided, overrides num_train_epochs.\n",
        "    'sharpen': 0.0,\n",
        "    'exposure': 0.0,\n",
        "    'vr_mode': False,\n",
        "    'name_of_your_model': '',\n",
        "    'save_model': True,\n",
        "    'where_to_save_model': 'Public HuggingFace',\n",
        "    'resolution': 768,\n",
        "    'image_path': '',\n",
        "    'readme_description': '',\n",
        "    'urls': [],\n",
        "}\n",
        "\n",
        "def buildInstantNGP(page):\n",
        "    global prefs, instant_ngp_prefs\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "        if pref is not None:\n",
        "          try:\n",
        "            if ptype == \"int\":\n",
        "              instant_ngp_prefs[pref] = int(e.control.value)\n",
        "            elif ptype == \"float\":\n",
        "              instant_ngp_prefs[pref] = float(e.control.value)\n",
        "            else:\n",
        "              instant_ngp_prefs[pref] = e.control.value\n",
        "          except Exception:\n",
        "            alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "            pass\n",
        "    def add_to_instant_ngp_output(o):\n",
        "        page.instant_ngp_output.controls.append(o)\n",
        "        page.instant_ngp_output.update()\n",
        "    def clear_output(e):\n",
        "        if prefs['enable_sounds']: page.snd_delete.play()\n",
        "        page.instant_ngp_output.controls = []\n",
        "        page.instant_ngp_output.update()\n",
        "        clear_button.visible = False\n",
        "        clear_button.update()\n",
        "    def instant_ngp_help(e):\n",
        "        def close_instant_ngp_dlg(e):\n",
        "          nonlocal instant_ngp_help_dlg\n",
        "          instant_ngp_help_dlg.open = False\n",
        "          page.update()\n",
        "        instant_ngp_help_dlg = AlertDialog(title=Text(\"üíÅ   Help with Instant-NGP\"), content=Column([\n",
        "            Text(\"Ever wanted to train a NeRF model of a fox in under 5 seconds? Or fly around a scene captured from photos of a factory robot? Of course you have! Here you will find an implementation of four neural graphics primitives, being neural radiance fields (NeRF), signed distance functions (SDFs), neural images, and neural volumes. In each case, we train and render a MLP with multiresolution hash input encoding using the tiny-cuda-nn framework.\"),\n",
        "            Text(\"We demonstrate near-instant training of neural graphics primitives on a single GPU for multiple tasks. In gigapixel image we represent an image by a neural network. SDF learns a signed distance function in 3D space whose zero level-set represents a 2D surface. NeRF [Mildenhall et al. 2020] uses 2D images and their camera poses to reconstruct a volumetric radiance-and-density field that is visualized using ray marching. Lastly, neural volume learns a denoised radiance and density field directly from a volumetric path tracer. In all tasks, our encoding and its efficient implementation provide clear benefits: instant training, high quality, and simplicity. Our encoding is task-agnostic: we use the same implementation and hyperparameters across all tasks and only vary the hash table size which trades off quality and performance.\"),\n",
        "            Text(\"Neural graphics primitives, parameterized by fully connected neural networks, can be costly to train and evaluate. We reduce this cost with a versatile new input encoding that permits the use of a smaller network without sacrificing quality, thus significantly reducing the number of floating point and memory access operations. A small neural network is augmented by a multiresolution hash table of trainable feature vectors whose values are optimized through stochastic gradient descent. The multiresolution structure allows the network to disambiguate hash collisions, making for a simple architecture that is trivial to parallelize on modern GPUs. We leverage this parallelism by implementing the whole system using fully-fused CUDA kernels with a focus on minimizing wasted bandwidth and compute operations. We achieve a combined speedup of several orders of magnitude, enabling training of high-quality neural graphics primitives in a matter of seconds, and rendering in tens of milliseconds at a resolution of 1920x1080.\"),\n",
        "            Markdown(\"[Project page](https://nvlabs.github.io/instant-ngp) / [Paper](https://nvlabs.github.io/instant-ngp/assets/mueller2022instant.pdf) / [Video](https://nvlabs.github.io/instant-ngp/assets/mueller2022instant.mp4) / [Presentation](https://tom94.net/data/publications/mueller22instant/mueller22instant-gtc.mp4) / [Real-Time Live](https://tom94.net/data/publications/mueller22instant/mueller22instant-rtl.mp4) / [BibTeX](https://nvlabs.github.io/instant-ngp/assets/mueller2022instant.bib)\", on_tap_link=lambda e: e.page.launch_url(e.data)),\n",
        "          ], scroll=ScrollMode.AUTO), actions=[TextButton(\"üå†  Like Magic... \", on_click=close_instant_ngp_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "        page.dialog = instant_ngp_help_dlg\n",
        "        instant_ngp_help_dlg.open = True\n",
        "        page.update()\n",
        "    def delete_image(e):\n",
        "        f = e.control.data\n",
        "        if os.path.isfile(f):\n",
        "          os.remove(f)\n",
        "          for i, fl in enumerate(page.instant_ngp_file_list.controls):\n",
        "            if fl.title.value == f:\n",
        "              del page.instant_ngp_file_list.controls[i]\n",
        "              page.instant_ngp_file_list.update()\n",
        "              continue\n",
        "    def delete_all_images(e):\n",
        "        for fl in page.instant_ngp_file_list.controls:\n",
        "          f = fl.title.value\n",
        "          if os.path.isfile(f):\n",
        "            os.remove(f)\n",
        "        page.instant_ngp_file_list.controls.clear()\n",
        "        page.instant_ngp_file_list.update()\n",
        "    def image_details(e):\n",
        "        img = e.control.data\n",
        "        alert_msg(e.page, \"Image Details\", content=Image(src=img), sound=False)\n",
        "    def add_file(fpath, update=True):\n",
        "        page.instant_ngp_file_list.controls.append(ListTile(title=Text(fpath), dense=False, trailing=PopupMenuButton(icon=icons.MORE_VERT,\n",
        "          items=[\n",
        "              PopupMenuItem(icon=icons.INFO, text=\"Image Details\", on_click=image_details, data=fpath),\n",
        "              PopupMenuItem(icon=icons.DELETE, text=\"Delete Image\", on_click=delete_image, data=fpath),\n",
        "              PopupMenuItem(icon=icons.DELETE_SWEEP, text=\"Delete All\", on_click=delete_all_images, data=fpath),\n",
        "          ]), data=fpath, on_click=image_details))\n",
        "        if update: page.instant_ngp_file_list.update()\n",
        "    def file_picker_result(e: FilePickerResultEvent):\n",
        "        if e.files != None:\n",
        "          upload_files(e)\n",
        "    save_dir = os.path.join(root_dir, 'my_ngp')\n",
        "    def on_upload_progress(e: FilePickerUploadEvent):\n",
        "        if e.progress == 1:\n",
        "          if not os.path.exists(save_dir):\n",
        "            os.mkdir(save_dir)\n",
        "          if not slash in e.file_name:\n",
        "            fname = os.path.join(root_dir, e.file_name)\n",
        "            fpath = os.path.join(save_dir, e.file_name)\n",
        "          else:\n",
        "            fname = e.file_name\n",
        "            fpath = os.path.join(save_dir, e.file_name.rpartition(slash)[2])\n",
        "          original_img = PILImage.open(fname)\n",
        "          width, height = original_img.size\n",
        "          width, height = scale_dimensions(width, height, instant_ngp_prefs['resolution'])\n",
        "          original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert(\"RGB\")\n",
        "          original_img.save(fpath)\n",
        "          if page.web:\n",
        "            os.remove(fname)\n",
        "          #shutil.move(fname, fpath)\n",
        "          add_file(fpath)\n",
        "    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)\n",
        "    def pick_path(e):\n",
        "        file_picker.pick_files(allow_multiple=True, allowed_extensions=[\"png\", \"PNG\", \"jpg\", \"jpeg\", 'obj', 'stl', 'nvdb'], dialog_title=\"Pick Image File to Enlarge\")\n",
        "    def upload_files(e):\n",
        "        uf = []\n",
        "        if file_picker.result != None and file_picker.result.files != None:\n",
        "            for f in file_picker.result.files:\n",
        "              if page.web:\n",
        "                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))\n",
        "              else:\n",
        "                on_upload_progress(FilePickerUploadEvent(f.path, 1, \"\"))\n",
        "            file_picker.upload(uf)\n",
        "    page.overlay.append(file_picker)\n",
        "    def add_image(e):\n",
        "        save_dir = os.path.join(root_dir, 'my_ngp')\n",
        "        if not os.path.exists(save_dir):\n",
        "          os.mkdir(save_dir)\n",
        "        if image_path.value.startswith('http'):\n",
        "          import requests\n",
        "          from io import BytesIO\n",
        "          response = requests.get(image_path.value)\n",
        "          fpath = os.path.join(save_dir, image_path.value.rpartition(slash)[2])\n",
        "          model_image = PILImage.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "          width, height = model_image.size\n",
        "          width, height = scale_dimensions(width, height, instant_ngp_prefs['resolution'])\n",
        "          model_image = model_image.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert(\"RGB\")\n",
        "          model_image.save(fpath)\n",
        "          add_file(fpath)\n",
        "        elif os.path.isfile(image_path.value):\n",
        "          fpath = os.path.join(save_dir, image_path.value.rpartition(slash)[2])\n",
        "          original_img = PILImage.open(image_path.value)\n",
        "          width, height = original_img.size\n",
        "          width, height = scale_dimensions(width, height, instant_ngp_prefs['resolution'])\n",
        "          original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert(\"RGB\")\n",
        "          original_img.save(fpath)\n",
        "          #shutil.copy(image_path.value, fpath)\n",
        "          add_file(fpath)\n",
        "        elif os.path.isdir(image_path.value):\n",
        "          for f in os.listdir(image_path.value):\n",
        "            file_path = os.path.join(image_path.value, f)\n",
        "            if os.path.isdir(file_path): continue\n",
        "            if f.lower().endswith(('.png', '.jpg', '.jpeg', 'obj', 'stl', 'nvdb')):\n",
        "              fpath = os.path.join(save_dir, f)\n",
        "              original_img = PILImage.open(file_path)\n",
        "              width, height = original_img.size\n",
        "              width, height = scale_dimensions(width, height, instant_ngp_prefs['resolution'])\n",
        "              original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert(\"RGB\")\n",
        "              original_img.save(fpath)\n",
        "              #shutil.copy(file_path, fpath)\n",
        "              add_file(fpath)\n",
        "        else:\n",
        "          if bool(image_path.value):\n",
        "            alert_msg(page, \"Couldn't find a valid File, Path or URL...\")\n",
        "          else:\n",
        "            pick_path(e)\n",
        "          return\n",
        "        image_path.value = \"\"\n",
        "        image_path.update()\n",
        "    def load_images():\n",
        "        if os.path.exists(save_dir):\n",
        "          for f in os.listdir(save_dir):\n",
        "            existing = os.path.join(save_dir, f)\n",
        "            if os.path.isdir(existing): continue\n",
        "            if f.lower().endswith(('.png', '.jpg', '.jpeg', 'obj', 'stl', 'nvdb')):\n",
        "              add_file(existing, update=False)\n",
        "    #instance_prompt = Container(content=Tooltip(message=\"The prompt with identifier specifying the instance\", content=TextField(label=\"Instance Prompt Token Text\", value=instant_ngp_prefs['instance_prompt'], on_change=lambda e:changed(e,'instance_prompt'))), col={'md':9})\n",
        "    name_of_your_model = TextField(label=\"Name of your Model\", value=instant_ngp_prefs['name_of_your_model'], on_change=lambda e:changed(e,'name_of_your_model'))\n",
        "    train_steps = Tooltip(message=\"Total number of training steps to perform.  More the better, even around 35000..\", content=TextField(label=\"Max Training Steps\", value=instant_ngp_prefs['train_steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'train_steps', ptype='int'), width = 160))\n",
        "    #save_model = Checkbox(label=\"Save Model to HuggingFace   \", tooltip=\"\", value=instant_ngp_prefs['save_model'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'save_model'))\n",
        "    #save_model = Tooltip(message=\"Requires WRITE access on API Key to Upload Checkpoint\", content=Switch(label=\"Save Model to HuggingFace    \", value=instant_ngp_prefs['save_model'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_save))\n",
        "    #where_to_save_model = Dropdown(label=\"Where to Save Model\", width=250, options=[dropdown.Option(\"Public HuggingFace\"), dropdown.Option(\"Private HuggingFace\")], value=instant_ngp_prefs['where_to_save_model'], on_change=lambda e: changed(e, 'where_to_save_model'))\n",
        "    #class_data_dir = TextField(label=\"Prior Preservation Class Folder\", value=instant_ngp_prefs['class_data_dir'], on_change=lambda e:changed(e,'class_data_dir'))\n",
        "    #readme_description = TextField(label=\"Extra README Description\", value=instant_ngp_prefs['readme_description'], on_change=lambda e:changed(e,'readme_description'))\n",
        "    max_row = SliderRow(label=\"Max Resolution Size\", min=256, max=1024, divisions=6, multiple=64, suffix=\"px\", pref=instant_ngp_prefs, key='resolution')\n",
        "    \n",
        "    sharpen = Row([Text(\" Shapen Images:\"), Slider(label=\"{value}\", min=0, max=1, divisions=10, expand=True, value=instant_ngp_prefs['sharpen'], on_change=lambda e: changed(e, 'sharpen'))], col={'lg':6})\n",
        "    exposure = Row([Text(\" Image Exposure:\"), Slider(label=\"{value}\", min=0, max=1, divisions=10, expand=True, value=instant_ngp_prefs['exposure'], on_change=lambda e: changed(e, 'exposure'))], col={'lg':6})\n",
        "    vr_mode = Checkbox(label=\"Output VR Mode\", tooltip=\"Render to a VR headset\", value=instant_ngp_prefs['vr_mode'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'vr_mode'))\n",
        "    image_path = TextField(label=\"Image Files or Folder Path or URL to Train\", value=instant_ngp_prefs['image_path'], on_change=lambda e:changed(e,'image_path'), suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_path), expand=1)\n",
        "    add_image_button = ElevatedButton(content=Text(\"Add File or Folder\"), on_click=add_image)\n",
        "    page.instant_ngp_file_list = Column([], tight=True, spacing=0)\n",
        "    load_images()\n",
        "    #where_to_save_model.visible = instant_ngp_prefs['save_model']\n",
        "    #readme_description.visible = instant_ngp_prefs['save_model']\n",
        "    #lambda_entropy = TextField(label=\"Lambda Entropy\", value=dreamfusinstant_ngp_prefsion_prefs['lambda_entropy'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'lambda_entropy', ptype='float'), width = 160)\n",
        "    #max_steps = TextField(label=\"Max Steps\", value=instant_ngp_prefs['max_steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'max_steps', ptype='int'), width = 160)\n",
        "    page.instant_ngp_output = Column([])\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(page.instant_ngp_output.controls) > 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üéë  Instant Neural Graphics Primitives by NVidia\", \"Convert series of images into 3D Models with Multiresolution Hash Encoding...\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with Instant NGP Settings\", on_click=instant_ngp_help)]),\n",
        "        Row([name_of_your_model]),\n",
        "        Row([train_steps, vr_mode]),\n",
        "        ResponsiveRow([sharpen, exposure]),\n",
        "        #Row([save_model, where_to_save_model]),\n",
        "        #readme_description,\n",
        "        #Row([class_data_dir]),\n",
        "        max_row,\n",
        "        Text(\"The scene to load. Can be full path to the training data, multiple image angles, NeRF dataset, a *.obj/*.stl mesh for training a SDF, images, or a *.nvdb volume.\"),\n",
        "        Row([image_path, add_image_button]),\n",
        "        page.instant_ngp_file_list,\n",
        "        Row([ElevatedButton(content=Text(\"üéá  Run Instant-NGP\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_instant_ngp(page))]),\n",
        "        page.instant_ngp_output,\n",
        "        clear_button,\n",
        "      ]\n",
        "    ))], scroll=ScrollMode.AUTO)\n",
        "    return c\n",
        "\n",
        "\n",
        "repaint_prefs = {\n",
        "    'original_image': '',\n",
        "    'mask_image': '',\n",
        "    'num_inference_steps': 500,\n",
        "    'eta': 0.0,\n",
        "    'jump_length': 10,\n",
        "    'jump_n_sample': 10,\n",
        "    'seed': 0,\n",
        "    'file_name': '',\n",
        "    'max_size': 1024,\n",
        "    'invert_mask': False,\n",
        "}\n",
        "def buildRepainter(page):\n",
        "    global repaint_prefs, prefs, pipe_repaint\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            repaint_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            repaint_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            repaint_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def add_to_repaint_output(o):\n",
        "      page.repaint_output.controls.append(o)\n",
        "      page.repaint_output.update()\n",
        "      if not clear_button.visible:\n",
        "        clear_button.visible = True\n",
        "        clear_button.update()\n",
        "    def clear_output(e):\n",
        "      if prefs['enable_sounds']: page.snd_delete.play()\n",
        "      page.repaint_output.controls = []\n",
        "      page.repaint_output.update()\n",
        "      clear_button.visible = False\n",
        "      clear_button.update()\n",
        "    def repaint_help(e):\n",
        "      def close_repaint_dlg(e):\n",
        "        nonlocal repaint_help_dlg\n",
        "        repaint_help_dlg.open = False\n",
        "        page.update()\n",
        "      repaint_help_dlg = AlertDialog(title=Text(\"üíÅ   Help with Repainter\"), content=Column([\n",
        "          Text(\"It's difficult to explain exactly what all these parameters do, but keep it close to defaults, keep prompt simple, or experiment to see what's what, we don't know.\"),\n",
        "        ], scroll=ScrollMode.AUTO), actions=[TextButton(\"üò™  Okay then... \", on_click=close_repaint_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = repaint_help_dlg\n",
        "      repaint_help_dlg.open = True\n",
        "      page.update()\n",
        "    def file_picker_result(e: FilePickerResultEvent):\n",
        "        if e.files != None:\n",
        "          upload_files(e)\n",
        "    def on_upload_progress(e: FilePickerUploadEvent):\n",
        "      nonlocal pick_type\n",
        "      if e.progress == 1:\n",
        "        if not slash in e.file_name:\n",
        "          fname = os.path.join(root_dir, e.file_name)\n",
        "          repaint_prefs['file_name'] = e.file_name.rpartition('.')[0]\n",
        "        else:\n",
        "          fname = e.file_name\n",
        "          repaint_prefs['file_name'] = e.file_name.rpartition(slash)[2].rpartition('.')[0]\n",
        "        if pick_type == \"original\":\n",
        "          original_image.value = fname\n",
        "          original_image.update()\n",
        "          repaint_prefs['original_image'] = fname\n",
        "        elif pick_type == \"mask\":\n",
        "          mask_image.value = fname\n",
        "          mask_image.update()\n",
        "          repaint_prefs['mask_image'] = fname\n",
        "        page.update()\n",
        "    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)\n",
        "    def upload_files(e):\n",
        "        uf = []\n",
        "        if file_picker.result != None and file_picker.result.files != None:\n",
        "            for f in file_picker.result.files:\n",
        "              if page.web:\n",
        "                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))\n",
        "              else:\n",
        "                on_upload_progress(FilePickerUploadEvent(f.path, 1, \"\"))\n",
        "            file_picker.upload(uf)\n",
        "    page.overlay.append(file_picker)\n",
        "    pick_type = \"\"\n",
        "    #page.overlay.append(pick_files_dialog)\n",
        "    def pick_original(e):\n",
        "        nonlocal pick_type\n",
        "        pick_type = \"original\"\n",
        "        file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"png\", \"PNG\", \"jpg\", \"jpeg\"], dialog_title=\"Pick Original Image File\")\n",
        "    def pick_mask(e):\n",
        "        nonlocal pick_type\n",
        "        pick_type = \"mask\"\n",
        "        file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"png\", \"PNG\", \"jpg\", \"jpeg\"], dialog_title=\"Pick Black & White Mask Image\")\n",
        "    def change_eta(e):\n",
        "        changed(e, 'eta', ptype=\"float\")\n",
        "        eta_value.value = f\" {repaint_prefs['eta']}\"\n",
        "        eta_value.update()\n",
        "        eta_row.update()\n",
        "    original_image = TextField(label=\"Original Image\", value=repaint_prefs['original_image'], expand=1, on_change=lambda e:changed(e,'original_image'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_original))\n",
        "    mask_image = TextField(label=\"Mask Image\", value=repaint_prefs['mask_image'], expand=1, on_change=lambda e:changed(e,'mask_image'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD_OUTLINED, on_click=pick_mask))\n",
        "    invert_mask = Checkbox(label=\"Invert\", tooltip=\"Swaps the Black & White of your Mask Image\", value=repaint_prefs['invert_mask'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'invert_mask'))\n",
        "    jump_length = TextField(label=\"Jump Length\", width=130, tooltip=\"The number of steps taken forward in time before going backward in time for a single jump\", value=repaint_prefs['jump_length'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'jump_length', ptype='int'))\n",
        "    jump_n_sample = TextField(label=\"Jump # of Sample\", width=130, tooltip=\"The number of times we will make forward time jump for a given chosen time sample.\", value=repaint_prefs['jump_n_sample'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'jump_n_sample', ptype='int'))\n",
        "    seed = TextField(label=\"Seed\", width=90, value=str(repaint_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip=\"0 or -1 picks a Random seed\", on_change=lambda e:changed(e,'seed', ptype='int'))\n",
        "    #num_inference_steps = TextField(label=\"Inference Steps\", value=str(repaint_prefs['num_inference_steps']), keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'num_inference_steps', ptype='int'))\n",
        "    num_inference_row = SliderRow(label=\"Number of Inference Steps\", min=10, max=3000, divisions=2990, pref=repaint_prefs, key='num_inference_steps', tooltip=\"The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.\")   \n",
        "    #eta = TextField(label=\"ETA\", value=str(repaint_prefs['eta']), keyboard_type=KeyboardType.NUMBER, hint_text=\"Amount of Noise\", on_change=lambda e:changed(e,'eta', ptype='float'))\n",
        "    eta = Slider(min=0.0, max=1.0, divisions=20, label=\"{value}\", value=float(repaint_prefs['eta']), tooltip=\"The weight of noise for added noise in a diffusion step. Its value is between 0.0 and 1.0 - 0.0 is DDIM and 1.0 is DDPM scheduler respectively.\", expand=True, on_change=change_eta)\n",
        "    eta_value = Text(f\" {repaint_prefs['eta']}\", weight=FontWeight.BOLD)\n",
        "    eta_row = Row([Text(\"ETA:\"), eta_value, Text(\"  DDIM\"), eta, Text(\"DDPM\")])\n",
        "    page.etas.append(eta_row)\n",
        "    max_row = SliderRow(label=\"Max Resolution Size\", min=256, max=1280, divisions=64, multiple=16, suffix=\"px\", pref=repaint_prefs, key='max_size')\n",
        "    page.repaint_output = Column([])\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(page.repaint_output.controls) > 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üíÖ  Repaint masked areas of an image\", \"Fills in areas of picture with what it thinks it should be, without a prompt...\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with Repainter Settings\", on_click=repaint_help)]),\n",
        "        Row([original_image, mask_image, invert_mask]),\n",
        "        num_inference_row,\n",
        "        eta_row,\n",
        "        max_row,\n",
        "        Row([jump_length, jump_n_sample, seed]),\n",
        "        ElevatedButton(content=Text(\"üñåÔ∏è  Run Repainter\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_repainter(page)),\n",
        "        page.repaint_output,\n",
        "        clear_button,\n",
        "      ]\n",
        "    ))], scroll=ScrollMode.AUTO)\n",
        "    return c\n",
        "\n",
        "image_variation_prefs = {\n",
        "    'init_image': '',\n",
        "    'guidance_scale': 7.5,\n",
        "    'num_inference_steps': 50,\n",
        "    'eta': 0.4,\n",
        "    'seed': 0,\n",
        "    'num_images': 1,\n",
        "    'file_name': '',\n",
        "    'max_size': 1024,\n",
        "    'width': 960,\n",
        "    'height': 512,\n",
        "}\n",
        "def buildImageVariation(page):\n",
        "    global image_variation_prefs, prefs, pipe_image_variation\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            image_variation_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            image_variation_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            image_variation_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def add_to_image_variation_output(o):\n",
        "      page.image_variation_output.controls.append(o)\n",
        "      page.image_variation_output.update()\n",
        "      if not clear_button.visible:\n",
        "        clear_button.visible = True\n",
        "        clear_button.update()\n",
        "    page.add_to_image_variation_output = add_to_image_variation_output\n",
        "    def clear_output(e):\n",
        "      if prefs['enable_sounds']: page.snd_delete.play()\n",
        "      page.image_variation_output.controls = []\n",
        "      page.image_variation_output.update()\n",
        "      clear_button.visible = False\n",
        "      clear_button.update()\n",
        "    def image_variation_help(e):\n",
        "      def close_image_variation_dlg(e):\n",
        "        nonlocal image_variation_help_dlg\n",
        "        image_variation_help_dlg.open = False\n",
        "        page.update()\n",
        "      image_variation_help_dlg = AlertDialog(title=Text(\"üôÖ   Help with Image Variations\"), content=Column([\n",
        "          Text(\"Give it any of your favorite images and create variations of it.... Simple as that, no prompt needed.\"),\n",
        "        ], scroll=ScrollMode.AUTO), actions=[TextButton(\"ü§ó  Sounds Fun... \", on_click=close_image_variation_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = image_variation_help_dlg\n",
        "      image_variation_help_dlg.open = True\n",
        "      page.update()\n",
        "    def file_picker_result(e: FilePickerResultEvent):\n",
        "        if e.files != None:\n",
        "          upload_files(e)\n",
        "    def on_upload_progress(e: FilePickerUploadEvent):\n",
        "      if e.progress == 1:\n",
        "        if not slash in e.file_name:\n",
        "          fname = os.path.join(root_dir, e.file_name)\n",
        "          image_variation_prefs['file_name'] = e.file_name.rpartition('.')[0]\n",
        "        else:\n",
        "          fname = e.file_name\n",
        "          image_variation_prefs['file_name'] = e.file_name.rpartition(slash)[2].rpartition('.')[0]\n",
        "        init_image.value = fname\n",
        "        init_image.update()\n",
        "        image_variation_prefs['init_image'] = fname\n",
        "        page.update()\n",
        "    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)\n",
        "    def upload_files(e):\n",
        "        uf = []\n",
        "        if file_picker.result != None and file_picker.result.files != None:\n",
        "            for f in file_picker.result.files:\n",
        "              if page.web:\n",
        "                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))\n",
        "              else:\n",
        "                on_upload_progress(FilePickerUploadEvent(f.path, 1, \"\"))\n",
        "            file_picker.upload(uf)\n",
        "    page.overlay.append(file_picker)\n",
        "    def pick_init(e):\n",
        "        file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"png\", \"PNG\", \"jpg\", \"jpeg\"], dialog_title=\"Pick init Image File\")\n",
        "    init_image = TextField(label=\"Initial Image\", value=image_variation_prefs['init_image'], on_change=lambda e:changed(e,'init_image'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_init))\n",
        "    seed = TextField(label=\"Seed\", width=90, value=str(image_variation_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip=\"0 or -1 picks a Random seed\", on_change=lambda e:changed(e,'seed', ptype='int'))\n",
        "    guidance = SliderRow(label=\"Guidance Scale\", min=0, max=50, divisions=100, round=1, pref=image_variation_prefs, key='guidance_scale')\n",
        "    num_inference_row = SliderRow(label=\"Number of Inference Steps\", min=1, max=100, divisions=99, pref=image_variation_prefs, key='num_inference_steps', tooltip=\"The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.\")   \n",
        "    #eta = TextField(label=\"ETA\", value=str(image_variation_prefs['eta']), keyboard_type=KeyboardType.NUMBER, hint_text=\"Amount of Noise\", on_change=lambda e:changed(e,'eta', ptype='float'))\n",
        "    eta = Slider(min=0.0, max=1.0, divisions=20, label=\"{value}\", value=float(image_variation_prefs['eta']), tooltip=\"The weight of noise for added noise in a diffusion step. Its value is between 0.0 and 1.0 - 0.0 is DDIM and 1.0 is DDPM scheduler respectively.\", expand=True, on_change=lambda e:changed(e,'eta', ptype='float'))\n",
        "    eta_row = Row([Text(\"DDIM ETA: \"), eta])\n",
        "    page.etas.append(eta_row)\n",
        "    max_row = SliderRow(label=\"Max Resolution Size\", min=256, max=1280, divisions=64, multiple=16, suffix=\"px\", pref=image_variation_prefs, key='max_size')\n",
        "    page.image_variation_output = Column([])\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(page.image_variation_output.controls) > 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"ü™©  Image Variations of any Init Image\", \"Creates a new version of your picture, without a prompt...\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with Image Variation Settings\", on_click=image_variation_help)]),\n",
        "        init_image,\n",
        "        #Row([init_image, mask_image, invert_mask]),\n",
        "        num_inference_row,\n",
        "        guidance,\n",
        "        eta_row,\n",
        "        max_row,\n",
        "        Row([NumberPicker(label=\"Number of Images: \", min=1, max=8, value=image_variation_prefs['num_images'], on_change=lambda e: changed(e, 'num_images')), seed]),\n",
        "        ElevatedButton(content=Text(\"üñçÔ∏è  Get Image Variation\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_image_variation(page)),\n",
        "        page.image_variation_output,\n",
        "        clear_button,\n",
        "      ]\n",
        "    ))], scroll=ScrollMode.AUTO, auto_scroll=False)\n",
        "    return c\n",
        "\n",
        "EDICT_prefs = {\n",
        "    'target_prompt': '',\n",
        "    'negative_prompt': '',\n",
        "    'base_prompt': '',\n",
        "    'init_image': '',\n",
        "    'guidance_scale': 3.0,\n",
        "    'num_inference_steps': 50,\n",
        "    'strength': 0.8,\n",
        "    'seed': 0,\n",
        "    'num_images': 1,\n",
        "    'batch_folder_name': '',\n",
        "    'max_size': 512,\n",
        "    \"apply_ESRGAN_upscale\": prefs['apply_ESRGAN_upscale'],\n",
        "    \"enlarge_scale\": 4.0,\n",
        "    \"display_upscaled_image\": False,\n",
        "}\n",
        "\n",
        "def buildEDICT(page):\n",
        "    global EDICT_prefs, prefs, pipe_EDICT\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            EDICT_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            EDICT_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            EDICT_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def add_to_EDICT_output(o):\n",
        "      page.EDICT_output.controls.append(o)\n",
        "      page.EDICT_output.update()\n",
        "      if not clear_button.visible:\n",
        "        clear_button.visible = True\n",
        "        clear_button.update()\n",
        "    page.add_to_EDICT_output = add_to_EDICT_output\n",
        "    def clear_output(e):\n",
        "      if prefs['enable_sounds']: page.snd_delete.play()\n",
        "      page.EDICT_output.controls = []\n",
        "      page.EDICT_output.update()\n",
        "      clear_button.visible = False\n",
        "      clear_button.update()\n",
        "    def EDICT_help(e):\n",
        "      def close_EDICT_dlg(e):\n",
        "        nonlocal EDICT_help_dlg\n",
        "        EDICT_help_dlg.open = False\n",
        "        page.update()\n",
        "      EDICT_help_dlg = AlertDialog(title=Text(\"üôÖ   Help with EDICT Edit\"), content=Column([\n",
        "          Text(\"EDICT: Exact Diffusion Inversion via Coupled Transformations\"),\n",
        "          Text(\"Using the iterative denoising diffusion principle, denoising diffusion models (DDMs) trained with web-scale data can generate highly realistic images conditioned on input text, layouts, and scene graphs. After image generation, the next important application of DDMs being explored by the research community is that of image editing. Models such as DALL-E-2 and Stable Diffusion [24] can perform inpainting, allowing users to edit images through manual annotation. Methods such as SDEdit have demonstrated that both synthetic and real images can be edited using stroke or composite guidance via DDMs. However, the goal of a holistic image editing tool that can edit any real/artificial image using purely text has still not been achieved, until now.\"),\n",
        "          Markdown(\"[Read Arxiv Paper](https://arxiv.org/pdf/2211.12446.pdf)\", on_tap_link=lambda e: e.page.launch_url(e.data)),\n",
        "        ], scroll=ScrollMode.AUTO), actions=[TextButton(\"ü•¥  Not Complicated... \", on_click=close_EDICT_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = EDICT_help_dlg\n",
        "      EDICT_help_dlg.open = True\n",
        "      page.update()\n",
        "    def toggle_ESRGAN(e):\n",
        "        ESRGAN_settings.height = None if e.control.value else 0\n",
        "        EDICT_prefs['apply_ESRGAN_upscale'] = e.control.value\n",
        "        ESRGAN_settings.update()\n",
        "    def file_picker_result(e: FilePickerResultEvent):\n",
        "        if e.files != None:\n",
        "          upload_files(e)\n",
        "    def on_upload_progress(e: FilePickerUploadEvent):\n",
        "      if e.progress == 1:\n",
        "        if not slash in e.file_name:\n",
        "          fname = os.path.join(root_dir, e.file_name)\n",
        "          EDICT_prefs['file_name'] = e.file_name.rpartition('.')[0]\n",
        "        else:\n",
        "          fname = e.file_name\n",
        "          EDICT_prefs['file_name'] = e.file_name.rpartition(slash)[2].rpartition('.')[0]\n",
        "        init_image.value = fname\n",
        "        init_image.update()\n",
        "        EDICT_prefs['init_image'] = fname\n",
        "        page.update()\n",
        "    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)\n",
        "    def upload_files(e):\n",
        "        uf = []\n",
        "        if file_picker.result != None and file_picker.result.files != None:\n",
        "            for f in file_picker.result.files:\n",
        "              if page.web:\n",
        "                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))\n",
        "              else:\n",
        "                on_upload_progress(FilePickerUploadEvent(f.path, 1, \"\"))\n",
        "            file_picker.upload(uf)\n",
        "    page.overlay.append(file_picker)\n",
        "    def pick_init(e):\n",
        "        file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"png\", \"PNG\", \"jpg\", \"jpeg\"], dialog_title=\"Pick init Image File\")\n",
        "    base_prompt = TextField(label=\"Base Prompt Text (describe init image)\", value=EDICT_prefs['base_prompt'], col={'md': 12}, multiline=True, on_change=lambda e:changed(e,'base_prompt'))\n",
        "    target_prompt = TextField(label=\"Target Prompt Text\", value=EDICT_prefs['target_prompt'], col={'md': 9}, multiline=True, on_change=lambda e:changed(e,'target_prompt'))\n",
        "    negative_prompt  = TextField(label=\"Negative Prompt Text\", value=EDICT_prefs['negative_prompt'], col={'md':3}, multiline=True, on_change=lambda e:changed(e,'negative_prompt'))\n",
        "    init_image = TextField(label=\"Initial Image to Edit (crops square)\", value=EDICT_prefs['init_image'], on_change=lambda e:changed(e,'init_image'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_init))\n",
        "    seed = TextField(label=\"Seed\", width=90, value=str(EDICT_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip=\"0 or -1 picks a Random seed\", on_change=lambda e:changed(e,'seed', ptype='int'))\n",
        "    guidance = SliderRow(label=\"Guidance Scale\", min=0, max=50, divisions=100, round=1, pref=EDICT_prefs, key='guidance_scale')\n",
        "    num_inference_row = SliderRow(label=\"Number of Inference Steps\", min=1, max=100, divisions=99, pref=EDICT_prefs, key='num_inference_steps', tooltip=\"The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.\")   \n",
        "    strength = SliderRow(label=\"Strength\", min=0, max=1, divisions=20, round=2, pref=EDICT_prefs, key='strength')\n",
        "    max_row = SliderRow(label=\"Max Resolution Size\", min=256, max=1024, divisions=12, multiple=32, suffix=\"px\", pref=EDICT_prefs, key='max_size')\n",
        "    batch_folder_name = TextField(label=\"Batch Folder Name\", value=EDICT_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    apply_ESRGAN_upscale = Switch(label=\"Apply ESRGAN Upscale\", value=EDICT_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)\n",
        "    enlarge_scale_slider = SliderRow(label=\"Enlarge Scale\", min=1, max=4, divisions=6, round=1, suffix=\"x\", pref=EDICT_prefs, key='enlarge_scale')\n",
        "    display_upscaled_image = Checkbox(label=\"Display Upscaled Image\", value=EDICT_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))\n",
        "    ESRGAN_settings = Container(Column([enlarge_scale_slider, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_EDICT = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_EDICT.height = None if status['installed_ESRGAN'] else 0\n",
        "    if not unCLIP_interpolation_prefs['apply_ESRGAN_upscale']:\n",
        "        ESRGAN_settings.height = 0\n",
        "    page.EDICT_output = Column([])\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(page.EDICT_output.controls) > 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"ü§π  EDICT Image Editing\", \"Diffusion pipeline for text-guided image editing... Exact Diffusion Inversion via Coupled Transformations.\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with Image Variation Settings\", on_click=EDICT_help)]),\n",
        "        init_image,\n",
        "        base_prompt,\n",
        "        ResponsiveRow([target_prompt, negative_prompt]),\n",
        "        #Row([init_image, mask_image, invert_mask]),\n",
        "        num_inference_row,\n",
        "        guidance,\n",
        "        strength,\n",
        "        max_row,\n",
        "        Row([NumberPicker(label=\"Number of Images: \", min=1, max=8, value=EDICT_prefs['num_images'], on_change=lambda e: changed(e, 'num_images')), seed, batch_folder_name]),\n",
        "        page.ESRGAN_block_EDICT,\n",
        "        ElevatedButton(content=Text(\"üßù  Get EDICT Edit\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_EDICT(page)),\n",
        "        page.EDICT_output,\n",
        "        clear_button,\n",
        "      ]\n",
        "    ))], scroll=ScrollMode.AUTO, auto_scroll=False)\n",
        "    return c\n",
        "\n",
        "DiffEdit_prefs = {\n",
        "    'target_prompt': '',\n",
        "    'negative_prompt': '',\n",
        "    'source_prompt': '',\n",
        "    'init_image': '',\n",
        "    'guidance_scale': 3.0,\n",
        "    'num_inference_steps': 50,\n",
        "    'strength': 0.8,\n",
        "    'seed': 0,\n",
        "    'num_images': 1,\n",
        "    'batch_folder_name': '',\n",
        "    'max_size': 768,\n",
        "    \"apply_ESRGAN_upscale\": prefs['apply_ESRGAN_upscale'],\n",
        "    \"enlarge_scale\": 4.0,\n",
        "    \"display_upscaled_image\": False,\n",
        "}\n",
        "\n",
        "def buildDiffEdit(page):\n",
        "    global DiffEdit_prefs, prefs, pipe_DiffEdit\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            DiffEdit_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            DiffEdit_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            DiffEdit_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def add_to_DiffEdit_output(o):\n",
        "      page.DiffEdit_output.controls.append(o)\n",
        "      page.DiffEdit_output.update()\n",
        "      if not clear_button.visible:\n",
        "        clear_button.visible = True\n",
        "        clear_button.update()\n",
        "    page.add_to_DiffEdit_output = add_to_DiffEdit_output\n",
        "    def clear_output(e):\n",
        "      if prefs['enable_sounds']: page.snd_delete.play()\n",
        "      page.DiffEdit_output.controls = []\n",
        "      page.DiffEdit_output.update()\n",
        "      clear_button.visible = False\n",
        "      clear_button.update()\n",
        "    def DiffEdit_help(e):\n",
        "      def close_DiffEdit_dlg(e):\n",
        "        nonlocal DiffEdit_help_dlg\n",
        "        DiffEdit_help_dlg.open = False\n",
        "        page.update()\n",
        "      DiffEdit_help_dlg = AlertDialog(title=Text(\"üôÖ   Help with DiffEdit\"), content=Column([\n",
        "          Text(\"Image generation has recently seen tremendous advances, with diffusion models allowing to synthesize convincing images for a large variety of text prompts. In this article, we propose DiffEdit, a method to take advantage of text-conditioned diffusion models for the task of semantic image editing, where the goal is to edit an image based on a text query. Semantic image editing is an extension of image generation, with the additional constraint that the generated image should be as similar as possible to a given input image. Current editing methods based on diffusion models usually require to provide a mask, making the task much easier by treating it as a conditional inpainting task. In contrast, our main contribution is able to automatically generate a mask highlighting regions of the input image that need to be edited, by contrasting predictions of a diffusion model conditioned on different text prompts. Moreover, we rely on latent inference to preserve content in those regions of interest and show excellent synergies with mask-based diffusion. DiffEdit achieves state-of-the-art editing performance on ImageNet. In addition, we evaluate semantic image editing in more challenging settings, using images from the COCO dataset as well as text-based generated images.\"),\n",
        "          #Text(\"\"),\n",
        "          Markdown(\"[Read Arxiv Paper](https://arxiv.org/abs/2210.11427) - [Blog Post with Demo](https://blog.problemsolversguild.com/technical/research/2022/11/02/DiffEdit-Implementation.html) - [GitHub](https://github.com/Xiang-cd/DiffEdit-stable-diffusion/)\", on_tap_link=lambda e: e.page.launch_url(e.data)),\n",
        "        ], scroll=ScrollMode.AUTO), actions=[TextButton(\"üòá  Not Difficult... \", on_click=close_DiffEdit_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = DiffEdit_help_dlg\n",
        "      DiffEdit_help_dlg.open = True\n",
        "      page.update()\n",
        "    def toggle_ESRGAN(e):\n",
        "        ESRGAN_settings.height = None if e.control.value else 0\n",
        "        DiffEdit_prefs['apply_ESRGAN_upscale'] = e.control.value\n",
        "        ESRGAN_settings.update()\n",
        "    def file_picker_result(e: FilePickerResultEvent):\n",
        "        if e.files != None:\n",
        "          upload_files(e)\n",
        "    def on_upload_progress(e: FilePickerUploadEvent):\n",
        "      if e.progress == 1:\n",
        "        if not slash in e.file_name:\n",
        "          fname = os.path.join(root_dir, e.file_name)\n",
        "          DiffEdit_prefs['file_name'] = e.file_name.rpartition('.')[0]\n",
        "        else:\n",
        "          fname = e.file_name\n",
        "          DiffEdit_prefs['file_name'] = e.file_name.rpartition(slash)[2].rpartition('.')[0]\n",
        "        init_image.value = fname\n",
        "        init_image.update()\n",
        "        DiffEdit_prefs['init_image'] = fname\n",
        "        page.update()\n",
        "    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)\n",
        "    def upload_files(e):\n",
        "        uf = []\n",
        "        if file_picker.result != None and file_picker.result.files != None:\n",
        "            for f in file_picker.result.files:\n",
        "              if page.web:\n",
        "                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))\n",
        "              else:\n",
        "                on_upload_progress(FilePickerUploadEvent(f.path, 1, \"\"))\n",
        "            file_picker.upload(uf)\n",
        "    page.overlay.append(file_picker)\n",
        "    def pick_init(e):\n",
        "        file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"png\", \"PNG\", \"jpg\", \"jpeg\"], dialog_title=\"Pick init Image File\")\n",
        "    source_prompt = TextField(label=\"Source Prompt Text (blank to auto-caption)\", value=DiffEdit_prefs['source_prompt'], col={'md': 12}, multiline=True, on_change=lambda e:changed(e,'source_prompt'))\n",
        "    target_prompt = TextField(label=\"Target Prompt Text (describe edits)\", value=DiffEdit_prefs['target_prompt'], col={'md': 9}, multiline=True, on_change=lambda e:changed(e,'target_prompt'))\n",
        "    negative_prompt  = TextField(label=\"Negative Prompt Text\", value=DiffEdit_prefs['negative_prompt'], col={'md':3}, multiline=True, on_change=lambda e:changed(e,'negative_prompt'))\n",
        "    init_image = TextField(label=\"Initial Image to Edit (crops square)\", value=DiffEdit_prefs['init_image'], on_change=lambda e:changed(e,'init_image'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_init))\n",
        "    seed = TextField(label=\"Seed\", width=90, value=str(DiffEdit_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip=\"0 or -1 picks a Random seed\", on_change=lambda e:changed(e,'seed', ptype='int'))\n",
        "    guidance = SliderRow(label=\"Guidance Scale\", min=0, max=50, divisions=100, round=1, pref=DiffEdit_prefs, key='guidance_scale')\n",
        "    num_inference_row = SliderRow(label=\"Number of Inference Steps\", min=1, max=100, divisions=99, pref=DiffEdit_prefs, key='num_inference_steps', tooltip=\"The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.\")   \n",
        "    strength = SliderRow(label=\"Strength\", min=0, max=1, divisions=20, round=2, pref=DiffEdit_prefs, key='strength')\n",
        "    max_row = SliderRow(label=\"Max Resolution Size\", min=256, max=1024, divisions=12, multiple=32, suffix=\"px\", pref=DiffEdit_prefs, key='max_size')\n",
        "    batch_folder_name = TextField(label=\"Batch Folder Name\", value=DiffEdit_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    apply_ESRGAN_upscale = Switch(label=\"Apply ESRGAN Upscale\", value=DiffEdit_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)\n",
        "    enlarge_scale_slider = SliderRow(label=\"Enlarge Scale\", min=1, max=4, divisions=6, round=1, suffix=\"x\", pref=DiffEdit_prefs, key='enlarge_scale')\n",
        "    display_upscaled_image = Checkbox(label=\"Display Upscaled Image\", value=DiffEdit_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))\n",
        "    ESRGAN_settings = Container(Column([enlarge_scale_slider, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_DiffEdit = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_DiffEdit.height = None if status['installed_ESRGAN'] else 0\n",
        "    if not unCLIP_interpolation_prefs['apply_ESRGAN_upscale']:\n",
        "        ESRGAN_settings.height = 0\n",
        "    page.DiffEdit_output = Column([])\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(page.DiffEdit_output.controls) > 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üçí  DiffEdit Image Editing\", \"Zero-shot Diffusion-based Semantic Image Editing with Mask Guidance...\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with Image Variation Settings\", on_click=DiffEdit_help)]),\n",
        "        init_image,\n",
        "        source_prompt,\n",
        "        ResponsiveRow([target_prompt, negative_prompt]),\n",
        "        #Row([init_image, mask_image, invert_mask]),\n",
        "        num_inference_row,\n",
        "        guidance,\n",
        "        strength,\n",
        "        max_row,\n",
        "        Row([NumberPicker(label=\"Number of Images: \", min=1, max=8, value=DiffEdit_prefs['num_images'], on_change=lambda e: changed(e, 'num_images')), seed, batch_folder_name]),\n",
        "        page.ESRGAN_block_DiffEdit,\n",
        "        ElevatedButton(content=Text(\"üòÉ  Get DiffEdit\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_DiffEdit(page)),\n",
        "        page.DiffEdit_output,\n",
        "        clear_button,\n",
        "      ]\n",
        "    ))], scroll=ScrollMode.AUTO, auto_scroll=False)\n",
        "    return c\n",
        "\n",
        "\n",
        "unCLIP_prefs = {\n",
        "    'prompt': '',\n",
        "    'batch_folder_name': '',\n",
        "    'prior_guidance_scale': 4.0,\n",
        "    'decoder_guidance_scale': 8.0,\n",
        "    'prior_num_inference_steps': 25,\n",
        "    'decoder_num_inference_steps': 25,\n",
        "    'super_res_num_inference_steps': 7,\n",
        "    'seed': 0,\n",
        "    'num_images': 1,\n",
        "    'use_StableUnCLIP_pipeline': False,\n",
        "    #'variance_type': 'learned_range',#fixed_small_log\n",
        "    #'num_train_timesteps': 1000,\n",
        "    #'prediction_type': 'epsilon',#sample\n",
        "    #'clip_sample': True,\n",
        "    \"apply_ESRGAN_upscale\": prefs['apply_ESRGAN_upscale'],\n",
        "    \"enlarge_scale\": 4.0,\n",
        "    \"display_upscaled_image\": True,\n",
        "}\n",
        "def buildUnCLIP(page):\n",
        "    global unCLIP_prefs, prefs, pipe_unCLIP\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            unCLIP_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            unCLIP_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            unCLIP_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def add_to_unCLIP_output(o):\n",
        "      page.unCLIP_output.controls.append(o)\n",
        "      page.unCLIP_output.update()\n",
        "      if not clear_button.visible:\n",
        "        clear_button.visible = True\n",
        "        clear_button.update()\n",
        "    page.add_to_unCLIP_output = add_to_unCLIP_output\n",
        "    def clear_output(e):\n",
        "      if prefs['enable_sounds']: page.snd_delete.play()\n",
        "      page.unCLIP_output.controls = []\n",
        "      page.unCLIP_output.update()\n",
        "      clear_button.visible = False\n",
        "      clear_button.update()\n",
        "    def unCLIP_help(e):\n",
        "      def close_unCLIP_dlg(e):\n",
        "        nonlocal unCLIP_help_dlg\n",
        "        unCLIP_help_dlg.open = False\n",
        "        page.update()\n",
        "      unCLIP_help_dlg = AlertDialog(title=Text(\"üôÖ   Help with unCLIP Pipeline\"), content=Column([\n",
        "          Text(\"Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we implemented a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.\"),\n",
        "          Text(\"The scheduler is a modified DDPM that has some minor variations in how it calculates the learned range variance and dynamically re-calculates betas based off the timesteps it is skipping. The scheduler also uses a slightly different step ratio when computing timesteps to use for inference.\"),\n",
        "          Markdown(\"The unCLIP model in diffusers comes from kakaobrain's karlo and the original codebase can be found [here](https://github.com/kakaobrain/karlo). Additionally, lucidrains has a DALL-E 2 recreation [here](https://github.com/lucidrains/DALLE2-pytorch).\"),\n",
        "        ], scroll=ScrollMode.AUTO), actions=[TextButton(\"üòï  Tricky... \", on_click=close_unCLIP_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = unCLIP_help_dlg\n",
        "      unCLIP_help_dlg.open = True\n",
        "      page.update()\n",
        "    def toggle_ESRGAN(e):\n",
        "        ESRGAN_settings.height = None if e.control.value else 0\n",
        "        unCLIP_prefs['apply_ESRGAN_upscale'] = e.control.value\n",
        "        ESRGAN_settings.update()\n",
        "    def toggle_unCLIP(e):\n",
        "        unCLIP_prefs['use_StableUnCLIP_pipeline'] = e.control.value\n",
        "        if unCLIP_prefs['use_StableUnCLIP_pipeline']:\n",
        "            decoder_num_inference_row.visible = False\n",
        "            super_res_num_inference_row.visible = False\n",
        "            decoder_guidance.visible = False\n",
        "            decoder_num_inference_row.update()\n",
        "            super_res_num_inference_row.update()\n",
        "            decoder_guidance.update()\n",
        "        else:\n",
        "            decoder_num_inference_row.visible = True\n",
        "            super_res_num_inference_row.visible = True\n",
        "            decoder_guidance.visible = True\n",
        "            decoder_guidance.update()\n",
        "            super_res_num_inference_row.update()\n",
        "            decoder_num_inference_row.update()\n",
        "    prompt = TextField(label=\"Prompt Text\", value=unCLIP_prefs['prompt'], on_change=lambda e:changed(e,'prompt'))\n",
        "    seed = TextField(label=\"Seed\", width=90, value=str(unCLIP_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip=\"0 or -1 picks a Random seed\", on_change=lambda e:changed(e,'seed', ptype='int'))\n",
        "    prior_num_inference_row = SliderRow(label=\"Number of Prior Inference Steps\", min=1, max=100, divisions=99, pref=unCLIP_prefs, key='prior_num_inference_steps', tooltip=\"The number of Prior denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.\")\n",
        "    decoder_num_inference_row = SliderRow(label=\"Number of Decoder Inference Steps\", min=1, max=100, divisions=99, pref=unCLIP_prefs, key='decoder_num_inference_steps', tooltip=\"The number of Decoder denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.\")\n",
        "    super_res_num_inference_row = SliderRow(label=\"Number of Super-Res Inference Steps\", min=1, max=100, divisions=99, pref=unCLIP_prefs, key='decoder_num_inference_steps', tooltip=\"The number of Super-Res denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.\")\n",
        "    prior_guidance = SliderRow(label=\"Prior Guidance Scale\", min=0, max=50, divisions=100, round=1, pref=unCLIP_prefs, key='prior_guidance_scale')\n",
        "    decoder_guidance = SliderRow(label=\"Decoder Guidance Scale\", min=0, max=50, divisions=100, round=1, pref=unCLIP_prefs, key='decoder_guidance_scale')\n",
        "    batch_folder_name = TextField(label=\"Batch Folder Name\", value=unCLIP_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    use_StableUnCLIP_pipeline = Tooltip(message=\"Combines prior model (generate clip image embedding from text, UnCLIPPipeline) and decoder pipeline (decode clip image embedding to image, StableDiffusionImageVariationPipeline)\", content=Switch(label=\"Use Stable UnCLIP Pipeline Instead\", value=unCLIP_prefs['use_StableUnCLIP_pipeline'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_unCLIP))\n",
        "    #eta = TextField(label=\"ETA\", value=str(unCLIP_prefs['eta']), keyboard_type=KeyboardType.NUMBER, hint_text=\"Amount of Noise\", on_change=lambda e:changed(e,'eta', ptype='float'))\n",
        "    #eta = Slider(min=0.0, max=1.0, divisions=20, label=\"{value}\", value=float(unCLIP_prefs['eta']), tooltip=\"The weight of noise for added noise in a diffusion step. Its value is between 0.0 and 1.0 - 0.0 is DDIM and 1.0 is DDPM scheduler respectively.\", expand=True, on_change=lambda e:changed(e,'eta', ptype='float'))\n",
        "    #eta_row = Row([Text(\"DDIM ETA: \"), eta])\n",
        "    #max_size = Slider(min=256, max=1280, divisions=64, label=\"{value}px\", value=int(unCLIP_prefs['max_size']), expand=True, on_change=lambda e:changed(e,'max_size', ptype='int'))\n",
        "    #max_row = Row([Text(\"Max Resolution Size: \"), max_size])\n",
        "    apply_ESRGAN_upscale = Switch(label=\"Apply ESRGAN Upscale\", value=unCLIP_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)\n",
        "    enlarge_scale_slider = SliderRow(label=\"Enlarge Scale\", min=1, max=4, divisions=6, round=1, suffix=\"x\", pref=unCLIP_prefs, key='enlarge_scale')\n",
        "    display_upscaled_image = Checkbox(label=\"Display Upscaled Image\", value=unCLIP_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))\n",
        "    ESRGAN_settings = Container(Column([enlarge_scale_slider, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_unCLIP = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_unCLIP.height = None if status['installed_ESRGAN'] else 0\n",
        "    if not unCLIP_prefs['apply_ESRGAN_upscale']:\n",
        "        ESRGAN_settings.height = 0\n",
        "    page.unCLIP_output = Column([], auto_scroll=True)\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(page.unCLIP_output.controls) > 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üåê  unCLIP Text-to-Image Generator\", \"Hierarchical Text-Conditional Image Generation with CLIP Latents.  Similar results to DALL-E 2...\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with unCLIP Settings\", on_click=unCLIP_help)]),\n",
        "        prompt,\n",
        "        #Row([prompt, mask_image, invert_mask]),\n",
        "        prior_num_inference_row, decoder_num_inference_row, super_res_num_inference_row,\n",
        "        prior_guidance, decoder_guidance,\n",
        "        #eta_row, max_row,\n",
        "        use_StableUnCLIP_pipeline,\n",
        "        Row([NumberPicker(label=\"Number of Images: \", min=1, max=20, value=unCLIP_prefs['num_images'], on_change=lambda e: changed(e, 'num_images')), seed, batch_folder_name]),\n",
        "        page.ESRGAN_block_unCLIP,\n",
        "        Row([ElevatedButton(content=Text(\"üñáÔ∏è   Get unCLIP Generation\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_unCLIP(page)), \n",
        "             ElevatedButton(content=Text(value=\"üìú   Run from Prompts List\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_unCLIP(page, from_list=True))]),\n",
        "        page.unCLIP_output,\n",
        "        clear_button,\n",
        "      ]\n",
        "    )),\n",
        "    ], scroll=ScrollMode.AUTO, auto_scroll=False)\n",
        "    return c\n",
        "\n",
        "unCLIP_image_variation_prefs = {\n",
        "    'init_image': '',\n",
        "    'file_name': '',\n",
        "    'batch_folder_name': '',\n",
        "    'decoder_guidance_scale': 8.0,\n",
        "    'decoder_num_inference_steps': 25,\n",
        "    'super_res_num_inference_steps': 7,\n",
        "    'seed': 0,\n",
        "    'num_images': 1,\n",
        "    'max_size': 768,\n",
        "    #'variance_type': 'learned_range',#fixed_small_log\n",
        "    #'num_train_timesteps': 1000,\n",
        "    #'prediction_type': 'epsilon',#sample\n",
        "    #'clip_sample': True,\n",
        "    \"apply_ESRGAN_upscale\": prefs['apply_ESRGAN_upscale'],\n",
        "    \"enlarge_scale\": 4.0,\n",
        "    \"display_upscaled_image\": True,\n",
        "}\n",
        "def buildUnCLIP_ImageVariation(page):\n",
        "    global unCLIP_image_variation_prefs, prefs, pipe_unCLIP_image_variation\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            unCLIP_image_variation_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            unCLIP_image_variation_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            unCLIP_image_variation_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def add_to_unCLIP_image_variation_output(o):\n",
        "      page.unCLIP_image_variation_output.controls.append(o)\n",
        "      page.unCLIP_image_variation_output.update()\n",
        "      if not clear_button.visible:\n",
        "        clear_button.visible = True\n",
        "        clear_button.update()\n",
        "    page.add_to_unCLIP_image_variation_output = add_to_unCLIP_image_variation_output\n",
        "    def clear_output(e):\n",
        "      if prefs['enable_sounds']: page.snd_delete.play()\n",
        "      page.unCLIP_image_variation_output.controls = []\n",
        "      page.unCLIP_image_variation_output.update()\n",
        "      clear_button.visible = False\n",
        "      clear_button.update()\n",
        "    def unCLIP_image_variation_help(e):\n",
        "      def close_unCLIP_image_variation_dlg(e):\n",
        "        nonlocal unCLIP_image_variation_help_dlg\n",
        "        unCLIP_image_variation_help_dlg.open = False\n",
        "        page.update()\n",
        "      unCLIP_image_variation_help_dlg = AlertDialog(title=Text(\"üôÖ   Help with unCLIP Image Variation Pipeline\"), content=Column([\n",
        "          Text(\"Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we implemented a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.\"),\n",
        "          Text(\"The scheduler is a modified DDPM that has some minor variations in how it calculates the learned range variance and dynamically re-calculates betas based off the timesteps it is skipping. The scheduler also uses a slightly different step ratio when computing timesteps to use for inference.\"),\n",
        "          Markdown(\"The unCLIP Image Variation model in diffusers comes from kakaobrain's karlo and the original codebase can be found [here](https://github.com/kakaobrain/karlo). Additionally, lucidrains has a DALL-E 2 recreation [here](https://github.com/lucidrains/DALLE2-pytorch).\"),\n",
        "        ], scroll=ScrollMode.AUTO), actions=[TextButton(\"üêá  We'll see... \", on_click=close_unCLIP_image_variation_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = unCLIP_image_variation_help_dlg\n",
        "      unCLIP_image_variation_help_dlg.open = True\n",
        "      page.update()\n",
        "    def file_picker_result(e: FilePickerResultEvent):\n",
        "        if e.files != None:\n",
        "          upload_files(e)\n",
        "    def on_upload_progress(e: FilePickerUploadEvent):\n",
        "      if e.progress == 1:\n",
        "        if not slash in e.file_name:\n",
        "          fname = os.path.join(root_dir, e.file_name)\n",
        "          unCLIP_image_variation_prefs['file_name'] = e.file_name.rpartition('.')[0]\n",
        "        else:\n",
        "          fname = e.file_name\n",
        "          unCLIP_image_variation_prefs['file_name'] = e.file_name.rpartition(slash)[2].rpartition('.')[0]\n",
        "        init_image.value = fname\n",
        "        init_image.update()\n",
        "        unCLIP_image_variation_prefs['init_image'] = fname\n",
        "        page.update()\n",
        "    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)\n",
        "    def upload_files(e):\n",
        "        uf = []\n",
        "        if file_picker.result != None and file_picker.result.files != None:\n",
        "            for f in file_picker.result.files:\n",
        "              if page.web:\n",
        "                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))\n",
        "              else:\n",
        "                on_upload_progress(FilePickerUploadEvent(f.path, 1, \"\"))\n",
        "            file_picker.upload(uf)\n",
        "    page.overlay.append(file_picker)\n",
        "    def pick_init(e):\n",
        "        file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"png\", \"PNG\", \"jpg\", \"jpeg\"], dialog_title=\"Pick Init Image File\")\n",
        "    init_image = TextField(label=\"Initial Image\", value=unCLIP_image_variation_prefs['init_image'], on_change=lambda e:changed(e,'init_image'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_init))\n",
        "    def toggle_ESRGAN(e):\n",
        "        ESRGAN_settings.height = None if e.control.value else 0\n",
        "        unCLIP_image_variation_prefs['apply_ESRGAN_upscale'] = e.control.value\n",
        "        ESRGAN_settings.update()\n",
        "    def change_enlarge_scale(e):\n",
        "        enlarge_scale_slider.controls[1].value = f\" {float(e.control.value)}x\"\n",
        "        enlarge_scale_slider.update()\n",
        "        changed(e, 'enlarge_scale', ptype=\"float\")\n",
        "    #prompt = TextField(label=\"Prompt Text\", value=unCLIP_image_variation_prefs['prompt'], on_change=lambda e:changed(e,'prompt'))\n",
        "    seed = TextField(label=\"Seed\", width=90, value=str(unCLIP_image_variation_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip=\"0 or -1 picks a Random seed\", on_change=lambda e:changed(e,'seed', ptype='int'))\n",
        "    decoder_num_inference_row = SliderRow(label=\"Number of Decoder Inference Steps\", min=1, max=100, divisions=99, pref=unCLIP_image_variation_prefs, key='decoder_num_inference_steps', tooltip=\"The number of Decoder denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.\")\n",
        "    super_res_num_inference_row = SliderRow(label=\"Number of Super-Res Inference Steps\", min=1, max=100, divisions=99, pref=unCLIP_image_variation_prefs, key='decoder_num_inference_steps', tooltip=\"The number of Super-Res denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.\")\n",
        "    decoder_guidance = SliderRow(label=\"Decoder Guidance Scale\", min=0, max=50, divisions=100, round=1, pref=unCLIP_image_variation_prefs, key='decoder_guidance_scale')\n",
        "    batch_folder_name = TextField(label=\"Batch Folder Name\", value=unCLIP_image_variation_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    #eta = TextField(label=\"ETA\", value=str(unCLIP_image_variation_prefs['eta']), keyboard_type=KeyboardType.NUMBER, hint_text=\"Amount of Noise\", on_change=lambda e:changed(e,'eta', ptype='float'))\n",
        "    #eta = Slider(min=0.0, max=1.0, divisions=20, label=\"{value}\", value=float(unCLIP_image_variation_prefs['eta']), tooltip=\"The weight of noise for added noise in a diffusion step. Its value is between 0.0 and 1.0 - 0.0 is DDIM and 1.0 is DDPM scheduler respectively.\", expand=True, on_change=lambda e:changed(e,'eta', ptype='float'))\n",
        "    #eta_row = Row([Text(\"DDIM ETA: \"), eta])\n",
        "    max_row = SliderRow(label=\"Max Resolution Size\", min=256, max=1280, divisions=64, multiple=16, suffix=\"px\", pref=unCLIP_image_variation_prefs, key='max_size')\n",
        "    apply_ESRGAN_upscale = Switch(label=\"Apply ESRGAN Upscale\", value=unCLIP_image_variation_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)\n",
        "    enlarge_scale_value = Text(f\" {float(unCLIP_image_variation_prefs['enlarge_scale'])}x\", weight=FontWeight.BOLD)\n",
        "    enlarge_scale = Slider(min=1, max=4, divisions=6, label=\"{value}x\", value=unCLIP_image_variation_prefs['enlarge_scale'], on_change=change_enlarge_scale, expand=True)\n",
        "    enlarge_scale_slider = Row([Text(\"Enlarge Scale: \"), enlarge_scale_value, enlarge_scale])\n",
        "    display_upscaled_image = Checkbox(label=\"Display Upscaled Image\", value=unCLIP_image_variation_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))\n",
        "    ESRGAN_settings = Container(Column([enlarge_scale_slider, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_unCLIP_image_variation = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_unCLIP_image_variation.height = None if status['installed_ESRGAN'] else 0\n",
        "    if not unCLIP_image_variation_prefs['apply_ESRGAN_upscale']:\n",
        "        ESRGAN_settings.height = 0\n",
        "    page.unCLIP_image_variation_output = Column([], auto_scroll=True)\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(page.unCLIP_image_variation_output.controls) > 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üéÜ  unCLIP Image Variation Generator\", \"Generate Variations from an input image using unCLIP...\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with unCLIP Image Variation Settings\", on_click=unCLIP_image_variation_help)]),\n",
        "        init_image,\n",
        "        #Row([prompt, mask_image, invert_mask]),\n",
        "        decoder_num_inference_row, super_res_num_inference_row,\n",
        "        decoder_guidance,\n",
        "        #eta_row, \n",
        "        max_row,\n",
        "        Row([NumberPicker(label=\"Number of Images: \", min=1, max=20, value=unCLIP_image_variation_prefs['num_images'], on_change=lambda e: changed(e, 'num_images')), seed, batch_folder_name]),\n",
        "        page.ESRGAN_block_unCLIP_image_variation,\n",
        "        Row([ElevatedButton(content=Text(\"ü¶Ñ   Get unCLIP Image Variation\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_unCLIP_image_variation(page)), \n",
        "             ElevatedButton(content=Text(value=\"üìú   Run from Prompts List\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_unCLIP_image_variation(page, from_list=True))]),\n",
        "        page.unCLIP_image_variation_output,\n",
        "        clear_button,\n",
        "      ]\n",
        "    )), \n",
        "    ], scroll=ScrollMode.AUTO, auto_scroll=False)\n",
        "    return c\n",
        "\n",
        "unCLIP_interpolation_prefs = {\n",
        "    'prompt': '',\n",
        "    'end_prompt': '',\n",
        "    'steps': 5,\n",
        "    'batch_folder_name': '',\n",
        "    'prior_guidance_scale': 4.0,\n",
        "    'decoder_guidance_scale': 8.0,\n",
        "    'prior_num_inference_steps': 25,\n",
        "    'decoder_num_inference_steps': 25,\n",
        "    'super_res_num_inference_steps': 7,\n",
        "    'seed': 0,\n",
        "    'num_images': 1,\n",
        "    \"apply_ESRGAN_upscale\": prefs['apply_ESRGAN_upscale'],\n",
        "    \"enlarge_scale\": 4.0,\n",
        "    \"display_upscaled_image\": True,\n",
        "}\n",
        "def buildUnCLIP_Interpolation(page):\n",
        "    global unCLIP_interpolation_prefs, prefs, pipe_unCLIP_interpolation\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            unCLIP_interpolation_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            unCLIP_interpolation_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            unCLIP_interpolation_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def add_to_unCLIP_interpolation_output(o):\n",
        "      page.unCLIP_interpolation_output.controls.append(o)\n",
        "      page.unCLIP_interpolation_output.update()\n",
        "      if not clear_button.visible:\n",
        "        clear_button.visible = True\n",
        "        clear_button.update()\n",
        "    page.add_to_unCLIP_interpolation_output = add_to_unCLIP_interpolation_output\n",
        "    def clear_output(e):\n",
        "      if prefs['enable_sounds']: page.snd_delete.play()\n",
        "      page.unCLIP_interpolation_output.controls = []\n",
        "      page.unCLIP_interpolation_output.update()\n",
        "      clear_button.visible = False\n",
        "      clear_button.update()\n",
        "    def unCLIP_interpolation_help(e):\n",
        "      def close_unCLIP_interpolation_dlg(e):\n",
        "        nonlocal unCLIP_interpolation_help_dlg\n",
        "        unCLIP_interpolation_help_dlg.open = False\n",
        "        page.update()\n",
        "      unCLIP_interpolation_help_dlg = AlertDialog(title=Text(\"üôÖ   Help with unCLIP Text Interpolation Pipeline\"), content=Column([\n",
        "          Text(\"This Diffusion Pipeline takes two prompts and interpolates between the two input prompts using spherical interpolation ( slerp ). The input prompts are converted to text embeddings by the pipeline's text_encoder and the interpolation is done on the resulting text_embeddings over the number of steps specified. Defaults to 5 steps.\"),\n",
        "          Text(\"Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we implemented a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.\"),\n",
        "          Text(\"The scheduler is a modified DDPM that has some minor variations in how it calculates the learned range variance and dynamically re-calculates betas based off the timesteps it is skipping. The scheduler also uses a slightly different step ratio when computing timesteps to use for inference.\"),\n",
        "          Markdown(\"The unCLIP model in diffusers comes from kakaobrain's karlo and the original codebase can be found [here](https://github.com/kakaobrain/karlo). Additionally, lucidrains has a DALL-E 2 recreation [here](https://github.com/lucidrains/DALLE2-pytorch).\"),\n",
        "        ], scroll=ScrollMode.AUTO), actions=[TextButton(\"üòï  Transformative... \", on_click=close_unCLIP_interpolation_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = unCLIP_interpolation_help_dlg\n",
        "      unCLIP_interpolation_help_dlg.open = True\n",
        "      page.update()\n",
        "    def toggle_ESRGAN(e):\n",
        "        ESRGAN_settings.height = None if e.control.value else 0\n",
        "        unCLIP_interpolation_prefs['apply_ESRGAN_upscale'] = e.control.value\n",
        "        ESRGAN_settings.update()\n",
        "    def change_enlarge_scale(e):\n",
        "        enlarge_scale_slider.controls[1].value = f\" {float(e.control.value)}x\"\n",
        "        enlarge_scale_slider.update()\n",
        "        changed(e, 'enlarge_scale', ptype=\"float\")\n",
        "    prompt = TextField(label=\"Start Prompt Text\", value=unCLIP_interpolation_prefs['prompt'], on_change=lambda e:changed(e,'prompt'))\n",
        "    end_prompt = TextField(label=\"End Prompt Text\", value=unCLIP_interpolation_prefs['end_prompt'], on_change=lambda e:changed(e,'end_prompt'))\n",
        "    seed = TextField(label=\"Seed\", width=90, value=str(unCLIP_interpolation_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip=\"0 or -1 picks a Random seed\", on_change=lambda e:changed(e,'seed', ptype='int'))\n",
        "    interpolation_steps_row = SliderRow(label=\"Number of Interpolation Step\", min=1, max=50, divisions=49, pref=unCLIP_interpolation_prefs, key='steps', tooltip=\"The number of steps over which to interpolate from start_prompt to end_prompt.\")\n",
        "    prior_num_inference_row = SliderRow(label=\"Number of Prior Inference Steps\", min=1, max=100, divisions=99, pref=unCLIP_interpolation_prefs, key='prior_num_inference_steps', tooltip=\"The number of Prior denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.\")\n",
        "    decoder_num_inference_row = SliderRow(label=\"Number of Decoder Inference Steps\", min=1, max=100, divisions=99, pref=unCLIP_interpolation_prefs, key='decoder_num_inference_steps', tooltip=\"The number of Decoder denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.\")\n",
        "    super_res_num_inference_row = SliderRow(label=\"Number of Super-Res Inference Steps\", min=1, max=100, divisions=99, pref=unCLIP_interpolation_prefs, key='decoder_num_inference_steps', tooltip=\"The number of Super-Res denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.\")\n",
        "    prior_guidance = SliderRow(label=\"Prior Guidance Scale\", min=0, max=50, divisions=100, round=1, pref=unCLIP_interpolation_prefs, key='prior_guidance_scale')\n",
        "    decoder_guidance = SliderRow(label=\"Decoder Guidance Scale\", min=0, max=50, divisions=100, round=1, pref=unCLIP_interpolation_prefs, key='decoder_guidance_scale')\n",
        "    batch_folder_name = TextField(label=\"Batch Folder Name\", value=unCLIP_interpolation_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    #use_StableunCLIP_interpolation_pipeline = Tooltip(message=\"Combines prior model (generate clip image embedding from text, UnCLIPPipeline) and decoder pipeline (decode clip image embedding to image, StableDiffusionImageVariationPipeline)\", content=Switch(label=\"Use Stable UnCLIP Pipeline Instead\", value=unCLIP_interpolation_prefs['use_StableunCLIP_interpolation_pipeline'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'use_StableunCLIP_interpolation_pipeline')))\n",
        "    #eta = TextField(label=\"ETA\", value=str(unCLIP_interpolation_prefs['eta']), keyboard_type=KeyboardType.NUMBER, hint_text=\"Amount of Noise\", on_change=lambda e:changed(e,'eta', ptype='float'))\n",
        "    #eta = Slider(min=0.0, max=1.0, divisions=20, label=\"{value}\", value=float(unCLIP_interpolation_prefs['eta']), tooltip=\"The weight of noise for added noise in a diffusion step. Its value is between 0.0 and 1.0 - 0.0 is DDIM and 1.0 is DDPM scheduler respectively.\", expand=True, on_change=lambda e:changed(e,'eta', ptype='float'))\n",
        "    #eta_row = Row([Text(\"DDIM ETA: \"), eta])\n",
        "    #max_size = Slider(min=256, max=1280, divisions=64, label=\"{value}px\", value=int(unCLIP_interpolation_prefs['max_size']), expand=True, on_change=lambda e:changed(e,'max_size', ptype='int'))\n",
        "    #max_row = Row([Text(\"Max Resolution Size: \"), max_size])\n",
        "    apply_ESRGAN_upscale = Switch(label=\"Apply ESRGAN Upscale\", value=unCLIP_interpolation_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)\n",
        "    enlarge_scale_value = Text(f\" {float(unCLIP_interpolation_prefs['enlarge_scale'])}x\", weight=FontWeight.BOLD)\n",
        "    enlarge_scale = Slider(min=1, max=4, divisions=6, label=\"{value}x\", value=unCLIP_interpolation_prefs['enlarge_scale'], on_change=change_enlarge_scale, expand=True)\n",
        "    enlarge_scale_slider = Row([Text(\"Enlarge Scale: \"), enlarge_scale_value, enlarge_scale])\n",
        "    display_upscaled_image = Checkbox(label=\"Display Upscaled Image\", value=unCLIP_interpolation_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))\n",
        "    ESRGAN_settings = Container(Column([enlarge_scale_slider, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_unCLIP_interpolation = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_unCLIP_interpolation.height = None if status['installed_ESRGAN'] else 0\n",
        "    if not unCLIP_interpolation_prefs['apply_ESRGAN_upscale']:\n",
        "        ESRGAN_settings.height = 0\n",
        "    page.unCLIP_interpolation_output = Column([], auto_scroll=True)\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(page.unCLIP_interpolation_output.controls) > 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üåå  unCLIP Text Interpolation Generator\", \"Takes two prompts and interpolates between the two input prompts using spherical interpolation...\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with unCLIP Settings\", on_click=unCLIP_interpolation_help)]),\n",
        "        prompt,\n",
        "        end_prompt,\n",
        "        #Row([prompt, mask_image, invert_mask]),\n",
        "        interpolation_steps_row,\n",
        "        prior_num_inference_row, decoder_num_inference_row, super_res_num_inference_row,\n",
        "        prior_guidance, decoder_guidance,\n",
        "        #eta_row, max_row,\n",
        "        #use_StableunCLIP_interpolation_pipeline,\n",
        "        #NumberPicker(label=\"Number of Images: \", min=1, max=20, value=unCLIP_interpolation_prefs['num_images'], on_change=lambda e: changed(e, 'num_images')), \n",
        "        Row([seed, batch_folder_name]),\n",
        "        page.ESRGAN_block_unCLIP_interpolation,\n",
        "        Row([ElevatedButton(content=Text(\"üéÜ   Get unCLIP Interpolations\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_unCLIP_interpolation(page))]),\n",
        "             #ElevatedButton(content=Text(value=\"üìú   Run from Prompts List\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_unCLIP_interpolation(page, from_list=True))]),\n",
        "        page.unCLIP_interpolation_output,\n",
        "        clear_button,\n",
        "      ]\n",
        "    )), \n",
        "    ], scroll=ScrollMode.AUTO, auto_scroll=False)\n",
        "    return c\n",
        "\n",
        "unCLIP_image_interpolation_prefs = {\n",
        "    'init_image': '',\n",
        "    'end_image': '',\n",
        "    'file_name': '',\n",
        "    'batch_folder_name': '',\n",
        "    'interpolation_steps': 6,\n",
        "    'decoder_guidance_scale': 8.0,\n",
        "    'decoder_num_inference_steps': 25,\n",
        "    'super_res_num_inference_steps': 7,\n",
        "    'seed': 0,\n",
        "    'num_images': 1,\n",
        "    'max_size': 768,\n",
        "    #'variance_type': 'learned_range',#fixed_small_log\n",
        "    #'num_train_timesteps': 1000,\n",
        "    #'prediction_type': 'epsilon',#sample\n",
        "    #'clip_sample': True,\n",
        "    \"apply_ESRGAN_upscale\": prefs['apply_ESRGAN_upscale'],\n",
        "    \"enlarge_scale\": 4.0,\n",
        "    \"display_upscaled_image\": True,\n",
        "}\n",
        "def buildUnCLIP_ImageInterpolation(page):\n",
        "    global unCLIP_image_interpolation_prefs, prefs, pipe_unCLIP_image_interpolation\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            unCLIP_image_interpolation_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            unCLIP_image_interpolation_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            unCLIP_image_interpolation_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def add_to_unCLIP_image_interpolation_output(o):\n",
        "      page.unCLIP_image_interpolation_output.controls.append(o)\n",
        "      page.unCLIP_image_interpolation_output.update()\n",
        "      if not clear_button.visible:\n",
        "        clear_button.visible = True\n",
        "        clear_button.update()\n",
        "    page.add_to_unCLIP_image_interpolation_output = add_to_unCLIP_image_interpolation_output\n",
        "    def clear_output(e):\n",
        "      if prefs['enable_sounds']: page.snd_delete.play()\n",
        "      page.unCLIP_image_interpolation_output.controls = []\n",
        "      page.unCLIP_image_interpolation_output.update()\n",
        "      clear_button.visible = False\n",
        "      clear_button.update()\n",
        "    def unCLIP_image_interpolation_help(e):\n",
        "      def close_unCLIP_image_interpolation_dlg(e):\n",
        "        nonlocal unCLIP_image_interpolation_help_dlg\n",
        "        unCLIP_image_interpolation_help_dlg.open = False\n",
        "        page.update()\n",
        "      unCLIP_image_interpolation_help_dlg = AlertDialog(title=Text(\"üôÖ   Help with unCLIP Image Interpolation Pipeline\"), content=Column([\n",
        "          Text(\"This Diffusion Pipeline takes two images or an image_embeddings tensor of size 2 and interpolates between their embeddings using spherical interpolation ( slerp ). The input images/image_embeddings are converted to image embeddings by the pipeline's image_encoder and the interpolation is done on the resulting image_embeddings over the number of steps specified.\"),\n",
        "          #Text(\"\"),\n",
        "          #Markdown(\"The unCLIP Image Interpolation model in diffusers comes from kakaobrain's karlo and the original codebase can be found [here](https://github.com/kakaobrain/karlo). Additionally, lucidrains has a DALL-E 2 recreation [here](https://github.com/lucidrains/DALLE2-pytorch).\"),\n",
        "        ], scroll=ScrollMode.AUTO), actions=[TextButton(\"ü¶ø  Transformers Activate... \", on_click=close_unCLIP_image_interpolation_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = unCLIP_image_interpolation_help_dlg\n",
        "      unCLIP_image_interpolation_help_dlg.open = True\n",
        "      page.update()\n",
        "    pick_type = \"\"\n",
        "    def file_picker_result(e: FilePickerResultEvent):\n",
        "        if e.files != None:\n",
        "          upload_files(e)\n",
        "    def on_upload_progress(e: FilePickerUploadEvent):\n",
        "      nonlocal pick_type\n",
        "      if e.progress == 1:\n",
        "        if not slash in e.file_name:\n",
        "          fname = os.path.join(root_dir, e.file_name)\n",
        "          unCLIP_image_interpolation_prefs['file_name'] = e.file_name.rpartition('.')[0]\n",
        "        else:\n",
        "          fname = e.file_name\n",
        "          unCLIP_image_interpolation_prefs['file_name'] = e.file_name.rpartition(slash)[2].rpartition('.')[0]\n",
        "        if pick_type == \"init\":\n",
        "            init_image.value = fname\n",
        "            init_image.update()\n",
        "            unCLIP_image_interpolation_prefs['init_image'] = fname\n",
        "        elif pick_type == \"end\":\n",
        "            end_image.value = fname\n",
        "            end_image.update()\n",
        "            unCLIP_image_interpolation_prefs['end_image'] = fname\n",
        "        page.update()\n",
        "    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)\n",
        "    def upload_files(e):\n",
        "        uf = []\n",
        "        if file_picker.result != None and file_picker.result.files != None:\n",
        "            for f in file_picker.result.files:\n",
        "              if page.web:\n",
        "                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))\n",
        "              else:\n",
        "                on_upload_progress(FilePickerUploadEvent(f.path, 1, \"\"))\n",
        "            file_picker.upload(uf)\n",
        "    page.overlay.append(file_picker)\n",
        "    def pick_init(e):\n",
        "        nonlocal pick_type\n",
        "        pick_type = \"init\"\n",
        "        file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"png\", \"PNG\", \"jpg\", \"jpeg\"], dialog_title=\"Pick Init Image File\")\n",
        "    def pick_end(e):\n",
        "        nonlocal pick_type\n",
        "        pick_type = \"end\"\n",
        "        file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"png\", \"PNG\", \"jpg\", \"jpeg\"], dialog_title=\"Pick Ending Image File\")\n",
        "    init_image = TextField(label=\"Initial Image\", value=unCLIP_image_interpolation_prefs['init_image'], on_change=lambda e:changed(e,'init_image'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_init))\n",
        "    end_image = TextField(label=\"Ending Image\", value=unCLIP_image_interpolation_prefs['end_image'], on_change=lambda e:changed(e,'end_image'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_end))\n",
        "    def toggle_ESRGAN(e):\n",
        "        ESRGAN_settings.height = None if e.control.value else 0\n",
        "        unCLIP_image_interpolation_prefs['apply_ESRGAN_upscale'] = e.control.value\n",
        "        ESRGAN_settings.update()\n",
        "    def change_enlarge_scale(e):\n",
        "        enlarge_scale_slider.controls[1].value = f\" {float(e.control.value)}x\"\n",
        "        enlarge_scale_slider.update()\n",
        "        changed(e, 'enlarge_scale', ptype=\"float\")\n",
        "    #prompt = TextField(label=\"Prompt Text\", value=unCLIP_image_interpolation_prefs['prompt'], on_change=lambda e:changed(e,'prompt'))\n",
        "    seed = TextField(label=\"Seed\", width=90, value=str(unCLIP_image_interpolation_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip=\"0 or -1 picks a Random seed\", on_change=lambda e:changed(e,'seed', ptype='int'))\n",
        "    interpolation_steps = SliderRow(label=\"Interpolation Steps\", min=1, max=100, divisions=99, pref=unCLIP_image_interpolation_prefs, key='interpolation_steps', tooltip=\"The number of interpolation images to generate.\")\n",
        "    decoder_num_inference_row = SliderRow(label=\"Number of Decoder Inference Steps\", min=1, max=100, divisions=99, pref=unCLIP_image_interpolation_prefs, key='decoder_num_inference_steps', tooltip=\"The number of Decoder denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.\")\n",
        "    super_res_num_inference_row = SliderRow(label=\"Number of Super-Res Inference Steps\", min=1, max=100, divisions=99, pref=unCLIP_image_interpolation_prefs, key='decoder_num_inference_steps', tooltip=\"The number of Super-Res denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.\")\n",
        "    decoder_guidance = SliderRow(label=\"Decoder Guidance Scale\", min=0, max=50, divisions=100, round=1, pref=unCLIP_image_interpolation_prefs, key='decoder_guidance_scale')\n",
        "    batch_folder_name = TextField(label=\"Batch Folder Name\", value=unCLIP_image_interpolation_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    #eta = TextField(label=\"ETA\", value=str(unCLIP_image_interpolation_prefs['eta']), keyboard_type=KeyboardType.NUMBER, hint_text=\"Amount of Noise\", on_change=lambda e:changed(e,'eta', ptype='float'))\n",
        "    #eta = Slider(min=0.0, max=1.0, divisions=20, label=\"{value}\", value=float(unCLIP_image_interpolation_prefs['eta']), tooltip=\"The weight of noise for added noise in a diffusion step. Its value is between 0.0 and 1.0 - 0.0 is DDIM and 1.0 is DDPM scheduler respectively.\", expand=True, on_change=lambda e:changed(e,'eta', ptype='float'))\n",
        "    #eta_row = Row([Text(\"DDIM ETA: \"), eta])\n",
        "    max_row = SliderRow(label=\"Max Resolution Size\", min=256, max=1280, divisions=64, multiple=16, suffix=\"px\", pref=unCLIP_image_interpolation_prefs, key='max_size')\n",
        "    apply_ESRGAN_upscale = Switch(label=\"Apply ESRGAN Upscale\", value=unCLIP_image_interpolation_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)\n",
        "    enlarge_scale_value = Text(f\" {float(unCLIP_image_interpolation_prefs['enlarge_scale'])}x\", weight=FontWeight.BOLD)\n",
        "    enlarge_scale = Slider(min=1, max=4, divisions=6, label=\"{value}x\", value=unCLIP_image_interpolation_prefs['enlarge_scale'], on_change=change_enlarge_scale, expand=True)\n",
        "    enlarge_scale_slider = Row([Text(\"Enlarge Scale: \"), enlarge_scale_value, enlarge_scale])\n",
        "    display_upscaled_image = Checkbox(label=\"Display Upscaled Image\", value=unCLIP_image_interpolation_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))\n",
        "    ESRGAN_settings = Container(Column([enlarge_scale_slider, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_unCLIP_image_interpolation = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_unCLIP_image_interpolation.height = None if status['installed_ESRGAN'] else 0\n",
        "    if not unCLIP_image_interpolation_prefs['apply_ESRGAN_upscale']:\n",
        "        ESRGAN_settings.height = 0\n",
        "    page.unCLIP_image_interpolation_output = Column([], auto_scroll=True)\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(page.unCLIP_image_interpolation_output.controls) > 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"ü§ñ  unCLIP Image Interpolation Generator\", \"Pass two images and produces in-betweens while interpolating between their image-embeddings...\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with unCLIP Image Interpolation Settings\", on_click=unCLIP_image_interpolation_help)]),\n",
        "        init_image, end_image,\n",
        "        interpolation_steps,\n",
        "        #Row([prompt, mask_image, invert_mask]),\n",
        "        decoder_num_inference_row, super_res_num_inference_row,\n",
        "        decoder_guidance,\n",
        "        #eta_row, \n",
        "        max_row,\n",
        "        Row([NumberPicker(label=\"Number of Images: \", min=1, max=20, value=unCLIP_image_interpolation_prefs['num_images'], on_change=lambda e: changed(e, 'num_images')), seed, batch_folder_name]),\n",
        "        page.ESRGAN_block_unCLIP_image_interpolation,\n",
        "        Row([ElevatedButton(content=Text(\"ü¶æ   Get unCLIP Image Interpolation\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_unCLIP_image_interpolation(page)), \n",
        "             #ElevatedButton(content=Text(value=\"üìú   Run from Prompts List\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_unCLIP_image_interpolation(page, from_list=True))\n",
        "             ]),\n",
        "        \n",
        "      ]\n",
        "    )), page.unCLIP_image_interpolation_output,\n",
        "        clear_button,\n",
        "    ], scroll=ScrollMode.AUTO, auto_scroll=False)\n",
        "    return c\n",
        "\n",
        "\n",
        "magic_mix_prefs = {\n",
        "    'init_image': '',\n",
        "    'prompt': '',\n",
        "    'guidance_scale': 7.5,\n",
        "    'num_inference_steps': 50,\n",
        "    'mix_factor': 0.5,\n",
        "    'kmin': 0.3,\n",
        "    'kmax': 0.6,\n",
        "    'seed': 0,\n",
        "    'num_images': 1,\n",
        "    'max_size': 1024,\n",
        "    'scheduler_mode': 'DDIM',\n",
        "    'scheduler_last': '',\n",
        "    'batch_folder_name': '',\n",
        "    'file_name': '',\n",
        "    \"apply_ESRGAN_upscale\": prefs['apply_ESRGAN_upscale'],\n",
        "    \"enlarge_scale\": 4.0,\n",
        "    \"display_upscaled_image\": False,\n",
        "}\n",
        "def buildMagicMix(page):\n",
        "    global magic_mix_prefs, prefs, pipe_magic_mix\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            magic_mix_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            magic_mix_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            magic_mix_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def add_to_magic_mix_output(o):\n",
        "      page.MagicMix.controls.append(o)\n",
        "      page.MagicMix.update()\n",
        "      if not clear_button.visible:\n",
        "        clear_button.visible = True\n",
        "        clear_button.update()\n",
        "    page.add_to_magic_mix_output = add_to_magic_mix_output\n",
        "    def clear_output(e):\n",
        "      if prefs['enable_sounds']: page.snd_delete.play()\n",
        "      for i, c in enumerate(page.MagicMix.controls):\n",
        "        if i == 0: continue\n",
        "        else: del page.MagicMix.controls[i]\n",
        "      page.MagicMix.update()\n",
        "      clear_button.visible = False\n",
        "      clear_button.update()\n",
        "    def magic_mix_help(e):\n",
        "      def close_magic_mix_dlg(e):\n",
        "        nonlocal magic_mix_help_dlg\n",
        "        magic_mix_help_dlg.open = False\n",
        "        page.update()\n",
        "      magic_mix_help_dlg = AlertDialog(title=Text(\"üôÖ   Help with MagicMix\"), content=Column([\n",
        "          Text(\"Have you ever imagined what a corgi-alike coffee machine or a tiger-alike rabbit would look like? In this work, we attempt to answer these questions by exploring a new task called semantic mixing, aiming at blending two different semantics to create a new concept (e.g., corgi + coffee machine -- > corgi-alike coffee machine). Unlike style transfer, where an image is stylized according to the reference style without changing the image content, semantic blending mixes two different concepts in a semantic manner to synthesize a novel concept while preserving the spatial layout and geometry. To this end, we present MagicMix, a simple yet effective solution based on pre-trained text-conditioned diffusion models. Motivated by the progressive generation property of diffusion models where layout/shape emerges at early denoising steps while semantically meaningful details appear at later steps during the denoising process, our method first obtains a coarse layout (either by corrupting an image or denoising from a pure Gaussian noise given a text prompt), followed by injection of conditional prompt for semantic mixing. Our method does not require any spatial mask or re-training, yet is able to synthesize novel objects with high fidelity. To improve the mixing quality, we further devise two simple strategies to provide better control and flexibility over the synthesized content. With our method, we present our results over diverse downstream applications, including semantic style transfer, novel object synthesis, breed mixing, and concept removal, demonstrating the flexibility of our method.\"),\n",
        "        ], scroll=ScrollMode.AUTO), actions=[TextButton(\"üßô  Sounds like magic... \", on_click=close_magic_mix_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = magic_mix_help_dlg\n",
        "      magic_mix_help_dlg.open = True\n",
        "      page.update()\n",
        "    def file_picker_result(e: FilePickerResultEvent):\n",
        "        if e.files != None:\n",
        "          upload_files(e)\n",
        "    def on_upload_progress(e: FilePickerUploadEvent):\n",
        "      if e.progress == 1:\n",
        "        if not slash in e.file_name:\n",
        "          fname = os.path.join(root_dir, e.file_name)\n",
        "          magic_mix_prefs['file_name'] = e.file_name.rpartition('.')[0]\n",
        "        else:\n",
        "          fname = e.file_name\n",
        "          magic_mix_prefs['file_name'] = e.file_name.rpartition(slash)[2].rpartition('.')[0]\n",
        "        init_image.value = fname\n",
        "        init_image.update()\n",
        "        magic_mix_prefs['init_image'] = fname\n",
        "        page.update()\n",
        "    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)\n",
        "    def upload_files(e):\n",
        "        uf = []\n",
        "        if file_picker.result != None and file_picker.result.files != None:\n",
        "            for f in file_picker.result.files:\n",
        "              if page.web:\n",
        "                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))\n",
        "              else:\n",
        "                on_upload_progress(FilePickerUploadEvent(f.path, 1, \"\"))\n",
        "            file_picker.upload(uf)\n",
        "    page.overlay.append(file_picker)\n",
        "    def pick_init(e):\n",
        "        file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"png\", \"PNG\", \"jpg\", \"jpeg\"], dialog_title=\"Pick Init Image File\")\n",
        "    prompt = TextField(label=\"Prompt Text\", value=magic_mix_prefs['prompt'], on_change=lambda e:changed(e,'prompt'))\n",
        "    def toggle_ESRGAN(e):\n",
        "        ESRGAN_settings.height = None if e.control.value else 0\n",
        "        magic_mix_prefs['apply_ESRGAN_upscale'] = e.control.value\n",
        "        ESRGAN_settings.update()\n",
        "    init_image = TextField(label=\"Initial Image\", value=magic_mix_prefs['init_image'], on_change=lambda e:changed(e,'init_image'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_init))\n",
        "    seed = TextField(label=\"Seed\", width=90, value=str(magic_mix_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip=\"0 or -1 picks a Random seed\", on_change=lambda e:changed(e,'seed', ptype='int'))\n",
        "    scheduler_mode = Dropdown(label=\"Scheduler/Sampler Mode\", hint_text=\"They're very similar, with minor differences in the noise\", width=200,\n",
        "            options=[\n",
        "                dropdown.Option(\"DDIM\"),\n",
        "                dropdown.Option(\"LMS Discrete\"),\n",
        "                dropdown.Option(\"PNDM\"),\n",
        "            ], value=magic_mix_prefs['scheduler_mode'], autofocus=False, on_change=lambda e:changed(e, 'scheduler_mode'),\n",
        "        )\n",
        "    num_inference_row = SliderRow(label=\"Number of Inference Steps\", min=1, max=100, divisions=99, pref=magic_mix_prefs, key='num_inference_steps', tooltip=\"The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.\")   \n",
        "    guidance = SliderRow(label=\"Guidance Scale\", min=0, max=50, divisions=100, round=1, pref=magic_mix_prefs, key='guidance_scale')\n",
        "    mix_factor_row = SliderRow(label=\"Mix Factor\", min=0.0, max=1.0, divisions=20, round=2, pref=magic_mix_prefs, key='mix_factor', tooltip=\"Interpolation constant used in the layout generation phase. The greater the value of `mix_factor`, the greater the influence of the prompt on the layout generation process.\")\n",
        "    kmin_row = SliderRow(label=\"k-Min\", min=0.0, max=1.0, divisions=20, round=2, pref=magic_mix_prefs, key='kmin', tooltip=\"A higher value of kmin results in more steps for content generation process. Determine the range for the layout and content generation process.\")\n",
        "    kmax_row = SliderRow(label=\"k-Max\", min=0.0, max=1.0, divisions=20, round=2, pref=magic_mix_prefs, key='kmax', tooltip=\"A higher value of kmax results in loss of more information about the layout of the original image. Determine the range for the layout and content generation process.\")\n",
        "    max_row = SliderRow(label=\"Max Resolution Size\", min=256, max=1280, divisions=64, multiple=16, suffix=\"px\", pref=magic_mix_prefs, key='max_size')\n",
        "    batch_folder_name = TextField(label=\"Batch Folder Name\", value=magic_mix_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    apply_ESRGAN_upscale = Switch(label=\"Apply ESRGAN Upscale\", value=magic_mix_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)\n",
        "    enlarge_scale_slider = SliderRow(label=\"Enlarge Scale\", min=1, max=4, divisions=6, round=1, suffix=\"x\", pref=magic_mix_prefs, key='enlarge_scale')\n",
        "    display_upscaled_image = Checkbox(label=\"Display Upscaled Image\", value=magic_mix_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))\n",
        "    ESRGAN_settings = Container(Column([enlarge_scale_slider, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_magic_mix = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_magic_mix.height = None if status['installed_ESRGAN'] else 0\n",
        "    if not unCLIP_prefs['apply_ESRGAN_upscale']:\n",
        "        ESRGAN_settings.height = 0\n",
        "\n",
        "    page.magic_mix_output = Column([], auto_scroll=True)\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(page.magic_mix_output.controls) > 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üßö  MagicMix Init Image with Prompt\", \"Diffusion Pipeline for semantic mixing of an image and a text prompt...\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with MagicMix Settings\", on_click=magic_mix_help)]),\n",
        "        init_image,\n",
        "        prompt,\n",
        "        scheduler_mode,\n",
        "        num_inference_row,\n",
        "        guidance,\n",
        "        mix_factor_row,\n",
        "        ResponsiveRow([kmin_row, kmax_row]),\n",
        "        max_row,\n",
        "        Row([NumberPicker(label=\"Number of Images: \", min=1, max=8, value=magic_mix_prefs['num_images'], on_change=lambda e: changed(e, 'num_images')), seed, batch_folder_name]),\n",
        "        page.ESRGAN_block_magic_mix,\n",
        "        Row([ElevatedButton(content=Text(\"ü™Ñ  Make MagicMix\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_magic_mix(page)), \n",
        "             ElevatedButton(content=Text(value=\"üìú   Run from Prompts List\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_magic_mix(page, from_list=True))]),\n",
        "        page.magic_mix_output,\n",
        "        clear_button,\n",
        "      ]\n",
        "    ))], scroll=ScrollMode.AUTO, auto_scroll=False)\n",
        "    return c\n",
        "\n",
        "paint_by_example_prefs = {\n",
        "    'original_image': '',\n",
        "    'mask_image': '',\n",
        "    'example_image': '',\n",
        "    'num_inference_steps': 50,\n",
        "    'guidance_scale': 7.5,\n",
        "    'eta': 0.0,\n",
        "    'seed': 0,\n",
        "    'max_size': 768,\n",
        "    'alpha_mask': False,\n",
        "    'invert_mask': False,\n",
        "    'num_images': 1,\n",
        "    'batch_folder_name': '',\n",
        "    \"apply_ESRGAN_upscale\": prefs['apply_ESRGAN_upscale'],\n",
        "    \"enlarge_scale\": 2.0,\n",
        "    \"display_upscaled_image\": False,\n",
        "}\n",
        "def buildPaintByExample(page):\n",
        "    global paint_by_example_prefs, prefs, pipe_paint_by_example\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            paint_by_example_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            paint_by_example_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            paint_by_example_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def add_to_paint_by_example_output(o):\n",
        "      page.paint_by_example_output.controls.append(o)\n",
        "      page.paint_by_example_output.update()\n",
        "      if not clear_button.visible:\n",
        "        clear_button.visible = True\n",
        "        clear_button.update()\n",
        "    def clear_output(e):\n",
        "      if prefs['enable_sounds']: page.snd_delete.play()\n",
        "      page.paint_by_example_output.controls = []\n",
        "      page.paint_by_example_output.update()\n",
        "      clear_button.visible = False\n",
        "      clear_button.update()\n",
        "    def paint_by_example_help(e):\n",
        "      def close_paint_by_example_dlg(e):\n",
        "        nonlocal paint_by_example_help_dlg\n",
        "        paint_by_example_help_dlg.open = False\n",
        "        page.update()\n",
        "      paint_by_example_help_dlg = AlertDialog(title=Text(\"üíÅ   Help with Paint-by-Example\"), content=Column([\n",
        "          Text(\"Language-guided image editing has achieved great success recently. In this pipeline, we use exemplar-guided image editing for more precise control. We achieve this goal by leveraging self-supervised training to disentangle and re-organize the source image and the exemplar. However, the naive approach will cause obvious fusing artifacts. We carefully analyze it and propose an information bottleneck and strong augmentations to avoid the trivial solution of directly copying and pasting the exemplar image. Meanwhile, to ensure the controllability of the editing process, we design an arbitrary shape mask for the exemplar image and leverage the classifier-free guidance to increase the similarity to the exemplar image. The whole framework involves a single forward of the diffusion model without any iterative optimization. We demonstrate that our method achieves an impressive performance and enables controllable editing on in-the-wild images with high fidelity.  Credit goes to https://github.com/Fantasy-Studio/Paint-by-Example\"),\n",
        "        ], scroll=ScrollMode.AUTO), actions=[TextButton(\"üò∏  Sweetness... \", on_click=close_paint_by_example_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = paint_by_example_help_dlg\n",
        "      paint_by_example_help_dlg.open = True\n",
        "      page.update()\n",
        "    def file_picker_result(e: FilePickerResultEvent):\n",
        "        if e.files != None:\n",
        "          upload_files(e)\n",
        "    def on_upload_progress(e: FilePickerUploadEvent):\n",
        "      nonlocal pick_type\n",
        "      if e.progress == 1:\n",
        "        if not slash in e.file_name:\n",
        "          fname = os.path.join(root_dir, e.file_name)\n",
        "          paint_by_example_prefs['file_name'] = e.file_name.rpartition('.')[0]\n",
        "        else:\n",
        "          fname = e.file_name\n",
        "          paint_by_example_prefs['file_name'] = e.file_name.rpartition(slash)[2].rpartition('.')[0]\n",
        "        if pick_type == \"original\":\n",
        "          original_image.value = fname\n",
        "          original_image.update()\n",
        "          paint_by_example_prefs['original_image'] = fname\n",
        "        elif pick_type == \"mask\":\n",
        "          mask_image.value = fname\n",
        "          mask_image.update()\n",
        "          paint_by_example_prefs['mask_image'] = fname\n",
        "        elif pick_type == \"example\":\n",
        "          example_image.value = fname\n",
        "          example_image.update()\n",
        "          paint_by_example_prefs['example_image'] = fname\n",
        "        page.update()\n",
        "    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)\n",
        "    def upload_files(e):\n",
        "        uf = []\n",
        "        if file_picker.result != None and file_picker.result.files != None:\n",
        "            for f in file_picker.result.files:\n",
        "              if page.web:\n",
        "                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))\n",
        "              else:\n",
        "                on_upload_progress(FilePickerUploadEvent(f.path, 1, \"\"))\n",
        "            file_picker.upload(uf)\n",
        "    page.overlay.append(file_picker)\n",
        "    pick_type = \"\"\n",
        "    #page.overlay.append(pick_files_dialog)\n",
        "    def pick_original(e):\n",
        "        nonlocal pick_type\n",
        "        pick_type = \"original\"\n",
        "        file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"png\", \"PNG\", \"jpg\", \"jpeg\"], dialog_title=\"Pick Original Image File\")\n",
        "    def pick_mask(e):\n",
        "        nonlocal pick_type\n",
        "        pick_type = \"mask\"\n",
        "        file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"png\", \"PNG\", \"jpg\", \"jpeg\"], dialog_title=\"Pick Black & White Mask Image\")\n",
        "    def pick_example(e):\n",
        "        nonlocal pick_type\n",
        "        pick_type = \"example\"\n",
        "        file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"png\", \"PNG\", \"jpg\", \"jpeg\"], dialog_title=\"Pick Example Style Image\")\n",
        "    def toggle_ESRGAN(e):\n",
        "        ESRGAN_settings.height = None if e.control.value else 0\n",
        "        paint_by_example_prefs['apply_ESRGAN_upscale'] = e.control.value\n",
        "        ESRGAN_settings.update()\n",
        "    def change_eta(e):\n",
        "        changed(e, 'eta', ptype=\"float\")\n",
        "        eta_value.value = f\" {paint_by_example_prefs['eta']}\"\n",
        "        eta_value.update()\n",
        "        eta_row.update()\n",
        "    original_image = TextField(label=\"Original Image\", value=paint_by_example_prefs['original_image'], expand=1, on_change=lambda e:changed(e,'original_image'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_original))\n",
        "    mask_image = TextField(label=\"Mask Image\", value=paint_by_example_prefs['mask_image'], expand=1, on_change=lambda e:changed(e,'mask_image'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD_OUTLINED, on_click=pick_mask))\n",
        "    alpha_mask = Checkbox(label=\"Alpha Mask\", value=paint_by_example_prefs['alpha_mask'], tooltip=\"Use Transparent Alpha Channel of Init as Mask\", fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'alpha_mask'))\n",
        "    invert_mask = Checkbox(label=\"Invert\", tooltip=\"Swaps the Black & White of your Mask Image\", value=paint_by_example_prefs['invert_mask'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'invert_mask'))\n",
        "    example_image = TextField(label=\"Example Style Image\", value=paint_by_example_prefs['example_image'], on_change=lambda e:changed(e,'example_image'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_example))\n",
        "    seed = TextField(label=\"Seed\", width=90, value=str(paint_by_example_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip=\"0 or -1 picks a Random seed\", on_change=lambda e:changed(e,'seed', ptype='int'))\n",
        "    num_inference_row = SliderRow(label=\"Number of Inference Steps\", min=1, max=100, divisions=99, pref=paint_by_example_prefs, key='num_inference_steps', tooltip=\"The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.\")   \n",
        "    guidance = SliderRow(label=\"Guidance Scale\", min=0, max=50, divisions=100, round=1, pref=paint_by_example_prefs, key='guidance_scale')\n",
        "    #eta = TextField(label=\"ETA\", value=str(paint_by_example_prefs['eta']), keyboard_type=KeyboardType.NUMBER, hint_text=\"Amount of Noise\", on_change=lambda e:changed(e,'eta', ptype='float'))\n",
        "    eta = Slider(min=0.0, max=1.0, divisions=20, label=\"{value}\", value=float(paint_by_example_prefs['eta']), tooltip=\"The weight of noise for added noise in a diffusion step. Its value is between 0.0 and 1.0 - 0.0 is DDIM and 1.0 is DDPM scheduler respectively.\", expand=True, on_change=change_eta)\n",
        "    eta_value = Text(f\" {paint_by_example_prefs['eta']}\", weight=FontWeight.BOLD)\n",
        "    eta_row = Row([Text(\"ETA:\"), eta_value, Text(\"  DDIM\"), eta, Text(\"DDPM\")])\n",
        "    page.etas.append(eta_row)\n",
        "    max_row = SliderRow(label=\"Max Resolution Size\", min=256, max=1280, divisions=64, multiple=16, suffix=\"px\", pref=paint_by_example_prefs, key='max_size')\n",
        "    batch_folder_name = TextField(label=\"Batch Folder Name\", value=paint_by_example_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    apply_ESRGAN_upscale = Switch(label=\"Apply ESRGAN Upscale\", value=paint_by_example_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)\n",
        "    enlarge_scale_slider = SliderRow(label=\"Enlarge Scale\", min=1, max=4, divisions=6, round=1, suffix=\"x\", pref=paint_by_example_prefs, key='enlarge_scale')\n",
        "    display_upscaled_image = Checkbox(label=\"Display Upscaled Image\", value=paint_by_example_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))\n",
        "    ESRGAN_settings = Container(Column([enlarge_scale_slider, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_paint_by_example = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_paint_by_example.height = None if status['installed_ESRGAN'] else 0\n",
        "    page.paint_by_example_output = Column([])\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(page.paint_by_example_output.controls) > 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"ü¶Å  Paint-by-Example\", \"Image-guided Inpainting using an Example Image to Transfer Subject to Masked area...\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with Paint-by-Example Settings\", on_click=paint_by_example_help)]),\n",
        "        ResponsiveRow([Row([original_image, alpha_mask], col={'lg':6}), Row([mask_image, invert_mask], col={'lg':6})]),\n",
        "        example_image,\n",
        "        num_inference_row,\n",
        "        guidance,\n",
        "        eta_row,\n",
        "        max_row,\n",
        "        Row([NumberPicker(label=\"Number of Images: \", min=1, max=8, value=paint_by_example_prefs['num_images'], on_change=lambda e: changed(e, 'num_images')), seed, batch_folder_name]),\n",
        "        page.ESRGAN_block_paint_by_example,\n",
        "        #Row([jump_length, jump_n_sample, seed]),\n",
        "        ElevatedButton(content=Text(\"üêæ  Run Paint-by-Example\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_paint_by_example(page)),\n",
        "        page.paint_by_example_output,\n",
        "        clear_button,\n",
        "      ]\n",
        "    ))], scroll=ScrollMode.AUTO, auto_scroll=False)\n",
        "    return c\n",
        "\n",
        "instruct_pix2pix_prefs = {\n",
        "    'original_image': '',\n",
        "    'prompt': '',\n",
        "    'negative_prompt': '',\n",
        "    'num_inference_steps': 100,\n",
        "    'guidance_scale': 7.5,\n",
        "    'image_guidance_scale': 1.5,\n",
        "    'eta': 0.0,\n",
        "    'seed': 0,\n",
        "    'max_size': 768,\n",
        "    'num_images': 1,\n",
        "    'use_init_video': False,\n",
        "    'init_video': '',\n",
        "    'fps': 12,\n",
        "    'start_time': 0,\n",
        "    'end_time': 0,\n",
        "    'control_v': 'v1.1',\n",
        "    'batch_folder_name': '',\n",
        "    \"apply_ESRGAN_upscale\": prefs['apply_ESRGAN_upscale'],\n",
        "    \"enlarge_scale\": 2.0,\n",
        "    \"display_upscaled_image\": False,\n",
        "}\n",
        "\n",
        "def buildInstructPix2Pix(page):\n",
        "    global instruct_pix2pix_prefs, prefs, pipe_instruct_pix2pix\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            instruct_pix2pix_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            instruct_pix2pix_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            instruct_pix2pix_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def add_to_instruct_pix2pix_output(o):\n",
        "      page.instruct_pix2pix_output.controls.append(o)\n",
        "      page.instruct_pix2pix_output.update()\n",
        "      if not clear_button.visible:\n",
        "        clear_button.visible = True\n",
        "        clear_button.update()\n",
        "    def clear_output(e):\n",
        "      if prefs['enable_sounds']: page.snd_delete.play()\n",
        "      page.instruct_pix2pix_output.controls = []\n",
        "      page.instruct_pix2pix_output.update()\n",
        "      clear_button.visible = False\n",
        "      clear_button.update()\n",
        "    def instruct_pix2pix_help(e):\n",
        "      def close_instruct_pix2pix_dlg(e):\n",
        "        nonlocal instruct_pix2pix_help_dlg\n",
        "        instruct_pix2pix_help_dlg.open = False\n",
        "        page.update()\n",
        "      instruct_pix2pix_help_dlg = AlertDialog(title=Text(\"üíÅ   Help with Instruct-Pix2Pix\"), content=Column([\n",
        "          Text(\"A method for editing images from human instructions: given an input image and a written instruction that tells the model what to do, our model follows these instructions to edit the image. To obtain training data for this problem, we combine the knowledge of two large pretrained models -- a language model (GPT-3) and a text-to-image model (Stable Diffusion) -- to generate a large dataset of image editing examples. Our conditional diffusion model, InstructPix2Pix, is trained on our generated data, and generalizes to real images and user-written instructions at inference time. Since it performs edits in the forward pass and does not require per example fine-tuning or inversion, our model edits images quickly, in a matter of seconds. We show compelling editing results for a diverse collection of input images and written instructions.\"),\n",
        "        ], scroll=ScrollMode.AUTO), actions=[TextButton(\"üòé  Fun2Fun... \", on_click=close_instruct_pix2pix_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = instruct_pix2pix_help_dlg\n",
        "      instruct_pix2pix_help_dlg.open = True\n",
        "      page.update()\n",
        "    def file_picker_result(e: FilePickerResultEvent):\n",
        "        if e.files != None:\n",
        "          upload_files(e)\n",
        "    def on_upload_progress(e: FilePickerUploadEvent):\n",
        "      nonlocal pick_type\n",
        "      if e.progress == 1:\n",
        "        if not slash in e.file_name:\n",
        "          fname = os.path.join(root_dir, e.file_name)\n",
        "          instruct_pix2pix_prefs['file_name'] = e.file_name.rpartition('.')[0]\n",
        "        else:\n",
        "          fname = e.file_name\n",
        "          instruct_pix2pix_prefs['file_name'] = e.file_name.rpartition(slash)[2].rpartition('.')[0]\n",
        "        if pick_type == \"image\":\n",
        "          original_image.value = fname\n",
        "          original_image.update()\n",
        "          instruct_pix2pix_prefs['original_image'] = fname\n",
        "        elif pick_type == \"video\":\n",
        "          init_video.value = fname\n",
        "          init_video.update()\n",
        "          instruct_pix2pix_prefs['init_video'] = fname\n",
        "        page.update()\n",
        "    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)\n",
        "    def upload_files(e):\n",
        "        uf = []\n",
        "        if file_picker.result != None and file_picker.result.files != None:\n",
        "            for f in file_picker.result.files:\n",
        "              if page.web:\n",
        "                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))\n",
        "              else:\n",
        "                on_upload_progress(FilePickerUploadEvent(f.path, 1, \"\"))\n",
        "            file_picker.upload(uf)\n",
        "    page.overlay.append(file_picker)\n",
        "    pick_type = \"\"\n",
        "    def pick_original(e):\n",
        "        nonlocal pick_type\n",
        "        pick_type = \"image\"\n",
        "        file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"png\", \"PNG\", \"jpg\", \"jpeg\"], dialog_title=\"Pick Original Image File\")\n",
        "    def pick_video(e):\n",
        "        nonlocal pick_type\n",
        "        pick_type = \"video\"\n",
        "        file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"mp4\", \"avi\"], dialog_title=\"Pick Initial Video File\")\n",
        "    def toggle_ESRGAN(e):\n",
        "        ESRGAN_settings.height = None if e.control.value else 0\n",
        "        instruct_pix2pix_prefs['apply_ESRGAN_upscale'] = e.control.value\n",
        "        ESRGAN_settings.update()\n",
        "    def change_eta(e):\n",
        "        changed(e, 'eta', ptype=\"float\")\n",
        "        eta_value.value = f\" {instruct_pix2pix_prefs['eta']}\"\n",
        "        eta_value.update()\n",
        "        eta_row.update()\n",
        "    def toggle_init_video(e):\n",
        "        changed(e, 'use_init_video')\n",
        "        show = e.control.value\n",
        "        original_image.visible = not show\n",
        "        original_image.update()\n",
        "        init_video.visible = show\n",
        "        init_video.update()\n",
        "        vid_params.height = None if show else 0\n",
        "        vid_params.update()\n",
        "        run_prompt_list.visible = not show\n",
        "        run_prompt_list.update()\n",
        "    original_image = TextField(label=\"Original Image\", value=instruct_pix2pix_prefs['original_image'], expand=True, on_change=lambda e:changed(e,'original_image'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_original))\n",
        "    prompt = TextField(label=\"Editing Instructions Prompt Text\", value=instruct_pix2pix_prefs['prompt'], col={'md': 9}, multiline=True, on_change=lambda e:changed(e,'prompt'))\n",
        "    negative_prompt  = TextField(label=\"Negative Prompt Text\", value=instruct_pix2pix_prefs['negative_prompt'], col={'md':3}, multiline=True, on_change=lambda e:changed(e,'negative_prompt'))\n",
        "    seed = TextField(label=\"Seed\", width=90, value=str(instruct_pix2pix_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip=\"0 or -1 picks a Random seed\", on_change=lambda e:changed(e,'seed', ptype='int'))\n",
        "    use_init_video = Tooltip(message=\"Input a short mp4 file to animate with.\", content=Switch(label=\"Use Init Video\", value=instruct_pix2pix_prefs['use_init_video'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_init_video))\n",
        "    init_video = TextField(label=\"Init Video Clip\", value=instruct_pix2pix_prefs['init_video'], expand=True, visible=instruct_pix2pix_prefs['use_init_video'], on_change=lambda e:changed(e,'init_video'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_video))\n",
        "    fps = SliderRow(label=\"Frames per Second\", min=1, max=30, divisions=29, suffix='fps', pref=instruct_pix2pix_prefs, key='fps', tooltip=\"The FPS to extract from the init video clip.\")\n",
        "    start_time = TextField(label=\"Start Time (s)\", value=instruct_pix2pix_prefs['start_time'], width=145, keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'start_time', ptype=\"float\"))\n",
        "    end_time = TextField(label=\"End Time (0 for all)\", value=instruct_pix2pix_prefs['end_time'], width=145, keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'end_time', ptype=\"float\"))\n",
        "    vid_params = Container(content=Column([fps, Row([start_time, end_time])]), animate_size=animation.Animation(800, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE, height=None if instruct_pix2pix_prefs['use_init_video'] else 0)\n",
        "    num_inference_row = SliderRow(label=\"Number of Inference Steps\", min=1, max=150, divisions=149, pref=instruct_pix2pix_prefs, key='num_inference_steps', tooltip=\"The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.\")   \n",
        "    guidance = SliderRow(label=\"Guidance Scale\", min=0, max=50, divisions=100, round=1, pref=instruct_pix2pix_prefs, key='guidance_scale')\n",
        "    image_guidance = SliderRow(label=\"Image Guidance Scale\", min=0, max=200, divisions=400, round=1, pref=instruct_pix2pix_prefs, key='image_guidance_scale', tooltip=\"Image guidance scale is to push the generated image towards the inital image `image`. Higher image guidance scale encourages to generate images that are closely linked to the source image `image`, usually at the expense of lower image quality.\")\n",
        "    #eta = TextField(label=\"ETA\", value=str(instruct_pix2pix_prefs['eta']), keyboard_type=KeyboardType.NUMBER, hint_text=\"Amount of Noise\", on_change=lambda e:changed(e,'eta', ptype='float'))\n",
        "    eta = Slider(min=0.0, max=1.0, divisions=20, label=\"{value}\", value=float(instruct_pix2pix_prefs['eta']), tooltip=\"The weight of noise for added noise in a diffusion step. Its value is between 0.0 and 1.0 - 0.0 is DDIM and 1.0 is DDPM scheduler respectively.\", expand=True, on_change=change_eta)\n",
        "    eta_value = Text(f\" {instruct_pix2pix_prefs['eta']}\", weight=FontWeight.BOLD)\n",
        "    eta_row = Row([Text(\"ETA:\"), eta_value, Text(\"  DDIM\"), eta, Text(\"DDPM\")])\n",
        "    page.etas.append(eta_row)\n",
        "    max_row = SliderRow(label=\"Max Resolution Size\", min=256, max=1280, divisions=64, multiple=16, suffix=\"px\", pref=instruct_pix2pix_prefs, key='max_size')\n",
        "    batch_folder_name = TextField(label=\"Batch Folder Name\", value=instruct_pix2pix_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    apply_ESRGAN_upscale = Switch(label=\"Apply ESRGAN Upscale\", value=instruct_pix2pix_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)\n",
        "    enlarge_scale_slider = SliderRow(label=\"Enlarge Scale\", min=1, max=4, divisions=6, round=1, suffix=\"x\", pref=instruct_pix2pix_prefs, key='enlarge_scale')\n",
        "    display_upscaled_image = Checkbox(label=\"Display Upscaled Image\", value=instruct_pix2pix_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))\n",
        "    ESRGAN_settings = Container(Column([enlarge_scale_slider, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_instruct_pix2pix = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_instruct_pix2pix.height = None if status['installed_ESRGAN'] else 0\n",
        "    page.instruct_pix2pix_output = Column([])\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(page.instruct_pix2pix_output.controls) > 0\n",
        "    run_prompt_list = ElevatedButton(content=Text(value=\"üìú   Run from Prompts List\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_instruct_pix2pix(page, from_list=True))\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üèúÔ∏è  Instruct-Pix2Pix\", \"Text-Based Image Editing - Learning to Follow Image Editing Instructions...\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with Instruct-Pix2Pix Settings\", on_click=instruct_pix2pix_help)]),\n",
        "        #ResponsiveRow([Row([original_image, alpha_mask], col={'lg':6}), Row([mask_image, invert_mask], col={'lg':6})]),\n",
        "        Row([original_image, init_video, use_init_video]),\n",
        "        vid_params,\n",
        "        ResponsiveRow([prompt, negative_prompt]),\n",
        "        num_inference_row,\n",
        "        guidance,\n",
        "        image_guidance,\n",
        "        eta_row,\n",
        "        max_row,\n",
        "        Row([NumberPicker(label=\"Number of Images: \", min=1, max=8, value=instruct_pix2pix_prefs['num_images'], on_change=lambda e: changed(e, 'num_images')), seed, batch_folder_name]),\n",
        "        page.ESRGAN_block_instruct_pix2pix,\n",
        "        #Row([jump_length, jump_n_sample, seed]),\n",
        "        \n",
        "        Row([ElevatedButton(content=Text(\"üèñÔ∏è  Run Instruct Pix2Pix\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_instruct_pix2pix(page)),\n",
        "             run_prompt_list]),\n",
        "        page.instruct_pix2pix_output,\n",
        "        clear_button,\n",
        "      ]))], scroll=ScrollMode.AUTO, auto_scroll=False)\n",
        "    return c\n",
        "\n",
        "controlnet_prefs = {\n",
        "    'original_image': '',\n",
        "    'prompt': '',\n",
        "    'negative_prompt': 'lowres, text, watermark, cropped, low quality',\n",
        "    'control_task': 'Scribble',\n",
        "    'conditioning_scale': 1.0,\n",
        "    'multi_controlnets': [],\n",
        "    'batch_size': 1,\n",
        "    'max_size': 768,\n",
        "    'low_threshold': 100, #1-255\n",
        "    'high_threshold': 200, #1-255\n",
        "    'steps': 50, #100\n",
        "    'guidance_scale': 9, #30\n",
        "    'seed': 0,\n",
        "    'eta': 0,\n",
        "    'use_init_video': False,\n",
        "    'init_video': '',\n",
        "    'fps': 12,\n",
        "    'start_time': 0,\n",
        "    'end_time': 0,\n",
        "    'file_prefix': 'controlnet-',\n",
        "    'batch_folder_name': '',\n",
        "    \"apply_ESRGAN_upscale\": prefs['apply_ESRGAN_upscale'],\n",
        "    \"enlarge_scale\": 2.0,\n",
        "    \"display_upscaled_image\": False,\n",
        "}\n",
        "\n",
        "def buildControlNet(page):\n",
        "    global controlnet_prefs, prefs\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            controlnet_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            controlnet_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            controlnet_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def add_to_controlnet_output(o):\n",
        "      page.controlnet_output.controls.append(o)\n",
        "      page.controlnet_output.update()\n",
        "      if not clear_button.visible:\n",
        "        clear_button.visible = True\n",
        "        clear_button.update()\n",
        "    def clear_output(e):\n",
        "      if prefs['enable_sounds']: page.snd_delete.play()\n",
        "      page.controlnet_output.controls = []\n",
        "      page.controlnet_output.update()\n",
        "      clear_button.visible = False\n",
        "      clear_button.update()\n",
        "    def controlnet_help(e):\n",
        "      def close_controlnet_dlg(e):\n",
        "        nonlocal controlnet_help_dlg\n",
        "        controlnet_help_dlg.open = False\n",
        "        page.update()\n",
        "      controlnet_help_dlg = AlertDialog(title=Text(\"üíÅ   Help with ControlNet\"), content=Column([\n",
        "          Text('ControlNet is a neural network structure to control diffusion models by adding extra conditions. It copys the weights of neural network blocks into a \"locked\" copy and a \"trainable\" copy. The \"trainable\" one learns your condition. The \"locked\" one preserves your model. Thanks to this, training with small dataset of image pairs will not destroy the production-ready diffusion models. The \"zero convolution\" is 1√ó1 convolution with both weight and bias initialized as zeros. Before training, all zero convolutions output zeros, and ControlNet will not cause any distortion.  No layer is trained from scratch. You are still fine-tuning. Your original model is safe.  This allows training on small-scale or even personal devices. This is also friendly to merge/replacement/offsetting of models/weights/blocks/layers.'),\n",
        "          Markdown(\"This is an interface for running the [official codebase](https://github.com/lllyasviel/ControlNet#readme) for models described in [Adding Conditional Control to Text-to-Image Diffusion Models](https://arxiv.org/abs/2302.05543).\", on_tap_link=lambda e: e.page.launch_url(e.data)),\n",
        "          Text(\"Scribble - A hand-drawn monochrome image with white outlines on a black background.\"),\n",
        "          Text(\"Canny Map Edge - A monochrome image with white edges on a black background.\"),\n",
        "          Text(\"OpenPose - A OpenPose bone image.\"),\n",
        "          Text(\"Depth - A grayscale image with black representing deep areas and white representing shallow areas.\"),\n",
        "          Text(\"HED - A monochrome image with white soft edges on a black background.\"),\n",
        "          Text(\"M-LSD - A monochrome image composed only of white straight lines on a black background.\"),\n",
        "          Text(\"Normal Map - A normal mapped image.\"),\n",
        "          Text(\"Segmented - An ADE20K's semantic segmentation protocol image.\"),\n",
        "          Text(\"LineArt - An image with line art, usually black lines on a white background.\"),\n",
        "          Text(\"Shuffle - An image with shuffled patches or regions.\"),\n",
        "          Text(\"Brightness - An image based on brightness of init.\"),\n",
        "          Text(\"Instruct Pix2Pix - Trained with pixel to pixel instruction.\"),\n",
        "        ], scroll=ScrollMode.AUTO), actions=[TextButton(\"üçÑ  Too much control... \", on_click=close_controlnet_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = controlnet_help_dlg\n",
        "      controlnet_help_dlg.open = True\n",
        "      page.update()\n",
        "    def file_picker_result(e: FilePickerResultEvent):\n",
        "        if e.files != None:\n",
        "          upload_files(e)\n",
        "    def on_upload_progress(e: FilePickerUploadEvent):\n",
        "      if e.progress == 1:\n",
        "        if not slash in e.file_name:\n",
        "          fname = os.path.join(root_dir, e.file_name)\n",
        "          controlnet_prefs['file_name'] = e.file_name.rpartition('.')[0]\n",
        "        else:\n",
        "          fname = e.file_name\n",
        "          controlnet_prefs['file_name'] = e.file_name.rpartition(slash)[2].rpartition('.')[0]\n",
        "        if pick_type == \"image\":\n",
        "          original_image.value = fname\n",
        "          original_image.update()\n",
        "          controlnet_prefs['original_image'] = fname\n",
        "        elif pick_type == \"video\":\n",
        "          init_video.value = fname\n",
        "          init_video.update()\n",
        "          controlnet_prefs['init_video'] = fname\n",
        "        page.update()\n",
        "    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)\n",
        "    def upload_files(e):\n",
        "        uf = []\n",
        "        if file_picker.result != None and file_picker.result.files != None:\n",
        "            for f in file_picker.result.files:\n",
        "              if page.web:\n",
        "                uf.append(FilePickerUploadFile(f.name, upload_url=e.page.get_upload_url(f.name, 600)))\n",
        "              else:\n",
        "                on_upload_progress(FilePickerUploadEvent(f.path, 1, \"\"))\n",
        "            file_picker.upload(uf)\n",
        "    page.overlay.append(file_picker)\n",
        "    pick_type = \"\"\n",
        "    def pick_original(e):\n",
        "        nonlocal pick_type\n",
        "        pick_type = \"image\"\n",
        "        file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"png\", \"PNG\", \"jpg\", \"jpeg\"], dialog_title=\"Pick Original Image File\")\n",
        "    def pick_video(e):\n",
        "        nonlocal pick_type\n",
        "        pick_type = \"video\"\n",
        "        file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"mp4\", \"avi\"], dialog_title=\"Pick Initial Video File\")\n",
        "    def toggle_ESRGAN(e):\n",
        "        ESRGAN_settings.height = None if e.control.value else 0\n",
        "        controlnet_prefs['apply_ESRGAN_upscale'] = e.control.value\n",
        "        ESRGAN_settings.update()\n",
        "    def change_task(e):\n",
        "        task = e.control.value\n",
        "        show = task.startswith(\"Video\")# or task == \"Video OpenPose\"\n",
        "        update = controlnet_prefs['use_init_video'] != show\n",
        "        changed(e,'control_task')\n",
        "        threshold.height = None if controlnet_prefs['control_task'] == \"Canny Map Edge\" or controlnet_prefs['control_task'] == \"Video Canny Edge\" else 0\n",
        "        threshold.update()\n",
        "        if update:\n",
        "            original_image.visible = not show\n",
        "            original_image.update()\n",
        "            init_video.visible = show\n",
        "            init_video.update()\n",
        "            vid_params.height = None if show else 0\n",
        "            vid_params.update()\n",
        "            conditioning_scale.visible = not show\n",
        "            conditioning_scale.update()\n",
        "            add_layer_btn.visible = not show\n",
        "            add_layer_btn.update()\n",
        "            multi_layers.visible = not show\n",
        "            multi_layers.update()\n",
        "            run_prompt_list.visible = not show\n",
        "            run_prompt_list.update()\n",
        "            controlnet_prefs['use_init_video'] = show\n",
        "    def change_eta(e):\n",
        "        changed(e, 'eta', ptype=\"float\")\n",
        "        eta_value.value = f\" {controlnet_prefs['eta']}\"\n",
        "        eta_value.update()\n",
        "        eta_row.update()\n",
        "    def add_layer(e):\n",
        "        layer = {'control_task': controlnet_prefs['control_task'], 'original_image': controlnet_prefs['original_image'], 'conditioning_scale': controlnet_prefs['conditioning_scale'], 'use_init_video': False}\n",
        "        if controlnet_prefs['control_task'] == \"Video Canny Edge\" or controlnet_prefs['control_task'] == \"Video OpenPose\":\n",
        "          layer['use_init_video'] = True\n",
        "          layer['init_video'] = controlnet_prefs['init_video']\n",
        "          layer['fps'] = controlnet_prefs['fps']\n",
        "          layer['start_time'] = controlnet_prefs['start_time']\n",
        "          layer['end_time'] = controlnet_prefs['end_time']\n",
        "          controlnet_prefs['init_video'] = \"\"\n",
        "          init_video.value = \"\"\n",
        "          original_image.update()\n",
        "        controlnet_prefs['multi_controlnets'].append(layer)\n",
        "        multi_layers.controls.append(ListTile(title=Row([Text(layer['control_task'] + \" - \", weight=FontWeight.BOLD), Text(layer['init_video'] if layer['use_init_video'] else layer['original_image']), Text(f\"- Conditioning Scale: {layer['conditioning_scale']}\")]), dense=True, trailing=PopupMenuButton(icon=icons.MORE_VERT,\n",
        "          items=[\n",
        "              PopupMenuItem(icon=icons.DELETE, text=\"Delete Control Layer\", on_click=delete_layer, data=layer),\n",
        "              PopupMenuItem(icon=icons.DELETE_SWEEP, text=\"Delete All Layers\", on_click=delete_all_layers, data=layer),\n",
        "          ]), data=layer))\n",
        "        multi_layers.update()\n",
        "        controlnet_prefs['original_image'] = \"\"\n",
        "        original_image.value = \"\"\n",
        "        original_image.update()\n",
        "    def delete_layer(e):\n",
        "        controlnet_prefs['multi_controlnets'].remove(e.control.data)\n",
        "        for c in multi_layers.controls:\n",
        "          if c.data['original_image'] == e.control.data['original_image']:\n",
        "             multi_layers.controls.remove(c)\n",
        "             break\n",
        "        multi_layers.update()\n",
        "        \n",
        "    def delete_all_layers(e):\n",
        "        controlnet_prefs['multi_controlnets'].clear()\n",
        "        multi_layers.controls.clear()\n",
        "        multi_layers.update()\n",
        "    original_image = TextField(label=\"Original Drawing\", value=controlnet_prefs['original_image'], expand=True, on_change=lambda e:changed(e,'original_image'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_original))\n",
        "    prompt = TextField(label=\"Prompt Text\", value=controlnet_prefs['prompt'], col={'md': 8}, multiline=True, on_change=lambda e:changed(e,'prompt'))\n",
        "    #a_prompt  = TextField(label=\"Added Prompt Text\", value=controlnet_prefs['a_prompt'], col={'md':3}, on_change=lambda e:changed(e,'a_prompt'))\n",
        "    negative_prompt  = TextField(label=\"Negative Prompt Text\", value=controlnet_prefs['negative_prompt'], col={'md':4}, multiline=True, on_change=lambda e:changed(e,'negative_prompt'))\n",
        "    control_task = Dropdown(label=\"ControlNet Task\", width=200, options=[dropdown.Option(\"Scribble\"), dropdown.Option(\"Canny Map Edge\"), dropdown.Option(\"OpenPose\"), dropdown.Option(\"Depth\"), dropdown.Option(\"HED\"), dropdown.Option(\"M-LSD\"), dropdown.Option(\"Normal Map\"), dropdown.Option(\"Segmentation\"), dropdown.Option(\"LineArt\"), dropdown.Option(\"Shuffle\"), dropdown.Option(\"Instruct Pix2Pix\"), dropdown.Option(\"Brightness\"), dropdown.Option(\"Video Canny Edge\"), dropdown.Option(\"Video OpenPose\")], value=controlnet_prefs['control_task'], on_change=change_task)\n",
        "    conditioning_scale = SliderRow(label=\"Conditioning Scale\", min=0, max=2, divisions=20, round=1, pref=controlnet_prefs, key='conditioning_scale', tooltip=\"The outputs of the controlnet are multiplied by `controlnet_conditioning_scale` before they are added to the residual in the original unet.\")\n",
        "    #add_layer_btn = IconButton(icons.ADD, tooltip=\"Add Multi-ControlNet Layer\", on_click=add_layer)\n",
        "    add_layer_btn = ft.FilledButton(\"‚ûï Add Layer\", width=135, on_click=add_layer)\n",
        "    multi_layers = Column([], spacing=0)\n",
        "    seed = TextField(label=\"Seed\", width=90, value=str(controlnet_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip=\"0 or -1 picks a Random seed\", on_change=lambda e:changed(e,'seed', ptype='int'))\n",
        "    #use_init_video = Tooltip(message=\"Input a short mp4 file to animate with.\", content=Switch(label=\"Use Init Video\", value=controlnet_prefs['use_init_video'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_init_video))\n",
        "    init_video = TextField(label=\"Init Video Clip\", value=controlnet_prefs['init_video'], expand=True, visible=controlnet_prefs['use_init_video'], on_change=lambda e:changed(e,'init_video'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_video))\n",
        "    fps = SliderRow(label=\"Frames per Second\", min=1, max=30, divisions=29, suffix='fps', pref=controlnet_prefs, key='fps', tooltip=\"The FPS to extract from the init video clip.\")\n",
        "    start_time = TextField(label=\"Start Time (s)\", value=controlnet_prefs['start_time'], width=145, keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'start_time', ptype=\"float\"))\n",
        "    end_time = TextField(label=\"End Time (0 for all)\", value=controlnet_prefs['end_time'], width=145, keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'end_time', ptype=\"float\"))\n",
        "    vid_params = Container(content=Column([fps, Row([start_time, end_time])]), animate_size=animation.Animation(800, AnimationCurve.EASE_OUT), clip_behavior=ClipBehavior.HARD_EDGE, height=None if controlnet_prefs['use_init_video'] else 0)\n",
        "\n",
        "    num_inference_row = SliderRow(label=\"Number of Steps\", min=1, max=100, divisions=99, pref=controlnet_prefs, key='steps', tooltip=\"The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.\")   \n",
        "    guidance = SliderRow(label=\"Guidance Scale\", min=0, max=30, divisions=60, round=1, pref=controlnet_prefs, key='guidance_scale')\n",
        "    low_threshold_row = SliderRow(label=\"Canny Low Threshold\", min=1, max=255, divisions=254, pref=controlnet_prefs, key='low_threshold')\n",
        "    high_threshold_row = SliderRow(label=\"Canny High Threshold\", min=1, max=255, divisions=254, pref=controlnet_prefs, key='high_threshold')\n",
        "    threshold = Container(Column([low_threshold_row, high_threshold_row]), animate_size=animation.Animation(1000, AnimationCurve.EASE_IN), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    threshold.height = None if controlnet_prefs['control_task'] == \"Canny Map Edge\" else 0\n",
        "    eta = Slider(min=0.0, max=1.0, divisions=20, label=\"{value}\", value=float(controlnet_prefs['eta']), tooltip=\"The weight of noise for added noise in a diffusion step. Its value is between 0.0 and 1.0 - 0.0 is DDIM and 1.0 is DDPM scheduler respectively.\", expand=True, on_change=change_eta)\n",
        "    eta_value = Text(f\" {controlnet_prefs['eta']}\", weight=FontWeight.BOLD)\n",
        "    eta_row = Row([Text(\"ETA:\"), eta_value, Text(\"  DDIM\"), eta])\n",
        "    page.etas.append(eta_row)\n",
        "    max_row = SliderRow(label=\"Max Resolution Size\", min=256, max=1280, divisions=64, multiple=16, suffix=\"px\", pref=controlnet_prefs, key='max_size')\n",
        "    file_prefix = TextField(label=\"Filename Prefix\",  value=controlnet_prefs['file_prefix'], width=150, height=60, on_change=lambda e:changed(e, 'file_prefix'))\n",
        "    batch_folder_name = TextField(label=\"Batch Folder Name\", value=controlnet_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    apply_ESRGAN_upscale = Switch(label=\"Apply ESRGAN Upscale\", value=controlnet_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)\n",
        "    enlarge_scale_slider = SliderRow(label=\"Enlarge Scale\", min=1, max=4, divisions=6, round=1, suffix=\"x\", pref=controlnet_prefs, key='enlarge_scale')\n",
        "    display_upscaled_image = Checkbox(label=\"Display Upscaled Image\", value=controlnet_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))\n",
        "    ESRGAN_settings = Container(Column([enlarge_scale_slider, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_controlnet = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_controlnet.height = None if status['installed_ESRGAN'] else 0\n",
        "    page.controlnet_output = Column([])\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(page.controlnet_output.controls) > 0\n",
        "    run_prompt_list = ElevatedButton(content=Text(value=\"üìú   Run from Prompts List\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_controlnet(page, from_list=True))\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üï∏Ô∏è  ControlNet Image+Text2Image\", \"Adding Input Conditions To Pretrained Text-to-Image Diffusion Models...\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with ControlNet Settings\", on_click=controlnet_help)]),\n",
        "        Row([control_task, original_image, init_video, add_layer_btn]),\n",
        "        conditioning_scale,\n",
        "        multi_layers,\n",
        "        vid_params,\n",
        "        Divider(thickness=2, height=4),\n",
        "        ResponsiveRow([prompt, negative_prompt]),\n",
        "        threshold,\n",
        "        num_inference_row,\n",
        "        guidance,\n",
        "        eta_row,\n",
        "        max_row,\n",
        "        Row([NumberPicker(label=\"Batch Size: \", min=1, max=8, value=controlnet_prefs['batch_size'], on_change=lambda e: changed(e, 'batch_size')), seed, batch_folder_name, file_prefix]),\n",
        "        page.ESRGAN_block_controlnet,\n",
        "        Row([ElevatedButton(content=Text(\"üè∏  Run ControlNet\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_controlnet(page)),\n",
        "             run_prompt_list]),\n",
        "        page.controlnet_output,\n",
        "        clear_button,\n",
        "      ]\n",
        "    ))], scroll=ScrollMode.AUTO, auto_scroll=False)\n",
        "    return c\n",
        "\n",
        "deepfloyd_prefs = {\n",
        "    'prompt': '',\n",
        "    'negative_prompt': '',\n",
        "    'init_image': '',\n",
        "    'mask_image': '',\n",
        "    'alpha_mask': False,\n",
        "    'invert_mask': False,\n",
        "    'num_inference_steps': 100,\n",
        "    'guidance_scale': 10,\n",
        "    'image_strength': 0.7,\n",
        "    'superres_num_inference_steps': 50,\n",
        "    'superres_guidance_scale': 4,\n",
        "    'upscale_num_inference_steps': 75,\n",
        "    'upscale_guidance_scale': 9,\n",
        "    'eta': 0.0,\n",
        "    'seed': 0,\n",
        "    'max_size': 768,\n",
        "    'apply_watermark': True,\n",
        "    'low_memory': True,\n",
        "    'num_images': 1,\n",
        "    'batch_folder_name': '',\n",
        "    'file_prefix': 'IF-',\n",
        "    \"apply_ESRGAN_upscale\": prefs['apply_ESRGAN_upscale'],\n",
        "    \"enlarge_scale\": 2.0,\n",
        "    \"display_upscaled_image\": False,\n",
        "}\n",
        "\n",
        "def buildDeepFloyd(page):\n",
        "    global deepfloyd_prefs, prefs, pipe_deepfloyd\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            deepfloyd_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            deepfloyd_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            deepfloyd_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def add_to_deepfloyd_output(o):\n",
        "      page.deepfloyd_output.controls.append(o)\n",
        "      page.deepfloyd_output.update()\n",
        "      if not clear_button.visible:\n",
        "        clear_button.visible = True\n",
        "        clear_button.update()\n",
        "    def clear_output(e):\n",
        "      if prefs['enable_sounds']: page.snd_delete.play()\n",
        "      page.deepfloyd_output.controls = []\n",
        "      page.deepfloyd_output.update()\n",
        "      clear_button.visible = False\n",
        "      clear_button.update()\n",
        "    def deepfloyd_help(e):\n",
        "      def close_deepfloyd_dlg(e):\n",
        "        nonlocal deepfloyd_help_dlg\n",
        "        deepfloyd_help_dlg.open = False\n",
        "        page.update()\n",
        "      deepfloyd_help_dlg = AlertDialog(title=Text(\"üíÅ   Help with DeepFloyd-IF\"), content=Column([\n",
        "          Markdown(\"**You must accept the license on the model card of [DeepFloyd/IF-I-XL-v1.0](https://huggingface.co/DeepFloyd/IF-I-XL-v1.0) before using.**\", on_tap_link=lambda e: e.page.launch_url(e.data)),\n",
        "          Text('DeepFloyd IF is a novel state-of-the-art open-source text-to-image model with a high degree of photorealism and language understanding. Built by the team behind ruDALL-E (the Russian-language version of OpenAI\\'s DALL-E algorithm), inspired by Google\\'s \"Imagen\", and backed by the company behind Stable Diffusion, DeepFloyd\\'s IF outperforms all of those algorithms. DeepFloyd IF is particularly good at understanding complex prompts and relationships between objects. It is also very good at inserting legible text into images - even more so than Stable Diffusion XL. It can even understand prompts in multiple languages. IF, or \"Intelligent Fiction\", is a text2image generator that is designed to create text and captions in the images in response to a prompt. The model is a modular composed of a frozen text encoder and three cascaded pixel diffusion modules:'),\n",
        "          Markdown(\"\"\"* Stage 1: a base model that generates 64x64 px image based on text prompt,\n",
        "* Stage 2: a 64x64 px => 256x256 px super-resolution model, and a\n",
        "* Stage 3: a 256x256 px => 1024x1024 px super-resolution model Stage 1 and Stage 2 utilize a frozen text encoder based on the T5 transformer to extract text embeddings, which are then fed into a UNet architecture enhanced with cross-attention and attention pooling. Stage 3 is Stability's x4 Upscaling model. The result is a highly efficient model that outperforms current state-of-the-art models, achieving a zero-shot FID score of 6.66 on the COCO dataset. Our work underscores the potential of larger UNet architectures in the first stage of cascaded diffusion models and depicts a promising future for text-to-image synthesis.\"\"\", on_tap_link=lambda e: e.page.launch_url(e.data)),\n",
        "        ], scroll=ScrollMode.AUTO), actions=[TextButton(\"üòå  Let's go Deep... \", on_click=close_deepfloyd_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = deepfloyd_help_dlg\n",
        "      deepfloyd_help_dlg.open = True\n",
        "      page.update()\n",
        "    def file_picker_result(e: FilePickerResultEvent):\n",
        "        if e.files != None:\n",
        "          upload_files(e)\n",
        "    def on_upload_progress(e: FilePickerUploadEvent):\n",
        "      nonlocal pick_type\n",
        "      if e.progress == 1:\n",
        "        if not slash in e.file_name:\n",
        "          fname = os.path.join(root_dir, e.file_name)\n",
        "          deepfloyd_prefs['file_name'] = e.file_name.rpartition('.')[0]\n",
        "        else:\n",
        "          fname = e.file_name\n",
        "          deepfloyd_prefs['file_name'] = e.file_name.rpartition(slash)[2].rpartition('.')[0]\n",
        "        if pick_type == \"image\":\n",
        "          init_image.value = fname\n",
        "          init_image.update()\n",
        "          deepfloyd_prefs['init_image'] = fname\n",
        "        elif pick_type == \"mask\":\n",
        "          mask_image.value = fname\n",
        "          mask_image.update()\n",
        "          deepfloyd_prefs['mask_image'] = fname\n",
        "        page.update()\n",
        "    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)\n",
        "    def upload_files(e):\n",
        "        uf = []\n",
        "        if file_picker.result != None and file_picker.result.files != None:\n",
        "            for f in file_picker.result.files:\n",
        "              if page.web:\n",
        "                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))\n",
        "              else:\n",
        "                on_upload_progress(FilePickerUploadEvent(f.path, 1, \"\"))\n",
        "            file_picker.upload(uf)\n",
        "    page.overlay.append(file_picker)\n",
        "    pick_type = \"\"\n",
        "    def pick_init(e):\n",
        "        nonlocal pick_type\n",
        "        pick_type = \"image\"\n",
        "        file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"png\", \"PNG\", \"jpg\", \"jpeg\"], dialog_title=\"Pick Original Image File\")\n",
        "    def pick_mask(e):\n",
        "        nonlocal pick_type\n",
        "        pick_type = \"mask\"\n",
        "        file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"png\", \"PNG\", \"jpg\", \"jpeg\"], dialog_title=\"Pick Mask Image File\")\n",
        "    def toggle_ESRGAN(e):\n",
        "        ESRGAN_settings.height = None if e.control.value else 0\n",
        "        deepfloyd_prefs['apply_ESRGAN_upscale'] = e.control.value\n",
        "        ESRGAN_settings.update()\n",
        "    def change_eta(e):\n",
        "        changed(e, 'eta', ptype=\"float\")\n",
        "        eta_value.value = f\" {deepfloyd_prefs['eta']}\"\n",
        "        eta_value.update()\n",
        "        eta_row.update()\n",
        "\n",
        "    init_image = TextField(label=\"Original Image (optional)\", value=deepfloyd_prefs['init_image'], expand=True, on_change=lambda e:changed(e,'init_image'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_init))\n",
        "    mask_image = TextField(label=\"Mask Image (optional)\", value=deepfloyd_prefs['mask_image'], expand=1, on_change=lambda e:changed(e,'mask_image'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD_OUTLINED, on_click=pick_mask))\n",
        "    invert_mask = Checkbox(label=\"Invert\", tooltip=\"Swaps the Black & White of your Mask Image\", value=deepfloyd_prefs['invert_mask'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'invert_mask'))\n",
        "    alpha_mask = Checkbox(label=\"Alpha Mask\", value=deepfloyd_prefs['alpha_mask'], tooltip=\"Use Transparent Alpha Channel of Init as Mask\", fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'alpha_mask'))\n",
        "\n",
        "    prompt = TextField(label=\"Prompt Text\", value=deepfloyd_prefs['prompt'], col={'md': 9}, multiline=True, on_change=lambda e:changed(e,'prompt'))\n",
        "    negative_prompt  = TextField(label=\"Negative Prompt Text\", value=deepfloyd_prefs['negative_prompt'], col={'md':3}, multiline=True, on_change=lambda e:changed(e,'negative_prompt'))\n",
        "    seed = TextField(label=\"Seed\", width=90, value=str(deepfloyd_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip=\"0 or -1 picks a Random seed\", on_change=lambda e:changed(e,'seed', ptype='int'))\n",
        "    num_inference_row = SliderRow(label=\"Number of Inference Steps\", min=1, max=200, divisions=199, pref=deepfloyd_prefs, key='num_inference_steps', tooltip=\"The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.\")   \n",
        "    guidance = SliderRow(label=\"Guidance Scale\", min=0, max=50, divisions=100, round=1, pref=deepfloyd_prefs, key='guidance_scale')\n",
        "    \n",
        "    superres_num_inference_row = SliderRow(label=\"Super Res Inference Steps\", min=1, max=200, divisions=199, pref=deepfloyd_prefs, key='superres_num_inference_steps', tooltip=\"The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.\")   \n",
        "    superres_guidance = SliderRow(label=\"Super Res Guidance Scale\", min=0, max=50, divisions=100, round=1, pref=deepfloyd_prefs, key='superres_guidance_scale')\n",
        "    upscale_num_inference_row = SliderRow(label=\"Upscale Inference Steps\", min=1, max=200, divisions=199, pref=deepfloyd_prefs, key='upscale_num_inference_steps', tooltip=\"The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.\")   \n",
        "    upscale_guidance = SliderRow(label=\"Upscale Guidance Scale\", min=0, max=50, divisions=100, round=1, pref=deepfloyd_prefs, key='upscale_guidance_scale')\n",
        "    image_strength = SliderRow(label=\"Image Strength\", min=0, max=1, divisions=20, round=2, pref=deepfloyd_prefs, key='image_strength', tooltip=\"Conceptually, indicates how much to transform the reference `image`. Denoising steps depends on the amount of noise initially added.\")\n",
        "    #eta = TextField(label=\"ETA\", value=str(deepfloyd_prefs['eta']), keyboard_type=KeyboardType.NUMBER, hint_text=\"Amount of Noise\", on_change=lambda e:changed(e,'eta', ptype='float'))\n",
        "    eta = Slider(min=0.0, max=1.0, divisions=20, label=\"{value}\", value=float(deepfloyd_prefs['eta']), tooltip=\"The weight of noise for added noise in a diffusion step. Its value is between 0.0 and 1.0 - 0.0 is DDIM and 1.0 is DDPM scheduler respectively.\", expand=True, on_change=change_eta)\n",
        "    eta_value = Text(f\" {deepfloyd_prefs['eta']}\", weight=FontWeight.BOLD)\n",
        "    eta_row = Row([Text(\"ETA:\"), eta_value, Text(\"  DDIM\"), eta, Text(\"DDPM\")])\n",
        "    #page.etas.append(eta_row)\n",
        "    max_row = SliderRow(label=\"Max Resolution Size\", min=256, max=1280, divisions=64, multiple=16, suffix=\"px\", pref=deepfloyd_prefs, key='max_size', tooltip=\"Resizes your Init and Mask Image to save memory.\")\n",
        "    apply_watermark = Tooltip(message=\"Under the license, you are legally required to include the watermark on bottom right corner.\", content=Switch(label=\"Apply IF Watermark\", value=deepfloyd_prefs['apply_watermark'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e, 'apply_watermark')))\n",
        "    low_memory = Tooltip(message=\"Needed for < 16GB VRAM to run. If you have more power, disable for faster runs.\", content=Switch(label=\"Lower Memory\", value=deepfloyd_prefs['low_memory'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e, 'low_memory')))\n",
        "    file_prefix = TextField(label=\"Filename Prefix\",  value=deepfloyd_prefs['file_prefix'], width=150, height=60, on_change=lambda e:changed(e, 'file_prefix'))\n",
        "    batch_folder_name = TextField(label=\"Batch Folder Name\", value=deepfloyd_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    apply_ESRGAN_upscale = Switch(label=\"Apply ESRGAN Upscale\", value=deepfloyd_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)\n",
        "    enlarge_scale_slider = SliderRow(label=\"Enlarge Scale\", min=1, max=4, divisions=6, round=1, suffix=\"x\", pref=deepfloyd_prefs, key='enlarge_scale')\n",
        "    display_upscaled_image = Checkbox(label=\"Display Upscaled Image\", value=deepfloyd_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))\n",
        "    ESRGAN_settings = Container(Column([enlarge_scale_slider, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_deepfloyd = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_deepfloyd.height = None if status['installed_ESRGAN'] else 0\n",
        "    page.deepfloyd_output = Column([])\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(page.deepfloyd_output.controls) > 0\n",
        "    run_prompt_list = ElevatedButton(content=Text(value=\"üìú   Run from Prompts List\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_deepfloyd(page, from_list=True))\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üåà  DeepFloyd IF (under construction, may not work)\", \"A new AI image generator that achieves state-of-the-art results on numerous image-generation tasks...\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with IF-DeepFloyd Settings\", on_click=deepfloyd_help)]),\n",
        "        ResponsiveRow([prompt, negative_prompt]),\n",
        "        ResponsiveRow([Row([init_image, alpha_mask], col={'lg':6}), Row([mask_image, invert_mask], col={'lg':6})]),\n",
        "        #Row([init_image, mask_image, invert_mask]),\n",
        "        image_strength,\n",
        "        num_inference_row,\n",
        "        guidance,\n",
        "        superres_num_inference_row,\n",
        "        superres_guidance,\n",
        "        upscale_num_inference_row,\n",
        "        upscale_guidance,\n",
        "        eta_row,\n",
        "        max_row,\n",
        "        Row([apply_watermark, low_memory]),\n",
        "        Row([NumberPicker(label=\"Number of Images: \", min=1, max=8, value=deepfloyd_prefs['num_images'], on_change=lambda e: changed(e, 'num_images')), seed, batch_folder_name, file_prefix]),\n",
        "        page.ESRGAN_block_deepfloyd,\n",
        "        Row([ElevatedButton(content=Text(\"üéà  Run DeepFloyd\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_deepfloyd(page)),\n",
        "             run_prompt_list]),\n",
        "        page.deepfloyd_output,\n",
        "        clear_button,\n",
        "      ]))], scroll=ScrollMode.AUTO, auto_scroll=False)\n",
        "    return c\n",
        "\n",
        "\n",
        "text_to_video_prefs = {\n",
        "    'prompt': '',\n",
        "    'negative_prompt': 'text, words, watermark, shutterstock',\n",
        "    'num_inference_steps': 50,\n",
        "    'guidance_scale': 9.0,\n",
        "    'export_to_video': True,\n",
        "    'eta': 0.0,\n",
        "    'seed': 0,\n",
        "    'width': 256,\n",
        "    'height': 256,\n",
        "    'num_frames': 16,\n",
        "    'batch_folder_name': '',\n",
        "    \"apply_ESRGAN_upscale\": prefs['apply_ESRGAN_upscale'],\n",
        "    \"enlarge_scale\": 2.0,\n",
        "    \"display_upscaled_image\": False,\n",
        "    \"lower_memory\": True,\n",
        "}\n",
        "\n",
        "def buildTextToVideo(page):\n",
        "    global text_to_video_prefs, prefs, pipe_text_to_video, editing_prompt\n",
        "    editing_prompt = {'editing_prompt':'', 'edit_warmup_steps':10, 'edit_guidance_scale':5, 'edit_threshold':0.9, 'edit_weights':1, 'reverse_editing_direction': False}\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            text_to_video_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            text_to_video_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            text_to_video_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def clear_output(e):\n",
        "      if prefs['enable_sounds']: page.snd_delete.play()\n",
        "      page.text_to_video_output.controls = []\n",
        "      page.text_to_video_output.update()\n",
        "      clear_button.visible = False\n",
        "      clear_button.update()\n",
        "    def text_to_video_help(e):\n",
        "      def close_text_to_video_dlg(e):\n",
        "        nonlocal text_to_video_help_dlg\n",
        "        text_to_video_help_dlg.open = False\n",
        "        page.update()\n",
        "      text_to_video_help_dlg = AlertDialog(title=Text(\"üíÅ   Help with Text-To-Video\"), content=Column([\n",
        "          Text(\"Text-to-video synthesis from [ModelScope](https://modelscope.cn/) can be considered the same as Stable Diffusion structure-wise but it is extended to videos instead of static images. More specifically, this system allows us to generate videos from a natural language text prompt.\"),\n",
        "          Markdown(\"\"\"From the [model summary](https://huggingface.co/damo-vilab/modelscope-damo-text-to-video-synthesis):\n",
        "*This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. Only English input is supported.*\n",
        "Resources:\n",
        "* [Website](https://modelscope.cn/models/damo/text-to-video-synthesis/summary)\n",
        "* [GitHub repository](https://github.com/modelscope/modelscope/)\"\"\", on_tap_link=lambda e: e.page.launch_url(e.data)),\n",
        "        ], scroll=ScrollMode.AUTO), actions=[TextButton(\"üéû  What'll be next... \", on_click=close_text_to_video_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = text_to_video_help_dlg\n",
        "      text_to_video_help_dlg.open = True\n",
        "      page.update()\n",
        "    def toggle_ESRGAN(e):\n",
        "        ESRGAN_settings.height = None if e.control.value else 0\n",
        "        text_to_video_prefs['apply_ESRGAN_upscale'] = e.control.value\n",
        "        ESRGAN_settings.update()\n",
        "    prompt = TextField(label=\"Animation Prompt Text\", value=text_to_video_prefs['prompt'], col={'md': 9}, multiline=True, on_change=lambda e:changed(e,'prompt'))\n",
        "    negative_prompt  = TextField(label=\"Negative Prompt Text\", value=text_to_video_prefs['negative_prompt'], col={'md':3}, on_change=lambda e:changed(e,'negative_prompt'))\n",
        "    num_frames = SliderRow(label=\"Number of Frames\", min=1, max=300, divisions=299, pref=text_to_video_prefs, key='num_frames', tooltip=\"The number of video frames that are generated. Defaults to 16 frames which at 8 frames per seconds amounts to 2 seconds of video.\")   \n",
        "    num_inference_row = SliderRow(label=\"Number of Inference Steps\", min=1, max=150, divisions=149, pref=text_to_video_prefs, key='num_inference_steps', tooltip=\"The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.\")   \n",
        "    guidance = SliderRow(label=\"Guidance Scale\", min=0, max=50, divisions=100, round=1, pref=text_to_video_prefs, key='guidance_scale')\n",
        "    eta_slider = SliderRow(label=\"ETA\", min=0, max=1.0, divisions=20, round=1, pref=text_to_video_prefs, key='eta', tooltip=\"The weight of noise for added noise in a diffusion step. Its value is between 0.0 and 1.0 - 0.0 is DDIM and 1.0 is DDPM scheduler respectively.\")\n",
        "    #width_slider = SliderRow(label=\"Width\", min=256, max=1280, divisions=64, multiple=16, suffix=\"px\", pref=text_to_video_prefs, key='width')\n",
        "    #height_slider = SliderRow(label=\"Height\", min=256, max=1280, divisions=64, multiple=16, suffix=\"px\", pref=text_to_video_prefs, key='height')\n",
        "    export_to_video = Tooltip(message=\"Save mp4 file along with Image Sequence\", content=Switch(label=\"Export to Video\", value=text_to_video_prefs['export_to_video'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'export_to_video')))\n",
        "    lower_memory = Tooltip(message=\"Enable CPU offloading, VAE Tiling & Stitching\", content=Switch(label=\"Lower Memory Mode\", value=text_to_video_prefs['lower_memory'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'lower_memory')))\n",
        "    batch_folder_name = TextField(label=\"Batch Folder Name\", value=text_to_video_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    seed = TextField(label=\"Seed\", width=90, value=str(text_to_video_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip=\"0 or -1 picks a Random seed\", on_change=lambda e:changed(e,'seed', ptype='int'))\n",
        "    apply_ESRGAN_upscale = Switch(label=\"Apply ESRGAN Upscale\", value=text_to_video_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)\n",
        "    enlarge_scale_slider = SliderRow(label=\"Enlarge Scale\", min=1, max=4, divisions=6, round=1, suffix=\"x\", pref=text_to_video_prefs, key='enlarge_scale')\n",
        "    display_upscaled_image = Checkbox(label=\"Display Upscaled Image\", value=text_to_video_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))\n",
        "    ESRGAN_settings = Container(Column([enlarge_scale_slider, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_text_to_video = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_text_to_video.height = None if status['installed_ESRGAN'] else 0\n",
        "    page.text_to_video_output = Column([], scroll=ScrollMode.AUTO, auto_scroll=False)\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(page.text_to_video_output.controls) > 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üé•  Text-To-Video Synthesis\", \"Modelscope's Text-to-video-synthesis Model to Animate Diffusion\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with Instruct-Pix2Pix Settings\", on_click=text_to_video_help)]),\n",
        "        #ResponsiveRow([Row([original_image, alpha_mask], col={'lg':6}), Row([mask_image, invert_mask], col={'lg':6})]),\n",
        "        ResponsiveRow([prompt, negative_prompt]),\n",
        "        #Row([NumberPicker(label=\"Number of Frames: \", min=1, max=8, value=text_to_video_prefs['num_frames'], tooltip=\"The number of video frames that are generated. Defaults to 16 frames which at 8 frames per seconds amounts to 2 seconds of video.\", on_change=lambda e: changed(e, 'num_frames')), seed, batch_folder_name]),\n",
        "        Row([export_to_video, lower_memory]),\n",
        "        num_frames,\n",
        "        num_inference_row,\n",
        "        guidance,\n",
        "        eta_slider,\n",
        "        #width_slider, height_slider,\n",
        "        page.ESRGAN_block_text_to_video,\n",
        "        Row([seed, batch_folder_name]),\n",
        "        #Row([jump_length, jump_n_sample, seed]),\n",
        "        Row([\n",
        "            ElevatedButton(content=Text(\"üìπ  Run Text-To-Video\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_text_to_video(page)),\n",
        "             #ElevatedButton(content=Text(value=\"üìú   Run from Prompts List\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_text_to_video(page, from_list=True))\n",
        "        ]),\n",
        "        page.text_to_video_output,\n",
        "        clear_button,\n",
        "      ]\n",
        "    ))], scroll=ScrollMode.AUTO, auto_scroll=False)\n",
        "    return c\n",
        "\n",
        "text_to_video_zero_prefs = {\n",
        "    'prompt': '',\n",
        "    'negative_prompt': 'text, words, watermark, shutterstock',\n",
        "    'num_inference_steps': 50,\n",
        "    'guidance_scale': 9.0,\n",
        "    'export_to_video': True,\n",
        "    'eta': 0.0,\n",
        "    'seed': 0,\n",
        "    'width': 1024,\n",
        "    'height': 1024,\n",
        "    'num_frames': 8,\n",
        "    'motion_field_strength_x': 12,\n",
        "    'motion_field_strength_y': 12,\n",
        "    't0': 42,\n",
        "    't1': 47,\n",
        "    'input_video': '',\n",
        "    'prep_type': 'Zero Shot',\n",
        "    'batch_folder_name': '',\n",
        "    \"apply_ESRGAN_upscale\": prefs['apply_ESRGAN_upscale'],\n",
        "    \"enlarge_scale\": 2.0,\n",
        "    \"display_upscaled_image\": False,\n",
        "    \"lower_memory\": True,\n",
        "}\n",
        "\n",
        "def buildTextToVideoZero(page):\n",
        "    global text_to_video_zero_prefs, prefs, pipe_text_to_video_zero\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            text_to_video_zero_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            text_to_video_zero_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            text_to_video_zero_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def clear_output(e):\n",
        "      if prefs['enable_sounds']: page.snd_delete.play()\n",
        "      page.text_to_video_zero_output.controls = []\n",
        "      page.text_to_video_zero_output.update()\n",
        "      clear_button.visible = False\n",
        "      clear_button.update()\n",
        "    def text_to_video_zero_help(e):\n",
        "      def close_text_to_video_zero_dlg(e):\n",
        "        nonlocal text_to_video_zero_help_dlg\n",
        "        text_to_video_zero_help_dlg.open = False\n",
        "        page.update()\n",
        "      text_to_video_zero_help_dlg = AlertDialog(title=Text(\"üíÅ   Help with Text-To-Video Zero\"), content=Column([\n",
        "          Text(\"Recent text-to-video generation approaches rely on computationally heavy training and require large-scale video datasets. In this paper, we introduce a new task of zero-shot text-to-video generation and propose a low-cost approach (without any training or optimization) by leveraging the power of existing text-to-image synthesis methods (e.g., Stable Diffusion), making them suitable for the video domain. Our key modifications include (i) enriching the latent codes of the generated frames with motion dynamics to keep the global scene and the background time consistent; and (ii) reprogramming frame-level self-attention using a new cross-frame attention of each frame on the first frame, to preserve the context, appearance, and identity of the foreground object.\"),\n",
        "          Text(\"Experiments show that this leads to low overhead, yet high-quality and remarkably consistent video generation. Moreover, our approach is not limited to text-to-video synthesis but is also applicable to other tasks such as conditional and content-specialized video generation, and Video Instruct-Pix2Pix, i.e., instruction-guided video editing. As experiments show, our method performs comparably or sometimes better than recent approaches, despite not being trained on additional video data.\"),\n",
        "          Markdown(\"\"\"* [Project Page](https://text2video-zero.github.io/)\n",
        "* [Paper](https://arxiv.org/abs/2303.13439)\n",
        "* [Original Code](https://github.com/Picsart-AI-Research/Text2Video-Zero)\"\"\", on_tap_link=lambda e: e.page.launch_url(e.data)),\n",
        "        ], scroll=ScrollMode.AUTO), actions=[TextButton(\"üéû  Let's go crazy... \", on_click=close_text_to_video_zero_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = text_to_video_zero_help_dlg\n",
        "      text_to_video_zero_help_dlg.open = True\n",
        "      page.update()\n",
        "    def toggle_ESRGAN(e):\n",
        "        ESRGAN_settings.height = None if e.control.value else 0\n",
        "        text_to_video_zero_prefs['apply_ESRGAN_upscale'] = e.control.value\n",
        "        ESRGAN_settings.update()\n",
        "    def change_steps(e):\n",
        "        steps = e.control.value\n",
        "        t0.set_max(steps - 1)\n",
        "        t0.set_max(steps - 1)\n",
        "        t0.set_divisions(steps - 1)\n",
        "        if text_to_video_zero_prefs['t0'] > steps - 1:\n",
        "            text_to_video_zero_prefs['t0'] = steps - 1\n",
        "            t0.set_value(text_to_video_zero_prefs['t0'])\n",
        "        t1.set_min(text_to_video_zero_prefs['t0'] + 1)\n",
        "        t1.set_max(steps - 1)\n",
        "        if text_to_video_zero_prefs['t1'] > steps - 1:\n",
        "            text_to_video_zero_prefs['t1'] = steps - 1\n",
        "            t1.set_value(text_to_video_zero_prefs['t1'])\n",
        "        if text_to_video_zero_prefs['t1'] < t1.min:\n",
        "            text_to_video_zero_prefs['t1'] = t1.min\n",
        "            t1.set_value(text_to_video_zero_prefs['t1'])\n",
        "        t1.set_divisions(t1.max - t1.min)\n",
        "        t0.update_slider()\n",
        "        t1.update_slider()\n",
        "    def change_t0(e):\n",
        "        t0_value = e.control.value\n",
        "        steps = num_inference_row.value\n",
        "        t1.set_min(t0_value + 1)\n",
        "        if text_to_video_zero_prefs['t1'] > steps - 1:\n",
        "            text_to_video_zero_prefs['t1'] = steps - 1\n",
        "            t1.set_value(text_to_video_zero_prefs['t1'])\n",
        "        if text_to_video_zero_prefs['t1'] < t1.min:\n",
        "            text_to_video_zero_prefs['t1'] = t1.min\n",
        "            t1.set_value(text_to_video_zero_prefs['t1'])\n",
        "        t1.set_divisions(t1.max - t1.min)\n",
        "        t1.update_slider()\n",
        "    prompt = TextField(label=\"Animation Prompt Text\", value=text_to_video_zero_prefs['prompt'], col={'md': 9}, multiline=True, on_change=lambda e:changed(e,'prompt'))\n",
        "    negative_prompt  = TextField(label=\"Negative Prompt Text\", value=text_to_video_zero_prefs['negative_prompt'], col={'md':3}, on_change=lambda e:changed(e,'negative_prompt'))\n",
        "    num_frames = SliderRow(label=\"Number of Frames\", min=1, max=300, divisions=299, pref=text_to_video_zero_prefs, key='num_frames', tooltip=\"The number of video frames that are generated. Defaults to 16 frames which at 8 frames per seconds amounts to 2 seconds of video.\")   \n",
        "    num_inference_row = SliderRow(label=\"Number of Inference Steps\", min=1, max=150, divisions=149, pref=text_to_video_zero_prefs, key='num_inference_steps', on_change=change_steps, tooltip=\"The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.\")   \n",
        "    guidance = SliderRow(label=\"Guidance Scale\", min=0, max=50, divisions=100, round=1, pref=text_to_video_zero_prefs, key='guidance_scale')\n",
        "    eta_slider = SliderRow(label=\"ETA\", min=0, max=1.0, divisions=20, round=1, pref=text_to_video_zero_prefs, key='eta', tooltip=\"The weight of noise for added noise in a diffusion step. Its value is between 0.0 and 1.0 - 0.0 is DDIM and 1.0 is DDPM scheduler respectively.\")\n",
        "    motion_field_strength_x = SliderRow(label=\"Motion Field Strength X\", min=1, max=30, divisions=29, pref=text_to_video_zero_prefs, key='motion_field_strength_x', tooltip=\"Strength of motion in generated video along x-axis\")   \n",
        "    motion_field_strength_y = SliderRow(label=\"Motion Field Strength Y\", min=1, max=30, divisions=29, pref=text_to_video_zero_prefs, key='motion_field_strength_y', tooltip=\"Strength of motion in generated video along y-axis\")   \n",
        "    t0 = SliderRow(label=\"Timestep t0\", min=0, max=50, divisions=50, pref=text_to_video_zero_prefs, key='t0', on_change=change_t0, tooltip=\"Should be in the range [0, num_inference_steps - 1]\")\n",
        "    t1 = SliderRow(label=\"Timestep t1\", min=43, max=50, divisions=7, pref=text_to_video_zero_prefs, key='t1', tooltip=\"Should be in the range [t0 + 1, num_inference_steps - 1]\")   \n",
        "    #width_slider = SliderRow(label=\"Width\", min=256, max=1280, divisions=64, multiple=16, suffix=\"px\", pref=text_to_video_zero_prefs, key='width')\n",
        "    #height_slider = SliderRow(label=\"Height\", min=256, max=1280, divisions=64, multiple=16, suffix=\"px\", pref=text_to_video_zero_prefs, key='height')\n",
        "    export_to_video = Tooltip(message=\"Save mp4 file along with Image Sequence\", content=Switch(label=\"Export to Video\", value=text_to_video_zero_prefs['export_to_video'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'export_to_video')))\n",
        "    #lower_memory = Tooltip(message=\"Enable CPU offloading, VAE Tiling & Stitching\", content=Switch(label=\"Lower Memory Mode\", value=text_to_video_zero_prefs['lower_memory'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'lower_memory')))\n",
        "    batch_folder_name = TextField(label=\"Batch Folder Name\", value=text_to_video_zero_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    seed = TextField(label=\"Seed\", width=90, value=str(text_to_video_zero_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip=\"0 or -1 picks a Random seed\", on_change=lambda e:changed(e,'seed', ptype='int'))\n",
        "    apply_ESRGAN_upscale = Switch(label=\"Apply ESRGAN Upscale\", value=text_to_video_zero_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)\n",
        "    enlarge_scale_slider = SliderRow(label=\"Enlarge Scale\", min=1, max=4, divisions=6, round=1, suffix=\"x\", pref=text_to_video_zero_prefs, key='enlarge_scale')\n",
        "    display_upscaled_image = Checkbox(label=\"Display Upscaled Image\", value=text_to_video_zero_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))\n",
        "    ESRGAN_settings = Container(Column([enlarge_scale_slider, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_text_to_video_zero = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_text_to_video_zero.height = None if status['installed_ESRGAN'] else 0\n",
        "    page.text_to_video_zero_output = Column([], scroll=ScrollMode.AUTO, auto_scroll=False)\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(page.text_to_video_zero_output.controls) > 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üé•  Text-To-Video Zero\", \"Text-to-Image Diffusion Models for Zero-Shot Video Generators\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with Instruct-Pix2Pix Settings\", on_click=text_to_video_zero_help)]),\n",
        "        #ResponsiveRow([Row([original_image, alpha_mask], col={'lg':6}), Row([mask_image, invert_mask], col={'lg':6})]),\n",
        "        ResponsiveRow([prompt, negative_prompt]),\n",
        "        #Row([NumberPicker(label=\"Number of Frames: \", min=1, max=8, value=text_to_video_zero_prefs['num_frames'], tooltip=\"The number of video frames that are generated. Defaults to 16 frames which at 8 frames per seconds amounts to 2 seconds of video.\", on_change=lambda e: changed(e, 'num_frames')), seed, batch_folder_name]),\n",
        "        Row([export_to_video]),\n",
        "        num_frames,\n",
        "        num_inference_row,\n",
        "        guidance,\n",
        "        eta_slider,\n",
        "        motion_field_strength_x, motion_field_strength_y,\n",
        "        t0, t1,\n",
        "        #width_slider, height_slider,\n",
        "        page.ESRGAN_block_text_to_video_zero,\n",
        "        Row([seed, batch_folder_name]),\n",
        "        #Row([jump_length, jump_n_sample, seed]),\n",
        "        Row([\n",
        "            ElevatedButton(content=Text(\"üìπ  Run Text2Video-Zero\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_text_to_video_zero(page)),\n",
        "             #ElevatedButton(content=Text(value=\"üìú   Run from Prompts List\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_text_to_video_zero(page, from_list=True))\n",
        "        ]),\n",
        "        page.text_to_video_zero_output,\n",
        "        clear_button,\n",
        "      ]\n",
        "    ))], scroll=ScrollMode.AUTO, auto_scroll=False)\n",
        "    return c\n",
        "\n",
        "stable_animation_prefs = {\n",
        "    #'prompt': '',\n",
        "    'animation_prompt': '{\\n0:\"\",\\n}',\n",
        "    #'animation_prompts': {},\n",
        "    'negative_prompt': 'text, words, watermark, shutterstock',\n",
        "    #'frame': 0,\n",
        "    'num_inference_steps': 50,\n",
        "    'guidance_scale': 9.0,\n",
        "    'export_to_video': True,\n",
        "    'seed': 0,\n",
        "    'width': 512,\n",
        "    'height': 512,\n",
        "    'max_frames': 300,\n",
        "    'steps_curve': \"0:(30)\",\n",
        "    'model': 'stable-diffusion-v1-5',\n",
        "    'style_preset': \"None\",\n",
        "    'sampler': 'K_dpmpp_2m',\n",
        "    'clip_guidance': \"None\",\n",
        "    'steps_strength_adj': True,\n",
        "    'interpolate_prompts': False,\n",
        "    'locked_seed': False,\n",
        "    'noise_add_curve': \"0:(0.02)\",\n",
        "    'noise_scale_curve': \"0:(0.99)\",\n",
        "    'strength_curve': \"0:(0.65)\",\n",
        "    'diffusion_cadence_curve': \"0:(1.0)\",\n",
        "    'cadence_interp': 'Mix',\n",
        "    'cadence_spans': False,\n",
        "    'inpaint_border': False,\n",
        "    'border': \"replicate\",\n",
        "    'use_inpainting_model': False,\n",
        "    'fps': 12,\n",
        "    'mask_min_value': \"0:(0.25)\",\n",
        "    'mask_binarization_thr': 0.5,\n",
        "    #Colour & Depth Parameters\n",
        "    'color_coherence': 'LAB',\n",
        "    'brightness_curve': \"0:(1.0)\",\n",
        "    'contrast_curve': \"0:(1.0)\",\n",
        "    'hue_curve': \"0:(0.0)\",\n",
        "    'saturation_curve': \"0:(1.0)\",\n",
        "    'lightness_curve': \"0:(0.0)\",\n",
        "    'color_match_animate': True,\n",
        "    'depth_model_weight': 0.3,\n",
        "    'near_plane': 200,\n",
        "    'far_plane': 10000,\n",
        "    'fov_curve': \"0:(25)\", #-180\n",
        "    'depth_blur_curve': \"0:(0.0)\", #-7\n",
        "    'depth_warp_curve': \"0:(1.0)\", #-1\n",
        "    #2D & 3D Parameters\n",
        "    'translation_x': \"0:(0)\",\n",
        "    'translation_y': \"0:(0)\",\n",
        "    'translation_z': \"0:(0)\",\n",
        "    'angle': \"0:(0)\",\n",
        "    'zoom': \"0:(1)\",\n",
        "    'rotation_x': \"0:(0)\",\n",
        "    'rotation_y': \"0:(0)\",\n",
        "    'rotation_z': \"0:(0)\",\n",
        "    'camera_type': \"Perspective\",#Orthographic\n",
        "    'render_mode': \"Mesh\",#Pointcloud\n",
        "    'animation_mode': \"3D warp\",\n",
        "    'mask_power': 0.3, #-4\n",
        "    #Input Parameters\n",
        "    'init_image': \"\",\n",
        "    'init_sizing': \"Stretch\",\n",
        "    'mask_image': \"\",\n",
        "    'mask_invert': False,\n",
        "    'video_init_path': \"\",\n",
        "    'video_flow_warp': True,\n",
        "    'video_mix_in_curve': \"0:(0.02)\",\n",
        "    'extract_nth_frame': 1,\n",
        "    'video_init_fps': 12,\n",
        "    #Post-Processor Parameters\n",
        "    'output_fps': 24,\n",
        "    'frame_interpolation_mode': \"Rife\", #film, none\n",
        "    'frame_interpolation_factor': \"2\", #4, 8\n",
        "    'batch_folder_name': '',\n",
        "    \"apply_ESRGAN_upscale\": prefs['apply_ESRGAN_upscale'],\n",
        "    \"enlarge_scale\": 2.0,\n",
        "    \"display_upscaled_image\": False,\n",
        "    \"lower_memory\": True,\n",
        "}\n",
        "stable_animation_prefs_default = stable_animation_prefs.copy()\n",
        "\n",
        "def buildStableAnimation(page):\n",
        "    global stable_animation_prefs, prefs, pipe_stable_animation\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            stable_animation_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            stable_animation_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            stable_animation_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def clear_output(e):\n",
        "      if prefs['enable_sounds']: page.snd_delete.play()\n",
        "      page.stable_animation_output.controls = []\n",
        "      page.stable_animation_output.update()\n",
        "      clear_button.visible = False\n",
        "      clear_button.update()\n",
        "    def stable_animation_help(e):\n",
        "      def close_stable_animation_dlg(e):\n",
        "        nonlocal stable_animation_help_dlg\n",
        "        stable_animation_help_dlg.open = False\n",
        "        page.update()\n",
        "      stable_animation_help_dlg = AlertDialog(title=Text(\"üíÅ   Help with Stable Animation\"), content=Column([\n",
        "          Text(\"With our Animation SDK, artists and developers have the ability to use the Stable Diffusion family of models to generate stunning animations. Create animations purely from prompts, start with an initial image, or drive an animation from a source video. \"),\n",
        "          Text(\"Artists have the ability to use all of our available inference models to generate animations. We currently support text-to-animation, image-to-animation, and video-to-animation. To see animated previews of how these parameters affect the resulting animation, please check out our Animation Handbook.\"),\n",
        "          Markdown(\" [Project Page](https://platform.stability.ai/docs/features/animation) | [Colab Gradio](https://colab.research.google.com/github/Stability-AI/stability-sdk/blob/animation/nbs/animation_gradio.ipynb) | [Animation Handbook](https://docs.google.com/document/d/1iHcAu_5rG11guGFie8sXBXPGuM4yKzqdd13MJ_1LU8U/edit?usp=sharing)\", on_tap_link=lambda e: e.page.launch_url(e.data)),\n",
        "        ], scroll=ScrollMode.AUTO), actions=[TextButton(\"üí∏  Worth it... \", on_click=close_stable_animation_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = stable_animation_help_dlg\n",
        "      stable_animation_help_dlg.open = True\n",
        "      page.update()\n",
        "    def toggle_ESRGAN(e):\n",
        "        ESRGAN_settings.height = None if e.control.value else 0\n",
        "        stable_animation_prefs['apply_ESRGAN_upscale'] = e.control.value\n",
        "        ESRGAN_settings.update()\n",
        "    def file_picker_result(e: FilePickerResultEvent):\n",
        "        if e.files != None:\n",
        "          upload_files(e)\n",
        "    def on_upload_progress(e: FilePickerUploadEvent):\n",
        "      nonlocal pick_type\n",
        "      if e.progress == 1:\n",
        "        if not slash in e.file_name:\n",
        "          fname = os.path.join(root_dir, e.file_name)\n",
        "          stable_animation_prefs['file_name'] = e.file_name.rpartition('.')[0]\n",
        "        else:\n",
        "          fname = e.file_name\n",
        "          stable_animation_prefs['file_name'] = e.file_name.rpartition(slash)[2].rpartition('.')[0]\n",
        "        if pick_type == \"image\":\n",
        "          init_image.value = fname\n",
        "          init_image.update()\n",
        "          stable_animation_prefs['init_image'] = fname\n",
        "        if pick_type == \"mask\":\n",
        "          mask_image.value = fname\n",
        "          mask_image.update()\n",
        "          stable_animation_prefs['mask_image'] = fname\n",
        "        elif pick_type == \"video\":\n",
        "          init_video.value = fname\n",
        "          init_video.update()\n",
        "          stable_animation_prefs['init_video'] = fname\n",
        "        page.update()\n",
        "    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)\n",
        "    def upload_files(e):\n",
        "        uf = []\n",
        "        if file_picker.result != None and file_picker.result.files != None:\n",
        "            for f in file_picker.result.files:\n",
        "              if page.web:\n",
        "                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))\n",
        "              else:\n",
        "                on_upload_progress(FilePickerUploadEvent(f.path, 1, \"\"))\n",
        "            file_picker.upload(uf)\n",
        "    page.overlay.append(file_picker)\n",
        "    pick_type = \"\"\n",
        "    def pick_init(e):\n",
        "        nonlocal pick_type\n",
        "        pick_type = \"image\"\n",
        "        file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"png\", \"PNG\", \"jpg\", \"jpeg\"], dialog_title=\"Pick Init Image File\")\n",
        "    def pick_mask(e):\n",
        "        nonlocal pick_type\n",
        "        pick_type = \"mask\"\n",
        "        file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"png\", \"PNG\", \"jpg\", \"jpeg\"], dialog_title=\"Pick Mask Image File\")\n",
        "    def pick_video(e):\n",
        "        nonlocal pick_type\n",
        "        pick_type = \"video\"\n",
        "        file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"mp4\", \"avi\"], dialog_title=\"Pick Initial Video File\")\n",
        "    def save_preset(e):\n",
        "      def copy_preset(pl):\n",
        "        nonlocal text_list, enter_text\n",
        "        page.set_clipboard(enter_text.value)\n",
        "        page.snack_bar = SnackBar(content=Text(f\"üìã   Animation Preset copied to clipboard...\"))\n",
        "        page.snack_bar.open = True\n",
        "        close_dlg(e)\n",
        "      def close_dlg(e):\n",
        "          dlg_copy.open = False\n",
        "          page.update()\n",
        "      text_list = json.dumps(stable_animation_prefs, indent = 4)\n",
        "      enter_text = TextField(label=\"Stable Animation Preset JSON\", value=text_list.strip(), expand=True, filled=True, multiline=True, autofocus=True)\n",
        "      dlg_copy = AlertDialog(modal=False, title=Text(\"üìù  Stable Animation as JSON\"), content=Container(Column([enter_text], alignment=MainAxisAlignment.START, tight=True, width=(page.window_width or page.width) - 180, height=(page.window_height or page.height) - 100, scroll=\"none\"), width=(page.window_width or page.width) - 180, height=(page.window_height or page.height) - 100), actions=[TextButton(content=Text(\"Cancel\", size=18), on_click=close_dlg), ElevatedButton(content=Text(value=emojize(\":floppy_disk:\") + \"  Copy Preset JSON to Clipboard\", size=19, weight=FontWeight.BOLD), data=text_list, on_click=lambda ev: copy_preset(text_list))], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = dlg_copy\n",
        "      dlg_copy.open = True\n",
        "      page.update()\n",
        "    def paste_preset(e):\n",
        "      def save_preset(e):\n",
        "        try:\n",
        "          preset_json = json.loads(enter_text.value.strip())\n",
        "        except UnicodeDecodeError:\n",
        "          close_dlg(e)\n",
        "          alert_msg(\"Error Parsing JSON Data...\")\n",
        "          return\n",
        "        load_preset(preset_json)\n",
        "        close_dlg(e)\n",
        "      def close_dlg(e):\n",
        "          dlg_paste.open = False\n",
        "          page.update()\n",
        "      enter_text = TextField(label=\"Enter Stable Animation Preset JSON\", expand=True, filled=True, min_lines=30, multiline=True, autofocus=True)\n",
        "      dlg_paste = AlertDialog(modal=False, title=Text(\"üìù  Paste Saved Preset JSON\"), content=Container(Column([enter_text], alignment=MainAxisAlignment.START, tight=True, width=(page.window_width or page.width) - 180, height=(page.window_height or page.height) - 100, scroll=\"none\"), width=(page.window_width or page.width) - 180, height=(page.window_height or page.height) - 100), actions=[TextButton(content=Text(\"Cancel\", size=18), on_click=close_dlg), ElevatedButton(content=Text(value=emojize(\":floppy_disk:\") + \"  Load Preset JSON Values \", size=19, weight=FontWeight.BOLD), on_click=save_preset)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = dlg_paste\n",
        "      dlg_paste.open = True\n",
        "      page.update()\n",
        "    def load_preset(d):\n",
        "      global stable_animation_prefs\n",
        "      stable_animation_prefs = d.copy()\n",
        "      prompt.value = d['animation_prompt']\n",
        "      negative_prompt.value = d['negative_prompt']\n",
        "      max_frames.value = d['max_frames']\n",
        "      fps.set_value(d['fps'])\n",
        "      num_inference_row.set_value(d['num_inference_steps'])\n",
        "      guidance.set_value(d['guidance_scale'])\n",
        "      model_checkpoint.value = d['model']\n",
        "      generation_sampler.value = d['sampler']\n",
        "      style_preset.value = d['style_preset']\n",
        "      clip_guidance.value = d['clip_guidance']\n",
        "      steps_strength_adj.value = d['steps_strength_adj']\n",
        "      interpolate_prompts.value = d['interpolate_prompts']\n",
        "      locked_seed.value = d['locked_seed']\n",
        "      noise_add_curve.value = d['noise_add_curve']\n",
        "      noise_scale_curve.value = d['noise_scale_curve']\n",
        "      strength_curve.value = d['strength_curve']\n",
        "      diffusion_cadence_curve.value = d['diffusion_cadence_curve']\n",
        "      cadence_interp.value = d['cadence_interp']\n",
        "      cadence_spans.value = d['cadence_spans']\n",
        "      inpaint_border.value = d['inpaint_border']\n",
        "      border.value = d['border']\n",
        "      use_inpainting_model.value = d['use_inpainting_model']\n",
        "      mask_min_value.value = d['mask_min_value']\n",
        "      mask_binarization_thr.set_value(d['mask_binarization_thr'])\n",
        "      color_coherence.value = d['color_coherence']\n",
        "      brightness_curve.value = d['brightness_curve']\n",
        "      contrast_curve.value = d['contrast_curve']\n",
        "      hue_curve.value = d['hue_curve']\n",
        "      saturation_curve.value = d['saturation_curve']\n",
        "      lightness_curve.value = d['lightness_curve']\n",
        "      color_match_animate.value = d['color_match_animate']\n",
        "      depth_model_weight.set_value(d['depth_model_weight'])\n",
        "      fov_curve.value = d['fov_curve']\n",
        "      depth_blur_curve.value = d['depth_blur_curve']\n",
        "      depth_warp_curve.value = d['depth_warp_curve']\n",
        "      translation_x.value = d['translation_x']\n",
        "      translation_y.value = d['translation_y']\n",
        "      translation_z.value = d['translation_z']\n",
        "      angle.value = d['angle']\n",
        "      zoom.value = d['zoom']\n",
        "      rotation_x.value = d['rotation_x']\n",
        "      rotation_y.value = d['rotation_y']\n",
        "      rotation_z.value = d['rotation_z']\n",
        "      camera_type.value = d['camera_type']\n",
        "      render_mode.value = d['render_mode']\n",
        "      mask_power.set_value(d['mask_power'])\n",
        "      init_image.value = d['init_image']\n",
        "      init_sizing.value = d['init_sizing']\n",
        "      mask_image.value = d['mask_image']\n",
        "      mask_invert.value = d['mask_invert']\n",
        "      video_mix_in_curve.value = d['video_mix_in_curve']\n",
        "      animation_mode.value = d['animation_mode']\n",
        "      init_video.value = d['video_init_path']\n",
        "      video_mix_in_curve.value = d['video_mix_in_curve']\n",
        "      video_flow_warp.value = d['video_flow_warp']\n",
        "      extract_nth_frame.value = d['extract_nth_frame']\n",
        "      video_init_fps.value = d['video_init_fps']\n",
        "      output_fps.value = d['output_fps']\n",
        "      frame_interpolation_mode.value = d['frame_interpolation_mode']\n",
        "      frame_interpolation_factor.value = d['frame_interpolation_factor']\n",
        "      width_slider.set_value(d['width'])\n",
        "      height_slider.set_value(d['height'])\n",
        "      seed.value = d['seed']\n",
        "      batch_folder_name.value = d['batch_folder_name']\n",
        "      #.set_value(d[''])\n",
        "      page.update()\n",
        "    def load_default(e):\n",
        "      global stable_animation_prefs_default\n",
        "      load_preset(stable_animation_prefs_default)\n",
        "    copy_preset_button = IconButton(icons.COPY_ALL, tooltip=\"Save Animation Presets as JSON\", on_click=save_preset)\n",
        "    paste_preset_button = IconButton(icons.CONTENT_PASTE, tooltip=\"Load Animation Presets as JSON\", on_click=paste_preset)\n",
        "    default_preset_button = IconButton(icons.REFRESH, tooltip=\"Reset Animation Presets to Default\", on_click=load_default)\n",
        "    #prompt = TextField(label=\"Animation Prompt Text\", value=stable_animation_prefs['prompt'], col={'md': 9}, multiline=True, on_change=lambda e:changed(e,'prompt'))\n",
        "    prompt = TextField(label=\"Animation Prompt Text\", value=stable_animation_prefs['animation_prompt'], col={'md': 9}, multiline=True, on_change=lambda e:changed(e,'animation_prompt'))\n",
        "    negative_prompt  = TextField(label=\"Negative Prompt Text\", value=stable_animation_prefs['negative_prompt'], col={'md':3}, multiline=True, on_change=lambda e:changed(e,'negative_prompt'))\n",
        "    max_frames  = TextField(label=\"Max Number of Frames\", width=200, hint_text=\"The number of video frames that are generated. Defaults to 16 frames which at 8 frames per seconds amounts to 2 seconds of video.\", value=stable_animation_prefs['max_frames'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, on_change=lambda e:changed(e,'max_frames', ptype='int'))\n",
        "    #max_frames = SliderRow(label=\"Max Number of Frames\", min=1, max=300, divisions=299, pref=stable_animation_prefs, key='max_frames', tooltip=\"The number of video frames that are generated. Defaults to 16 frames which at 8 frames per seconds amounts to 2 seconds of video.\")   \n",
        "    fps = SliderRow(label=\"Frames per Second\", min=1, max=30, divisions=29, suffix='fps', expand=True, pref=stable_animation_prefs, key='fps', tooltip=\"The FPS to extract from the init video clip.\")\n",
        "    num_inference_row = SliderRow(label=\"Number of Inference Steps\", min=1, max=150, divisions=149, pref=stable_animation_prefs, key='num_inference_steps', on_change=lambda e:changed(e,'num_inference_steps'), tooltip=\"The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.\")   \n",
        "    guidance = SliderRow(label=\"Guidance Scale\", min=0, max=50, divisions=100, round=1, pref=stable_animation_prefs, key='guidance_scale')\n",
        "    #eta_slider = SliderRow(label=\"ETA\", min=0, max=1.0, divisions=20, round=1, pref=stable_animation_prefs, key='eta', tooltip=\"The weight of noise for added noise in a diffusion step. Its value is between 0.0 and 1.0 - 0.0 is DDIM and 1.0 is DDPM scheduler respectively.\")\n",
        "    model_checkpoint = Dropdown(label=\"Model Checkpoint\", hint_text=\"\", width=270, options=[dropdown.Option(\"stable-diffusion-xl-beta-v2-2-2\"), dropdown.Option(\"stable-diffusion-768-v2-1\"), dropdown.Option(\"stable-diffusion-512-v2-1\"), dropdown.Option(\"stable-diffusion-768-v2-0\"), dropdown.Option(\"stable-diffusion-512-v2-0\"), dropdown.Option(\"stable-diffusion-v1-5\"), dropdown.Option(\"stable-diffusion-v1\"), dropdown.Option(\"stable-inpainting-512-v2-0\"), dropdown.Option(\"stable-inpainting-v1-0\")], value=stable_animation_prefs['model'], autofocus=False, on_change=lambda e:changed(e, 'model'))\n",
        "    generation_sampler = Dropdown(label=\"Generation Sampler\", hint_text=\"\", width=230, options=[dropdown.Option(\"DDIM\"), dropdown.Option(\"PLMS\"), dropdown.Option(\"K_euler\"), dropdown.Option(\"K_euler_ancestral\"), dropdown.Option(\"K_heun\"), dropdown.Option(\"K_dpmpp_2m\"), dropdown.Option(\"K_dpm_2_ancestral\"), dropdown.Option(\"K_lms\"), dropdown.Option(\"K_dpmpp_2s_ancestral\"), dropdown.Option(\"K_dpm_2\")], value=stable_animation_prefs['sampler'], autofocus=False, on_change=lambda e:changed(e, 'sampler'))\n",
        "    #DDIM, PLMS, K_euler, K_euler_ancestral, K_heun, K_dpm_2, K_dpm_2_ancestral, K_lms, K_dpmpp_2m, K_dpmpp_2s_ancestral\n",
        "    style_preset = Dropdown(label=\"Style Preset\", hint_text=\"\", width=240, options=[dropdown.Option(\"None\"), dropdown.Option(\"3d-model\"), dropdown.Option(\"analog-film\"), dropdown.Option(\"anime\"), dropdown.Option(\"cinematic\"), dropdown.Option(\"comic-book\"), dropdown.Option(\"digital-art\"), dropdown.Option(\"enhance fantasy-art\"), dropdown.Option(\"isometric\"), dropdown.Option(\"line-art\"), dropdown.Option(\"low-poly\"), dropdown.Option(\"modeling-compound\"), dropdown.Option(\"neon-punk\"), dropdown.Option(\"origami\"), dropdown.Option(\"photographic\"), dropdown.Option(\"pixel-art\")], value=stable_animation_prefs['style_preset'], autofocus=False, on_change=lambda e:changed(e, 'style_preset'))\n",
        "    clip_guidance = Dropdown(label=\"Clip Guidance\", hint_text=\"\", width=240, options=[dropdown.Option(\"None\"), dropdown.Option(\"Simple\"), dropdown.Option(\"FastBlue\"), dropdown.Option(\"FastGreen\")], value=stable_animation_prefs['clip_guidance'], autofocus=False, on_change=lambda e:changed(e, 'clip_guidance'))\n",
        "    steps_strength_adj = Checkbox(label=\"Steps Strength Adjustment\", tooltip=\"Adjusts number of diffusion steps based on current previous frame strength value.\", value=stable_animation_prefs['steps_strength_adj'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'steps_strength_adj'))\n",
        "    interpolate_prompts = Checkbox(label=\"Interpolate Prompts\", tooltip=\"Smoothly interpolate prompts between keyframes.\", value=stable_animation_prefs['interpolate_prompts'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'interpolate_prompts'))\n",
        "    locked_seed = Checkbox(label=\"Locked Seed\", tooltip=\"Keep the same seed for all frames.\", value=stable_animation_prefs['locked_seed'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'locked_seed'))\n",
        "    noise_add_curve  = TextField(label=\"Noise Add Curve\", value=stable_animation_prefs['noise_add_curve'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, multiline=True, on_change=lambda e:changed(e,'noise_add_curve'))\n",
        "    noise_scale_curve  = TextField(label=\"Noise Scale Curve\", value=stable_animation_prefs['noise_scale_curve'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, multiline=True, on_change=lambda e:changed(e,'noise_scale_curve'))\n",
        "    strength_curve  = TextField(label=\"Strength Curve\", value=stable_animation_prefs['strength_curve'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, multiline=True, on_change=lambda e:changed(e,'strength_curve'))\n",
        "    diffusion_cadence_curve  = TextField(label=\"Diffusion Cadence Curve\", hint_text=\"One greater than the number of frames between diffusion operations. A cadence of 1 performs diffusion on each frame. Values greater than one will generate frames using interpolation methods.\", value=stable_animation_prefs['diffusion_cadence_curve'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, multiline=True, on_change=lambda e:changed(e,'diffusion_cadence_curve'))\n",
        "    cadence_interp = Dropdown(label=\"Cadence Interpolation\", hint_text=\"\", width=200, options=[dropdown.Option(\"Mix\"), dropdown.Option(\"RIFE\"), dropdown.Option(\"VAE-LERP\"), dropdown.Option(\"VAE-SLERP\")], value=stable_animation_prefs['cadence_interp'], autofocus=False, on_change=lambda e:changed(e, 'cadence_interp'))\n",
        "    cadence_spans = Checkbox(label=\"Cadence Spans\", tooltip=\"Experimental diffusion cadence mode for better outpainting\", value=stable_animation_prefs['cadence_spans'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'cadence_spans'))\n",
        "    inpaint_border = Checkbox(label=\"Inpaint Border\", tooltip=\"Use inpainting on top of border regions for 2D and 3D warp modes.\", value=stable_animation_prefs['inpaint_border'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'inpaint_border'))\n",
        "    border = Dropdown(label=\"Border\", hint_text=\"Method that will be used to fill empty regions, e.g. after a rotation transform.\", width=200, options=[dropdown.Option(\"reflect\"), dropdown.Option(\"replicate\"), dropdown.Option(\"wrap\"), dropdown.Option(\"zero\"), dropdown.Option(\"prefill\")], value=stable_animation_prefs['border'], autofocus=False, on_change=lambda e:changed(e, 'border'))\n",
        "    use_inpainting_model = Checkbox(label=\"Use Inpainting Model\", tooltip=\"\", value=stable_animation_prefs['use_inpainting_model'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'use_inpainting_model'))\n",
        "    mask_min_value  = TextField(label=\"Mask Minimum\", value=stable_animation_prefs['mask_min_value'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, multiline=True, on_change=lambda e:changed(e,'mask_min_value'))\n",
        "    mask_binarization_thr = SliderRow(label=\"Mask Binarization Threshold\", min=0, max=1.0, divisions=20, round=1, pref=stable_animation_prefs, key='mask_binarization_thr', expand=True, tooltip=\"Grayscale mask values lower than this value will be set to 0, values that are higher ‚Äî to 1.\")\n",
        "    color_coherence = Dropdown(label=\"Color Coherance\", hint_text=\"Color space that will be used for inter-frame color adjustments.\", width=350, col={'xs':12, 'md':6, 'lg':4, 'xl':3}, options=[dropdown.Option(\"None\"), dropdown.Option(\"HSV\"), dropdown.Option(\"LAB\"), dropdown.Option(\"RGB\")], value=stable_animation_prefs['color_coherence'], autofocus=False, on_change=lambda e:changed(e, 'color_coherence'))\n",
        "    brightness_curve  = TextField(label=\"Brightness Curve\", value=stable_animation_prefs['brightness_curve'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, multiline=True, on_change=lambda e:changed(e,'brightness_curve'))\n",
        "    contrast_curve  = TextField(label=\"Contrast Curve\", value=stable_animation_prefs['contrast_curve'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, multiline=True, on_change=lambda e:changed(e,'contrast_curve'))\n",
        "    hue_curve  = TextField(label=\"Hue Curve\", value=stable_animation_prefs['hue_curve'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, multiline=True, on_change=lambda e:changed(e,'hue_curve'))\n",
        "    saturation_curve  = TextField(label=\"Saturation Curve\", value=stable_animation_prefs['saturation_curve'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, multiline=True, on_change=lambda e:changed(e,'saturation_curve'))\n",
        "    lightness_curve  = TextField(label=\"Lightness Curve\", value=stable_animation_prefs['lightness_curve'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, multiline=True, on_change=lambda e:changed(e,'lightness_curve'))\n",
        "    color_match_animate = Checkbox(label=\"Color Match Animate\", tooltip=\"Animate color match between key frames.\", value=stable_animation_prefs['color_match_animate'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'color_match_animate'))\n",
        "    depth_model_weight = SliderRow(label=\"Depth Model Weight\", min=0, max=1.0, divisions=20, round=1, pref=stable_animation_prefs, key='depth_model_weight', expand=True, tooltip=\"Blend factor between AdaBins and MiDaS depth models.\")\n",
        "    fov_curve  = TextField(label=\"FOV Curve\", hint_text=\"FOV angle of camera volume in degrees. Max 180.\", value=stable_animation_prefs['fov_curve'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, multiline=True, on_change=lambda e:changed(e,'fov_curve'))\n",
        "    depth_blur_curve  = TextField(label=\"Depth Blur Curve\", hint_text=\"\", value=stable_animation_prefs['depth_blur_curve'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, multiline=True, on_change=lambda e:changed(e,'depth_blur_curve'))\n",
        "    depth_warp_curve  = TextField(label=\"Depth Warp Curve\", hint_text=\"\", value=stable_animation_prefs['depth_warp_curve'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, multiline=True, on_change=lambda e:changed(e,'depth_warp_curve'))\n",
        "    translation_x  = TextField(label=\"Translation X\", hint_text=\"\", value=stable_animation_prefs['translation_x'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, multiline=True, on_change=lambda e:changed(e,'translation_x'))\n",
        "    translation_y  = TextField(label=\"Translation Y\", hint_text=\"\", value=stable_animation_prefs['translation_y'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, multiline=True, on_change=lambda e:changed(e,'translation_y'))\n",
        "    translation_z  = TextField(label=\"Translation Z\", hint_text=\"\", value=stable_animation_prefs['translation_z'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, multiline=True, on_change=lambda e:changed(e,'translation_z'))\n",
        "    angle  = TextField(label=\"Angle\", hint_text=\"\", value=stable_animation_prefs['angle'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, multiline=True, on_change=lambda e:changed(e,'angle'))\n",
        "    zoom  = TextField(label=\"Zoom\", hint_text=\"\", value=stable_animation_prefs['zoom'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, multiline=True, on_change=lambda e:changed(e,'zoom'))\n",
        "    rotation_x  = TextField(label=\"Rotation X\", hint_text=\"\", value=stable_animation_prefs['rotation_x'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, multiline=True, on_change=lambda e:changed(e,'rotation_x'))\n",
        "    rotation_y  = TextField(label=\"Rotation Y\", hint_text=\"\", value=stable_animation_prefs['rotation_y'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, multiline=True, on_change=lambda e:changed(e,'rotation_y'))\n",
        "    rotation_z  = TextField(label=\"Rotation Z\", hint_text=\"\", value=stable_animation_prefs['rotation_z'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, multiline=True, on_change=lambda e:changed(e,'rotation_z'))\n",
        "    camera_type = Dropdown(label=\"Camera Type\", hint_text=\"\", width=200, options=[dropdown.Option(\"Perspective\"), dropdown.Option(\"Orthographic\")], value=stable_animation_prefs['camera_type'], autofocus=False, on_change=lambda e:changed(e, 'camera_type'))\n",
        "    render_mode = Dropdown(label=\"Render Mode\", hint_text=\"\", width=200, options=[dropdown.Option(\"Mesh\"), dropdown.Option(\"Pointcloud\")], value=stable_animation_prefs['render_mode'], autofocus=False, col={'xs':12, 'md':6, 'lg':4, 'xl':3}, on_change=lambda e:changed(e, 'render_mode'))\n",
        "    mask_power = SliderRow(label=\"Mesh Power\", min=0, max=4.0, divisions=80, round=1, expand=True, pref=stable_animation_prefs, key='mask_power', tooltip=\"\")\n",
        "    init_image = TextField(label=\"Initial Image\", value=stable_animation_prefs['init_image'], on_change=lambda e:changed(e,'init_image'), expand=True, height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_init))\n",
        "    init_sizing = Dropdown(label=\"Init Sizing\", hint_text=\"\", width=200, options=[dropdown.Option(\"Cover\"), dropdown.Option(\"Stretch\"), dropdown.Option(\"Resize-Canvas\")], value=stable_animation_prefs['init_sizing'], autofocus=False, on_change=lambda e:changed(e, 'init_sizing'))\n",
        "    mask_image = TextField(label=\"Mask Image\", value=stable_animation_prefs['mask_image'], on_change=lambda e:changed(e,'mask_image'), expand=True, height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_mask))\n",
        "    mask_invert = Checkbox(label=\"Invert Mask\", tooltip=\"White in mask marks areas to change by default.\", value=stable_animation_prefs['mask_invert'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'mask_invert'))\n",
        "    video_mix_in_curve  = TextField(label=\"Video Mixin Curve\", hint_text=\"\", value=stable_animation_prefs['video_mix_in_curve'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, multiline=True, on_change=lambda e:changed(e,'video_mix_in_curve'))\n",
        "    animation_mode = Dropdown(label=\"Animation Mode\", hint_text=\"\", width=200, options=[dropdown.Option(\"2D\"), dropdown.Option(\"3D warp\"), dropdown.Option(\"3D render\"), dropdown.Option(\"Video Input\")], value=stable_animation_prefs['animation_mode'], autofocus=False, on_change=lambda e:changed(e, 'animation_mode'))\n",
        "    init_video = TextField(label=\"Init Video File\", value=stable_animation_prefs['video_init_path'], on_change=lambda e:changed(e,'video_init_path'), expand=True, height=60, suffix=IconButton(icon=icons.VIDEO_CALL, on_click=pick_video))\n",
        "    video_mix_in_curve  = TextField(label=\"Video Mixin Curve\", hint_text=\"\", value=stable_animation_prefs['video_mix_in_curve'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, multiline=True, on_change=lambda e:changed(e,'video_mix_in_curve'))\n",
        "    video_flow_warp = Checkbox(label=\"Video Flow Warp\", tooltip=\"Whether or not to transfer the optical flow from the video to the generated animation as a warp effect.\", value=stable_animation_prefs['video_flow_warp'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'video_flow_warp'))\n",
        "    extract_nth_frame  = TextField(label=\"Extract Nth Frame\", width=200, hint_text=\"Only use every Nth frame of the video\", value=stable_animation_prefs['extract_nth_frame'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, on_change=lambda e:changed(e,'extract_nth_frame', ptype='int'))\n",
        "    video_init_fps  = TextField(label=\"Init Video FPS\", width=200, hint_text=\"\", value=stable_animation_prefs['video_init_fps'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, on_change=lambda e:changed(e,'video_init_fps', ptype='int'))\n",
        "    output_fps  = TextField(label=\"Output FPS\", width=200, hint_text=\"Frame rate to use when generating video output.\", value=stable_animation_prefs['output_fps'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, on_change=lambda e:changed(e,'output_fps', ptype='int'))\n",
        "    frame_interpolation_mode = Dropdown(label=\"Frame Interpolation Mode\", hint_text=\"\", width=200, options=[dropdown.Option(\"Rife\"), dropdown.Option(\"Film\"), dropdown.Option(\"None\")], value=stable_animation_prefs['frame_interpolation_mode'], autofocus=False, on_change=lambda e:changed(e, 'frame_interpolation_mode'))\n",
        "    frame_interpolation_factor = Dropdown(label=\"Frame Interpolation Factor\", hint_text=\"\", width=200, options=[dropdown.Option(\"2\"), dropdown.Option(\"4\"), dropdown.Option(\"8\")], value=stable_animation_prefs['frame_interpolation_factor'], autofocus=False, on_change=lambda e:changed(e, 'frame_interpolation_factor'))\n",
        "    width_slider = SliderRow(label=\"Width\", min=256, max=1280, divisions=64, multiple=16, suffix=\"px\", pref=stable_animation_prefs, key='width')\n",
        "    height_slider = SliderRow(label=\"Height\", min=256, max=1280, divisions=64, multiple=16, suffix=\"px\", pref=stable_animation_prefs, key='height')\n",
        "    export_to_video = Tooltip(message=\"Save mp4 file along with Image Sequence\", content=Switch(label=\"Export to Video\", value=stable_animation_prefs['export_to_video'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'export_to_video')))\n",
        "    #lower_memory = Tooltip(message=\"Enable CPU offloading, VAE Tiling & Stitching\", content=Switch(label=\"Lower Memory Mode\", value=stable_animation_prefs['lower_memory'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'lower_memory')))\n",
        "    seed = TextField(label=\"Seed\", width=90, value=str(stable_animation_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip=\"0 or -1 picks a Random seed\", on_change=lambda e:changed(e,'seed', ptype='int'))\n",
        "    batch_folder_name = TextField(label=\"Batch Folder Name\", value=stable_animation_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    apply_ESRGAN_upscale = Switch(label=\"Apply ESRGAN Upscale\", value=stable_animation_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)\n",
        "    enlarge_scale_slider = SliderRow(label=\"Enlarge Scale\", min=1, max=4, divisions=6, round=1, suffix=\"x\", pref=stable_animation_prefs, key='enlarge_scale')\n",
        "    display_upscaled_image = Checkbox(label=\"Display Upscaled Image\", value=stable_animation_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))\n",
        "    ESRGAN_settings = Container(Column([enlarge_scale_slider, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_stable_animation = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_stable_animation.height = None if status['installed_ESRGAN'] else 0\n",
        "    page.stable_animation_output = Column([], scroll=ScrollMode.AUTO, auto_scroll=False)\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(page.stable_animation_output.controls) > 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"ü•è  Stable Animation SDK\", \"Use Stability.ai API Credits for Advanced Video Generation\", actions=[copy_preset_button, paste_preset_button, default_preset_button, IconButton(icon=icons.HELP, tooltip=\"Help with Instruct-Pix2Pix Settings\", on_click=stable_animation_help)]),\n",
        "        ResponsiveRow([prompt, negative_prompt], vertical_alignment=CrossAxisAlignment.START),\n",
        "        Row([max_frames, fps]),\n",
        "        num_inference_row,\n",
        "        guidance,\n",
        "        Row([model_checkpoint, generation_sampler, style_preset, clip_guidance], wrap=True),\n",
        "        Row([steps_strength_adj, interpolate_prompts, locked_seed], wrap=True),\n",
        "        ResponsiveRow([noise_add_curve, noise_scale_curve, strength_curve, diffusion_cadence_curve]),\n",
        "        Row([cadence_interp, cadence_spans]),\n",
        "        Row([border, inpaint_border, use_inpainting_model]),\n",
        "        Row([mask_min_value, mask_binarization_thr]),\n",
        "        ResponsiveRow([color_coherence, brightness_curve, contrast_curve, hue_curve, saturation_curve, lightness_curve]),\n",
        "        Row([color_match_animate, depth_model_weight]),\n",
        "        ResponsiveRow([fov_curve, depth_blur_curve, depth_warp_curve]),\n",
        "        ResponsiveRow([translation_x, translation_y, translation_z, zoom]),\n",
        "        ResponsiveRow([rotation_x, rotation_y, rotation_z, angle]),\n",
        "        Row([camera_type, render_mode, mask_power]),\n",
        "        Row([init_image, init_sizing]),\n",
        "        Row([mask_image, mask_invert]),\n",
        "        Row([init_video, animation_mode]),\n",
        "        Row([video_flow_warp, video_mix_in_curve]),\n",
        "        Row([extract_nth_frame, video_init_fps, output_fps], wrap=True),\n",
        "        Row([export_to_video, frame_interpolation_mode, frame_interpolation_factor]),\n",
        "        width_slider, height_slider,\n",
        "        page.ESRGAN_block_stable_animation,\n",
        "        Row([seed, batch_folder_name]),\n",
        "        Row([\n",
        "            ElevatedButton(content=Text(\"üßå  Run Stable Animation\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_stable_animation(page)),\n",
        "             #ElevatedButton(content=Text(value=\"üìú   Run from Prompts List\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_stable_animation(page, from_list=True))\n",
        "        ]),\n",
        "        page.stable_animation_output,\n",
        "        clear_button,\n",
        "      ]\n",
        "    ))], scroll=ScrollMode.AUTO, auto_scroll=False)\n",
        "    return c\n",
        "\n",
        "\n",
        "materialdiffusion_prefs = {\n",
        "    \"material_prompt\": '',\n",
        "    \"batch_folder_name\": '',\n",
        "    \"file_prefix\": \"material-\",\n",
        "    \"num_outputs\": 1,\n",
        "    \"steps\":50,\n",
        "    \"eta\":0.4,\n",
        "    \"width\": 512,\n",
        "    \"height\":512,\n",
        "    \"guidance_scale\":7.5,\n",
        "    \"seed\":0,\n",
        "    \"init_image\": '',\n",
        "    \"prompt_strength\": 0.5,\n",
        "    \"mask_image\": '',\n",
        "    \"invert_mask\": False,\n",
        "    \"apply_ESRGAN_upscale\": prefs['apply_ESRGAN_upscale'],\n",
        "    \"enlarge_scale\": prefs['enlarge_scale'],\n",
        "    #\"face_enhance\": prefs['face_enhance'],\n",
        "    \"display_upscaled_image\": prefs['display_upscaled_image'],\n",
        "}\n",
        "\n",
        "def buildMaterialDiffusion(page):\n",
        "    global prefs, materialdiffusion_prefs, status\n",
        "\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            materialdiffusion_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            materialdiffusion_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            materialdiffusion_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def pick_files_result(e: FilePickerResultEvent):\n",
        "        if e.files:\n",
        "            img = e.files\n",
        "            uf = []\n",
        "            fname = img[0]\n",
        "            print(\", \".join(map(lambda f: f.name, e.files)))\n",
        "            src_path = page.get_upload_url(fname.name, 600)\n",
        "            uf.append(FilePickerUploadFile(fname.name, upload_url=src_path))\n",
        "            pick_files_dialog.upload(uf)\n",
        "            print(str(src_path))\n",
        "            #src_path = ''.join(src_path)\n",
        "            print(str(uf[0]))\n",
        "            dst_path = os.path.join(root_dir, fname.name)\n",
        "            print(f'Copy {src_path} to {dst_path}')\n",
        "            #shutil.copy(src_path, dst_path)\n",
        "            # TODO: is init or mask?\n",
        "            init_image.value = dst_path\n",
        "\n",
        "    pick_files_dialog = FilePicker(on_result=pick_files_result)\n",
        "    page.overlay.append(pick_files_dialog)\n",
        "    #selected_files = Text()\n",
        "\n",
        "    def file_picker_result(e: FilePickerResultEvent):\n",
        "        if e.files != None:\n",
        "            upload_files(e)\n",
        "    def on_upload_progress(e: FilePickerUploadEvent):\n",
        "        nonlocal pick_type\n",
        "        if e.progress == 1:\n",
        "            if not slash in e.file_name:\n",
        "              fname = os.path.join(root_dir, e.file_name)\n",
        "            else:\n",
        "              fname = e.file_name\n",
        "            if pick_type == \"init\":\n",
        "                init_image.value = fname\n",
        "                init_image.update()\n",
        "                materialdiffusion_prefs['init_image'] = fname\n",
        "            elif pick_type == \"mask\":\n",
        "                mask_image.value = fname\n",
        "                mask_image.update()\n",
        "                materialdiffusion_prefs['mask_image'] = fname\n",
        "            page.update()\n",
        "    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)\n",
        "    def upload_files(e):\n",
        "        uf = []\n",
        "        if file_picker.result != None and file_picker.result.files != None:\n",
        "            for f in file_picker.result.files:\n",
        "              if page.web:\n",
        "                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))\n",
        "              else:\n",
        "                on_upload_progress(FilePickerUploadEvent(f.path, 1, \"\"))\n",
        "            file_picker.upload(uf)\n",
        "    page.overlay.append(file_picker)\n",
        "    pick_type = \"\"\n",
        "    #page.overlay.append(pick_files_dialog)\n",
        "    def pick_init(e):\n",
        "        nonlocal pick_type\n",
        "        pick_type = \"init\"\n",
        "        file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"png\", \"PNG\"], dialog_title=\"Pick Init Image File\")\n",
        "    def pick_mask(e):\n",
        "        nonlocal pick_type\n",
        "        pick_type = \"mask\"\n",
        "        file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"png\", \"PNG\"], dialog_title=\"Pick Black & White Mask Image\")\n",
        "    def toggle_ESRGAN(e):\n",
        "        ESRGAN_settings.height = None if e.control.value else 0\n",
        "        materialdiffusion_prefs['apply_ESRGAN_upscale'] = e.control.value\n",
        "        ESRGAN_settings.update()\n",
        "        has_changed = True\n",
        "    def change_strength(e):\n",
        "        strength_value.value = f\" {int(e.control.value * 100)}%\"\n",
        "        strength_value.update()\n",
        "        guidance.update()\n",
        "        changed(e, 'prompt_strength', ptype=\"float\")\n",
        "\n",
        "    material_prompt = TextField(label=\"Material Prompt\", value=materialdiffusion_prefs['material_prompt'], multiline=True, on_change=lambda e:changed(e,'material_prompt'))\n",
        "    batch_folder_name = TextField(label=\"Batch Folder Name\", value=materialdiffusion_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    file_prefix = TextField(label=\"Filename Prefix\", value=materialdiffusion_prefs['file_prefix'], width=150, on_change=lambda e:changed(e,'file_prefix'))\n",
        "    #num_outputs = NumberPicker(label=\"Num of Outputs\", min=1, max=4, step=4, value=materialdiffusion_prefs['num_outputs'], on_change=lambda e:changed(e,'num_outputs', ptype=\"int\"))\n",
        "    #num_outputs = TextField(label=\"num_outputs\", value=materialdiffusion_prefs['num_outputs'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'num_outputs', ptype=\"int\"))\n",
        "    #n_iterations = TextField(label=\"Number of Iterations\", value=materialdiffusion_prefs['n_iterations'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'n_iterations', ptype=\"int\"))\n",
        "    steps = TextField(label=\"Inference Steps\", value=materialdiffusion_prefs['steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'steps', ptype=\"int\"))\n",
        "    eta = TextField(label=\"DDIM ETA\", value=materialdiffusion_prefs['eta'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'eta', ptype=\"float\"))\n",
        "    steps = SliderRow(label=\"Inference Steps\", min=0, max=200, divisions=200, pref=materialdiffusion_prefs, key='steps')\n",
        "    eta = SliderRow(label=\"DDIM ETA\", min=0, max=1, divisions=20, round=1, pref=materialdiffusion_prefs, key='eta')\n",
        "    seed = TextField(label=\"Seed\", value=materialdiffusion_prefs['seed'], keyboard_type=KeyboardType.NUMBER, width=120, on_change=lambda e:changed(e,'seed', ptype=\"int\"))\n",
        "    param_rows = ResponsiveRow([Column([batch_folder_name, file_prefix, NumberPicker(label=\"Output Images\", min=1, max=8, step=1, value=materialdiffusion_prefs['num_outputs'], on_change=lambda e:changed(e,'num_outputs', ptype=\"int\"))], col={'xs':12, 'md':6}), \n",
        "                      Column([steps, eta, seed], col={'xs':12, 'lg':6})], vertical_alignment=CrossAxisAlignment.START)\n",
        "    batch_row = Row([batch_folder_name, file_prefix], col={'xs':12, 'lg':6})\n",
        "    number_row = Row([NumberPicker(label=\"Output Images\", min=1, max=8, step=1, value=materialdiffusion_prefs['num_outputs'], on_change=lambda e:changed(e,'num_outputs', ptype=\"int\")), seed], col={'xs':12, 'md':6})\n",
        "    param_rows = ResponsiveRow([number_row, batch_row], vertical_alignment=CrossAxisAlignment.START)\n",
        "    guidance = SliderRow(label=\"Guidance Scale\", min=0, max=50, divisions=100, round=1, pref=materialdiffusion_prefs, key='guidance_scale')\n",
        "    width_slider = SliderRow(label=\"Width\", min=128, max=1024, divisions=14, multiple=32, suffix=\"px\", pref=materialdiffusion_prefs, key='width')\n",
        "    height_slider = SliderRow(label=\"Height\", min=128, max=1024, divisions=14, multiple=32, suffix=\"px\", pref=materialdiffusion_prefs, key='height')\n",
        "    init_image = TextField(label=\"Init Image\", value=materialdiffusion_prefs['init_image'], on_change=lambda e:changed(e,'init_image'), expand=True, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_init), col={'xs':12, 'md':6})\n",
        "    mask_image = TextField(label=\"Mask Image\", value=materialdiffusion_prefs['mask_image'], on_change=lambda e:changed(e,'mask_image'), expand=True, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD_OUTLINED, on_click=pick_mask), col={'xs':10, 'md':5})\n",
        "    invert_mask = Checkbox(label=\"Invert\", tooltip=\"Swaps the Black & White of your Mask Image\", value=materialdiffusion_prefs['invert_mask'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'invert_mask'), col={'xs':2, 'md':1})\n",
        "    image_pickers = Container(content=ResponsiveRow([init_image, mask_image, invert_mask]), padding=padding.only(top=5), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    prompt_strength = Slider(min=0.1, max=0.9, divisions=16, label=\"{value}%\", value=materialdiffusion_prefs['prompt_strength'], on_change=change_strength, expand=True)\n",
        "    strength_value = Text(f\" {int(materialdiffusion_prefs['prompt_strength'] * 100)}%\", weight=FontWeight.BOLD)\n",
        "    strength_slider = Row([Text(\"Prompt Strength: \"), strength_value, prompt_strength])\n",
        "    #strength_slider = SliderRow(label=\"Prompt Strength\", min=0.1, max=0.9, divisions=16, suffix=\"%\", pref=materialdiffusion_prefs, key='prompt_strength')\n",
        "    img_block = Container(Column([image_pickers, strength_slider, Divider(height=9, thickness=2)]), padding=padding.only(top=5), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    apply_ESRGAN_upscale = Switch(label=\"Apply ESRGAN Upscale\", value=materialdiffusion_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)\n",
        "    enlarge_scale_slider = SliderRow(label=\"Enlarge Scale\", min=1, max=4, divisions=6, round=1, suffix=\"x\", pref=materialdiffusion_prefs, key='enlarge_scale')\n",
        "    #face_enhance = Checkbox(label=\"Use Face Enhance GPFGAN\", value=materialdiffusion_prefs['face_enhance'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'face_enhance'))\n",
        "    display_upscaled_image = Checkbox(label=\"Display Upscaled Image\", value=materialdiffusion_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))\n",
        "    ESRGAN_settings = Container(Column([enlarge_scale_slider, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_material = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_material.height = None if status['installed_ESRGAN'] else 0\n",
        "    if not materialdiffusion_prefs['apply_ESRGAN_upscale']:\n",
        "        ESRGAN_settings.height = 0\n",
        "    parameters_button = ElevatedButton(content=Text(value=\"üí®   Run Material Diffusion\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_materialdiffusion(page))\n",
        "\n",
        "    parameters_row = Row([parameters_button], alignment=MainAxisAlignment.SPACE_BETWEEN)\n",
        "    page.materialdiffusion_output = Column([])\n",
        "    c = Column([Container(\n",
        "        padding=padding.only(18, 14, 20, 10), content=Column([\n",
        "            Header(\"üß±  Replicate Material Diffusion\", \"Create Seamless Tiled Textures with your Prompt. Requires account at Replicate.com and your Key.\"),\n",
        "            material_prompt,\n",
        "            steps,\n",
        "            guidance,\n",
        "            eta,\n",
        "            width_slider, height_slider, #Divider(height=9, thickness=2), \n",
        "            img_block, page.ESRGAN_block_material,\n",
        "            #(img_block if status['installed_img2img'] or status['installed_stability'] else Container(content=None)), (clip_block if prefs['install_CLIP_guided'] else Container(content=None)), (ESRGAN_block if prefs['install_ESRGAN'] else Container(content=None)), \n",
        "            param_rows,\n",
        "            #batch_row,\n",
        "            #number_row,\n",
        "            parameters_row,\n",
        "            page.materialdiffusion_output\n",
        "        ],\n",
        "    ))], scroll=ScrollMode.AUTO)#batch_folder_name, batch_size, n_iterations, steps, eta, seed, \n",
        "    return c\n",
        "\n",
        "ImageNet_classes = {'ATM': 480, 'Acinonyx jubatus': 293, 'Aepyceros melampus': 352, 'Afghan': 160, 'Afghan hound': 160, 'African chameleon': 47, 'African crocodile': 49, 'African elephant': 386, 'African gray': 87, 'African grey': 87, 'African hunting dog': 275, 'Ailuropoda melanoleuca': 388, 'Ailurus fulgens': 387, 'Airedale': 191, 'Airedale terrier': 191, 'Alaska crab': 121, 'Alaska king crab': 121, 'Alaskan king crab': 121, 'Alaskan malamute': 249, 'Alligator mississipiensis': 50, 'Alopex lagopus': 279, 'Ambystoma maculatum': 28, 'Ambystoma mexicanum': 29, 'American Staffordshire terrier': 180, 'American alligator': 50, 'American black bear': 295, 'American chameleon': 40, 'American coot': 137, 'American eagle': 22, 'American egret': 132, 'American lobster': 122, 'American pit bull terrier': 180, 'American robin': 15, 'Angora': 332, 'Angora rabbit': 332, 'Anolis carolinensis': 40, 'Appenzeller': 240, 'Aptenodytes patagonica': 145, 'Arabian camel': 354, 'Aramus pictus': 135, 'Aranea diademata': 74, 'Araneus cavaticus': 73, 'Arctic fox': 279, 'Arctic wolf': 270, 'Arenaria interpres': 139, 'Argiope aurantia': 72, 'Ascaphus trui': 32, 'Asiatic buffalo': 346, 'Ateles geoffroyi': 381, 'Australian terrier': 193, 'Band Aid': 419, 'Bedlington terrier': 181, 'Bernese mountain dog': 239, 'Biro': 418, 'Blenheim spaniel': 156, 'Bonasa umbellus': 82, 'Border collie': 232, 'Border terrier': 182, 'Boston bull': 195, 'Boston terrier': 195, 'Bouvier des Flandres': 233, 'Bouviers des Flandres': 233, 'Brabancon griffon': 262, 'Bradypus tridactylus': 364, 'Brittany spaniel': 215, 'Bubalus bubalis': 346, 'CD player': 485, 'CRO': 688, 'CRT screen': 782, 'Cacatua galerita': 89, 'Camelus dromedarius': 354, 'Cancer irroratus': 119, 'Cancer magister': 118, 'Canis dingo': 273, 'Canis latrans': 272, 'Canis lupus': 269, 'Canis lupus tundrarum': 270, 'Canis niger': 271, 'Canis rufus': 271, 'Cape hunting dog': 275, 'Capra ibex': 350, 'Carassius auratus': 1, 'Carcharodon carcharias': 2, 'Cardigan': 264, 'Cardigan Welsh corgi': 264, 'Carduelis carduelis': 11, 'Caretta caretta': 33, 'Carphophis amoenus': 52, 'Carpodacus mexicanus': 12, 'Cavia cobaya': 338, 'Cebus capucinus': 378, 'Cerastes cornutus': 66, 'Chamaeleo chamaeleon': 47, 'Chesapeake Bay retriever': 209, 'Chihuahua': 151, 'Chlamydosaurus kingi': 43, 'Christmas stocking': 496, 'Ciconia ciconia': 127, 'Ciconia nigra': 128, 'Constrictor constrictor': 61, 'Crock Pot': 521, 'Crocodylus niloticus': 49, 'Crotalus adamanteus': 67, 'Crotalus cerastes': 68, 'Cuon alpinus': 274, 'Cygnus atratus': 100, 'Cypripedium calceolus': 986, 'Cypripedium parviflorum': 986, 'Danaus plexippus': 323, 'Dandie Dinmont': 194, 'Dandie Dinmont terrier': 194, 'Dermochelys coriacea': 34, 'Doberman': 236, 'Doberman pinscher': 236, 'Dugong dugon': 149, 'Dungeness crab': 118, 'Dutch oven': 544, 'Egretta albus': 132, 'Egretta caerulea': 131, 'Egyptian cat': 285, 'Elephas maximus': 385, 'English cocker spaniel': 219, 'English foxhound': 167, 'English setter': 212, 'English springer': 217, 'English springer spaniel': 217, 'EntleBucher': 241, 'Erolia alpina': 140, 'Erythrocebus patas': 371, 'Eschrichtius gibbosus': 147, 'Eschrichtius robustus': 147, 'Eskimo dog': 248, 'Euarctos americanus': 295, 'European fire salamander': 25, 'European gallinule': 136, 'Felis concolor': 286, 'Felis onca': 290, 'French bulldog': 245, 'French horn': 566, 'French loaf': 930, 'Fringilla montifringilla': 10, 'Fulica americana': 137, 'Galeocerdo cuvieri': 3, 'German police dog': 235, 'German shepherd': 235, 'German shepherd dog': 235, 'German short-haired pointer': 210, 'Gila monster': 45, 'Gordon setter': 214, 'Gorilla gorilla': 366, 'Granny Smith': 948, 'Great Dane': 246, 'Great Pyrenees': 257, 'Greater Swiss Mountain dog': 238, 'Grifola frondosa': 996, 'Haliaeetus leucocephalus': 22, 'Heloderma suspectum': 45, 'Hippopotamus amphibius': 344, 'Holocanthus tricolor': 392, 'Homarus americanus': 122, 'Hungarian pointer': 211, 'Hylobates lar': 368, 'Hylobates syndactylus': 369, 'Hypsiglena torquata': 60, 'Ibizan Podenco': 173, 'Ibizan hound': 173, 'Iguana iguana': 39, 'Indian cobra': 63, 'Indian elephant': 385, 'Indri brevicaudatus': 384, 'Indri indri': 384, 'Irish setter': 213, 'Irish terrier': 184, 'Irish water spaniel': 221, 'Irish wolfhound': 170, 'Italian greyhound': 171, 'Japanese spaniel': 152, 'Kakatoe galerita': 89, 'Kerry blue terrier': 183, 'Komodo dragon': 48, 'Komodo lizard': 48, 'Labrador retriever': 208, 'Lacerta viridis': 46, 'Lakeland terrier': 189, 'Latrodectus mactans': 75, 'Lemur catta': 383, 'Leonberg': 255, 'Lepisosteus osseus': 395, 'Lhasa': 204, 'Lhasa apso': 204, 'Loafer': 630, 'Loxodonta africana': 386, 'Lycaon pictus': 275, 'Madagascar cat': 383, 'Maine lobster': 122, 'Maltese': 153, 'Maltese dog': 153, 'Maltese terrier': 153, 'Melursus ursinus': 297, 'Mergus serrator': 98, 'Mexican hairless': 268, 'Model T': 661, 'Mustela nigripes': 359, 'Mustela putorius': 358, 'Naja naja': 63, 'Nasalis larvatus': 376, 'Newfoundland': 256, 'Newfoundland dog': 256, 'Nile crocodile': 49, 'Norfolk terrier': 185, 'Northern lobster': 122, 'Norwegian elkhound': 174, 'Norwich terrier': 186, 'Old English sheepdog': 229, 'Oncorhynchus kisutch': 391, 'Orcinus orca': 148, 'Ornithorhynchus anatinus': 103, 'Ovis canadensis': 349, 'Pan troglodytes': 367, 'Panthera leo': 291, 'Panthera onca': 290, 'Panthera pardus': 288, 'Panthera tigris': 292, 'Panthera uncia': 289, 'Paralithodes camtschatica': 121, 'Passerina cyanea': 14, 'Peke': 154, 'Pekinese': 154, 'Pekingese': 154, 'Pembroke': 263, 'Pembroke Welsh corgi': 263, 'Persian cat': 283, 'Petri dish': 712, 'Phalangium opilio': 70, 'Phascolarctos cinereus': 105, 'Polaroid Land camera': 732, 'Polaroid camera': 732, 'Polyporus frondosus': 996, 'Pomeranian': 259, 'Pongo pygmaeus': 365, 'Porphyrio porphyrio': 136, 'Psittacus erithacus': 87, 'Python sebae': 62, 'R.V.': 757, 'RV': 757, 'Rana catesbeiana': 30, 'Rhodesian ridgeback': 159, 'Rocky Mountain bighorn': 349, 'Rocky Mountain sheep': 349, 'Rottweiler': 234, 'Russian wolfhound': 169, 'Saimiri sciureus': 382, 'Saint Bernard': 247, 'Salamandra salamandra': 25, 'Saluki': 176, 'Samoyed': 258, 'Samoyede': 258, 'Sciurus niger': 335, 'Scotch terrier': 199, 'Scottie': 199, 'Scottish deerhound': 177, 'Scottish terrier': 199, 'Sealyham': 190, 'Sealyham terrier': 190, 'Shetland': 230, 'Shetland sheep dog': 230, 'Shetland sheepdog': 230, 'Shih-Tzu': 155, 'Siamese': 284, 'Siamese cat': 284, 'Siberian husky': 250, 'St Bernard': 247, 'Staffordshire bull terrier': 179, 'Staffordshire bullterrier': 179, 'Staffordshire terrier': 180, 'Strix nebulosa': 24, 'Struthio camelus': 9, 'Sus scrofa': 342, 'Sussex spaniel': 220, 'Sydney silky': 201, 'Symphalangus syndactylus': 369, 'T-shirt': 610, 'Thalarctos maritimus': 296, 'Tibetan mastiff': 244, 'Tibetan terrier': 200, 'Tinca tinca': 0, 'Tringa totanus': 141, 'Triturus vulgaris': 26, 'Turdus migratorius': 15, 'U-boat': 833, 'Urocyon cinereoargenteus': 280, 'Ursus Maritimus': 296, 'Ursus americanus': 295, 'Ursus arctos': 294, 'Ursus ursinus': 297, 'Varanus komodoensis': 48, 'Virginia fence': 912, 'Vulpes macrotis': 278, 'Vulpes vulpes': 277, 'Walker foxhound': 166, 'Walker hound': 166, 'Weimaraner': 178, 'Welsh springer spaniel': 218, 'West Highland white terrier': 203, 'Windsor tie': 906, 'Yorkshire terrier': 187, 'abacus': 398, 'abaya': 399, 'academic gown': 400, 'academic robe': 400, 'accordion': 401, 'acorn': 988, 'acorn squash': 941, 'acoustic guitar': 402, 'admiral': 321, 'aegis': 461, 'affenpinscher': 252, 'agama': 42, 'agaric': 992, 'ai': 364, 'aircraft carrier': 403, 'airliner': 404, 'airship': 405, 'albatross': 146, 'all-terrain bike': 671, 'alligator lizard': 44, 'alp': 970, 'alsatian': 235, 'altar': 406, 'ambulance': 407, 'amphibian': 408, 'amphibious vehicle': 408, 'analog clock': 409, 'ananas': 953, 'anemone': 108, 'anemone fish': 393, 'anole': 40, 'ant': 310, 'anteater': 102, 'apiary': 410, 'apron': 411, 'armadillo': 363, 'armored combat vehicle': 847, 'armoured combat vehicle': 847, 'army tank': 847, 'artichoke': 944, 'articulated lorry': 867, 'ash bin': 412, 'ash-bin': 412, 'ashbin': 412, 'ashcan': 412, 'assault gun': 413, 'assault rifle': 413, 'attack aircraft carrier': 403, 'automated teller': 480, 'automated teller machine': 480, 'automatic teller': 480, 'automatic teller machine': 480, 'automatic washer': 897, 'axolotl': 29, 'baboon': 372, 'back pack': 414, 'backpack': 414, 'badger': 362, 'bagel': 931, 'bakehouse': 415, 'bakery': 415, 'bakeshop': 415, 'balance beam': 416, 'bald eagle': 22, 'balloon': 417, 'ballpen': 418, 'ballplayer': 981, 'ballpoint': 418, 'ballpoint pen': 418, 'balusters': 421, 'balustrade': 421, 'banana': 954, 'bandeau': 459, 'banded gecko': 38, 'banister': 421, 'banjo': 420, 'bannister': 421, 'barbell': 422, 'barber chair': 423, 'barbershop': 424, 'barn': 425, 'barn spider': 73, 'barometer': 426, 'barracouta': 389, 'barrel': 427, 'barrow': 428, 'bars': 702, 'baseball': 429, 'baseball player': 981, 'basenji': 253, 'basketball': 430, 'basset': 161, 'basset hound': 161, 'bassinet': 431, 'bassoon': 432, 'bath': 435, 'bath towel': 434, 'bathing cap': 433, 'bathing trunks': 842, 'bathing tub': 435, 'bathroom tissue': 999, 'bathtub': 435, 'beach waggon': 436, 'beach wagon': 436, 'beacon': 437, 'beacon light': 437, 'beagle': 162, 'beaker': 438, 'beam': 416, 'bear cat': 387, 'bearskin': 439, 'beaver': 337, 'bee': 309, 'bee eater': 92, 'bee house': 410, 'beer bottle': 440, 'beer glass': 441, 'beigel': 931, 'bell': 494, 'bell cot': 442, 'bell cote': 442, 'bell pepper': 945, 'bell toad': 32, 'bib': 443, 'bicycle-built-for-two': 444, 'bighorn': 349, 'bighorn sheep': 349, 'bikini': 445, 'billfish': 395, 'billfold': 893, 'billiard table': 736, 'binder': 446, 'binoculars': 447, 'birdhouse': 448, 'bison': 347, 'bittern': 133, 'black Maria': 734, 'black and gold garden spider': 72, 'black bear': 295, 'black grouse': 80, 'black stork': 128, 'black swan': 100, 'black widow': 75, 'black-and-tan coonhound': 165, 'black-footed ferret': 359, 'bloodhound': 163, 'blow drier': 589, 'blow dryer': 589, 'blower': 545, 'blowfish': 397, 'blue jack': 391, 'blue jean': 608, 'bluetick': 164, 'boa': 552, 'boa constrictor': 61, 'boar': 342, 'board': 532, 'boat paddle': 693, 'boathouse': 449, 'bob': 450, 'bobsled': 450, 'bobsleigh': 450, 'bobtail': 229, 'bola': 451, 'bola tie': 451, 'bolete': 997, 'bolo': 451, 'bolo tie': 451, 'bonnet': 452, 'book jacket': 921, 'bookcase': 453, 'bookshop': 454, 'bookstall': 454, 'bookstore': 454, 'borzoi': 169, 'bottle screw': 512, 'bottlecap': 455, 'bow': 456, 'bow tie': 457, 'bow-tie': 457, 'bowtie': 457, 'box tortoise': 37, 'box turtle': 37, 'boxer': 242, 'bra': 459, 'brain coral': 109, 'brambling': 10, 'brass': 458, 'brassiere': 459, 'breakwater': 460, 'breastplate': 461, 'briard': 226, 'bridegroom': 982, 'broccoli': 937, 'broom': 462, 'brown bear': 294, 'bruin': 294, 'brush kangaroo': 104, 'brush wolf': 272, 'bubble': 971, 'bucket': 463, 'buckeye': 990, 'buckle': 464, 'buckler': 787, 'bulbul': 16, 'bull mastiff': 243, 'bullet': 466, 'bullet train': 466, 'bulletproof vest': 465, 'bullfrog': 30, 'bulwark': 460, 'burrito': 965, 'busby': 439, 'bustard': 138, 'butcher shop': 467, 'butternut squash': 942, 'cab': 468, 'cabbage butterfly': 324, 'cairn': 192, 'cairn terrier': 192, 'caldron': 469, 'can opener': 473, 'candle': 470, 'candy store': 509, 'cannon': 471, 'canoe': 472, 'capitulum': 998, 'capuchin': 378, 'car mirror': 475, 'car wheel': 479, 'carabid beetle': 302, 'carbonara': 959, 'cardigan': 474, 'cardoon': 946, 'carousel': 476, \"carpenter's kit\": 477, \"carpenter's plane\": 726, 'carriage': 705, 'carriage dog': 251, 'carrier': 403, 'carrion fungus': 994, 'carrousel': 476, 'carton': 478, 'cash dispenser': 480, 'cash machine': 480, 'cask': 427, 'cassette': 481, 'cassette player': 482, 'castle': 483, 'cat bear': 387, 'catamaran': 484, 'catamount': 287, 'cathode-ray oscilloscope': 688, 'cauldron': 469, 'cauliflower': 938, 'cell': 487, 'cello': 486, 'cellphone': 487, 'cellular phone': 487, 'cellular telephone': 487, 'centipede': 79, 'cerastes': 66, 'chain': 488, 'chain armor': 490, 'chain armour': 490, 'chain mail': 490, 'chain saw': 491, 'chainlink fence': 489, 'chainsaw': 491, 'chambered nautilus': 117, 'cheeseburger': 933, 'cheetah': 293, 'chest': 492, 'chetah': 293, 'chickadee': 19, 'chiffonier': 493, 'chime': 494, 'chimp': 367, 'chimpanzee': 367, 'china cabinet': 495, 'china closet': 495, 'chiton': 116, 'chocolate sauce': 960, 'chocolate syrup': 960, 'chopper': 499, 'chow': 260, 'chow chow': 260, 'chrysanthemum dog': 200, 'chrysomelid': 304, 'church': 497, 'church building': 497, 'chute': 701, 'cicada': 316, 'cicala': 316, 'cimarron': 349, 'cinema': 498, 'claw': 600, 'cleaver': 499, 'cliff': 972, 'cliff dwelling': 500, 'cloak': 501, 'clog': 502, 'closet': 894, 'clumber': 216, 'clumber spaniel': 216, 'coach': 705, 'coach dog': 251, 'coast': 978, 'coat-of-mail shell': 116, 'cock': 7, 'cocker': 219, 'cocker spaniel': 219, 'cockroach': 314, 'cocktail shaker': 503, 'coffee mug': 504, 'coffeepot': 505, 'coho': 391, 'coho salmon': 391, 'cohoe': 391, 'coil': 506, 'collie': 231, 'colobus': 375, 'colobus monkey': 375, 'combination lock': 507, 'comfort': 750, 'comforter': 750, 'comic book': 917, 'commode': 493, 'common iguana': 39, 'common newt': 26, 'computer keyboard': 508, 'computer mouse': 673, 'conch': 112, 'confectionary': 509, 'confectionery': 509, 'conker': 990, 'consomme': 925, 'container ship': 510, 'container vessel': 510, 'containership': 510, 'convertible': 511, 'coon bear': 388, 'coral fungus': 991, 'coral reef': 973, 'corkscrew': 512, 'corn': 987, 'cornet': 513, 'cot': 520, 'cottontail': 330, 'cottontail rabbit': 330, 'coucal': 91, 'cougar': 286, 'courgette': 939, 'cowboy boot': 514, 'cowboy hat': 515, 'coyote': 272, 'cradle': 516, 'crampfish': 5, 'crane': 517, 'crash helmet': 518, 'crate': 519, 'crawdad': 124, 'crawdaddy': 124, 'crawfish': 124, 'crayfish': 124, 'crib': 520, 'cricket': 312, 'crinoline': 601, 'croquet ball': 522, 'crossword': 918, 'crossword puzzle': 918, 'crutch': 523, 'cucumber': 943, 'cuirass': 524, 'cuke': 943, 'cup': 968, 'curly-coated retriever': 206, 'custard apple': 956, 'daddy longlegs': 70, 'daisy': 985, 'dalmatian': 251, 'dam': 525, 'damselfly': 320, 'dark glasses': 837, 'darning needle': 319, 'day bed': 831, 'deerhound': 177, 'denim': 608, 'desk': 526, 'desktop computer': 527, \"devil's darning needle\": 319, 'devilfish': 147, 'dhole': 274, 'dial phone': 528, 'dial telephone': 528, 'diamondback': 67, 'diamondback rattlesnake': 67, 'diaper': 529, 'digital clock': 530, 'digital watch': 531, 'dike': 525, 'dingo': 273, 'dining table': 532, 'dipper': 20, 'dirigible': 405, 'disc brake': 535, 'dish washer': 534, 'dishcloth': 533, 'dishrag': 533, 'dishwasher': 534, 'dishwashing machine': 534, 'disk brake': 535, 'dock': 536, 'dockage': 536, 'docking facility': 536, 'dog sled': 537, 'dog sleigh': 537, 'dogsled': 537, 'dome': 538, 'doormat': 539, 'dough': 961, 'dowitcher': 142, 'dragon lizard': 48, 'dragonfly': 319, 'drake': 97, 'drilling platform': 540, 'dromedary': 354, 'drop': 972, 'drop-off': 972, 'drum': 541, 'drumstick': 542, 'duck-billed platypus': 103, 'duckbill': 103, 'duckbilled platypus': 103, 'dugong': 149, 'dumbbell': 543, 'dung beetle': 305, 'dunlin': 140, 'dust cover': 921, 'dust jacket': 921, 'dust wrapper': 921, 'dustbin': 412, 'dustcart': 569, 'dyke': 525, 'ear': 998, 'earthstar': 995, 'eastern fox squirrel': 335, 'eatery': 762, 'eating house': 762, 'eating place': 762, 'echidna': 102, 'eel': 390, 'eft': 27, 'eggnog': 969, 'egis': 461, 'electric fan': 545, 'electric guitar': 546, 'electric locomotive': 547, 'electric ray': 5, 'electric switch': 844, 'electrical switch': 844, 'elkhound': 174, 'emmet': 310, 'entertainment center': 548, 'envelope': 549, 'espresso': 967, 'espresso maker': 550, 'essence': 711, 'estate car': 436, 'ewer': 725, 'face powder': 551, 'feather boa': 552, 'ferret': 359, 'fiddle': 889, 'fiddler crab': 120, 'field glasses': 447, 'fig': 952, 'file': 553, 'file cabinet': 553, 'filing cabinet': 553, 'fire engine': 555, 'fire screen': 556, 'fire truck': 555, 'fireboat': 554, 'fireguard': 556, 'fitch': 358, 'fixed disk': 592, 'flagpole': 557, 'flagstaff': 557, 'flamingo': 130, 'flat-coated retriever': 205, 'flattop': 403, 'flatworm': 110, 'flowerpot': 738, 'flute': 558, 'fly': 308, 'folding chair': 559, 'food market': 582, 'football helmet': 560, 'footstall': 708, 'foreland': 976, 'forklift': 561, 'foulmart': 358, 'foumart': 358, 'fountain': 562, 'fountain pen': 563, 'four-poster': 564, 'fox squirrel': 335, 'freight car': 565, 'frilled lizard': 43, 'frying pan': 567, 'frypan': 567, 'fur coat': 568, 'gar': 395, 'garbage can': 412, 'garbage truck': 569, 'garden cart': 428, 'garden spider': 74, 'garfish': 395, 'garpike': 395, 'garter snake': 57, 'gas helmet': 570, 'gas pump': 571, 'gasmask': 570, 'gasoline pump': 571, 'gazelle': 353, 'gazelle hound': 176, 'geta': 502, 'geyser': 974, 'giant lizard': 48, 'giant panda': 388, 'giant schnauzer': 197, 'gibbon': 368, 'glasshouse': 580, 'globe artichoke': 944, 'globefish': 397, 'go-kart': 573, 'goblet': 572, 'golden retriever': 207, 'goldfinch': 11, 'goldfish': 1, 'golf ball': 574, 'golf cart': 575, 'golfcart': 575, 'gondola': 576, 'gong': 577, 'goose': 99, 'gorilla': 366, 'gown': 578, 'grampus': 148, 'grand': 579, 'grand piano': 579, 'grass snake': 57, 'grasshopper': 311, 'gray fox': 280, 'gray whale': 147, 'gray wolf': 269, 'great gray owl': 24, 'great grey owl': 24, 'great white heron': 132, 'great white shark': 2, 'green lizard': 46, 'green mamba': 64, 'green snake': 55, 'greenhouse': 580, 'grey fox': 280, 'grey whale': 147, 'grey wolf': 269, 'grille': 581, 'grocery': 582, 'grocery store': 582, 'groenendael': 224, 'groin': 460, 'groom': 982, 'ground beetle': 302, 'groyne': 460, 'grunter': 341, 'guacamole': 924, 'guenon': 370, 'guenon monkey': 370, 'guillotine': 583, 'guinea pig': 338, 'gyromitra': 993, 'hack': 468, 'hair drier': 589, 'hair dryer': 589, 'hair slide': 584, 'hair spray': 585, 'half track': 586, 'hammer': 587, 'hammerhead': 4, 'hammerhead shark': 4, 'hamper': 588, 'hamster': 333, 'hand blower': 589, 'hand-held computer': 590, 'hand-held microcomputer': 590, 'handbasin': 896, 'handkerchief': 591, 'handrail': 421, 'hankey': 591, 'hankie': 591, 'hanky': 591, 'hard disc': 592, 'hard disk': 592, 'hare': 331, 'harmonica': 593, 'harp': 594, 'hartebeest': 351, 'harvester': 595, 'harvestman': 70, 'hatchet': 596, 'hautbois': 683, 'hautboy': 683, 'haversack': 414, 'hay': 958, 'head': 976, 'head cabbage': 936, 'headland': 976, 'hedgehog': 334, 'helix': 506, 'hen': 8, 'hen of the woods': 996, 'hen-of-the-woods': 996, 'hermit crab': 125, 'high bar': 602, 'hip': 989, 'hippo': 344, 'hippopotamus': 344, 'hockey puck': 746, 'hodometer': 685, 'hog': 341, 'hognose snake': 54, 'holothurian': 329, 'holster': 597, 'home theater': 598, 'home theatre': 598, 'honeycomb': 599, 'hook': 600, 'hoopskirt': 601, 'hopper': 311, 'horizontal bar': 602, 'horn': 566, 'hornbill': 93, 'horned asp': 66, 'horned rattlesnake': 68, 'horned viper': 66, 'horse cart': 603, 'horse chestnut': 990, 'horse-cart': 603, 'hot dog': 934, 'hot pot': 926, 'hotdog': 934, 'hotpot': 926, 'hourglass': 604, 'house finch': 12, 'howler': 379, 'howler monkey': 379, 'hummingbird': 94, 'hunting spider': 77, 'husky': 248, 'hussar monkey': 371, 'hyaena': 276, 'hyena': 276, 'hyena dog': 275, 'iPod': 605, 'ibex': 350, 'ice bear': 296, 'ice cream': 928, 'ice lolly': 929, 'icebox': 760, 'icecream': 928, 'igniter': 626, 'ignitor': 626, 'iguana': 39, 'impala': 352, 'indigo bird': 14, 'indigo bunting': 14, 'indigo finch': 14, 'indri': 384, 'indris': 384, 'internet site': 916, 'iron': 606, 'island dispenser': 571, 'isopod': 126, 'jacamar': 95, 'jack': 955, \"jack-o'-lantern\": 607, 'jackfruit': 955, 'jaguar': 290, 'jak': 955, 'jammies': 697, 'jay': 17, 'jean': 608, 'jeep': 609, 'jellyfish': 107, 'jersey': 610, 'jetty': 460, \"jeweler's loupe\": 633, 'jigsaw puzzle': 611, 'jinrikisha': 612, 'joystick': 613, \"judge's robe\": 400, 'junco': 13, 'kangaroo bear': 105, 'keeshond': 261, 'kelpie': 227, 'keypad': 508, 'killer': 148, 'killer whale': 148, 'kimono': 614, 'king crab': 121, 'king of beasts': 291, 'king penguin': 145, 'king snake': 56, 'kingsnake': 56, 'kit fox': 278, 'kite': 21, 'knapsack': 414, 'knee pad': 615, 'knot': 616, 'koala': 105, 'koala bear': 105, 'komondor': 228, 'kuvasz': 222, 'lab coat': 617, 'laboratory coat': 617, 'labyrinth': 646, 'lacewing': 318, 'lacewing fly': 318, 'ladle': 618, 'lady beetle': 301, 'ladybeetle': 301, 'ladybird': 301, 'ladybird beetle': 301, 'ladybug': 301, 'lakeshore': 975, 'lakeside': 975, 'lamp shade': 619, 'lampshade': 619, 'landrover': 609, 'langouste': 123, 'langur': 374, 'laptop': 620, 'laptop computer': 620, 'lavabo': 896, 'lawn cart': 428, 'lawn mower': 621, 'leaf beetle': 304, 'leafhopper': 317, 'leatherback': 34, 'leatherback turtle': 34, 'leathery turtle': 34, 'lemon': 951, 'lens cap': 622, 'lens cover': 622, 'leopard': 288, 'lesser panda': 387, 'letter box': 637, 'letter opener': 623, 'library': 624, 'lifeboat': 625, 'light': 626, 'lighter': 626, 'lighthouse': 437, 'limo': 627, 'limousine': 627, 'limpkin': 135, 'liner': 628, 'linnet': 12, 'lion': 291, 'lionfish': 396, 'lip rouge': 629, 'lipstick': 629, 'little blue heron': 131, 'llama': 355, 'loggerhead': 33, 'loggerhead turtle': 33, 'lollipop': 929, 'lolly': 929, 'long-horned beetle': 303, 'longicorn': 303, 'longicorn beetle': 303, 'lorikeet': 90, 'lotion': 631, 'loudspeaker': 632, 'loudspeaker system': 632, 'loupe': 633, 'lumbermill': 634, 'lycaenid': 326, 'lycaenid butterfly': 326, 'lynx': 287, 'macaque': 373, 'macaw': 88, 'magnetic compass': 635, 'magpie': 18, 'mail': 490, 'mailbag': 636, 'mailbox': 637, 'maillot': 639, 'malamute': 249, 'malemute': 249, 'malinois': 225, 'man-eater': 2, 'man-eating shark': 2, 'maned wolf': 271, 'manhole cover': 640, 'mantid': 315, 'mantis': 315, 'manufactured home': 660, 'maraca': 641, 'marimba': 642, 'market': 582, 'marmoset': 377, 'marmot': 336, 'marsh hen': 137, 'mashed potato': 935, 'mask': 643, 'matchstick': 644, 'maypole': 645, 'maze': 646, 'measuring cup': 647, 'meat cleaver': 499, 'meat loaf': 962, 'meat market': 467, 'meatloaf': 962, 'medicine cabinet': 648, 'medicine chest': 648, 'meerkat': 299, 'megalith': 649, 'megalithic structure': 649, 'membranophone': 541, 'memorial tablet': 458, 'menu': 922, 'merry-go-round': 476, 'microphone': 650, 'microwave': 651, 'microwave oven': 651, 'mierkat': 299, 'mike': 650, 'mileometer': 685, 'military plane': 895, 'military uniform': 652, 'milk can': 653, 'milkweed butterfly': 323, 'milometer': 685, 'mini': 655, 'miniature pinscher': 237, 'miniature poodle': 266, 'miniature schnauzer': 196, 'minibus': 654, 'miniskirt': 655, 'minivan': 656, 'mink': 357, 'missile': 744, 'mitten': 658, 'mixing bowl': 659, 'mobile home': 660, 'mobile phone': 487, 'modem': 662, 'mole': 460, 'mollymawk': 146, 'monarch': 323, 'monarch butterfly': 323, 'monastery': 663, 'mongoose': 298, 'monitor': 664, 'monkey dog': 252, 'monkey pinscher': 252, 'monocycle': 880, 'mop': 840, 'moped': 665, 'mortar': 666, 'mortarboard': 667, 'mosque': 668, 'mosquito hawk': 319, 'mosquito net': 669, 'motor scooter': 670, 'mountain bike': 671, 'mountain lion': 286, 'mountain tent': 672, 'mouse': 673, 'mousetrap': 674, 'mouth harp': 593, 'mouth organ': 593, 'movie house': 498, 'movie theater': 498, 'movie theatre': 498, 'moving van': 675, 'mower': 621, 'mud hen': 137, 'mud puppy': 29, 'mud turtle': 35, 'mushroom': 947, 'muzzle': 676, 'nail': 677, 'napkin': 529, 'nappy': 529, 'native bear': 105, 'nautilus': 117, 'neck brace': 678, 'necklace': 679, 'nematode': 111, 'nematode worm': 111, 'night snake': 60, 'nipple': 680, 'notebook': 681, 'notebook computer': 681, 'notecase': 893, 'nudibranch': 115, 'numbfish': 5, 'nursery': 580, 'obelisk': 682, 'oboe': 683, 'ocarina': 684, 'ocean liner': 628, 'odometer': 685, 'off-roader': 671, 'offshore rig': 540, 'oil filter': 686, 'one-armed bandit': 800, 'opera glasses': 447, 'orang': 365, 'orange': 950, 'orangutan': 365, 'orangutang': 365, 'orca': 148, 'organ': 687, 'oscilloscope': 688, 'ostrich': 9, 'otter': 360, 'otter hound': 175, 'otterhound': 175, 'ounce': 289, 'overskirt': 689, 'ox': 345, 'oxcart': 690, 'oxygen mask': 691, 'oyster catcher': 143, 'oystercatcher': 143, 'packet': 692, 'packsack': 414, 'paddle': 693, 'paddle wheel': 694, 'paddlewheel': 694, 'paddy wagon': 734, 'padlock': 695, 'pail': 463, 'paintbrush': 696, 'painter': 286, 'pajama': 697, 'palace': 698, 'paling': 716, 'panda': 388, 'panda bear': 388, 'pandean pipe': 699, 'panpipe': 699, 'panther': 290, 'paper knife': 623, 'paper towel': 700, 'paperknife': 623, 'papillon': 157, 'parachute': 701, 'parallel bars': 702, 'park bench': 703, 'parking meter': 704, 'partridge': 86, 'passenger car': 705, 'patas': 371, 'patio': 706, 'patrol wagon': 734, 'patten': 502, 'pay-phone': 707, 'pay-station': 707, 'peacock': 84, 'pearly nautilus': 117, 'pedestal': 708, 'pelican': 144, 'pencil box': 709, 'pencil case': 709, 'pencil eraser': 767, 'pencil sharpener': 710, 'penny bank': 719, 'perfume': 711, 'petrol pump': 571, 'pharos': 437, 'photocopier': 713, 'piano accordion': 401, 'pick': 714, 'pickelhaube': 715, 'picket fence': 716, 'pickup': 717, 'pickup truck': 717, 'picture palace': 498, 'pier': 718, 'pig': 341, 'pigboat': 833, 'piggy bank': 719, 'pill bottle': 720, 'pillow': 721, 'pineapple': 953, 'ping-pong ball': 722, 'pinwheel': 723, 'pipe organ': 687, 'pirate': 724, 'pirate ship': 724, 'pismire': 310, 'pit bull terrier': 180, 'pitcher': 725, 'pizza': 963, 'pizza pie': 963, \"pj's\": 697, 'plane': 726, 'planetarium': 727, 'plaque': 458, 'plastic bag': 728, 'plate': 923, 'plate rack': 729, 'platyhelminth': 110, 'platypus': 103, 'plectron': 714, 'plectrum': 714, 'plinth': 708, 'plough': 730, 'plow': 730, \"plumber's helper\": 731, 'plunger': 731, 'pocketbook': 893, 'poke bonnet': 452, 'polar bear': 296, 'pole': 733, 'polecat': 361, 'police van': 734, 'police wagon': 734, 'polyplacophore': 116, 'pomegranate': 957, 'poncho': 735, 'pool table': 736, 'pop bottle': 737, 'popsicle': 929, 'porcupine': 334, 'postbag': 636, 'pot': 738, 'potpie': 964, \"potter's wheel\": 739, 'power drill': 740, 'prairie chicken': 83, 'prairie fowl': 83, 'prairie grouse': 83, 'prairie wolf': 272, 'prayer mat': 741, 'prayer rug': 741, 'press': 894, 'pretzel': 932, 'printer': 742, 'prison': 743, 'prison house': 743, 'proboscis monkey': 376, 'projectile': 744, 'projector': 745, 'promontory': 976, 'ptarmigan': 81, 'puck': 746, 'puff': 750, 'puff adder': 54, 'puffer': 397, 'pufferfish': 397, 'pug': 254, 'pug-dog': 254, 'puma': 286, 'punch bag': 747, 'punchball': 747, 'punching bag': 747, 'punching ball': 747, 'purse': 748, 'pyjama': 697, 'quail': 85, 'quill': 749, 'quill pen': 749, 'quilt': 750, 'race car': 751, 'racer': 751, 'racing car': 751, 'racket': 752, 'racquet': 752, 'radiator': 753, 'radiator grille': 581, 'radio': 754, 'radio reflector': 755, 'radio telescope': 755, 'rain barrel': 756, 'ram': 348, 'rapeseed': 984, 'reaper': 595, 'recreational vehicle': 757, 'red fox': 277, 'red hot': 934, 'red panda': 387, 'red setter': 213, 'red wine': 966, 'red wolf': 271, 'red-backed sandpiper': 140, 'red-breasted merganser': 98, 'redbone': 168, 'redshank': 141, 'reel': 758, 'reflex camera': 759, 'refrigerator': 760, 'remote': 761, 'remote control': 761, 'respirator': 570, 'restaurant': 762, 'revolver': 763, 'rhinoceros beetle': 306, 'ribbed toad': 32, 'ricksha': 612, 'rickshaw': 612, 'rifle': 764, 'rig': 867, 'ring armor': 490, 'ring armour': 490, 'ring mail': 490, 'ring snake': 53, 'ring-binder': 446, 'ring-necked snake': 53, 'ring-tailed lemur': 383, 'ringlet': 322, 'ringlet butterfly': 322, 'ringneck snake': 53, 'ringtail': 378, 'river horse': 344, 'roach': 314, 'robin': 15, 'rock beauty': 392, 'rock crab': 119, 'rock lobster': 123, 'rock python': 62, 'rock snake': 62, 'rocker': 765, 'rocking chair': 765, 'rose hip': 989, 'rosehip': 989, 'rotisserie': 766, 'roundabout': 476, 'roundworm': 111, 'rubber': 767, 'rubber eraser': 767, 'rucksack': 414, 'ruddy turnstone': 139, 'ruffed grouse': 82, 'rugby ball': 768, 'rule': 769, 'ruler': 769, 'running shoe': 770, 'sabot': 502, 'safe': 771, 'safety pin': 772, 'salt shaker': 773, 'saltshaker': 773, 'sand bar': 977, 'sand viper': 66, 'sandal': 774, 'sandbar': 977, 'sarong': 775, 'sawmill': 634, 'sax': 776, 'saxophone': 776, 'scabbard': 777, 'scale': 778, 'schipperke': 223, 'school bus': 779, 'schooner': 780, 'scooter': 670, 'scope': 688, 'scoreboard': 781, 'scorpion': 71, 'screen': 782, 'screw': 783, 'screwdriver': 784, 'scuba diver': 983, 'sea anemone': 108, 'sea cradle': 116, 'sea crawfish': 123, 'sea cucumber': 329, 'sea lion': 150, 'sea slug': 115, 'sea snake': 65, 'sea star': 327, 'sea urchin': 328, 'sea wolf': 148, 'sea-coast': 978, 'seacoast': 978, 'seashore': 978, 'seat belt': 785, 'seatbelt': 785, 'seawall': 460, 'semi': 867, 'sewing machine': 786, 'sewing needle': 319, 'shades': 837, 'shako': 439, 'shield': 787, 'shoe shop': 788, 'shoe store': 788, 'shoe-shop': 788, 'shoji': 789, 'shopping basket': 790, 'shopping cart': 791, 'shovel': 792, 'shower cap': 793, 'shower curtain': 794, 'siamang': 369, 'sidewinder': 68, 'silky terrier': 201, 'silver salmon': 391, 'site': 916, 'six-gun': 763, 'six-shooter': 763, 'skeeter hawk': 319, 'ski': 795, 'ski mask': 796, 'skillet': 567, 'skunk': 361, 'sleeping bag': 797, 'sleuthhound': 163, 'slide rule': 798, 'sliding door': 799, 'slipstick': 798, 'slot': 800, 'sloth bear': 297, 'slug': 114, 'smoothing iron': 606, 'snail': 113, 'snake doctor': 319, 'snake feeder': 319, 'snake fence': 912, 'snake-rail fence': 912, 'snoek': 389, 'snooker table': 736, 'snorkel': 801, 'snow leopard': 289, 'snowbird': 13, 'snowmobile': 802, 'snowplough': 803, 'snowplow': 803, 'soap dispenser': 804, 'soccer ball': 805, 'sock': 806, 'soda bottle': 737, 'soft-coated wheaten terrier': 202, 'solar collector': 807, 'solar dish': 807, 'solar furnace': 807, 'sombrero': 808, 'sorrel': 339, 'soup bowl': 809, 'space bar': 810, 'space heater': 811, 'space shuttle': 812, 'spaghetti squash': 940, 'spatula': 813, 'speaker': 632, 'speaker system': 632, 'speaker unit': 632, 'speedboat': 814, 'spider monkey': 381, 'spider web': 815, \"spider's web\": 815, 'spike': 998, 'spindle': 816, 'spiny anteater': 102, 'spiny lobster': 123, 'spiral': 506, 'spoonbill': 129, 'sport car': 817, 'sports car': 817, 'spot': 818, 'spotlight': 818, 'spotted salamander': 28, 'squealer': 341, 'squeeze box': 401, 'squirrel monkey': 382, 'stage': 819, 'standard poodle': 267, 'standard schnauzer': 198, 'starfish': 327, 'station waggon': 436, 'station wagon': 436, 'steam locomotive': 820, 'steel arch bridge': 821, 'steel drum': 822, 'stethoscope': 823, 'stick insect': 313, 'stingray': 6, 'stinkhorn': 994, 'stole': 824, 'stone wall': 825, 'stop watch': 826, 'stoplight': 920, 'stopwatch': 826, 'stove': 827, 'strainer': 828, 'strawberry': 949, 'street sign': 919, 'streetcar': 829, 'stretcher': 830, 'studio couch': 831, 'stupa': 832, 'sturgeon': 394, 'sub': 833, 'submarine': 833, 'suit': 834, 'suit of clothes': 834, 'sulfur butterfly': 325, 'sulphur butterfly': 325, 'sulphur-crested cockatoo': 89, 'sun blocker': 838, 'sunblock': 838, 'sundial': 835, 'sunglass': 836, 'sunglasses': 837, 'sunscreen': 838, 'suspension bridge': 839, 'swab': 840, 'sweatshirt': 841, 'sweet potato': 684, 'swimming cap': 433, 'swimming trunks': 842, 'swing': 843, 'switch': 844, 'swob': 840, 'syringe': 845, 'syrinx': 699, 'tabby': 281, 'tabby cat': 281, 'table lamp': 846, 'tailed frog': 32, 'tailed toad': 32, 'tam-tam': 577, 'tandem': 444, 'tandem bicycle': 444, 'tank': 847, 'tank suit': 639, 'tape player': 848, 'taper': 470, 'tarantula': 76, 'taxi': 468, 'taxicab': 468, 'teapot': 849, 'teddy': 850, 'teddy bear': 850, 'tee shirt': 610, 'television': 851, 'television system': 851, 'ten-gallon hat': 515, 'tench': 0, 'tennis ball': 852, 'terrace': 706, 'terrapin': 36, 'thatch': 853, 'thatched roof': 853, 'theater curtain': 854, 'theatre curtain': 854, 'thimble': 855, 'thrasher': 856, 'three-toed sloth': 364, 'thresher': 856, 'threshing machine': 856, 'throne': 857, 'thunder snake': 52, 'tick': 78, 'tiger': 292, 'tiger beetle': 300, 'tiger cat': 282, 'tiger shark': 3, 'tile roof': 858, 'timber wolf': 269, 'tin opener': 473, 'titi': 380, 'titi monkey': 380, 'toaster': 859, 'tobacco shop': 860, 'tobacconist': 860, 'tobacconist shop': 860, 'toilet paper': 999, 'toilet seat': 861, 'toilet tissue': 999, 'tool kit': 477, 'tope': 832, 'torch': 862, 'torpedo': 5, 'totem pole': 863, 'toucan': 96, 'tow car': 864, 'tow truck': 864, 'toy poodle': 265, 'toy terrier': 158, 'toyshop': 865, 'trackless trolley': 874, 'tractor': 866, 'tractor trailer': 867, 'traffic light': 920, 'traffic signal': 920, 'trailer truck': 867, 'tram': 829, 'tramcar': 829, 'transverse flute': 558, 'trash barrel': 412, 'trash bin': 412, 'trash can': 412, 'tray': 868, 'tree frog': 31, 'tree-frog': 31, 'trench coat': 869, 'triceratops': 51, 'tricycle': 870, 'trifle': 927, 'trike': 870, 'trilobite': 69, 'trimaran': 871, 'tripod': 872, 'triumphal arch': 873, 'trolley': 829, 'trolley car': 829, 'trolley coach': 874, 'trolleybus': 874, 'trombone': 875, 'trucking rig': 867, 'trump': 513, 'trumpet': 513, 'tub': 876, 'tup': 348, 'turnstile': 877, 'tusker': 101, 'two-piece': 445, 'tympan': 541, 'typewriter keyboard': 878, 'umbrella': 879, 'unicycle': 880, 'upright': 881, 'upright piano': 881, 'vacuum': 882, 'vacuum cleaner': 882, 'vale': 979, 'valley': 979, 'vase': 883, 'vat': 876, 'vault': 884, 'velocipede': 870, 'velvet': 885, 'vending machine': 886, 'vestment': 887, 'viaduct': 888, 'vine snake': 59, 'violin': 889, 'violoncello': 486, 'vizsla': 211, 'volcano': 980, 'volleyball': 890, 'volute': 506, 'vulture': 23, 'waffle iron': 891, 'waggon': 436, 'wagon': 734, 'walking stick': 313, 'walkingstick': 313, 'wall clock': 892, 'wallaby': 104, 'wallet': 893, 'wardrobe': 894, 'warplane': 895, 'warragal': 273, 'warrigal': 273, 'warthog': 343, 'wash-hand basin': 896, 'washbasin': 896, 'washbowl': 896, 'washer': 897, 'washing machine': 897, 'wastebin': 412, 'water bottle': 898, 'water buffalo': 346, 'water hen': 137, 'water jug': 899, 'water ouzel': 20, 'water ox': 346, 'water snake': 58, 'water tower': 900, 'wax light': 470, 'weasel': 356, 'web site': 916, 'website': 916, 'weevil': 307, 'weighing machine': 778, 'welcome mat': 539, 'wheelbarrow': 428, 'whippet': 172, 'whiptail': 41, 'whiptail lizard': 41, 'whirligig': 476, 'whiskey jug': 901, 'whistle': 902, 'white fox': 279, 'white shark': 2, 'white stork': 127, 'white wolf': 270, 'whorl': 506, 'wig': 903, 'wild boar': 342, 'window screen': 904, 'window shade': 905, 'wine bottle': 907, 'wing': 908, 'wire-haired fox terrier': 188, 'wireless': 754, 'wok': 909, 'wolf spider': 77, 'wombat': 106, 'wood pussy': 361, 'wood rabbit': 330, 'wooden spoon': 910, 'woodworking plane': 726, 'wool': 911, 'woolen': 911, 'woollen': 911, 'worm fence': 912, 'worm snake': 52, 'wreck': 913, 'wrecker': 864, 'xylophone': 642, 'yawl': 914, \"yellow lady's slipper\": 986, 'yellow lady-slipper': 986, 'yurt': 915, 'zebra': 340, 'zucchini': 939}\n",
        "DiT_prefs = {\n",
        "    'prompt': '',\n",
        "    'batch_folder_name': '',\n",
        "    'guidance_scale': 4.0,\n",
        "    'num_inference_steps': 50,\n",
        "    'seed': 0,\n",
        "    'num_images': 1,\n",
        "    #'variance_type': 'learned_range',#fixed_small_log\n",
        "    #'num_train_timesteps': 1000,\n",
        "    #'prediction_type': 'epsilon',#sample\n",
        "    #'clip_sample': True,\n",
        "    \"apply_ESRGAN_upscale\": prefs['apply_ESRGAN_upscale'],\n",
        "    \"enlarge_scale\": 4.0,\n",
        "    \"display_upscaled_image\": True,\n",
        "}\n",
        "def buildDiT(page):\n",
        "    global DiT_prefs, prefs, pipe_DiT\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            DiT_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            DiT_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            DiT_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def add_to_DiT_output(o):\n",
        "      page.DiT_output.controls.append(o)\n",
        "      page.DiT_output.update()\n",
        "      if not clear_button.visible:\n",
        "        clear_button.visible = True\n",
        "        clear_button.update()\n",
        "    page.add_to_DiT_output = add_to_DiT_output\n",
        "    def clear_output(e):\n",
        "      if prefs['enable_sounds']: page.snd_delete.play()\n",
        "      page.DiT_output.controls = []\n",
        "      page.DiT_output.update()\n",
        "      clear_button.visible = False\n",
        "      clear_button.update()\n",
        "    def DiT_help(e):\n",
        "      def close_DiT_dlg(e):\n",
        "        nonlocal DiT_help_dlg\n",
        "        DiT_help_dlg.open = False\n",
        "        page.update()\n",
        "      DiT_help_dlg = AlertDialog(title=Text(\"üôÖ   Help with DiT Pipeline\"), content=Column([\n",
        "          Text(\"Provide a comma separated list of general ImageNet Classes to create images. Press Class List to see availble classes, click to copy a token to clipboard then paste in textfield.\"),\n",
        "          Text(\"We explore a new class of diffusion models based on the transformer architecture. We train latent diffusion models of images, replacing the commonly-used U-Net backbone with a transformer that operates on latent patches. We analyze the scalability of our Diffusion Transformers (DiTs) through the lens of forward pass complexity as measured by Gflops. We find that DiTs with higher Gflops -- through increased transformer depth/width or increased number of input tokens -- consistently have lower FID. In addition to possessing good scalability properties, our largest DiT-XL/2 models outperform all prior diffusion models on the class-conditional ImageNet 512x512 and 256x256 benchmarks, achieving a state-of-the-art FID of 2.27 on the latter.\"),\n",
        "          Markdown(\"The DiT model in diffusers comes from  can be found here: [Scalable Diffusion Models with Transformers](https://www.wpeebles.com/DiT) (DiT) and [facebookresearch/dit](https://github.com/facebookresearch/dit)..\", on_tap_link=lambda e: e.page.launch_url(e.data)),\n",
        "        ], scroll=ScrollMode.AUTO), actions=[TextButton(\"üòï  Interesting... \", on_click=close_DiT_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = DiT_help_dlg\n",
        "      DiT_help_dlg.open = True\n",
        "      page.update()\n",
        "    def copy_class(e):\n",
        "      page.set_clipboard(e.control.text)\n",
        "      page.snack_bar = SnackBar(content=Text(f\"üìã   Class {e.control.text} copied to clipboard...\"))\n",
        "      page.snack_bar.open = True\n",
        "      page.update()\n",
        "    def show_classes(e):\n",
        "      classes = []\n",
        "      for c in ImageNet_classes.keys():\n",
        "        #TODO Copy to clipboard on click\n",
        "        classes.append(TextButton(c, col={'sm':4, 'md':3, 'lg':2,}, on_click=copy_class))\n",
        "      alert_msg(page, \"ImageNET Class List\", content=Container(Column([ResponsiveRow(\n",
        "        controls=classes,\n",
        "        expand=True,\n",
        "      )], spacing=0), width=(page.window_width or page.width) - 150), okay=\"üò≤  That's a lot...\", sound=False)\n",
        "    guidance_scale = SliderRow(label=\"Guidance Scale\", min=0, max=50, divisions=100, round=1, pref=DiT_prefs, key='guidance_scale')\n",
        "    def toggle_ESRGAN(e):\n",
        "        ESRGAN_settings.height = None if e.control.value else 0\n",
        "        DiT_prefs['apply_ESRGAN_upscale'] = e.control.value\n",
        "        ESRGAN_settings.update()\n",
        "    prompt = TextField(label=\"ImageNet Class Names (separated by commas)\", value=DiT_prefs['prompt'], on_change=lambda e:changed(e,'prompt'))\n",
        "    seed = TextField(label=\"Seed\", width=90, value=str(DiT_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip=\"0 or -1 picks a Random seed\", on_change=lambda e:changed(e,'seed', ptype='int'))\n",
        "    num_inference_row = SliderRow(label=\"Number of Inference Steps\", min=1, max=100, divisions=99, pref=DiT_prefs, key='num_inference_steps', tooltip=\"The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.\")   \n",
        "    batch_folder_name = TextField(label=\"Batch Folder Name\", value=DiT_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    #eta = TextField(label=\"ETA\", value=str(DiT_prefs['eta']), keyboard_type=KeyboardType.NUMBER, hint_text=\"Amount of Noise\", on_change=lambda e:changed(e,'eta', ptype='float'))\n",
        "    #eta = Slider(min=0.0, max=1.0, divisions=20, label=\"{value}\", value=float(DiT_prefs['eta']), tooltip=\"The weight of noise for added noise in a diffusion step. Its value is between 0.0 and 1.0 - 0.0 is DDIM and 1.0 is DDPM scheduler respectively.\", expand=True, on_change=lambda e:changed(e,'eta', ptype='float'))\n",
        "    #eta_row = Row([Text(\"DDIM ETA: \"), eta])\n",
        "    #max_size = Slider(min=256, max=1280, divisions=64, label=\"{value}px\", value=int(DiT_prefs['max_size']), expand=True, on_change=lambda e:changed(e,'max_size', ptype='int'))\n",
        "    #max_row = Row([Text(\"Max Resolution Size: \"), max_size])\n",
        "    apply_ESRGAN_upscale = Switch(label=\"Apply ESRGAN Upscale\", value=DiT_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)\n",
        "    enlarge_scale_slider = SliderRow(label=\"Enlarge Scale\", min=1, max=4, divisions=6, round=1, suffix=\"x\", pref=DiT_prefs, key='enlarge_scale')\n",
        "    display_upscaled_image = Checkbox(label=\"Display Upscaled Image\", value=DiT_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))\n",
        "    ESRGAN_settings = Container(Column([enlarge_scale_slider, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_DiT = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_DiT.height = None if status['installed_ESRGAN'] else 0\n",
        "    if not DiT_prefs['apply_ESRGAN_upscale']:\n",
        "        ESRGAN_settings.height = 0\n",
        "    page.DiT_output = Column([], auto_scroll=True)\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(page.DiT_output.controls) > 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"‚ößÔ∏è  DiT Models with Transformers Class-to-Image Generator\", \"Scalable Diffusion Models with Transformers...\", actions=[ft.OutlinedButton(\"Class List\", on_click=show_classes), IconButton(icon=icons.HELP, tooltip=\"Help with DiT Settings\", on_click=DiT_help)]),\n",
        "        prompt,\n",
        "        #Row([prompt, mask_image, invert_mask]),\n",
        "        num_inference_row,\n",
        "        guidance_scale,\n",
        "        Row([NumberPicker(label=\"Number of Images: \", min=1, max=20, value=DiT_prefs['num_images'], on_change=lambda e: changed(e, 'num_images')), seed, batch_folder_name]),\n",
        "        page.ESRGAN_block_DiT,\n",
        "        Row([ElevatedButton(content=Text(\"üîÄ   Get DiT Generation\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_DiT(page)), \n",
        "             #ElevatedButton(content=Text(value=\"üìú   Run from Prompts List\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_DiT(page, from_list=True))\n",
        "             ]),\n",
        "        \n",
        "      ]\n",
        "    )), page.DiT_output,\n",
        "        clear_button,\n",
        "    ], scroll=ScrollMode.AUTO, auto_scroll=False)\n",
        "    return c\n",
        "\n",
        "dall_e_prefs = {\n",
        "    'prompt': '',\n",
        "    'size': '512x512',\n",
        "    'num_images': 1,\n",
        "    'init_image': '',\n",
        "    'mask_image': '',\n",
        "    'variation': False,\n",
        "    \"invert_mask\": False,\n",
        "    'file_prefix': 'dalle-',\n",
        "    \"apply_ESRGAN_upscale\": prefs['apply_ESRGAN_upscale'],\n",
        "    \"enlarge_scale\": prefs['enlarge_scale'],\n",
        "    \"face_enhance\": prefs['face_enhance'],\n",
        "    \"display_upscaled_image\": prefs['display_upscaled_image'],\n",
        "    \"batch_folder_name\": '',\n",
        "}\n",
        "\n",
        "def buildDallE2(page):\n",
        "    global dall_e_prefs\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            dall_e_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            dall_e_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            dall_e_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def pick_files_result(e: FilePickerResultEvent):\n",
        "        if e.files:\n",
        "            img = e.files\n",
        "            dalle = []\n",
        "            fname = img[0]\n",
        "            print(\", \".join(map(lambda f: f.name, e.files)))\n",
        "            src_path = page.get_upload_url(fname.name, 600)\n",
        "            dalle.append(FilePickerUploadFile(fname.name, upload_url=src_path))\n",
        "            pick_files_dialog.upload(dalle)\n",
        "            print(str(src_path))\n",
        "            #src_path = ''.join(src_path)\n",
        "            print(str(dalle[0]))\n",
        "            dst_path = os.path.join(root_dir, fname.name)\n",
        "            print(f'Copy {src_path} to {dst_path}')\n",
        "            #shutil.copy(src_path, dst_path)\n",
        "            # TODO: is init or mask?\n",
        "            init_image.value = dst_path\n",
        "\n",
        "    pick_files_dialog = FilePicker(on_result=pick_files_result)\n",
        "    page.overlay.append(pick_files_dialog)\n",
        "    #selected_files = Text()\n",
        "\n",
        "    def file_picker_result(e: FilePickerResultEvent):\n",
        "        if e.files != None:\n",
        "            upload_files(e)\n",
        "    def on_upload_progress(e: FilePickerUploadEvent):\n",
        "        nonlocal pick_type\n",
        "        if e.progress == 1:\n",
        "            if not slash in e.file_name:\n",
        "              fname = os.path.join(root_dir, e.file_name)\n",
        "            else:\n",
        "              fname = e.file_name\n",
        "            if pick_type == \"init\":\n",
        "                init_image.value = fname\n",
        "                init_image.update()\n",
        "                dall_e_prefs['init_image'] = fname\n",
        "            elif pick_type == \"mask\":\n",
        "                mask_image.value = fname\n",
        "                mask_image.update()\n",
        "                dall_e_prefs['mask_image'] = fname\n",
        "            page.update()\n",
        "    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)\n",
        "    def upload_files(e):\n",
        "        dalle = []\n",
        "        if file_picker.result != None and file_picker.result.files != None:\n",
        "            for f in file_picker.result.files:\n",
        "              if page.web:\n",
        "                dalle.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))\n",
        "              else:\n",
        "                on_upload_progress(FilePickerUploadEvent(f.path, 1, \"\"))\n",
        "            file_picker.upload(dalle)\n",
        "    page.overlay.append(file_picker)\n",
        "    pick_type = \"\"\n",
        "    #page.overlay.append(pick_files_dialog)\n",
        "    def pick_init(e):\n",
        "        nonlocal pick_type\n",
        "        pick_type = \"init\"\n",
        "        file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"png\", \"PNG\"], dialog_title=\"Pick Init Image File\")\n",
        "    def pick_mask(e):\n",
        "        nonlocal pick_type\n",
        "        pick_type = \"mask\"\n",
        "        file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"png\", \"PNG\"], dialog_title=\"Pick Black & White Mask Image\")\n",
        "    def toggle_ESRGAN(e):\n",
        "        ESRGAN_settings.height = None if e.control.value else 0\n",
        "        dall_e_prefs['apply_ESRGAN_upscale'] = e.control.value\n",
        "        ESRGAN_settings.update()\n",
        "    def change_enlarge_scale(e):\n",
        "        enlarge_scale_slider.controls[1].value = f\" {float(e.control.value)}x\"\n",
        "        enlarge_scale_slider.update()\n",
        "        changed(e, 'enlarge_scale', ptype=\"float\")\n",
        "\n",
        "    prompt = TextField(label=\"Prompt Text\", value=dall_e_prefs['prompt'], multiline=True, on_change=lambda e:changed(e,'prompt'))\n",
        "    batch_folder_name = TextField(label=\"Batch Folder Name\", value=dall_e_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    file_prefix = TextField(label=\"Filename Prefix\", value=dall_e_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))\n",
        "    #num_images = NumberPicker(label=\"Num of Outputs\", min=1, max=4, step=4, value=dall_e_prefs['num_images'], on_change=lambda e:changed(e,'num_images', ptype=\"int\"))\n",
        "    #num_images = TextField(label=\"num_images\", value=dall_e_prefs['num_images'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'num_images', ptype=\"int\"))\n",
        "    #n_iterations = TextField(label=\"Number of Iterations\", value=dall_e_prefs['n_iterations'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'n_iterations', ptype=\"int\"))\n",
        "    #steps = TextField(label=\"Inference Steps\", value=dall_e_prefs['steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'steps', ptype=\"int\"))\n",
        "    #eta = TextField(label=\"DDIM ETA\", value=dall_e_prefs['eta'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'eta', ptype=\"float\"))\n",
        "    #seed = TextField(label=\"Seed\", value=dall_e_prefs['seed'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'seed', ptype=\"int\"))\n",
        "    size = Dropdown(label=\"Image Size\", width=120, options=[dropdown.Option(\"256x256\"), dropdown.Option(\"512x512\"), dropdown.Option(\"1024x1024\")], value=dall_e_prefs['size'], on_change=lambda e:changed(e,'size'))\n",
        "    param_rows = ResponsiveRow([Row([batch_folder_name, file_prefix], col={'lg':6}), Row([size, NumberPicker(label=\" Number of Images\", min=1, max=10, step=1, value=dall_e_prefs['num_images'], on_change=lambda e:changed(e,'num_images', ptype=\"int\"))], col={'lg':6})])\n",
        "    \n",
        "    #width = Slider(min=128, max=1024, divisions=6, label=\"{value}px\", value=dall_e_prefs['width'], on_change=change_width, expand=True)\n",
        "    #width_value = Text(f\" {int(dall_e_prefs['width'])}px\", weight=FontWeight.BOLD)\n",
        "    #width_slider = Row([Text(f\"Width: \"), width_value, width])\n",
        "    #height = Slider(min=128, max=1024, divisions=6, label=\"{value}px\", value=dall_e_prefs['height'], on_change=change_height, expand=True)\n",
        "    #height_value = Text(f\" {int(dall_e_prefs['height'])}px\", weight=FontWeight.BOLD)\n",
        "    #height_slider = Row([Text(f\"Height: \"), height_value, height])\n",
        "    init_image = TextField(label=\"Init Image\", value=dall_e_prefs['init_image'], on_change=lambda e:changed(e,'init_image'), expand=True, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_init, col={\"*\":1, \"md\":3}))\n",
        "    mask_image = TextField(label=\"Mask Image\", value=dall_e_prefs['mask_image'], on_change=lambda e:changed(e,'mask_image'), expand=True, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD_OUTLINED, on_click=pick_mask, col={\"*\":1, \"md\":3}))\n",
        "    variation = Checkbox(label=\"Variation   \", tooltip=\"Creates Variation of Init Image. Disregards the Prompt and Mask.\", value=dall_e_prefs['variation'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'variation'))\n",
        "    invert_mask = Checkbox(label=\"Invert\", tooltip=\"Swaps the Black & White of your Mask Image\", value=dall_e_prefs['invert_mask'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'invert_mask'))\n",
        "    image_pickers = Container(content=ResponsiveRow([Row([init_image, variation], col={\"md\":6}), Row([mask_image, invert_mask], col={\"md\":6})], run_spacing=2), padding=padding.only(top=5), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    #prompt_strength = Slider(min=0.1, max=0.9, divisions=16, label=\"{value}%\", value=dall_e_prefs['prompt_strength'], on_change=change_strength, expand=True)\n",
        "    #strength_value = Text(f\" {int(dall_e_prefs['prompt_strength'] * 100)}%\", weight=FontWeight.BOLD) \n",
        "    #strength_slider = Row([Text(\"Prompt Strength: \"), strength_value, prompt_strength])\n",
        "    img_block = Container(Column([image_pickers, Divider(height=9, thickness=2)]), padding=padding.only(top=5), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    apply_ESRGAN_upscale = Switch(label=\"Apply ESRGAN Upscale\", value=dall_e_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)\n",
        "    enlarge_scale_value = Text(f\" {float(dall_e_prefs['enlarge_scale'])}x\", weight=FontWeight.BOLD)\n",
        "    enlarge_scale = Slider(min=1, max=4, divisions=6, label=\"{value}x\", value=dall_e_prefs['enlarge_scale'], on_change=change_enlarge_scale, expand=True)\n",
        "    enlarge_scale_slider = Row([Text(\"Enlarge Scale: \"), enlarge_scale_value, enlarge_scale])\n",
        "    face_enhance = Checkbox(label=\"Use Face Enhance GPFGAN\", value=dall_e_prefs['face_enhance'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'face_enhance'))\n",
        "    display_upscaled_image = Checkbox(label=\"Display Upscaled Image\", value=dall_e_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))\n",
        "    ESRGAN_settings = Container(Column([enlarge_scale_slider, face_enhance, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_dalle = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_dalle.height = None if status['installed_ESRGAN'] else 0\n",
        "    if not dall_e_prefs['apply_ESRGAN_upscale']:\n",
        "        ESRGAN_settings.height = 0\n",
        "    list_button = ElevatedButton(content=Text(value=\"üìú   Run from Prompts List\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_dall_e(page, from_list=True))\n",
        "    parameters_button = ElevatedButton(content=Text(value=\"üñºÔ∏è   Run Dall-E 2\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_dall_e(page))\n",
        "\n",
        "    parameters_row = Row([parameters_button, list_button], spacing=22)#, alignment=MainAxisAlignment.SPACE_BETWEEN)\n",
        "    page.dall_e_output = Column([])\n",
        "    c = Column([Container(\n",
        "        padding=padding.only(18, 14, 20, 10), content=Column([\n",
        "            Header(\"üë∫  OpenAI Dall-E 2\", \"Generates Images using your OpenAI API Key. Note: Uses same credits as official website.\"),\n",
        "            prompt,\n",
        "            param_rows,\n",
        "            img_block, page.ESRGAN_block_dalle,\n",
        "            #(img_block if status['installed_img2img'] or status['installed_stability'] else Container(content=None)), (clip_block if prefs['install_CLIP_guided'] else Container(content=None)), (ESRGAN_block if prefs['install_ESRGAN'] else Container(content=None)), \n",
        "            parameters_row,\n",
        "            page.dall_e_output\n",
        "        ],\n",
        "    ))], scroll=ScrollMode.AUTO)\n",
        "    return c\n",
        "\n",
        "kandinsky_prefs = {\n",
        "    \"prompt\": '',\n",
        "    \"batch_folder_name\": '',\n",
        "    \"file_prefix\": \"kandinsky-\",\n",
        "    \"num_images\": 1,\n",
        "    \"steps\":100,\n",
        "    \"ddim_eta\":0.05,\n",
        "    \"width\": 512,\n",
        "    \"height\":512,\n",
        "    \"guidance_scale\":8,\n",
        "    'prior_cf_scale': 4,\n",
        "    'prior_steps': \"25\",\n",
        "    \"dynamic_threshold_v\":99.5,\n",
        "    \"sampler\": \"ddim_sampler\",\n",
        "    \"denoised_type\": \"dynamic_threshold\",\n",
        "    \"init_image\": '',\n",
        "    \"strength\": 0.5,\n",
        "    \"mask_image\": '',\n",
        "    \"invert_mask\": False,\n",
        "    \"apply_ESRGAN_upscale\": prefs['apply_ESRGAN_upscale'],\n",
        "    \"enlarge_scale\": prefs['enlarge_scale'],\n",
        "    \"face_enhance\": prefs['face_enhance'],\n",
        "    \"display_upscaled_image\": prefs['display_upscaled_image'],\n",
        "}\n",
        "\n",
        "def buildKandinsky(page):\n",
        "    global prefs, kandinsky_prefs, status\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            kandinsky_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            kandinsky_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            kandinsky_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def kandinsky_help(e):\n",
        "      def close_kandinsky_dlg(e):\n",
        "        nonlocal kandinsky_help_dlg\n",
        "        kandinsky_help_dlg.open = False\n",
        "        page.update()\n",
        "      kandinsky_help_dlg = AlertDialog(title=Text(\"üôÖ   Help with Kandinsky Pipeline\"), content=Column([\n",
        "          Text(\"NOTE: Right now, installing this may be incompatible with Diffusers packages, so it may not work if you first installed HuggingFace & Stable Diffusion. It's recommended to run this on a fresh runtime, only installing ESRGAN to upscale. We hope to fix this soon, but works great.\"),\n",
        "          Text(\"Kandinsky 2.1 inherits best practicies from Dall-E 2 and Latent diffusion, while introducing some new ideas.\"),\n",
        "          Text(\"As text and image encoder it uses CLIP model and diffusion image prior (mapping) between latent spaces of CLIP modalities. This approach increases the visual performance of the model and unveils new horizons in blending images and text-guided image manipulation. For diffusion mapping of latent spaces we use transformer with num_layers=20, num_heads=32 and hidden_size=2048. Kandinsky 2.1 was trained on a large-scale image-text dataset LAION HighRes and fine-tuned on our internal datasets. These encoders and multilingual training datasets unveil the real multilingual text-to-image generation experience!\"),\n",
        "          Text(\"The decision to make changes to the architecture came after continuing to learn the Kandinsky 2.0 version and trying to get stable text embeddings of the mT5 multilingual language model. The logical conclusion was that the use of only text embedding was not enough for high-quality image synthesis. After analyzing once again the existing DALL-E 2 solution from OpenAI, it was decided to experiment with the image prior model (allows you to generate visual embedding CLIP by text prompt or text embedding CLIP), while remaining in the latent visual space paradigm, so that you do not have to retrain the diffusion part of the UNet model Kandinsky 2.0. Now a little more details about the learning process of Kandinsky 2.1.\"),\n",
        "          Markdown(\"[Kandinsky GitHub](https://github.com/ai-forever/Kandinsky-2) | [Kandinsky 2.1 Blog](https://habr.com/ru/companies/sberbank/articles/725282/) | [FusionBrain Demo](https://fusionbrain.ai/diffusion)\"),\n",
        "        ], scroll=ScrollMode.AUTO), actions=[TextButton(\"ü§§  Quality... \", on_click=close_kandinsky_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = kandinsky_help_dlg\n",
        "      kandinsky_help_dlg.open = True\n",
        "      page.update()\n",
        "    def pick_files_result(e: FilePickerResultEvent):\n",
        "        if e.files:\n",
        "            img = e.files\n",
        "            uf = []\n",
        "            fname = img[0]\n",
        "            #print(\", \".join(map(lambda f: f.name, e.files)))\n",
        "            src_path = page.get_upload_url(fname.name, 600)\n",
        "            uf.append(FilePickerUploadFile(fname.name, upload_url=src_path))\n",
        "            pick_files_dialog.upload(uf)\n",
        "            #print(str(src_path))\n",
        "            #src_path = ''.join(src_path)\n",
        "            #print(str(uf[0]))\n",
        "            dst_path = os.path.join(root_dir, fname.name)\n",
        "            #print(f'Copy {src_path} to {dst_path}')\n",
        "            #shutil.copy(src_path, dst_path)\n",
        "            # TODO: is init or mask?\n",
        "            init_image.value = dst_path\n",
        "    pick_files_dialog = FilePicker(on_result=pick_files_result)\n",
        "    page.overlay.append(pick_files_dialog)\n",
        "    #selected_files = Text()\n",
        "    def file_picker_result(e: FilePickerResultEvent):\n",
        "        if e.files != None:\n",
        "            upload_files(e)\n",
        "    def on_upload_progress(e: FilePickerUploadEvent):\n",
        "        nonlocal pick_type\n",
        "        if e.progress == 1:\n",
        "            if not slash in e.file_name:\n",
        "              fname = os.path.join(root_dir, e.file_name)\n",
        "            else:\n",
        "              fname = e.file_name\n",
        "            if pick_type == \"init\":\n",
        "                init_image.value = fname\n",
        "                init_image.update()\n",
        "                kandinsky_prefs['init_image'] = fname\n",
        "            elif pick_type == \"mask\":\n",
        "                mask_image.value = fname\n",
        "                mask_image.update()\n",
        "                kandinsky_prefs['mask_image'] = fname\n",
        "            page.update()\n",
        "    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)\n",
        "    def upload_files(e):\n",
        "        uf = []\n",
        "        if file_picker.result != None and file_picker.result.files != None:\n",
        "            for f in file_picker.result.files:\n",
        "              if page.web:\n",
        "                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))\n",
        "              else:\n",
        "                on_upload_progress(FilePickerUploadEvent(f.path, 1, \"\"))\n",
        "            file_picker.upload(uf)\n",
        "    page.overlay.append(file_picker)\n",
        "    pick_type = \"\"\n",
        "    #page.overlay.append(pick_files_dialog)\n",
        "    def pick_init(e):\n",
        "        nonlocal pick_type\n",
        "        pick_type = \"init\"\n",
        "        file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"png\", \"PNG\"], dialog_title=\"Pick Init Image File\")\n",
        "    def pick_mask(e):\n",
        "        nonlocal pick_type\n",
        "        pick_type = \"mask\"\n",
        "        file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"png\", \"PNG\"], dialog_title=\"Pick Black & White Mask Image\")\n",
        "    def toggle_ESRGAN(e):\n",
        "        ESRGAN_settings.height = None if e.control.value else 0\n",
        "        kandinsky_prefs['apply_ESRGAN_upscale'] = e.control.value\n",
        "        ESRGAN_settings.update()\n",
        "    prompt = TextField(label=\"Prompt Text\", value=kandinsky_prefs['prompt'], multiline=True, on_change=lambda e:changed(e,'prompt'))\n",
        "    batch_folder_name = TextField(label=\"Batch Folder Name\", value=kandinsky_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    file_prefix = TextField(label=\"Filename Prefix\", value=kandinsky_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))\n",
        "    #num_outputs = NumberPicker(label=\"Num of Outputs\", min=1, max=4, step=4, value=kandinsky_prefs['num_outputs'], on_change=lambda e:changed(e,'num_outputs', ptype=\"int\"))\n",
        "    #num_outputs = TextField(label=\"num_outputs\", value=kandinsky_prefs['num_outputs'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'num_outputs', ptype=\"int\"))\n",
        "    #n_iterations = TextField(label=\"Number of Iterations\", value=kandinsky_prefs['n_iterations'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'n_iterations', ptype=\"int\"))\n",
        "    steps = TextField(label=\"Number of Steps\", value=kandinsky_prefs['steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'steps', ptype=\"int\"))\n",
        "    ddim_eta = TextField(label=\"DDIM ETA\", value=kandinsky_prefs['ddim_eta'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'ddim_eta', ptype=\"float\"))\n",
        "    dynamic_threshold_v = TextField(label=\"Dynamic Threshold\", value=kandinsky_prefs['dynamic_threshold_v'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'dynamic_threshold_v', ptype=\"float\"))\n",
        "    sampler = Dropdown(label=\"Sampler\", width=200, options=[dropdown.Option(\"ddim_sampler\"), dropdown.Option(\"p_sampler\")], value=kandinsky_prefs['sampler'], on_change=lambda e:changed(e,'sampler'), col={'xs':12, 'md':6})\n",
        "    n_images = NumberPicker(label=\"Number of Images\", min=1, max=9, step=1, value=kandinsky_fuse_prefs['num_images'], on_change=lambda e:changed(e,'num_images', ptype=\"int\"))\n",
        "    param_rows = ResponsiveRow([Column([batch_folder_name, n_images], col={'xs':12, 'md':6}), \n",
        "                      Column([file_prefix, sampler], col={'xs':12, 'md':6})\n",
        "                      #Column([steps, ddim_eta, dynamic_threshold_v], col={'xs':12, 'md':6})\n",
        "                      ], vertical_alignment=CrossAxisAlignment.START)\n",
        "    denoised_type = Dropdown(label=\"Denoised Type\", width=180, options=[dropdown.Option(\"dynamic_threshold\"), dropdown.Option(\"clip_denoised\")], value=kandinsky_prefs['denoised_type'], on_change=lambda e:changed(e,'denoised_type'), col={'xs':12, 'md':6})\n",
        "    dropdown_row = ResponsiveRow([sampler])#, denoised_type])\n",
        "    steps = SliderRow(label=\"Number of Steps\", min=0, max=200, divisions=200, pref=kandinsky_prefs, key='steps')\n",
        "    prior_cf_scale = SliderRow(label=\"Prior CF Scale\", min=0, max=10, divisions=10, pref=kandinsky_prefs, key='prior_cf_scale')\n",
        "    prior_steps = SliderRow(label=\"Prior Steps\", min=0, max=50, divisions=50, pref=kandinsky_prefs, key='prior_steps')\n",
        "    guidance = SliderRow(label=\"Guidance Scale\", min=0, max=50, divisions=50, pref=kandinsky_prefs, key='guidance_scale')\n",
        "    width_slider = SliderRow(label=\"Width\", min=128, max=1024, divisions=14, multiple=32, suffix=\"px\", pref=kandinsky_prefs, key='width')\n",
        "    height_slider = SliderRow(label=\"Height\", min=128, max=1024, divisions=14, multiple=32, suffix=\"px\", pref=kandinsky_prefs, key='height')\n",
        "    init_image = TextField(label=\"Init Image\", value=kandinsky_prefs['init_image'], on_change=lambda e:changed(e,'init_image'), expand=True, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_init), col={'xs':12, 'md':6})\n",
        "    mask_image = TextField(label=\"Mask Image\", value=kandinsky_prefs['mask_image'], on_change=lambda e:changed(e,'mask_image'), expand=True, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD_OUTLINED, on_click=pick_mask), col={'xs':10, 'md':5})\n",
        "    invert_mask = Checkbox(label=\"Invert\", tooltip=\"Swaps the Black & White of your Mask Image\", value=kandinsky_prefs['invert_mask'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'invert_mask'), col={'xs':2, 'md':1})\n",
        "    image_pickers = Container(content=ResponsiveRow([init_image, mask_image, invert_mask]), padding=padding.only(top=5), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    strength_slider = SliderRow(label=\"Init Image Strength\", min=0.1, max=0.9, divisions=16, round=2, pref=kandinsky_prefs, key='strength')\n",
        "    img_block = Container(Column([image_pickers, strength_slider, Divider(height=9, thickness=2)]), padding=padding.only(top=5), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    apply_ESRGAN_upscale = Switch(label=\"Apply ESRGAN Upscale\", value=kandinsky_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)\n",
        "    enlarge_scale_slider = SliderRow(label=\"Enlarge Scale\", min=1, max=4, divisions=6, round=1, suffix=\"x\", pref=kandinsky_prefs, key='enlarge_scale')\n",
        "    face_enhance = Checkbox(label=\"Use Face Enhance GPFGAN\", value=kandinsky_prefs['face_enhance'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'face_enhance'))\n",
        "    display_upscaled_image = Checkbox(label=\"Display Upscaled Image\", value=kandinsky_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))\n",
        "    ESRGAN_settings = Container(Column([enlarge_scale_slider, face_enhance, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_kandinsky = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_kandinsky.height = None if status['installed_ESRGAN'] else 0\n",
        "    if not kandinsky_prefs['apply_ESRGAN_upscale']:\n",
        "        ESRGAN_settings.height = 0\n",
        "    parameters_button = ElevatedButton(content=Text(value=\"‚ú®   Run Kandinsky 2.1\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_kandinsky(page))\n",
        "\n",
        "    parameters_row = Row([parameters_button], alignment=MainAxisAlignment.SPACE_BETWEEN)\n",
        "    page.kandinsky_output = Column([])\n",
        "    c = Column([Container(\n",
        "        padding=padding.only(18, 14, 20, 10), content=Column([\n",
        "            Header(\"üéé  Kandinsky 2.1\", \"A Latent Diffusion model with two Multilingual text encoders, supports 100+ languages, made in Russia.\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with Kandinsky Settings\", on_click=kandinsky_help)]),\n",
        "            prompt,\n",
        "            #param_rows, #dropdown_row, \n",
        "            steps, prior_steps, prior_cf_scale,\n",
        "            guidance, width_slider, height_slider, #Divider(height=9, thickness=2), \n",
        "            img_block,\n",
        "            Row([batch_folder_name, file_prefix]),\n",
        "            Row([n_images, sampler]),\n",
        "            page.ESRGAN_block_kandinsky,\n",
        "            #(img_block if status['installed_img2img'] or status['installed_stability'] else Container(content=None)), (clip_block if prefs['install_CLIP_guided'] else Container(content=None)), (ESRGAN_block if prefs['install_ESRGAN'] else Container(content=None)), \n",
        "            parameters_row,\n",
        "            page.kandinsky_output\n",
        "        ],\n",
        "    ))], scroll=ScrollMode.AUTO)#batch_folder_name, batch_size, n_iterations, steps, ddim_eta, seed, \n",
        "    return c\n",
        "\n",
        "kandinsky_fuse_prefs = {\n",
        "    \"prompt\": '',\n",
        "    \"batch_folder_name\": '',\n",
        "    \"file_prefix\": \"kandinsky-\",\n",
        "    \"num_images\": 1,\n",
        "    \"mixes\": [],\n",
        "    \"steps\":100,\n",
        "    \"width\": 512,\n",
        "    \"height\":512,\n",
        "    \"guidance_scale\":8,\n",
        "    'prior_cf_scale': 4,\n",
        "    'prior_steps': \"25\",\n",
        "    \"sampler\": \"ddim_sampler\",\n",
        "    \"init_image\": '',\n",
        "    \"weight\": 0.5,\n",
        "    \"mask_image\": '',\n",
        "    \"invert_mask\": False,\n",
        "    \"apply_ESRGAN_upscale\": prefs['apply_ESRGAN_upscale'],\n",
        "    \"enlarge_scale\": prefs['enlarge_scale'],\n",
        "    \"face_enhance\": prefs['face_enhance'],\n",
        "    \"display_upscaled_image\": prefs['display_upscaled_image'],\n",
        "}\n",
        "\n",
        "def buildKandinskyFuse(page):\n",
        "    global prefs, kandinsky_fuse_prefs, status\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            kandinsky_fuse_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            kandinsky_fuse_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            kandinsky_fuse_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def pick_files_result(e: FilePickerResultEvent):\n",
        "        if e.files:\n",
        "            img = e.files\n",
        "            uf = []\n",
        "            fname = img[0]\n",
        "            #print(\", \".join(map(lambda f: f.name, e.files)))\n",
        "            src_path = page.get_upload_url(fname.name, 600)\n",
        "            uf.append(FilePickerUploadFile(fname.name, upload_url=src_path))\n",
        "            pick_files_dialog.upload(uf)\n",
        "            #print(str(src_path))\n",
        "            #src_path = ''.join(src_path)\n",
        "            #print(str(uf[0]))\n",
        "            dst_path = os.path.join(root_dir, fname.name)\n",
        "            #print(f'Copy {src_path} to {dst_path}')\n",
        "            #shutil.copy(src_path, dst_path)\n",
        "            init_image.value = dst_path\n",
        "    pick_files_dialog = FilePicker(on_result=pick_files_result)\n",
        "    page.overlay.append(pick_files_dialog)\n",
        "    #selected_files = Text()\n",
        "    def file_picker_result(e: FilePickerResultEvent):\n",
        "        if e.files != None:\n",
        "            upload_files(e)\n",
        "    def on_upload_progress(e: FilePickerUploadEvent):\n",
        "        nonlocal pick_type\n",
        "        if e.progress == 1:\n",
        "            if not slash in e.file_name:\n",
        "              fname = os.path.join(root_dir, e.file_name)\n",
        "            else:\n",
        "              fname = e.file_name\n",
        "            init_image.value = fname\n",
        "            init_image.update()\n",
        "            kandinsky_fuse_prefs['init_image'] = fname\n",
        "            page.update()\n",
        "    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)\n",
        "    def upload_files(e):\n",
        "        uf = []\n",
        "        if file_picker.result != None and file_picker.result.files != None:\n",
        "            for f in file_picker.result.files:\n",
        "              if page.web:\n",
        "                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))\n",
        "              else:\n",
        "                on_upload_progress(FilePickerUploadEvent(f.path, 1, \"\"))\n",
        "            file_picker.upload(uf)\n",
        "    page.overlay.append(file_picker)\n",
        "    pick_type = \"\"\n",
        "    #page.overlay.append(pick_files_dialog)\n",
        "    def pick_init(e):\n",
        "        nonlocal pick_type\n",
        "        pick_type = \"init\"\n",
        "        file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"png\", \"PNG\"], dialog_title=\"Pick Init Image File\")\n",
        "    def toggle_ESRGAN(e):\n",
        "        ESRGAN_settings.height = None if e.control.value else 0\n",
        "        kandinsky_fuse_prefs['apply_ESRGAN_upscale'] = e.control.value\n",
        "        ESRGAN_settings.update()\n",
        "    def kandinsky_help(e):\n",
        "      def close_kandinsky_dlg(e):\n",
        "        nonlocal kandinsky_help_dlg\n",
        "        kandinsky_help_dlg.open = False\n",
        "        page.update()\n",
        "      kandinsky_help_dlg = AlertDialog(title=Text(\"üôÖ   Help with Kandinsky Fuse Pipeline\"), content=Column([\n",
        "          Text(\"NOTE: Right now, installing this may be incompatible with Diffusers packages, so it may not work if you first installed HuggingFace & Stable Diffusion. It's recommended to run this on a fresh runtime, only installing ESRGAN to upscale. We hope to fix this soon, but works great.\"),\n",
        "          Text(\"This variation lets you fuse together many images together with multiple text prompts to create a mix. Set the weights of the prompts and images to adjust the amount of influence it has on the generated style. Get experimental\"),\n",
        "          Text(\"Kandinsky 2.1 inherits best practicies from Dall-E 2 and Latent diffusion, while introducing some new ideas.\"),\n",
        "          Text(\"As text and image encoder it uses CLIP model and diffusion image prior (mapping) between latent spaces of CLIP modalities. This approach increases the visual performance of the model and unveils new horizons in blending images and text-guided image manipulation. For diffusion mapping of latent spaces we use transformer with num_layers=20, num_heads=32 and hidden_size=2048. Kandinsky 2.1 was trained on a large-scale image-text dataset LAION HighRes and fine-tuned on our internal datasets. These encoders and multilingual training datasets unveil the real multilingual text-to-image generation experience!\"),\n",
        "          Text(\"The decision to make changes to the architecture came after continuing to learn the Kandinsky 2.0 version and trying to get stable text embeddings of the mT5 multilingual language model. The logical conclusion was that the use of only text embedding was not enough for high-quality image synthesis. After analyzing once again the existing DALL-E 2 solution from OpenAI, it was decided to experiment with the image prior model (allows you to generate visual embedding CLIP by text prompt or text embedding CLIP), while remaining in the latent visual space paradigm, so that you do not have to retrain the diffusion part of the UNet model Kandinsky 2.0. Now a little more details about the learning process of Kandinsky 2.1.\"),\n",
        "          Markdown(\"[Kandinsky GitHub](https://github.com/ai-forever/Kandinsky-2) | [Kandinsky 2.1 Blog](https://habr.com/ru/companies/sberbank/articles/725282/) | [FusionBrain Demo](https://fusionbrain.ai/diffusion)\"),\n",
        "        ], scroll=ScrollMode.AUTO), actions=[TextButton(\"ü´¢  Possibility Overload... \", on_click=close_kandinsky_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = kandinsky_help_dlg\n",
        "      kandinsky_help_dlg.open = True\n",
        "      page.update()\n",
        "    def add_image(e):\n",
        "        if not bool(kandinsky_fuse_prefs['init_image']): return\n",
        "        layer = {'init_image': kandinsky_fuse_prefs['init_image'], 'weight': kandinsky_fuse_prefs['weight']}\n",
        "        kandinsky_fuse_prefs['mixes'].append(layer)\n",
        "        fuse_layers.controls.append(ListTile(title=Row([Text(layer['init_image'], weight=FontWeight.BOLD), Text(f\"Weight: {layer['weight']}\")], alignment=MainAxisAlignment.SPACE_BETWEEN), dense=True, trailing=PopupMenuButton(icon=icons.MORE_VERT,\n",
        "          items=[\n",
        "              PopupMenuItem(icon=icons.EDIT, text=\"Edit Image Layer\", on_click=edit_layer, data=layer),\n",
        "              PopupMenuItem(icon=icons.DELETE, text=\"Delete Image Layer\", on_click=delete_layer, data=layer),\n",
        "              PopupMenuItem(icon=icons.DELETE_SWEEP, text=\"Delete All Layers\", on_click=delete_all_layers, data=layer),\n",
        "              PopupMenuItem(icon=icons.ARROW_UPWARD, text=\"Move Up\", on_click=move_up, data=layer),\n",
        "              PopupMenuItem(icon=icons.ARROW_DOWNWARD, text=\"Move Down\", on_click=move_down, data=layer),\n",
        "          ]), data=layer, on_click=edit_layer))\n",
        "        fuse_layers.update()\n",
        "        kandinsky_fuse_prefs['init_image'] = \"\"\n",
        "        init_image.value = \"\"\n",
        "        init_image.update()\n",
        "    def add_prompt(e):\n",
        "        if not bool(kandinsky_fuse_prefs['prompt']): return\n",
        "        layer = {'prompt': kandinsky_fuse_prefs['prompt'], 'weight': kandinsky_fuse_prefs['weight']}\n",
        "        kandinsky_fuse_prefs['mixes'].append(layer)\n",
        "        fuse_layers.controls.append(ListTile(title=Row([Text(layer['prompt'], weight=FontWeight.BOLD), Text(f\"Weight: {layer['weight']}\")], alignment=MainAxisAlignment.SPACE_BETWEEN), dense=True, trailing=PopupMenuButton(icon=icons.MORE_VERT,\n",
        "          items=[\n",
        "              PopupMenuItem(icon=icons.EDIT, text=\"Edit Text Layer\", on_click=edit_layer, data=layer),\n",
        "              PopupMenuItem(icon=icons.DELETE, text=\"Delete Text Layer\", on_click=delete_layer, data=layer),\n",
        "              PopupMenuItem(icon=icons.DELETE_SWEEP, text=\"Delete All Layers\", on_click=delete_all_layers, data=layer),\n",
        "              PopupMenuItem(icon=icons.ARROW_UPWARD, text=\"Move Up\", on_click=move_up, data=layer),\n",
        "              PopupMenuItem(icon=icons.ARROW_DOWNWARD, text=\"Move Down\", on_click=move_down, data=layer),\n",
        "          ]), data=layer, on_click=edit_layer))\n",
        "        fuse_layers.update()\n",
        "        kandinsky_fuse_prefs['prompt'] = \"\"\n",
        "        prompt.value = \"\"\n",
        "        prompt.update()\n",
        "    def delete_layer(e):\n",
        "        kandinsky_fuse_prefs['mixes'].remove(e.control.data)\n",
        "        for c in fuse_layers.controls:\n",
        "            if 'prompt' in c.data:\n",
        "                if c.data['prompt'] == e.control.data['prompt']:\n",
        "                    fuse_layers.controls.remove(c)\n",
        "                    break\n",
        "            else:\n",
        "                if c.data['init_image'] == e.control.data['init_image']:\n",
        "                    fuse_layers.controls.remove(c)\n",
        "                    break\n",
        "        fuse_layers.update()\n",
        "    def delete_all_layers(e):\n",
        "        kandinsky_fuse_prefs['mixes'].clear()\n",
        "        fuse_layers.controls.clear()\n",
        "        fuse_layers.update()\n",
        "    def move_down(e):\n",
        "        idx = kandinsky_fuse_prefs['mixes'].index(e.control.data)\n",
        "        if idx < (len(kandinsky_fuse_prefs['mixes']) - 1):\n",
        "          d = kandinsky_fuse_prefs['mixes'].pop(idx)\n",
        "          kandinsky_fuse_prefs['mixes'].insert(idx+1, d)\n",
        "          dr = fuse_layers.controls.pop(idx)\n",
        "          fuse_layers.controls.insert(idx+1, dr)\n",
        "          fuse_layers.update()\n",
        "    def move_up(e):\n",
        "        idx = kandinsky_fuse_prefs['mixes'].index(e.control.data)\n",
        "        if idx > 0:\n",
        "          d = kandinsky_fuse_prefs['mixes'].pop(idx)\n",
        "          kandinsky_fuse_prefs['mixes'].insert(idx-1, d)\n",
        "          dr = fuse_layers.controls.pop(idx)\n",
        "          fuse_layers.controls.insert(idx-1, dr)\n",
        "          fuse_layers.update()\n",
        "    def edit_layer(e):\n",
        "        data = e.control.data\n",
        "        layer_type = \"prompt\" if \"prompt\" in data else \"image\"\n",
        "        def pick_files_result(e: FilePickerResultEvent):\n",
        "            if e.files:\n",
        "                img = e.files\n",
        "                uf = []\n",
        "                fname = img[0]\n",
        "                src_path = page.get_upload_url(fname.name, 600)\n",
        "                uf.append(FilePickerUploadFile(fname.name, upload_url=src_path))\n",
        "                pick_files_dialog.upload(uf)\n",
        "                dst_path = os.path.join(root_dir, fname.name)\n",
        "                image_mix.value = dst_path\n",
        "        pick_files_dialog = FilePicker(on_result=pick_files_result)\n",
        "        page.overlay.append(pick_files_dialog)\n",
        "        def file_picker_result(e: FilePickerResultEvent):\n",
        "            if e.files != None:\n",
        "                upload_files(e)\n",
        "        def on_upload_progress(e: FilePickerUploadEvent):\n",
        "            if e.progress == 1:\n",
        "                if not slash in e.file_name:\n",
        "                  fname = os.path.join(root_dir, e.file_name)\n",
        "                else:\n",
        "                  fname = e.file_name\n",
        "                image_mix.value = fname\n",
        "                image_mix.update()\n",
        "                data['init_image'] = fname\n",
        "                page.update()\n",
        "        file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)\n",
        "        def upload_files(e):\n",
        "            uf = []\n",
        "            if file_picker.result != None and file_picker.result.files != None:\n",
        "                for f in file_picker.result.files:\n",
        "                  if page.web:\n",
        "                    uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))\n",
        "                  else:\n",
        "                    on_upload_progress(FilePickerUploadEvent(f.path, 1, \"\"))\n",
        "                file_picker.upload(uf)\n",
        "        page.overlay.append(file_picker)\n",
        "        def pick_image(e):\n",
        "            file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"png\", \"PNG\"], dialog_title=\"Pick Init Image File\")\n",
        "        #name = e.control.title.controls[0].value\n",
        "        #path = e.control.title.controls[1].value\n",
        "        if layer_type == \"prompt\":\n",
        "            prompt_value = data[\"prompt\"]\n",
        "            image_value = \"\"\n",
        "        else:\n",
        "            prompt_value = \"\"\n",
        "            image_value = data[\"init_image\"]\n",
        "        def close_dlg(e):\n",
        "            dlg_edit.open = False\n",
        "            page.update()\n",
        "        def save_layer(e):\n",
        "            layer = None\n",
        "            for l in kandinsky_fuse_prefs['mixes']:\n",
        "                if \"prompt\" in l:\n",
        "                  if layer_type == \"prompt\":\n",
        "                      if data[\"prompt\"] == l[\"prompt\"]:\n",
        "                        layer = l\n",
        "                        layer['prompt'] = prompt_text.value\n",
        "                        break\n",
        "                else:\n",
        "                    if layer_type == \"image\":\n",
        "                      if data[\"init_image\"] == l[\"init_image\"]:\n",
        "                        layer = l\n",
        "                        layer['init_image'] = image_mix.value\n",
        "                        break\n",
        "            for c in fuse_layers.controls:\n",
        "                if 'prompt' in c.data:\n",
        "                    if 'prompt' not in data: continue\n",
        "                    if c.data['prompt'] == data['prompt']:\n",
        "                        c.title.controls[0].value = layer['prompt']\n",
        "                        c.title.controls[1].value = f\"Weight: {layer['weight']}\"\n",
        "                        c.update()\n",
        "                        break\n",
        "                else:\n",
        "                    if 'init_image' not in data: continue\n",
        "                    if c.data['init_image'] == data['init_image']:\n",
        "                        c.title.controls[0].value = layer['init_image']\n",
        "                        c.title.controls[1].value = f\"Weight: {layer['weight']}\"\n",
        "                        c.update()\n",
        "                        break\n",
        "            layer['prompt'] = prompt_text.value\n",
        "            #layer['weight'] = model_path.value\n",
        "            dlg_edit.open = False\n",
        "            e.control.update()\n",
        "            page.update()\n",
        "        prompt_text = TextField(label=\"Fuse Prompt Text\", value=prompt_value, multiline=True, visible=layer_type == \"prompt\")\n",
        "        image_mix = TextField(label=\"Fuse Image Path\", value=image_value, visible=layer_type == \"image\", height=65, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_image))\n",
        "        edit_weights = SliderRow(label=\"Weight/Strength\", min=0, max=1, divisions=20, round=1, pref=data, key='weight', tooltip=\"Indicates how much each individual concept should influence the overall guidance. If no weights are provided all concepts are applied equally.\")\n",
        "        dlg_edit = AlertDialog(modal=False, title=Text(f\"üß≥ Edit Kandinsky Fuse {layer_type.title()} Mix\"), content=Container(Column([prompt_text, image_mix, edit_weights], alignment=MainAxisAlignment.START, tight=True, scroll=ScrollMode.AUTO, width=(page.window_width or page.width) - 100)), actions=[TextButton(content=Text(\"Cancel\", size=18), on_click=close_dlg), ElevatedButton(content=Text(value=emojize(\":floppy_disk:\") + \"  Save Layer \", size=19, weight=FontWeight.BOLD), on_click=save_layer)], actions_alignment=MainAxisAlignment.END)\n",
        "        page.dialog = dlg_edit\n",
        "        dlg_edit.open = True\n",
        "        page.update()\n",
        "    add_prompt_btn = ft.FilledButton(\"‚ûï Add Prompt\", width=135, on_click=add_prompt)\n",
        "    #add_prompt_btn = IconButton(icons.ADD, tooltip=\"Add Text Prompt\", on_click=add_prompt)\n",
        "    add_image_btn = ft.FilledButton(\"‚ûï Add Image\", width=135, on_click=add_image)\n",
        "    #add_image_btn = IconButton(icons.ADD, tooltip=\"Add Image to Mix\", on_click=add_image)\n",
        "    prompt = TextField(label=\"Mix Prompt Text\", value=kandinsky_fuse_prefs['prompt'], expand=True, multiline=True, on_submit=add_prompt, on_change=lambda e:changed(e,'prompt'))\n",
        "    prompt_row = Row([prompt, add_prompt_btn])\n",
        "    init_image = TextField(label=\"Mixing Image\", value=kandinsky_fuse_prefs['init_image'], on_change=lambda e:changed(e,'init_image'), expand=True, height=65, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_init), col={'xs':12, 'md':6})\n",
        "    image_row = Row([init_image, add_image_btn])\n",
        "    weight_slider = SliderRow(label=\"Text or Image Weight\", min=0.1, max=0.9, divisions=16, round=2, pref=kandinsky_fuse_prefs, key='weight')\n",
        "    fuse_layers = Column([], spacing=0)\n",
        "    batch_folder_name = TextField(label=\"Batch Folder Name\", value=kandinsky_fuse_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    file_prefix = TextField(label=\"Filename Prefix\", value=kandinsky_fuse_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))\n",
        "    #num_outputs = NumberPicker(label=\"Num of Outputs\", min=1, max=4, step=4, value=kandinsky_fuse_prefs['num_outputs'], on_change=lambda e:changed(e,'num_outputs', ptype=\"int\"))\n",
        "    #num_outputs = TextField(label=\"num_outputs\", value=kandinsky_fuse_prefs['num_outputs'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'num_outputs', ptype=\"int\"))\n",
        "    #n_iterations = TextField(label=\"Number of Iterations\", value=kandinsky_fuse_prefs['n_iterations'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'n_iterations', ptype=\"int\"))\n",
        "    steps = TextField(label=\"Number of Steps\", value=kandinsky_fuse_prefs['steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'steps', ptype=\"int\"))\n",
        "    sampler = Dropdown(label=\"Sampler\", width=200, options=[dropdown.Option(\"ddim_sampler\"), dropdown.Option(\"p_sampler\")], value=kandinsky_fuse_prefs['sampler'], on_change=lambda e:changed(e,'sampler'), col={'xs':12, 'md':6})\n",
        "    n_images = NumberPicker(label=\"Number of Images\", min=1, max=9, step=1, value=kandinsky_fuse_prefs['num_images'], on_change=lambda e:changed(e,'num_images', ptype=\"int\"))\n",
        "    param_rows = ResponsiveRow([Column([batch_folder_name, n_images], col={'xs':12, 'md':6}), \n",
        "                      Column([file_prefix, sampler], col={'xs':12, 'md':6})\n",
        "                      #Column([steps, ddim_eta, dynamic_threshold_v], col={'xs':12, 'md':6})\n",
        "                      ], vertical_alignment=CrossAxisAlignment.START)\n",
        "    steps = SliderRow(label=\"Number of Steps\", min=0, max=200, divisions=200, pref=kandinsky_fuse_prefs, key='steps')\n",
        "    prior_cf_scale = SliderRow(label=\"Prior CF Scale\", min=0, max=10, divisions=10, pref=kandinsky_fuse_prefs, key='prior_cf_scale')\n",
        "    prior_steps = SliderRow(label=\"Prior Steps\", min=0, max=50, divisions=50, pref=kandinsky_fuse_prefs, key='prior_steps')\n",
        "    guidance = SliderRow(label=\"Guidance Scale\", min=0, max=50, divisions=50, pref=kandinsky_fuse_prefs, key='guidance_scale')\n",
        "    width_slider = SliderRow(label=\"Width\", min=128, max=1024, divisions=14, multiple=32, suffix=\"px\", pref=kandinsky_fuse_prefs, key='width')\n",
        "    height_slider = SliderRow(label=\"Height\", min=128, max=1024, divisions=14, multiple=32, suffix=\"px\", pref=kandinsky_fuse_prefs, key='height')\n",
        "    #mask_image = TextField(label=\"Mask Image\", value=kandinsky_fuse_prefs['mask_image'], on_change=lambda e:changed(e,'mask_image'), expand=True, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD_OUTLINED, on_click=pick_mask), col={'xs':10, 'md':5})\n",
        "    #invert_mask = Checkbox(label=\"Invert\", tooltip=\"Swaps the Black & White of your Mask Image\", value=kandinsky_fuse_prefs['invert_mask'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'invert_mask'), col={'xs':2, 'md':1})\n",
        "    #image_pickers = Container(content=ResponsiveRow([init_image, mask_image, invert_mask]), padding=padding.only(top=5), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    #img_block = Container(Column([image_pickers, weight_slider, Divider(height=9, thickness=2)]), padding=padding.only(top=5), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    apply_ESRGAN_upscale = Switch(label=\"Apply ESRGAN Upscale\", value=kandinsky_fuse_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)\n",
        "    enlarge_scale_slider = SliderRow(label=\"Enlarge Scale\", min=1, max=4, divisions=6, round=1, suffix=\"x\", pref=kandinsky_fuse_prefs, key='enlarge_scale')\n",
        "    face_enhance = Checkbox(label=\"Use Face Enhance GPFGAN\", value=kandinsky_fuse_prefs['face_enhance'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'face_enhance'))\n",
        "    display_upscaled_image = Checkbox(label=\"Display Upscaled Image\", value=kandinsky_fuse_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))\n",
        "    ESRGAN_settings = Container(Column([enlarge_scale_slider, face_enhance, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_kandinsky_fuse = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_kandinsky_fuse.height = None if status['installed_ESRGAN'] else 0\n",
        "    if not kandinsky_fuse_prefs['apply_ESRGAN_upscale']:\n",
        "        ESRGAN_settings.height = 0\n",
        "    parameters_button = ElevatedButton(content=Text(value=\"üí•   Run Kandinsky 2.1 Fuser\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_kandinsky_fuse(page))\n",
        "\n",
        "    parameters_row = Row([parameters_button], alignment=MainAxisAlignment.SPACE_BETWEEN)\n",
        "    page.kandinsky_fuse_output = Column([])\n",
        "    c = Column([Container(\n",
        "        padding=padding.only(18, 14, 20, 10), content=Column([\n",
        "            Header(\"üí£  Kandinsky 2.1 Fuse\", \"Mix multiple Images and Prompts together. A Latent Diffusion model with two Multilingual text encoders, supports 100+ languages, made in Russia.\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with Kandinsky Settings\", on_click=kandinsky_help)]),\n",
        "            prompt_row,\n",
        "            image_row,\n",
        "            weight_slider,\n",
        "            Divider(height=5, thickness=4),\n",
        "            fuse_layers,\n",
        "            #Divider(height=2, thickness=2),\n",
        "            #param_rows, #dropdown_row, \n",
        "            steps, prior_steps, prior_cf_scale,\n",
        "            guidance, width_slider, height_slider, #Divider(height=9, thickness=2), \n",
        "            Row([batch_folder_name, file_prefix]),\n",
        "            Row([n_images, sampler]),\n",
        "            page.ESRGAN_block_kandinsky_fuse,\n",
        "            #(img_block if status['installed_img2img'] or status['installed_stability'] else Container(content=None)), (clip_block if prefs['install_CLIP_guided'] else Container(content=None)), (ESRGAN_block if prefs['install_ESRGAN'] else Container(content=None)), \n",
        "            parameters_row,\n",
        "            page.kandinsky_fuse_output\n",
        "        ],\n",
        "    ))], scroll=ScrollMode.AUTO)#batch_folder_name, batch_size, n_iterations, steps, ddim_eta, seed, \n",
        "    return c\n",
        "\n",
        "deep_daze_prefs = {\n",
        "    'prompt': '',\n",
        "    'num_layers': 32,\n",
        "    'save_every': 20,\n",
        "    'max_size': 512,\n",
        "    'save_progress': False,\n",
        "    'learning_rate': 1e-5,\n",
        "    'iterations': 1050,\n",
        "    'num_images': 1,\n",
        "    'batch_folder_name': '',\n",
        "    'file_prefix': 'daze-',\n",
        "    \"apply_ESRGAN_upscale\": prefs['apply_ESRGAN_upscale'],\n",
        "    \"enlarge_scale\": 2.0,\n",
        "    \"display_upscaled_image\": False,\n",
        "}\n",
        "\n",
        "def buildDeepDaze(page):\n",
        "    global deep_daze_prefs, prefs, pipe_deep_daze\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            deep_daze_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            deep_daze_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            deep_daze_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def add_to_deep_daze_output(o):\n",
        "      page.deep_daze_output.controls.append(o)\n",
        "      page.deep_daze_output.update()\n",
        "      if not clear_button.visible:\n",
        "        clear_button.visible = True\n",
        "        clear_button.update()\n",
        "    page.add_to_deep_daze_output = add_to_deep_daze_output\n",
        "    def clear_output(e):\n",
        "      if prefs['enable_sounds']: page.snd_delete.play()\n",
        "      page.deep_daze_output.controls = []\n",
        "      page.deep_daze_output.update()\n",
        "      clear_button.visible = False\n",
        "      clear_button.update()\n",
        "    def deep_daze_help(e):\n",
        "      def close_deep_daze_dlg(e):\n",
        "        nonlocal deep_daze_help_dlg\n",
        "        deep_daze_help_dlg.open = False\n",
        "        page.update()\n",
        "      deep_daze_help_dlg = AlertDialog(title=Text(\"üôÖ   Help with DeepDaze Pipeline\"), content=Column([\n",
        "          Text(\"Text to image generation using OpenAI's CLIP and Siren. Credit goes to Ryan Murdock for the discovery of this technique (and for coming up with the great name)!\"),\n",
        "          Text(\"Heavily influenced by Alexander Mordvintsev's Deep Dream, this work uses CLIP to match an image learned by a SIREN network with a given textual description.\"),\n",
        "          #Markdown(\"\"),\n",
        "        ], scroll=ScrollMode.AUTO), actions=[TextButton(\"üòµ‚Äçüí´  Why not... \", on_click=close_deep_daze_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = deep_daze_help_dlg\n",
        "      deep_daze_help_dlg.open = True\n",
        "      page.update()\n",
        "    def toggle_ESRGAN(e):\n",
        "        ESRGAN_settings.height = None if e.control.value else 0\n",
        "        deep_daze_prefs['apply_ESRGAN_upscale'] = e.control.value\n",
        "        ESRGAN_settings.update()\n",
        "    prompt = TextField(label=\"Prompt Text\", value=deep_daze_prefs['prompt'], on_change=lambda e:changed(e,'prompt'))\n",
        "    max_row = SliderRow(label=\"Max Resolution Size\", min=256, max=1024, divisions=12, multiple=32, suffix=\"px\", pref=deep_daze_prefs, key='max_size')\n",
        "    #num_layers = TextField(label=\"Inference Steps\", value=str(deep_daze_prefs['num_layers']), keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'num_layers', ptype='int'))\n",
        "    learning_rate = TextField(label=\"Learning Rate\", width=130, value=float(deep_daze_prefs['learning_rate']), keyboard_type=KeyboardType.NUMBER, tooltip=\"The learning rate of the neural net.\", on_change=lambda e:changed(e,'learning_rate', ptype='float'))\n",
        "    num_layers_row = SliderRow(label=\"Number of Layers\", min=1, max=100, divisions=99, pref=deep_daze_prefs, key='num_layers', tooltip=\"The number of hidden layers to use with Siren neural network\")\n",
        "    save_every_row = SliderRow(label=\"Save/Show Every x Steps\", min=1, max=100, divisions=99, pref=deep_daze_prefs, key='save_every', tooltip=\"Generate an image every time iterations is a multiple of this number.\")\n",
        "    iterations_row = SliderRow(label=\"Number of Iterations\", min=1, max=2000, divisions=1999, pref=deep_daze_prefs, key='iterations', tooltip=\"The number of times to calculate and backpropogate loss in a given epoch.\")\n",
        "    batch_folder_name = TextField(label=\"Batch Folder Name\", value=deep_daze_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    file_prefix = TextField(label=\"Filename Prefix\", value=deep_daze_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))\n",
        "    save_progress = Tooltip(message=\"Whether or not to save images generated before training Siren is complete.\", content=Switch(label=\"Save Progress Steps\", value=deep_daze_prefs['save_progress'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'save_progress')))\n",
        "    #eta = TextField(label=\"ETA\", value=str(deep_daze_prefs['eta']), keyboard_type=KeyboardType.NUMBER, hint_text=\"Amount of Noise\", on_change=lambda e:changed(e,'eta', ptype='float'))\n",
        "    #eta = Slider(min=0.0, max=1.0, divisions=20, label=\"{value}\", value=float(deep_daze_prefs['eta']), tooltip=\"The weight of noise for added noise in a diffusion step. Its value is between 0.0 and 1.0 - 0.0 is DDIM and 1.0 is DDPM scheduler respectively.\", expand=True, on_change=lambda e:changed(e,'eta', ptype='float'))\n",
        "    #eta_row = Row([Text(\"DDIM ETA: \"), eta])\n",
        "    #max_size = Slider(min=256, max=1280, divisions=64, label=\"{value}px\", value=int(deep_daze_prefs['max_size']), expand=True, on_change=lambda e:changed(e,'max_size', ptype='int'))\n",
        "    #max_row = Row([Text(\"Max Resolution Size: \"), max_size])\n",
        "    apply_ESRGAN_upscale = Switch(label=\"Apply ESRGAN Upscale\", value=deep_daze_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)\n",
        "    enlarge_scale_slider = SliderRow(label=\"Enlarge Scale\", min=1, max=4, divisions=6, round=1, suffix=\"x\", pref=deep_daze_prefs, key='enlarge_scale')\n",
        "    display_upscaled_image = Checkbox(label=\"Display Upscaled Image\", value=deep_daze_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))\n",
        "    ESRGAN_settings = Container(Column([enlarge_scale_slider, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_deep_daze = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_deep_daze.height = None if status['installed_ESRGAN'] else 0\n",
        "    if not deep_daze_prefs['apply_ESRGAN_upscale']:\n",
        "        ESRGAN_settings.height = 0\n",
        "    page.deep_daze_output = Column([], auto_scroll=True)\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(page.deep_daze_output.controls) > 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üëÄ  DeepDaze Text-to-Image Generator\", \"An alternative method using OpenAI's CLIP and Siren. Made a few years ago but still facinating results....\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with DeepDaze Settings\", on_click=deep_daze_help)]),\n",
        "        prompt,\n",
        "        num_layers_row,\n",
        "        iterations_row,\n",
        "        Row([learning_rate, save_progress]),\n",
        "        save_every_row,\n",
        "        max_row,\n",
        "        #NumberPicker(label=\"Number of Images: \", min=1, max=20, value=deep_daze_prefs['num_images'], on_change=lambda e: changed(e, 'num_images')), \n",
        "        Row([batch_folder_name, file_prefix]),\n",
        "        page.ESRGAN_block_deep_daze,\n",
        "        Row([ElevatedButton(content=Text(\"üò∂   Get DeepDaze Generation\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_deep_daze(page)), \n",
        "             #ElevatedButton(content=Text(value=\"üìú   Run from Prompts List\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_deep_daze(page, from_list=True))\n",
        "        ]),\n",
        "      ]\n",
        "    )), page.deep_daze_output,\n",
        "        clear_button,\n",
        "    ], scroll=ScrollMode.AUTO, auto_scroll=False)\n",
        "    return c\n",
        "\n",
        "CLIPstyler_prefs = {\n",
        "    'source':'a photo',\n",
        "    'prompt_text': 'Detailed oil painting',\n",
        "    'batch_folder_name': 'clipstyler',\n",
        "    'crop_size': 128,\n",
        "    'num_crops': 64,\n",
        "    'original_image': '',\n",
        "    'image_dir': \"\",\n",
        "    'training_iterations': 100,\n",
        "    'width': 512,\n",
        "    'height': 512,\n",
        "    \"apply_ESRGAN_upscale\": prefs['apply_ESRGAN_upscale'],\n",
        "    \"enlarge_scale\": prefs['enlarge_scale'],\n",
        "    \"display_upscaled_image\": prefs['display_upscaled_image'],\n",
        "}\n",
        "\n",
        "def buildCLIPstyler(page):\n",
        "    global CLIPstyler, prefs\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            CLIPstyler_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            CLIPstyler_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            CLIPstyler_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def pick_files_result(e: FilePickerResultEvent):\n",
        "        if e.files:\n",
        "            img = e.files\n",
        "            uf = []\n",
        "            fname = img[0]\n",
        "            print(\", \".join(map(lambda f: f.name, e.files)))\n",
        "            src_path = page.get_upload_url(fname.name, 600)\n",
        "            uf.append(FilePickerUploadFile(fname.name, upload_url=src_path))\n",
        "            pick_files_dialog.upload(uf)\n",
        "            print(str(src_path))\n",
        "            #src_path = ''.join(src_path)\n",
        "            print(str(uf[0]))\n",
        "            dst_path = os.path.join(root_dir, fname.name)\n",
        "            print(f'Copy {src_path} to {dst_path}')\n",
        "            #shutil.copy(src_path, dst_path)\n",
        "            # TODO: is original or mask?\n",
        "            original_image.value = dst_path\n",
        "\n",
        "    pick_files_dialog = FilePicker(on_result=pick_files_result)\n",
        "    page.overlay.append(pick_files_dialog)\n",
        "    #selected_files = Text()\n",
        "\n",
        "    def file_picker_result(e: FilePickerResultEvent):\n",
        "        if e.files != None:\n",
        "            upload_files(e)\n",
        "    def on_upload_progress(e: FilePickerUploadEvent):\n",
        "        if e.progress == 1:\n",
        "            if not slash in e.file_name:\n",
        "              fname = os.path.join(root_dir, e.file_name)\n",
        "            else:\n",
        "              fname = e.file_name\n",
        "            original_image.value = fname\n",
        "            original_image.update()\n",
        "            CLIPstyler_prefs['original_image'] = fname\n",
        "            page.update()\n",
        "    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)\n",
        "    def upload_files(e):\n",
        "        uf = []\n",
        "        if file_picker.result != None and file_picker.result.files != None:\n",
        "            for f in file_picker.result.files:\n",
        "              if page.web:\n",
        "                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))\n",
        "              else:\n",
        "                on_upload_progress(FilePickerUploadEvent(f.path, 1, \"\"))\n",
        "            file_picker.upload(uf)\n",
        "    page.overlay.append(file_picker)\n",
        "    pick_type = \"\"\n",
        "    #page.overlay.append(pick_files_dialog)\n",
        "    def pick_original(e):\n",
        "        file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"png\", \"PNG\", \"jpg\", \"jpeg\"], dialog_title=\"Pick original Image File\")\n",
        "    def toggle_ESRGAN(e):\n",
        "        ESRGAN_settings.height = None if e.control.value else 0\n",
        "        CLIPstyler_prefs['apply_ESRGAN_upscale'] = e.control.value\n",
        "        ESRGAN_settings.update()\n",
        "        has_changed = True\n",
        "    prompt_text = TextField(label=\"Stylized Prompt Text\", value=CLIPstyler_prefs['prompt_text'], on_change=lambda e:changed(e,'prompt_text'))\n",
        "    batch_folder_name = TextField(label=\"Batch Folder Name\", value=CLIPstyler_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    source = TextField(label=\"Source Type\", value=CLIPstyler_prefs['source'], on_change=lambda e:changed(e,'source'))\n",
        "    #training_iterations = TextField(label=\"Training Iterations\", value=CLIPstyler_prefs['training_iterations'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'training_iterations', ptype=\"int\"))\n",
        "    crop_size = TextField(label=\"Crop Size\", value=CLIPstyler_prefs['crop_size'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'crop_size', ptype=\"int\"))\n",
        "    num_crops = TextField(label=\"Number of Crops\", value=CLIPstyler_prefs['num_crops'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'num_crops', ptype=\"int\"))\n",
        "    param_rows = Column([Row([batch_folder_name, source]), Row([crop_size, num_crops])])\n",
        "    iterations = SliderRow(label=\"Training Iterations\", min=50, max=500, divisions=90, pref=CLIPstyler_prefs, key='training_iterations')   \n",
        "    \n",
        "    width_slider = SliderRow(label=\"Width\", min=128, max=1024, divisions=14, multiple=32, suffix=\"px\", pref=CLIPstyler_prefs, key='width')\n",
        "    height_slider = SliderRow(label=\"Height\", min=128, max=1024, divisions=14, multiple=32, suffix=\"px\", pref=CLIPstyler_prefs, key='height')\n",
        "    original_image = TextField(label=\"Original Image\", value=CLIPstyler_prefs['original_image'], on_change=lambda e:changed(e,'original_image'), expand=True, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_original, col={\"*\":1, \"md\":3}))\n",
        "    #mask_image = TextField(label=\"Mask Image\", value=CLIPstyler_prefs['mask_image'], on_change=lambda e:changed(e,'mask_image'), expand=True, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD_OUTLINED, on_click=pick_mask, col={\"*\":1, \"md\":3}))\n",
        "    #invert_mask = Checkbox(label=\"Invert\", tooltip=\"Swaps the Black & White of your Mask Image\", value=CLIPstyler_prefs['invert_mask'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'invert_mask'))\n",
        "    image_picker = Container(content=Row([original_image]), padding=padding.only(top=5), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    #prompt_strength = Slider(min=0.1, max=0.9, divisions=16, label=\"{value}%\", value=CLIPstyler_prefs['prompt_strength'], on_change=change_strength, expand=True)\n",
        "    #strength_value = Text(f\" {int(CLIPstyler_prefs['prompt_strength'] * 100)}%\", weight=FontWeight.BOLD)\n",
        "    #strength_slider = Row([Text(\"Prompt Strength: \"), strength_value, prompt_strength])\n",
        "    #img_block = Container(Column([image_pickers, strength_slider, Divider(height=9, thickness=2)]), padding=padding.only(top=5), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    apply_ESRGAN_upscale = Switch(label=\"Apply ESRGAN Upscale\", value=CLIPstyler_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)\n",
        "    enlarge_scale_slider = SliderRow(label=\"Enlarge Scale\", min=1, max=4, divisions=6, round=1, suffix=\"x\", pref=CLIPstyler_prefs, key='enlarge_scale')\n",
        "    #face_enhance = Checkbox(label=\"Use Face Enhance GPFGAN\", value=CLIPstyler_prefs['face_enhance'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'face_enhance'))\n",
        "    display_upscaled_image = Checkbox(label=\"Display Upscaled Image\", value=CLIPstyler_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))\n",
        "    ESRGAN_settings = Container(Column([enlarge_scale_slider, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_styler = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_styler.height = None if status['installed_ESRGAN'] else 0\n",
        "    if not CLIPstyler_prefs['apply_ESRGAN_upscale']:\n",
        "        ESRGAN_settings.height = 0\n",
        "    parameters_button = ElevatedButton(content=Text(value=\"üìé   Run CLIP-Styler\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_CLIPstyler(page))\n",
        "\n",
        "    parameters_row = Row([parameters_button], alignment=MainAxisAlignment.SPACE_BETWEEN)\n",
        "    page.CLIPstyler_output = Column([])\n",
        "    c = Column([Container(\n",
        "        padding=padding.only(18, 14, 20, 10), content=Column([\n",
        "            Header(\"üòé   CLIP-Styler\", \"Transfers a Text Guided Style onto your Image From Prompt Description...\"),\n",
        "            image_picker, prompt_text,\n",
        "            param_rows, iterations, width_slider, height_slider, #Divider(height=9, thickness=2), \n",
        "            page.ESRGAN_block_styler,\n",
        "            #(img_block if status['installed_img2img'] or status['installed_stability'] else Container(content=None)), (clip_block if prefs['install_CLIP_guided'] else Container(content=None)), (ESRGAN_block if prefs['install_ESRGAN'] else Container(content=None)), \n",
        "            parameters_row,\n",
        "            page.CLIPstyler_output\n",
        "        ],\n",
        "    ))], scroll=ScrollMode.AUTO)#batch_folder_name, batch_size, n_iterations, steps, crop_size, num_crops, \n",
        "    return c\n",
        "\n",
        "semantic_prefs = {\n",
        "    'prompt': '',\n",
        "    'negative_prompt': '',\n",
        "    'num_inference_steps': 100,\n",
        "    'guidance_scale': 7.5,\n",
        "    'edit_momentum_scale': 0.1,\n",
        "    'edit_mom_beta': 0.4,\n",
        "    'editing_prompts': [],\n",
        "    'eta': 0.0,\n",
        "    'seed': 0,\n",
        "    'width': 960,\n",
        "    'height': 768,\n",
        "    'num_images': 1,\n",
        "    'batch_folder_name': '',\n",
        "    \"apply_ESRGAN_upscale\": prefs['apply_ESRGAN_upscale'],\n",
        "    \"enlarge_scale\": 2.0,\n",
        "    \"display_upscaled_image\": False,\n",
        "}\n",
        "\n",
        "def buildSemanticGuidance(page):\n",
        "    global semantic_prefs, prefs, pipe_semantic, editing_prompt\n",
        "    editing_prompt = {'editing_prompt':'', 'edit_warmup_steps':10, 'edit_guidance_scale':5, 'edit_threshold':0.9, 'edit_weights':1, 'reverse_editing_direction': False}\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "      if pref is not None:\n",
        "        try:\n",
        "          if ptype == \"int\":\n",
        "            semantic_prefs[pref] = int(e.control.value)\n",
        "          elif ptype == \"float\":\n",
        "            semantic_prefs[pref] = float(e.control.value)\n",
        "          else:\n",
        "            semantic_prefs[pref] = e.control.value\n",
        "        except Exception:\n",
        "          alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "          pass\n",
        "    def add_to_semantic_output(o):\n",
        "      page.semantic_output.controls.append(o)\n",
        "      page.semantic_output.update()\n",
        "      if not clear_button.visible:\n",
        "        clear_button.visible = True\n",
        "        clear_button.update()\n",
        "    def clear_output(e):\n",
        "      if prefs['enable_sounds']: page.snd_delete.play()\n",
        "      page.semantic_output.controls = []\n",
        "      page.semantic_output.update()\n",
        "      clear_button.visible = False\n",
        "      clear_button.update()\n",
        "    def semantic_help(e):\n",
        "      def close_semantic_dlg(e):\n",
        "        nonlocal semantic_help_dlg\n",
        "        semantic_help_dlg.open = False\n",
        "        page.update()\n",
        "      semantic_help_dlg = AlertDialog(title=Text(\"üíÅ   Help with Semantic Guidance\"), content=Column([\n",
        "          Text(\"SEGA allows applying or removing one or more concepts from an image. The strength of the concept can also be controlled. I.e. the smile concept can be used to incrementally increase or decrease the smile of a portrait. Similar to how classifier free guidance provides guidance via empty prompt inputs, SEGA provides guidance on conceptual prompts. Multiple of these conceptual prompts can be applied simultaneously. Each conceptual prompt can either add or remove their concept depending on if the guidance is applied positively or negatively.\"),\n",
        "          Text(\"Unlike Pix2Pix Zero or Attend and Excite, SEGA directly interacts with the diffusion process instead of performing any explicit gradient-based optimization.\"),\n",
        "          Text(\"Semantic Guidance for Diffusion Models was proposed in SEGA: Instructing Diffusion using Semantic Dimensions and provides strong semantic control over the image generation. Small changes to the text prompt usually result in entirely different output images. However, with SEGA a variety of changes to the image are enabled that can be controlled easily and intuitively, and stay true to the original image composition.\"),\n",
        "          Text(\"Text-to-image diffusion models have recently received a lot of interest for their astonishing ability to produce high-fidelity images from text only. However, achieving one-shot generation that aligns with the user's intent is nearly impossible, yet small changes to the input prompt often result in very different images. This leaves the user with little semantic control. To put the user in control, we show how to interact with the diffusion process to flexibly steer it along semantic directions. This semantic guidance (SEGA) allows for subtle and extensive edits, changes in composition and style, as well as optimizing the overall artistic conception. We demonstrate SEGA's effectiveness on a variety of tasks and provide evidence for its versatility and flexibility.\"),\n",
        "          Markdown(\"[HuggingFace Documentation](https://huggingface.co/docs/diffusers/main/en/api/pipelines/semantic_stable_diffusion) - [Paper](https://arxiv.org/abs/2301.12247)\", on_tap_link=lambda e: e.page.launch_url(e.data)),\n",
        "        ], scroll=ScrollMode.AUTO), actions=[TextButton(\"üòñ  More Control Please... \", on_click=close_semantic_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "      page.dialog = semantic_help_dlg\n",
        "      semantic_help_dlg.open = True\n",
        "      page.update()\n",
        "    def toggle_ESRGAN(e):\n",
        "        ESRGAN_settings.height = None if e.control.value else 0\n",
        "        semantic_prefs['apply_ESRGAN_upscale'] = e.control.value\n",
        "        ESRGAN_settings.update()\n",
        "    def change_eta(e):\n",
        "        changed(e, 'eta', ptype=\"float\")\n",
        "        eta_value.value = f\" {semantic_prefs['eta']}\"\n",
        "        eta_value.update()\n",
        "        eta_row.update()\n",
        "    def semantic_tile(semantic_prompt):\n",
        "        params = []\n",
        "        for k, v in semantic_prompt.items():\n",
        "            if k == 'editing_prompt': continue\n",
        "            params.append(f'{to_title(k)}: {v}')\n",
        "        sub = ', '.join(params)\n",
        "        return ListTile(title=Text(semantic_prompt['editing_prompt'], max_lines=6, style=TextThemeStyle.BODY_LARGE), subtitle=Text(sub), dense=True, data=semantic_prompt, trailing=PopupMenuButton(icon=icons.MORE_VERT,\n",
        "          items=[PopupMenuItem(icon=icons.EDIT, text=\"Edit Semantic Prompt\", on_click=lambda e: edit_semantic(semantic_prompt), data=semantic_prompt),\n",
        "                 PopupMenuItem(icon=icons.DELETE, text=\"Delete Semantic Prompt\", on_click=lambda e: del_semantic(semantic_prompt), data=semantic_prompt)]), on_click=lambda e: edit_semantic(semantic_prompt))\n",
        "    def edit_semantic(edit=None):\n",
        "        semantic_prompt = edit if bool(edit) else editing_prompt.copy()\n",
        "        edit_prompt = edit['editing_prompt'] if bool(edit) else \"\"\n",
        "        def close_dlg(e):\n",
        "            dlg_edit.open = False\n",
        "            page.update()\n",
        "        def changed_p(e, pref=None):\n",
        "            if pref is not None:\n",
        "                semantic_prompt[pref] = e.control.value\n",
        "        def save_semantic_prompt(e):\n",
        "            if edit == None:\n",
        "                semantic_prefs['editing_prompts'].append(semantic_prompt)\n",
        "                page.semantic_prompts.controls.append(semantic_tile(semantic_prompt))\n",
        "                page.semantic_prompts.update()\n",
        "            else:\n",
        "                for s in semantic_prefs['editing_prompts']:\n",
        "                    if s['editing_prompt'] == edit_prompt:\n",
        "                        s = semantic_prompt\n",
        "                        break\n",
        "                for t in page.semantic_prompts.controls:\n",
        "                    if t.data['editing_prompt'] == edit_prompt:\n",
        "                        params = []\n",
        "                        for k, v in semantic_prompt.items():\n",
        "                            if k == 'editing_prompt': continue\n",
        "                            params.append(f'{to_title(k)}: {v}')\n",
        "                        sub = ', '.join(params)\n",
        "                        t.title = Text(semantic_prompt['editing_prompt'], max_lines=6, style=TextThemeStyle.BODY_LARGE)\n",
        "                        t.subtitle = Text(sub)\n",
        "                        t.data = semantic_prompt\n",
        "                        t.update()\n",
        "                        break\n",
        "            dlg_edit.open = False\n",
        "            e.control.update()\n",
        "            page.update()\n",
        "        semantic_editing_prompt = TextField(label=\"Semantic Editing Prompt Modifier\", value=semantic_prompt['editing_prompt'], autofocus=True, on_change=lambda e:changed_p(e,'editing_prompt'))\n",
        "        edit_warmup_steps = SliderRow(label=\"Edit Warmup Steps\", min=0, max=50, divisions=50, pref=semantic_prompt, key='edit_warmup_steps', tooltip=\"Number of diffusion steps (for each prompt) for which semantic guidance will not be applied. Momentum will still be calculated for those steps and applied once all warmup periods are over.\")\n",
        "        edit_guidance_scale = SliderRow(label=\"Edit Guidance Scale\", min=0, max=20, divisions=40, round=1, pref=semantic_prompt, key='edit_guidance_scale', tooltip=\"Guidance scale for semantic guidance. If provided as list values should correspond to `editing_prompt`.\")\n",
        "        edit_threshold = SliderRow(label=\"Edit Threshold\", min=0, max=1, divisions=40, round=3, pref=semantic_prompt, key='edit_threshold', tooltip=\"Threshold of semantic guidance.\")\n",
        "        edit_weights = SliderRow(label=\"Edit Weights\", min=0, max=10, divisions=20, round=1, pref=semantic_prompt, key='edit_weights', tooltip=\"Indicates how much each individual concept should influence the overall guidance. If no weights are provided all concepts are applied equally.\")\n",
        "        reverse_editing_direction = Checkbox(label=\"Reverse Editing Direction\", value=semantic_prompt['reverse_editing_direction'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed_p(e,'reverse_editing_direction'), tooltip=\"Whether the corresponding prompt in `editing_prompt` should be increased or decreased.\")\n",
        "        dlg_edit = AlertDialog(modal=False, title=Text(f\"‚ôüÔ∏è {'Edit' if bool(edit) else 'Add'} Semantic Prompt\"), content=Container(Column([\n",
        "            semantic_editing_prompt, edit_warmup_steps, edit_guidance_scale, edit_threshold, edit_weights, reverse_editing_direction,\n",
        "        ], alignment=MainAxisAlignment.START, tight=True, scroll=ScrollMode.AUTO), width=(page.width or page.window_width) - 180), actions=[TextButton(content=Text(\"Cancel\", size=18), on_click=close_dlg), ElevatedButton(content=Text(value=emojize(\":floppy_disk:\") + \"  Save Prompt \", size=19, weight=FontWeight.BOLD), on_click=save_semantic_prompt)], actions_alignment=MainAxisAlignment.END)\n",
        "        page.dialog = dlg_edit\n",
        "        dlg_edit.open = True\n",
        "        page.update()\n",
        "    def del_semantic(edit=None):\n",
        "        for s in semantic_prefs['editing_prompts']:\n",
        "            if s['editing_prompt'] == edit['editing_prompt']:\n",
        "                semantic_prefs['editing_prompts'].remove(s)\n",
        "                break\n",
        "        for t in page.semantic_prompts.controls:\n",
        "            if t.data['editing_prompt'] == edit['editing_prompt']:\n",
        "                page.semantic_prompts.controls.remove(t)\n",
        "                break\n",
        "        page.semantic_prompts.update()\n",
        "        if prefs['enable_sounds']: page.snd_delete.play()\n",
        "    def clear_semantic_prompts(e):\n",
        "        semantic_prefs['editing_prompts'].clear()\n",
        "        page.semantic_prompts.controls.clear()\n",
        "        page.semantic_prompts.update()\n",
        "        if prefs['enable_sounds']: page.snd_delete.play()\n",
        "    prompt = TextField(label=\"Base Prompt Text\", value=semantic_prefs['prompt'], col={'md': 9}, multiline=True, on_change=lambda e:changed(e,'prompt'))\n",
        "    negative_prompt  = TextField(label=\"Negative Prompt Text\", value=semantic_prefs['negative_prompt'], col={'md':3}, multiline=True, on_change=lambda e:changed(e,'negative_prompt'))\n",
        "    seed = TextField(label=\"Seed\", width=90, value=str(semantic_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip=\"0 or -1 picks a Random seed\", on_change=lambda e:changed(e,'seed', ptype='int'))\n",
        "    num_inference_row = SliderRow(label=\"Number of Inference Steps\", min=1, max=150, divisions=149, pref=semantic_prefs, key='num_inference_steps', tooltip=\"The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.\")   \n",
        "    guidance = SliderRow(label=\"Guidance Scale\", min=0, max=50, divisions=100, round=1, pref=semantic_prefs, key='guidance_scale')\n",
        "    edit_momentum_scale = SliderRow(label=\"Edit Momentum Scale\", min=0, max=1, divisions=20, round=1, pref=semantic_prefs, key='edit_momentum_scale', tooltip=\"Scale of the momentum to be added to the semantic guidance at each diffusion step. Momentum is already built up during warmup, i.e. for diffusion steps smaller than `sld_warmup_steps`. Momentum will only be added to latent guidance once all warmup periods are finished.\")\n",
        "    edit_mom_beta = SliderRow(label=\"Edit Momentum Beta\", min=0, max=1, divisions=20, round=1, pref=semantic_prefs, key='edit_mom_beta', tooltip=\"Defines how semantic guidance momentum builds up. `edit_mom_beta` indicates how much of the previous momentum will be kept. Momentum is already built up during warmup, i.e. for diffusion steps smaller than `edit_warmup_steps`.\")\n",
        "    #eta = TextField(label=\"ETA\", value=str(semantic_prefs['eta']), keyboard_type=KeyboardType.NUMBER, hint_text=\"Amount of Noise\", on_change=lambda e:changed(e,'eta', ptype='float'))\n",
        "    eta = Slider(min=0.0, max=1.0, divisions=20, label=\"{value}\", value=float(semantic_prefs['eta']), tooltip=\"The weight of noise for added noise in a diffusion step. Its value is between 0.0 and 1.0 - 0.0 is DDIM and 1.0 is DDPM scheduler respectively.\", expand=True, on_change=change_eta)\n",
        "    eta_value = Text(f\" {semantic_prefs['eta']}\", weight=FontWeight.BOLD)\n",
        "    eta_row = Row([Text(\"ETA:\"), eta_value, eta])\n",
        "    page.etas.append(eta_row)\n",
        "    width_slider = SliderRow(label=\"Width\", min=256, max=1280, divisions=64, multiple=16, suffix=\"px\", pref=semantic_prefs, key='width')\n",
        "    height_slider = SliderRow(label=\"Height\", min=256, max=1280, divisions=64, multiple=16, suffix=\"px\", pref=semantic_prefs, key='height')\n",
        "    batch_folder_name = TextField(label=\"Batch Folder Name\", value=semantic_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    apply_ESRGAN_upscale = Switch(label=\"Apply ESRGAN Upscale\", value=semantic_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)\n",
        "    enlarge_scale_slider = SliderRow(label=\"Enlarge Scale\", min=1, max=4, divisions=6, round=1, suffix=\"x\", pref=semantic_prefs, key='enlarge_scale')\n",
        "    display_upscaled_image = Checkbox(label=\"Display Upscaled Image\", value=semantic_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))\n",
        "    ESRGAN_settings = Container(Column([enlarge_scale_slider, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_semantic = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    page.ESRGAN_block_semantic.height = None if status['installed_ESRGAN'] else 0\n",
        "    page.semantic_prompts = Column([], spacing=0)\n",
        "    page.semantic_output = Column([])\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(page.semantic_output.controls) > 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üß©  Semantic Guidance for Diffusion Models - SEGA\", \"Text-to-Image Generation with Latent Editing to apply or remove multiple concepts from an image with advanced controls....\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with Instruct-Pix2Pix Settings\", on_click=semantic_help)]),\n",
        "        #ResponsiveRow([Row([original_image, alpha_mask], col={'lg':6}), Row([mask_image, invert_mask], col={'lg':6})]),\n",
        "        ResponsiveRow([prompt, negative_prompt]),\n",
        "        Row([Text(\"Editing Semantic Prompts\", style=TextThemeStyle.TITLE_LARGE, weight=FontWeight.BOLD), \n",
        "                    Row([ft.FilledTonalButton(\"Clear Prompts\", on_click=clear_semantic_prompts), ft.FilledButton(\"Add Editing Prompt\", on_click=lambda e: edit_semantic(None))])], alignment=MainAxisAlignment.SPACE_BETWEEN),\n",
        "        page.semantic_prompts,\n",
        "        Divider(thickness=2, height=4),\n",
        "        num_inference_row,\n",
        "        guidance,\n",
        "        edit_momentum_scale, edit_mom_beta,\n",
        "        eta_row,\n",
        "        width_slider, height_slider,\n",
        "        Row([NumberPicker(label=\"Number of Images: \", min=1, max=8, value=semantic_prefs['num_images'], on_change=lambda e: changed(e, 'num_images')), seed, batch_folder_name]),\n",
        "        page.ESRGAN_block_semantic,\n",
        "        #Row([jump_length, jump_n_sample, seed]),\n",
        "        Row([\n",
        "            ElevatedButton(content=Text(\"üé≥  Run Semantic Guidance\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_semantic(page)),\n",
        "             #ElevatedButton(content=Text(value=\"üìú   Run from Prompts List\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_semantic(page, from_list=True))\n",
        "        ]),\n",
        "        page.semantic_output,\n",
        "        clear_button,\n",
        "      ]\n",
        "    ))], scroll=ScrollMode.AUTO, auto_scroll=False)\n",
        "    return c\n",
        "\n",
        "#TODO: Waiting for Scribbler addon integration\n",
        "def buildDreamMask(page):\n",
        "    #prog_bars: Dict[str, ProgressRing] = {}\n",
        "    files = Ref[Column]()\n",
        "    #upload_button = Ref[ElevatedButton]()\n",
        "\n",
        "    def file_picker_result(e: FilePickerResultEvent):\n",
        "        files.current.controls.clear()\n",
        "        if e.files != None:\n",
        "          upload_files(e)\n",
        "    def on_upload_progress(e: FilePickerUploadEvent):\n",
        "      if e.progress == 1:\n",
        "        if not slash in e.file_name:\n",
        "          fname = os.path.join(root_dir, e.file_name)\n",
        "        else:\n",
        "          fname = e.file_name\n",
        "        files.current.controls.append(Row([Text(f\"Done uploading {root_dir}{e.file_name}\")]))\n",
        "        page.update()\n",
        "    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)\n",
        "    def upload_files(e):\n",
        "        uf = []\n",
        "        if file_picker.result != None and file_picker.result.files != None:\n",
        "            for f in file_picker.result.files:\n",
        "              if page.web:\n",
        "                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))\n",
        "              else:\n",
        "                on_upload_progress(FilePickerUploadEvent(f.path, 1, \"\"))\n",
        "            file_picker.upload(uf)\n",
        "    page.overlay.append(file_picker)\n",
        "\n",
        "    c = Column([\n",
        "        ElevatedButton(\n",
        "            \"Select Init Image to Mask...\",\n",
        "            icon=icons.FOLDER_OPEN,\n",
        "            on_click=lambda _: file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"png\", \"PNG\"], dialog_title=\"Pick Init Image File\" ),\n",
        "        ),\n",
        "        Column(ref=files),\n",
        "    ])\n",
        "    return c\n",
        "\n",
        "dreambooth_prefs = {\n",
        "    'instance_prompt': '',\n",
        "    'prior_preservation': False,\n",
        "    'prior_preservation_class_prompt': \"\",\n",
        "    'num_class_images': 12,\n",
        "    'sample_batch_size': 2,\n",
        "    'train_batch_size': 1,\n",
        "    'prior_loss_weight': 0.5,\n",
        "    'prior_preservation_class_folder': os.path.join(root_dir, \"class_images\"),\n",
        "    'learning_rate': 5e-06,\n",
        "    'max_train_steps': 450,\n",
        "    'seed': 222476,\n",
        "    'name_of_your_concept': \"\",\n",
        "    'save_concept': True,\n",
        "    'where_to_save_concept': \"Public Library\",\n",
        "    'max_size': 512,\n",
        "    'image_path': '',\n",
        "    'readme_description': '',\n",
        "    'urls': [],\n",
        "}\n",
        "\n",
        "def buildDreamBooth(page):\n",
        "    global prefs, dreambooth_prefs\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "        if pref is not None:\n",
        "          try:\n",
        "            if ptype == \"int\":\n",
        "              dreambooth_prefs[pref] = int(e.control.value)\n",
        "            elif ptype == \"float\":\n",
        "              dreambooth_prefs[pref] = float(e.control.value)\n",
        "            else:\n",
        "              dreambooth_prefs[pref] = e.control.value\n",
        "          except Exception:\n",
        "            alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "            pass\n",
        "    def add_to_dreambooth_output(o):\n",
        "        page.dreambooth_output.controls.append(o)\n",
        "        page.dreambooth_output.update()\n",
        "    def clear_output(e):\n",
        "        if prefs['enable_sounds']: page.snd_delete.play()\n",
        "        page.dreambooth_output.controls = []\n",
        "        page.dreambooth_output.update()\n",
        "        clear_button.visible = False\n",
        "        clear_button.update()\n",
        "    def db_help(e):\n",
        "        def close_db_dlg(e):\n",
        "          nonlocal db_help_dlg\n",
        "          db_help_dlg.open = False\n",
        "          page.update()\n",
        "        db_help_dlg = AlertDialog(title=Text(\"üíÅ   Help with DreamBooth\"), content=Column([\n",
        "            Text(\"First thing is to collect all your own images that you want to teach it to dream.  Feed it at least 5 square pictures of the object or style to learn, and it'll save your Custom Model Checkpoint.\"),\n",
        "            Text(\"Fine-tune your perameters, but be aware that the training process takes a long time to run, so careful with the settings if you don't have the patience or processor. Dream at your own risk.\"),\n",
        "          ], scroll=ScrollMode.AUTO), actions=[TextButton(emojize(':sleepy_face:') + \"  Got it... \", on_click=close_db_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "        page.dialog = db_help_dlg\n",
        "        db_help_dlg.open = True\n",
        "        page.update()\n",
        "    def delete_image(e):\n",
        "        f = e.control.data\n",
        "        if os.path.isfile(f):\n",
        "          os.remove(f)\n",
        "          for i, fl in enumerate(page.db_file_list.controls):\n",
        "            if fl.title.value == f:\n",
        "              del page.db_file_list.controls[i]\n",
        "              page.db_file_list.update()\n",
        "              continue\n",
        "    def delete_all_images(e):\n",
        "        for fl in page.db_file_list.controls:\n",
        "          f = fl.title.value\n",
        "          if os.path.isfile(f):\n",
        "            os.remove(f)\n",
        "        page.db_file_list.controls.clear()\n",
        "        page.db_file_list.update()\n",
        "    def add_file(fpath, update=True):\n",
        "        page.db_file_list.controls.append(ListTile(title=Text(fpath), dense=True, trailing=PopupMenuButton(icon=icons.MORE_VERT,\n",
        "          items=[#TODO: View Image\n",
        "              PopupMenuItem(icon=icons.DELETE, text=\"Delete Image\", on_click=delete_image, data=fpath),\n",
        "              PopupMenuItem(icon=icons.DELETE_SWEEP, text=\"Delete All\", on_click=delete_all_images, data=fpath),\n",
        "          ])))\n",
        "        if update: page.db_file_list.update()\n",
        "    def file_picker_result(e: FilePickerResultEvent):\n",
        "        if e.files != None:\n",
        "          upload_files(e)\n",
        "    save_dir = os.path.join(root_dir, 'my_concept')\n",
        "    def on_upload_progress(e: FilePickerUploadEvent):\n",
        "        if e.progress == 1:\n",
        "          if not os.path.exists(save_dir):\n",
        "            os.mkdir(save_dir)\n",
        "          if not slash in e.file_name:\n",
        "            fname = os.path.join(root_dir, e.file_name)\n",
        "            fpath = os.path.join(save_dir, e.file_name)\n",
        "          else:\n",
        "            fname = e.file_name\n",
        "            fpath = os.path.join(save_dir, e.file_name.rpartition(slash)[2])\n",
        "          original_img = PILImage.open(fname)\n",
        "          width, height = original_img.size\n",
        "          width, height = scale_dimensions(width, height, dreambooth_prefs['max_size'])\n",
        "          original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert(\"RGB\")\n",
        "          original_img.save(fpath)\n",
        "          if page.web: os.remove(fname)\n",
        "          #shutil.move(fname, fpath)\n",
        "          add_file(fpath)\n",
        "    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)\n",
        "    def pick_path(e):\n",
        "        file_picker.pick_files(allow_multiple=True, allowed_extensions=[\"png\", \"PNG\", \"jpg\", \"jpeg\"], dialog_title=\"Pick Image File to Enlarge\")\n",
        "    def upload_files(e):\n",
        "        uf = []\n",
        "        if file_picker.result != None and file_picker.result.files != None:\n",
        "            for f in file_picker.result.files:\n",
        "              if page.web:\n",
        "                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))\n",
        "              else:\n",
        "                on_upload_progress(FilePickerUploadEvent(f.path, 1, \"\"))\n",
        "            file_picker.upload(uf)\n",
        "    page.overlay.append(file_picker)\n",
        "    def add_image(e):\n",
        "        save_dir = os.path.join(root_dir, 'my_concept')\n",
        "        if not os.path.exists(save_dir):\n",
        "          os.mkdir(save_dir)\n",
        "        if image_path.value.startswith('http'):\n",
        "          import requests\n",
        "          from io import BytesIO\n",
        "          response = requests.get(image_path.value)\n",
        "          fpath = os.path.join(save_dir, image_path.value.rpartition(slash)[2])\n",
        "          concept_image = PILImage.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "          width, height = concept_image.size\n",
        "          width, height = scale_dimensions(width, height, dreambooth_prefs['max_size'])\n",
        "          concept_image = concept_image.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert(\"RGB\")\n",
        "          concept_image.save(fpath)\n",
        "          add_file(fpath)\n",
        "        elif os.path.isfile(image_path.value):\n",
        "          fpath = os.path.join(save_dir, image_path.value.rpartition(slash)[2])\n",
        "          original_img = PILImage.open(image_path.value)\n",
        "          width, height = original_img.size\n",
        "          width, height = scale_dimensions(width, height, dreambooth_prefs['max_size'])\n",
        "          original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert(\"RGB\")\n",
        "          original_img.save(fpath)\n",
        "          #shutil.copy(image_path.value, fpath)\n",
        "          add_file(fpath)\n",
        "        elif os.path.isdir(image_path.value):\n",
        "          for f in os.listdir(image_path.value):\n",
        "            file_path = os.path.join(image_path.value, f)\n",
        "            if os.path.isdir(file_path): continue\n",
        "            if f.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "              fpath = os.path.join(save_dir, f)\n",
        "              original_img = PILImage.open(file_path)\n",
        "              width, height = original_img.size\n",
        "              width, height = scale_dimensions(width, height, dreambooth_prefs['max_size'])\n",
        "              original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert(\"RGB\")\n",
        "              original_img.save(fpath)\n",
        "              #shutil.copy(file_path, fpath)\n",
        "              add_file(fpath)\n",
        "        else:\n",
        "          if bool(image_path.value):\n",
        "            alert_msg(page, \"Couldn't find a valid File, Path or URL...\")\n",
        "          else:\n",
        "            pick_path(e)\n",
        "          return\n",
        "        image_path.value = \"\"\n",
        "        image_path.update()\n",
        "    def load_images():\n",
        "        if os.path.exists(save_dir):\n",
        "          for f in os.listdir(save_dir):\n",
        "            existing = os.path.join(save_dir, f)\n",
        "            if os.path.isdir(existing): continue\n",
        "            if f.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "              add_file(existing, update=False)\n",
        "    instance_prompt = TextField(label=\"Instance Prompt Token Text\", value=dreambooth_prefs['instance_prompt'], on_change=lambda e:changed(e,'instance_prompt'))\n",
        "    prior_preservation_class_prompt = TextField(label=\"Prior Preservation Class Prompt\", value=dreambooth_prefs['prior_preservation_class_prompt'], on_change=lambda e:changed(e,'prior_preservation_class_prompt'))\n",
        "    prior_preservation = Checkbox(label=\"Prior Preservation\", tooltip=\"If you'd like class of the concept (e.g.: toy, dog, painting) is guaranteed to be preserved. This increases the quality and helps with generalization at the cost of training time\", value=dreambooth_prefs['prior_preservation'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'prior_preservation'))\n",
        "    num_class_images = TextField(label=\"Number of Class Images\", value=dreambooth_prefs['num_class_images'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'num_class_images', ptype='int'), width = 160)\n",
        "    sample_batch_size = TextField(label=\"Sample Batch Size\", value=dreambooth_prefs['sample_batch_size'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'sample_batch_size', ptype='int'), width = 160)\n",
        "    prior_loss_weight = TextField(label=\"Prior Loss Weight\", value=dreambooth_prefs['prior_loss_weight'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'prior_loss_weight', ptype='float'), width = 160)\n",
        "    max_train_steps = TextField(label=\"Max Training Steps\", value=dreambooth_prefs['max_train_steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'max_train_steps', ptype='int'), width = 160)\n",
        "    learning_rate = TextField(label=\"Learning Rate\", value=dreambooth_prefs['learning_rate'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'learning_rate', ptype='float'), width = 160)\n",
        "    seed = TextField(label=\"Seed\", value=dreambooth_prefs['seed'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'seed', ptype='int'), width = 160)\n",
        "    save_concept = Checkbox(label=\"Save Concept    \", tooltip=\"\", value=dreambooth_prefs['save_concept'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'save_concept'))\n",
        "    where_to_save_concept = Dropdown(label=\"Where to Save Concept\", width=250, options=[dropdown.Option(\"Public Library\"), dropdown.Option(\"Privately to my Profile\")], value=dreambooth_prefs['where_to_save_concept'], on_change=lambda e: changed(e, 'where_to_save_concept'))\n",
        "    prior_preservation_class_folder = TextField(label=\"Prior Preservation Class Folder\", value=dreambooth_prefs['prior_preservation_class_folder'], on_change=lambda e:changed(e,'prior_preservation_class_folder'))\n",
        "    name_of_your_concept = TextField(label=\"Name of your Concept\", value=dreambooth_prefs['name_of_your_concept'], on_change=lambda e:changed(e,'name_of_your_concept'))\n",
        "    readme_description = TextField(label=\"Extra README Description\", value=dreambooth_prefs['readme_description'], on_change=lambda e:changed(e,'readme_description'))\n",
        "    max_row = SliderRow(label=\"Max Resolution Size\", min=256, max=1024, divisions=12, multiple=32, suffix=\"px\", pref=dreambooth_prefs, key='max_size')\n",
        "    image_path = TextField(label=\"Image File or Folder Path or URL to Train\", value=dreambooth_prefs['image_path'], on_change=lambda e:changed(e,'image_path'), suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_path), expand=1)\n",
        "    add_image_button = ElevatedButton(content=Text(\"Add File or Folder\"), on_click=add_image)\n",
        "    page.db_file_list = Column([], tight=True, spacing=0)\n",
        "    load_images()\n",
        "    #seed = TextField(label=\"Seed\", value=dreambooth_prefs['seed'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'seed', ptype='int'), width = 160)\n",
        "    #lambda_entropy = TextField(label=\"Lambda Entropy\", value=dreamfusdreambooth_prefsion_prefs['lambda_entropy'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'lambda_entropy', ptype='float'), width = 160)\n",
        "    #max_steps = TextField(label=\"Max Steps\", value=dreambooth_prefs['max_steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'max_steps', ptype='int'), width = 160)\n",
        "    page.dreambooth_output = Column([])\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(page.dreambooth_output.controls) > 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üò∂‚Äçüå´Ô∏è  Create Custom DreamBooth Concept Model\", \"Provide a collection of images to conceptualize. Warning: May take over an hour to run the training...\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with DreamBooth Settings\", on_click=db_help)]),\n",
        "        Row([instance_prompt, name_of_your_concept]),\n",
        "        Row([num_class_images, sample_batch_size, prior_loss_weight]),\n",
        "        Row([max_train_steps, learning_rate, seed]),\n",
        "        Row([save_concept, where_to_save_concept]),\n",
        "        readme_description,\n",
        "        #Row([prior_preservation_class_folder]),\n",
        "        max_row,\n",
        "        Row([image_path, add_image_button]),\n",
        "        page.db_file_list,\n",
        "        Row([ElevatedButton(content=Text(\"üë®‚Äçüé®Ô∏è  Run DreamBooth\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_dreambooth(page)), ElevatedButton(content=Text(\"üë®‚Äçüé®Ô∏è  Run DreamBooth 2\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_dreambooth2(page))]),\n",
        "        page.dreambooth_output,\n",
        "        clear_button,\n",
        "      ]\n",
        "    ))], scroll=ScrollMode.AUTO)\n",
        "    return c\n",
        "\n",
        "textualinversion_prefs = {\n",
        "    'what_to_teach': 'object',\n",
        "    'placeholder_token': '',\n",
        "    'initializer_token': '',\n",
        "    'scale_lr': True,\n",
        "    'max_train_steps': 3000,\n",
        "    'train_batch_size': 1,\n",
        "    'gradient_accumulation_steps': 4,\n",
        "    'seed': 22276,\n",
        "    'repeats': 100,\n",
        "    'validation_prompt': '',\n",
        "    'validation_steps': 1,\n",
        "    'num_vectors': 2,\n",
        "    'output_dir': os.path.join(root_dir, \"sd-concept-output\"),\n",
        "    'learning_rate': 5e-04,\n",
        "    'name_of_your_concept': \"\",\n",
        "    'save_concept': True,\n",
        "    'where_to_save_concept': \"Public Library\",\n",
        "    'max_size': 512,\n",
        "    'image_path': '',\n",
        "    'readme_description': '',\n",
        "    'urls': [],\n",
        "}\n",
        "def buildTextualInversion(page):\n",
        "    global prefs, textualinversion_prefs\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "        if pref is not None:\n",
        "          try:\n",
        "            if ptype == \"int\":\n",
        "              textualinversion_prefs[pref] = int(e.control.value)\n",
        "            elif ptype == \"float\":\n",
        "              textualinversion_prefs[pref] = float(e.control.value)\n",
        "            else:\n",
        "              textualinversion_prefs[pref] = e.control.value\n",
        "          except Exception:\n",
        "            alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "            pass\n",
        "    def add_to_textualinversion_output(o):\n",
        "        page.textualinversion_output.controls.append(o)\n",
        "        page.textualinversion_output.update()\n",
        "    def clear_output(e):\n",
        "        if prefs['enable_sounds']: page.snd_delete.play()\n",
        "        page.textualinversion_output.controls = []\n",
        "        page.textualinversion_output.update()\n",
        "        clear_button.visible = False\n",
        "        clear_button.update()\n",
        "    def ti_help(e):\n",
        "        def close_ti_dlg(e):\n",
        "          nonlocal ti_help_dlg\n",
        "          ti_help_dlg.open = False\n",
        "          page.update()\n",
        "        ti_help_dlg = AlertDialog(title=Text(\"üíÅ   Help with Textual-Inversion\"), content=Column([\n",
        "            Text(\"\"),\n",
        "          ], scroll=ScrollMode.AUTO), actions=[TextButton(\"üò™  I'll figure it out... \", on_click=close_ti_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "        page.dialog = ti_help_dlg\n",
        "        ti_help_dlg.open = True\n",
        "        page.update()\n",
        "    def delete_image(e):\n",
        "        f = e.control.data\n",
        "        if os.path.isfile(f):\n",
        "          os.remove(f)\n",
        "          for i, fl in enumerate(page.ti_file_list.controls):\n",
        "            if fl.title.value == f:\n",
        "              del page.ti_file_list.controls[i]\n",
        "              page.ti_file_list.update()\n",
        "              continue\n",
        "    def delete_all_images(e):\n",
        "        for fl in page.ti_file_list.controls:\n",
        "          f = fl.title.value\n",
        "          if os.path.isfile(f):\n",
        "            os.remove(f)\n",
        "        page.ti_file_list.controls.clear()\n",
        "        page.ti_file_list.update()\n",
        "    def add_file(fpath, update=True):\n",
        "        page.ti_file_list.controls.append(ListTile(title=Text(fpath), dense=True, trailing=PopupMenuButton(icon=icons.MORE_VERT,\n",
        "          items=[#TODO: View Image\n",
        "              PopupMenuItem(icon=icons.DELETE, text=\"Delete Image\", on_click=delete_image, data=fpath),\n",
        "              PopupMenuItem(icon=icons.DELETE_SWEEP, text=\"Delete All\", on_click=delete_all_images, data=fpath),\n",
        "          ])))\n",
        "        if update: page.ti_file_list.update()\n",
        "    def file_picker_result(e: FilePickerResultEvent):\n",
        "        if e.files != None:\n",
        "          upload_files(e)\n",
        "    save_dir = os.path.join(root_dir, 'my_concept')\n",
        "    def on_upload_progress(e: FilePickerUploadEvent):\n",
        "        if e.progress == 1:\n",
        "          if not os.path.exists(save_dir):\n",
        "            os.mkdir(save_dir)\n",
        "          if not slash in e.file_name:\n",
        "            fname = os.path.join(root_dir, e.file_name)\n",
        "            fpath = os.path.join(save_dir, e.file_name)\n",
        "          else:\n",
        "            fname = e.file_name\n",
        "            fpath = os.path.join(save_dir, e.file_name.rpartition(slash)[2])\n",
        "          original_img = PILImage.open(fname)\n",
        "          width, height = original_img.size\n",
        "          width, height = scale_dimensions(width, height, textualinversion_prefs['max_size'])\n",
        "          original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert(\"RGB\")\n",
        "          original_img.save(fpath)\n",
        "          if page.web: os.remove(fname)\n",
        "          #shutil.move(fname, fpath)\n",
        "          add_file(fpath)\n",
        "    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)\n",
        "    def pick_path(e):\n",
        "        file_picker.pick_files(allow_multiple=True, allowed_extensions=[\"png\", \"PNG\", \"jpg\", \"jpeg\"], dialog_title=\"Pick Image File to Enlarge\")\n",
        "    def upload_files(e):\n",
        "        uf = []\n",
        "        if file_picker.result != None and file_picker.result.files != None:\n",
        "            for f in file_picker.result.files:\n",
        "              if page.web:\n",
        "                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))\n",
        "              else:\n",
        "                on_upload_progress(FilePickerUploadEvent(f.path, 1, \"\"))\n",
        "            file_picker.upload(uf)\n",
        "    page.overlay.append(file_picker)\n",
        "    def add_image(e):\n",
        "        save_dir = os.path.join(root_dir, 'my_concept')\n",
        "        if not os.path.exists(save_dir):\n",
        "          os.mkdir(save_dir)\n",
        "        if image_path.value.startswith('http'):\n",
        "          import requests\n",
        "          from io import BytesIO\n",
        "          response = requests.get(image_path.value)\n",
        "          fpath = os.path.join(save_dir, image_path.value.rpartition(slash)[2])\n",
        "          concept_image = PILImage.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "          width, height = concept_image.size\n",
        "          width, height = scale_dimensions(width, height, textualinversion_prefs['max_size'])\n",
        "          concept_image = concept_image.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert(\"RGB\")\n",
        "          concept_image.save(fpath)\n",
        "          add_file(fpath)\n",
        "        elif os.path.isfile(image_path.value):\n",
        "          fpath = os.path.join(save_dir, image_path.value.rpartition(slash)[2])\n",
        "          original_img = PILImage.open(image_path.value)\n",
        "          width, height = original_img.size\n",
        "          width, height = scale_dimensions(width, height, textualinversion_prefs['max_size'])\n",
        "          original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert(\"RGB\")\n",
        "          original_img.save(fpath)\n",
        "          #shutil.copy(image_path.value, fpath)\n",
        "          add_file(fpath)\n",
        "        elif os.path.isdir(image_path.value):\n",
        "          for f in os.listdir(image_path.value):\n",
        "            file_path = os.path.join(image_path.value, f)\n",
        "            if os.path.isdir(file_path): continue\n",
        "            if f.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "              fpath = os.path.join(save_dir, f)\n",
        "              original_img = PILImage.open(file_path)\n",
        "              width, height = original_img.size\n",
        "              width, height = scale_dimensions(width, height, textualinversion_prefs['max_size'])\n",
        "              original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert(\"RGB\")\n",
        "              original_img.save(fpath)\n",
        "              #shutil.copy(file_path, fpath)\n",
        "              add_file(fpath)\n",
        "        else:\n",
        "          if bool(image_path.value):\n",
        "            alert_msg(page, \"Couldn't find a valid File, Path or URL...\")\n",
        "          else:\n",
        "            pick_path(e)\n",
        "          return\n",
        "        image_path.value = \"\"\n",
        "        image_path.update()\n",
        "    def load_images():\n",
        "        if os.path.exists(save_dir):\n",
        "          for f in os.listdir(save_dir):\n",
        "            existing = os.path.join(save_dir, f)\n",
        "            if os.path.isdir(existing): continue\n",
        "            if f.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "              add_file(existing, update=False)\n",
        "    what_to_teach = Dropdown(label=\"What to Teach\", width=250, options=[dropdown.Option(\"object\"), dropdown.Option(\"style\")], value=textualinversion_prefs['what_to_teach'], on_change=lambda e: changed(e, 'what_to_teach'))\n",
        "    placeholder_token = TextField(label=\"Placeholder <Token> Keyword\", value=textualinversion_prefs['placeholder_token'], on_change=lambda e:changed(e,'placeholder_token'))\n",
        "    initializer_token = TextField(label=\"Initializer Token Category Summary\", value=textualinversion_prefs['initializer_token'], on_change=lambda e:changed(e,'initializer_token'))\n",
        "    validation_prompt = TextField(label=\"Validation <Token> Prompt\", value=textualinversion_prefs['validation_prompt'], on_change=lambda e:changed(e,'validation_prompt'))\n",
        "    gradient_accumulation_steps = TextField(label=\"Gradient Accumulation Steps\", value=textualinversion_prefs['gradient_accumulation_steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'gradient_accumulation_steps', ptype='int'), width = 160)\n",
        "    scale_lr = Checkbox(label=\"Scale Learning Rate\", tooltip=\"\", value=textualinversion_prefs['scale_lr'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'scale_lr'))\n",
        "    validation_steps = TextField(label=\"Validation Steps\", value=textualinversion_prefs['validation_steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'validation_steps', ptype='int'), width = 145)\n",
        "    num_vectors = TextField(label=\"Number of Vectors\", value=textualinversion_prefs['num_vectors'], tooltip=\"How many textual inversion vectors shall be used to learn the concept.\", keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'num_vectors', ptype='int'), width = 145)\n",
        "    repeats = TextField(label=\"Repeats\", value=textualinversion_prefs['repeats'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'repeats', ptype='int'), width = 160)\n",
        "    train_batch_size = TextField(label=\"Train Batch Size\", value=textualinversion_prefs['train_batch_size'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'train_batch_size', ptype='float'), width = 160)\n",
        "    max_train_steps = TextField(label=\"Max Training Steps\", value=textualinversion_prefs['max_train_steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'max_train_steps', ptype='int'), width = 160)\n",
        "    learning_rate = TextField(label=\"Learning Rate\", value=textualinversion_prefs['learning_rate'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'learning_rate', ptype='float'), width = 160)\n",
        "    seed = TextField(label=\"Seed\", value=textualinversion_prefs['seed'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'seed', ptype='int'), width = 160)\n",
        "    save_concept = Checkbox(label=\"Save Concept    \", tooltip=\"\", value=textualinversion_prefs['save_concept'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'save_concept'))\n",
        "    where_to_save_concept = Dropdown(label=\"Where to Save Concept\", width=250, options=[dropdown.Option(\"Public Library\"), dropdown.Option(\"Privately to my Profile\")], value=textualinversion_prefs['where_to_save_concept'], on_change=lambda e: changed(e, 'where_to_save_concept'))\n",
        "    output_dir = TextField(label=\"Prior Preservation Class Folder\", value=textualinversion_prefs['output_dir'], on_change=lambda e:changed(e,'output_dir'))\n",
        "    name_of_your_concept = TextField(label=\"Name of your Concept\", value=textualinversion_prefs['name_of_your_concept'], on_change=lambda e:changed(e,'name_of_your_concept'))\n",
        "    readme_description = TextField(label=\"Extra README Description\", value=textualinversion_prefs['readme_description'], on_change=lambda e:changed(e,'readme_description'))\n",
        "    max_row = SliderRow(label=\"Max Resolution Size\", min=256, max=1024, divisions=12, multiple=32, suffix=\"px\", pref=textualinversion_prefs, key='max_size')\n",
        "    image_path = TextField(label=\"Image File or Folder Path or URL to Train\", value=textualinversion_prefs['image_path'], on_change=lambda e:changed(e,'image_path'), suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_path), expand=1)\n",
        "    add_image_button = ElevatedButton(content=Text(\"Add File or Folder\"), on_click=add_image)\n",
        "    page.ti_file_list = Column([], tight=True, spacing=0)\n",
        "    load_images()\n",
        "    #seed = TextField(label=\"Seed\", value=textualinversion_prefs['seed'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'seed', ptype='int'), width = 160)\n",
        "    #lambda_entropy = TextField(label=\"Lambda Entropy\", value=dreamfustextualinversion_prefsion_prefs['lambda_entropy'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'lambda_entropy', ptype='float'), width = 160)\n",
        "    #max_steps = TextField(label=\"Max Steps\", value=textualinversion_prefs['max_steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'max_steps', ptype='int'), width = 160)\n",
        "    page.textualinversion_output = Column([])\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(page.textualinversion_output.controls) > 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üò∂‚Äçüå´Ô∏è  Create Cusom Textual-Inversion Concept Model\", \"Provide a collection of images to conceptualize. Warning: May take over an hour to run the training...\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with Textual-Inversion Settings\", on_click=ti_help)]),\n",
        "        Row([what_to_teach, initializer_token]),\n",
        "        Row([placeholder_token, name_of_your_concept]),\n",
        "        Row([validation_prompt, validation_steps, num_vectors]),\n",
        "        scale_lr,\n",
        "        Row([gradient_accumulation_steps, repeats, train_batch_size]),\n",
        "        Row([max_train_steps, learning_rate, seed]),\n",
        "        Row([save_concept, where_to_save_concept]),\n",
        "        readme_description,\n",
        "        #Row([output_dir]),\n",
        "        max_row,\n",
        "        Row([image_path, add_image_button]),\n",
        "        page.ti_file_list,\n",
        "        ElevatedButton(content=Text(\"üë®‚Äçüé®Ô∏è  Run Textual-Inversion\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_textualinversion(page)),\n",
        "        page.textualinversion_output,\n",
        "        clear_button,\n",
        "      ]\n",
        "    ))], scroll=ScrollMode.AUTO)\n",
        "    return c\n",
        "\n",
        "LoRA_dreambooth_prefs = {\n",
        "    'instance_prompt': '', #The prompt with identifier specifying the instance\n",
        "    'class_prompt': '',\n",
        "    'prior_preservation': False, #Flag to add prior preservation loss.\n",
        "    'num_class_images': 100, #Minimal class images for prior preservation loss. If there are not enough images already present in class_data_dir, additional images will be sampled with class_prompt.\n",
        "    'sample_batch_size': 4, #Batch size (per device) for sampling images.\n",
        "    'train_batch_size': 1, #\"Batch size (per device) for the training dataloader.\n",
        "    'gradient_accumulation_steps': 1, #Number of updates steps to accumulate before performing a backward/update pass.\n",
        "    'gradient_checkpointing': True, #Whether or not to use gradient checkpointing to save memory at the expense of slower backward pass.\n",
        "    'checkpointing_steps': 100,#Number of training steps between saving model checkpoints\n",
        "    'lr_scheduler': 'constant', #[\"linear\", \"cosine\", \"cosine_with_restarts\", \"polynomial\", \"constant\", \"constant_with_warmup\"]\n",
        "    'lr_warmup_steps': 500, #Number of steps for the warmup in the lr scheduler.\n",
        "    'lr_num_cycles': 1, #Number of hard resets of the lr in cosine_with_restarts scheduler.\n",
        "    'lr_power': 1, #Power factor of the polynomial scheduler.\n",
        "    'prior_loss_weight': 1.0, #The weight of prior preservation loss.\n",
        "    'class_data_dir': os.path.join(root_dir, \"class_images\"),\n",
        "    'learning_rate': 1e-4, #Initial learning rate (after the potential warmup period) to use.\n",
        "    'scale_lr': False, #Scale the learning rate by the number of GPUs, gradient accumulation steps, and batch size.\n",
        "    'max_train_steps': 500, #Total number of training steps to perform.  If provided, overrides num_train_epochs.\n",
        "    'seed': 0,\n",
        "    'name_of_your_model': '',\n",
        "    'save_model': True,\n",
        "    'where_to_save_model': 'Public HuggingFace',\n",
        "    'resolution': 512,\n",
        "    'image_path': '',\n",
        "    'readme_description': '',\n",
        "    'urls': [],\n",
        "}\n",
        "\n",
        "    #--lr_num_cycles=1 --lr_power=1 --prior_loss_weight=1.0 --sample_batch_size=4 --num_class_images=100\n",
        "LoRA_prefs = {\n",
        "    'instance_prompt': '', #The prompt with identifier specifying the instance\n",
        "    'class_prompt': '',\n",
        "    'prior_preservation': False, #Flag to add prior preservation loss.\n",
        "    #'num_class_images': 100, #Minimal class images for prior preservation loss. If there are not enough images already present in class_data_dir, additional images will be sampled with class_prompt.\n",
        "    #'sample_batch_size': 4, #Batch size (per device) for sampling images.\n",
        "    'train_batch_size': 1, #\"Batch size (per device) for the training dataloader.\n",
        "    'gradient_accumulation_steps': 1, #Number of updates steps to accumulate before performing a backward/update pass.\n",
        "    'gradient_checkpointing': True, #Whether or not to use gradient checkpointing to save memory at the expense of slower backward pass.\n",
        "    'checkpointing_steps': 100,#Number of training steps between saving model checkpoints\n",
        "    'resume_from_checkpoint': '', #Whether training should be resumed from a previous checkpoint. Use a path saved by\" `--checkpointing_steps`, or `latest` to automatically select the last available checkpoint.\n",
        "    'lr_scheduler': 'constant', #[\"linear\", \"cosine\", \"cosine_with_restarts\", \"polynomial\", \"constant\", \"constant_with_warmup\"]\n",
        "    'lr_warmup_steps': 500, #Number of steps for the warmup in the lr scheduler.\n",
        "    #'lr_num_cycles': 1, #Number of hard resets of the lr in cosine_with_restarts scheduler.\n",
        "    #'lr_power': 1, #Power factor of the polynomial scheduler.\n",
        "    #'prior_loss_weight': 1.0, #The weight of prior preservation loss.\n",
        "    'class_data_dir': os.path.join(root_dir, \"class_images\"),\n",
        "    'learning_rate': 1e-4, #Initial learning rate (after the potential warmup period) to use.\n",
        "    'scale_lr': False, #Scale the learning rate by the number of GPUs, gradient accumulation steps, and batch size.\n",
        "    'max_train_steps': 500, #Total number of training steps to perform.  If provided, overrides num_train_epochs.\n",
        "    'seed': 0,\n",
        "    'validation_prompt': '', #A prompt that is sampled during training for inference.\n",
        "    'num_validation_images': 4, #Number of images that should be generated during validation with `validation_prompt`.\n",
        "    'validation_epochs': 1, #Run fine-tuning validation every X epochs. The validation process consists of running the prompt\n",
        "    'name_of_your_model': '',\n",
        "    'save_model': True,\n",
        "    'where_to_save_model': 'Public HuggingFace',\n",
        "    'resolution': 512,\n",
        "    'image_path': '',\n",
        "    'readme_description': '',\n",
        "    'urls': [],\n",
        "}\n",
        "\n",
        "def buildLoRA_Dreambooth(page):\n",
        "    global prefs, LoRA_dreambooth_prefs\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "        if pref is not None:\n",
        "          try:\n",
        "            if ptype == \"int\":\n",
        "              LoRA_dreambooth_prefs[pref] = int(e.control.value)\n",
        "            elif ptype == \"float\":\n",
        "              LoRA_dreambooth_prefs[pref] = float(e.control.value)\n",
        "            else:\n",
        "              LoRA_dreambooth_prefs[pref] = e.control.value\n",
        "          except Exception:\n",
        "            alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "            pass\n",
        "    def add_to_LoRA_dreambooth_output(o):\n",
        "        page.LoRA_dreambooth_output.controls.append(o)\n",
        "        page.LoRA_dreambooth_output.update()\n",
        "    def clear_output(e):\n",
        "        if prefs['enable_sounds']: page.snd_delete.play()\n",
        "        page.LoRA_dreambooth_output.controls = []\n",
        "        page.LoRA_dreambooth_output.update()\n",
        "        clear_button.visible = False\n",
        "        clear_button.update()\n",
        "    def lora_dreambooth_help(e):\n",
        "        def close_lora_dreambooth_dlg(e):\n",
        "          nonlocal lora_dreambooth_help_dlg\n",
        "          lora_dreambooth_help_dlg.open = False\n",
        "          page.update()\n",
        "        lora_dreambooth_help_dlg = AlertDialog(title=Text(\"üíÅ   Help with LoRA DreamBooth\"), content=Column([\n",
        "            Text(\"First thing is to collect all your own images that you want to teach it to dream.  Feed it at least 5 square pictures of the object or style to learn, and it'll save your Custom Model Checkpoint.\"),\n",
        "            Markdown(\"\"\"Low-Rank Adaption of Large Language Models was first introduced by Microsoft in [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685) by *Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen*\n",
        "In a nutshell, LoRA allows to adapt pretrained models by adding pairs of rank-decomposition matrices to existing weights and **only** training those newly added weights. This has a couple of advantages:\n",
        "- Previous pretrained weights are kept frozen so that the model is not prone to [catastrophic forgetting](https://www.pnas.org/doi/10.1073/pnas.1611835114)\n",
        "- Rank-decomposition matrices have significantly fewer parameters than the original model, which means that trained LoRA weights are easily portable.\n",
        "- LoRA attention layers allow to control to which extent the model is adapted torwards new training images via a `scale` parameter.\"\"\", on_tap_link=lambda e: e.page.launch_url(e.data)),\n",
        "            Text(\"Fine-tune your perameters, but be aware that the training process takes a long time to run, so careful with the settings if you don't have the patience or processor. Dream at your own risk.\"),\n",
        "          ], scroll=ScrollMode.AUTO), actions=[TextButton(emojize(':sun_with_face:') + \"  Neato... \", on_click=close_lora_dreambooth_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "        page.dialog = lora_dreambooth_help_dlg\n",
        "        lora_dreambooth_help_dlg.open = True\n",
        "        page.update()\n",
        "    def delete_image(e):\n",
        "        f = e.control.data\n",
        "        if os.path.isfile(f):\n",
        "          os.remove(f)\n",
        "          for i, fl in enumerate(page.lora_dreambooth_file_list.controls):\n",
        "            if fl.title.value == f:\n",
        "              del page.lora_dreambooth_file_list.controls[i]\n",
        "              page.lora_dreambooth_file_list.update()\n",
        "              continue\n",
        "    def delete_all_images(e):\n",
        "        for fl in page.lora_dreambooth_file_list.controls:\n",
        "          f = fl.title.value\n",
        "          if os.path.isfile(f):\n",
        "            os.remove(f)\n",
        "        page.lora_dreambooth_file_list.controls.clear()\n",
        "        page.lora_dreambooth_file_list.update()\n",
        "    def image_details(e):\n",
        "        img = e.control.data\n",
        "        alert_msg(e.page, \"Image Details\", content=Image(src=img), sound=False)\n",
        "    def add_file(fpath, update=True):\n",
        "        page.lora_dreambooth_file_list.controls.append(ListTile(title=Text(fpath), dense=False, trailing=PopupMenuButton(icon=icons.MORE_VERT,\n",
        "          items=[#TODO: View Image\n",
        "              PopupMenuItem(icon=icons.INFO, text=\"Image Details\", on_click=image_details, data=fpath),\n",
        "              PopupMenuItem(icon=icons.DELETE, text=\"Delete Image\", on_click=delete_image, data=fpath),\n",
        "              PopupMenuItem(icon=icons.DELETE_SWEEP, text=\"Delete All\", on_click=delete_all_images, data=fpath),\n",
        "          ]), data=fpath, on_click=image_details))\n",
        "        if update: page.lora_dreambooth_file_list.update()\n",
        "    def file_picker_result(e: FilePickerResultEvent):\n",
        "        if e.files != None:\n",
        "          upload_files(e)\n",
        "    save_dir = os.path.join(root_dir, 'my_model')\n",
        "    def on_upload_progress(e: FilePickerUploadEvent):\n",
        "        if e.progress == 1:\n",
        "          if not os.path.exists(save_dir):\n",
        "            os.mkdir(save_dir)\n",
        "          if not slash in e.file_name:\n",
        "            fname = os.path.join(root_dir, e.file_name)\n",
        "            fpath = os.path.join(save_dir, e.file_name)\n",
        "          else:\n",
        "            fname = e.file_name\n",
        "            fpath = os.path.join(save_dir, e.file_name.rpartition(slash)[2])\n",
        "          original_img = PILImage.open(fname)\n",
        "          width, height = original_img.size\n",
        "          width, height = scale_dimensions(width, height, LoRA_dreambooth_prefs['resolution'])\n",
        "          original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert(\"RGB\")\n",
        "          original_img.save(fpath)\n",
        "          if page.web: os.remove(fname)\n",
        "          #shutil.move(fname, fpath)\n",
        "          add_file(fpath)\n",
        "    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)\n",
        "    def pick_path(e):\n",
        "        file_picker.pick_files(allow_multiple=True, allowed_extensions=[\"png\", \"PNG\", \"jpg\", \"jpeg\"], dialog_title=\"Pick Image File to Enlarge\")\n",
        "    def upload_files(e):\n",
        "        uf = []\n",
        "        if file_picker.result != None and file_picker.result.files != None:\n",
        "            for f in file_picker.result.files:\n",
        "              if page.web:\n",
        "                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))\n",
        "              else:\n",
        "                on_upload_progress(FilePickerUploadEvent(f.path, 1, \"\"))\n",
        "            file_picker.upload(uf)\n",
        "    page.overlay.append(file_picker)\n",
        "    def add_image(e):\n",
        "        save_dir = os.path.join(root_dir, 'my_model')\n",
        "        if not os.path.exists(save_dir):\n",
        "          os.mkdir(save_dir)\n",
        "        if image_path.value.startswith('http'):\n",
        "          import requests\n",
        "          from io import BytesIO\n",
        "          response = requests.get(image_path.value)\n",
        "          fpath = os.path.join(save_dir, image_path.value.rpartition(slash)[2])\n",
        "          model_image = PILImage.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "          width, height = model_image.size\n",
        "          width, height = scale_dimensions(width, height, LoRA_dreambooth_prefs['resolution'])\n",
        "          model_image = model_image.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert(\"RGB\")\n",
        "          model_image.save(fpath)\n",
        "          add_file(fpath)\n",
        "        elif os.path.isfile(image_path.value):\n",
        "          fpath = os.path.join(save_dir, image_path.value.rpartition(slash)[2])\n",
        "          original_img = PILImage.open(image_path.value)\n",
        "          width, height = original_img.size\n",
        "          width, height = scale_dimensions(width, height, LoRA_dreambooth_prefs['resolution'])\n",
        "          original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert(\"RGB\")\n",
        "          original_img.save(fpath)\n",
        "          #shutil.copy(image_path.value, fpath)\n",
        "          add_file(fpath)\n",
        "        elif os.path.isdir(image_path.value):\n",
        "          for f in os.listdir(image_path.value):\n",
        "            file_path = os.path.join(image_path.value, f)\n",
        "            if os.path.isdir(file_path): continue\n",
        "            if f.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "              fpath = os.path.join(save_dir, f)\n",
        "              original_img = PILImage.open(file_path)\n",
        "              width, height = original_img.size\n",
        "              width, height = scale_dimensions(width, height, LoRA_dreambooth_prefs['resolution'])\n",
        "              original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert(\"RGB\")\n",
        "              original_img.save(fpath)\n",
        "              #shutil.copy(file_path, fpath)\n",
        "              add_file(fpath)\n",
        "        else:\n",
        "          if bool(image_path.value):\n",
        "            alert_msg(page, \"Couldn't find a valid File, Path or URL...\")\n",
        "          else:\n",
        "            pick_path(e)\n",
        "          return\n",
        "        image_path.value = \"\"\n",
        "        image_path.update()\n",
        "    def load_images():\n",
        "        if os.path.exists(save_dir):\n",
        "          for f in os.listdir(save_dir):\n",
        "            existing = os.path.join(save_dir, f)\n",
        "            if os.path.isdir(existing): continue\n",
        "            if f.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "              add_file(existing, update=False)\n",
        "    def toggle_save(e):\n",
        "        changed(e, 'save_model')\n",
        "        where_to_save_model.visible = LoRA_dreambooth_prefs['save_model']\n",
        "        where_to_save_model.update()\n",
        "        readme_description.visible = LoRA_dreambooth_prefs['save_model']\n",
        "        readme_description.update()\n",
        "    instance_prompt = Container(content=Tooltip(message=\"The prompt with identifier specifying the instance\", content=TextField(label=\"Instance Prompt Token Text\", value=LoRA_dreambooth_prefs['instance_prompt'], on_change=lambda e:changed(e,'instance_prompt'))), col={'md':9})\n",
        "    name_of_your_model = TextField(label=\"Name of your Model\", value=LoRA_dreambooth_prefs['name_of_your_model'], on_change=lambda e:changed(e,'name_of_your_model'), col={'md':3})\n",
        "    class_prompt = TextField(label=\"Class Prompt\", value=LoRA_dreambooth_prefs['class_prompt'], on_change=lambda e:changed(e,'class_prompt'))\n",
        "    lr_scheduler = Dropdown(label=\"Learning Rate Scheduler\", width=250, options=[dropdown.Option(\"constant\"), dropdown.Option(\"constant_with_warmup\"), dropdown.Option(\"linear\"), dropdown.Option(\"cosine\"), dropdown.Option(\"cosine_with_restarts\"), dropdown.Option(\"polynomial\")], value=LoRA_dreambooth_prefs['lr_scheduler'], on_change=lambda e: changed(e, 'lr_scheduler'))\n",
        "    prior_preservation = Checkbox(label=\"Prior Preservation\", tooltip=\"If you'd like class of the model (e.g.: toy, dog, painting) is guaranteed to be preserved. This increases the quality and helps with generalization at the cost of training time\", value=LoRA_dreambooth_prefs['prior_preservation'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'prior_preservation'))\n",
        "    gradient_checkpointing = Checkbox(label=\"Gradient Checkpointing   \", tooltip=\"Whether or not to use gradient checkpointing to save memory at the expense of slower backward pass.\", value=LoRA_dreambooth_prefs['gradient_checkpointing'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'gradient_checkpointing'))\n",
        "    num_class_images = Tooltip(message=\"Minimal class images for prior preservation loss. If there are not enough images already present in class_data_dir, additional images will be sampled with class_prompt.\", content=TextField(label=\"Number of Class Images\", value=LoRA_dreambooth_prefs['num_class_images'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'num_class_images', ptype='int'), width = 160))\n",
        "    sample_batch_size = Tooltip(message=\"Batch size (per device) for sampling images.\", content=TextField(label=\"Sample Batch Size\", value=LoRA_dreambooth_prefs['sample_batch_size'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'sample_batch_size', ptype='int'), width = 160))\n",
        "    train_batch_size = Tooltip(message=\"Batch size (per device) for the training dataloader.\", content=TextField(label=\"Train Batch Size\", value=LoRA_dreambooth_prefs['train_batch_size'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'train_batch_size', ptype='int'), width = 160))\n",
        "    prior_loss_weight = Tooltip(message=\"The weight of prior preservation loss.\", content=TextField(label=\"Prior Loss Weight\", value=LoRA_dreambooth_prefs['prior_loss_weight'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'prior_loss_weight', ptype='float'), width = 160))\n",
        "    max_train_steps = Tooltip(message=\"Total number of training steps to perform.  If provided, overrides num_train_epochs.\", content=TextField(label=\"Max Training Steps\", value=LoRA_dreambooth_prefs['max_train_steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'max_train_steps', ptype='int'), width = 160))\n",
        "    gradient_accumulation_steps = Tooltip(message=\"Number of updates steps to accumulate before performing a backward/update pass.\", content=TextField(label=\"Gradient Accumulation Steps\", value=LoRA_dreambooth_prefs['gradient_accumulation_steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'gradient_accumulation_steps', ptype='int'), width = 160))\n",
        "    learning_rate = Tooltip(message=\"Initial learning rate (after the potential warmup period) to use.\", content=TextField(label=\"Learning Rate\", value=LoRA_dreambooth_prefs['learning_rate'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'learning_rate', ptype='float'), width = 160))\n",
        "    lr_warmup_steps = Tooltip(message=\"Number of steps for the warmup in the lr scheduler.\", content=TextField(label=\"LR Warmup Steps\", value=LoRA_dreambooth_prefs['lr_warmup_steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'lr_warmup_steps', ptype='int'), width = 160))\n",
        "    lr_num_cycles = Tooltip(message=\"Number of hard resets of the lr in cosine_with_restarts scheduler.\", content=TextField(label=\"LR Number of Cycles\", value=LoRA_dreambooth_prefs['lr_num_cycles'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'lr_num_cycles', ptype='int'), width = 160))\n",
        "    lr_power = Tooltip(message=\"Power factor of the polynomial scheduler.\", content=TextField(label=\"LR Power\", value=LoRA_dreambooth_prefs['lr_power'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'lr_power', ptype='int'), width = 160))\n",
        "    seed = Tooltip(message=\"0 or -1 for Random. Pick any number.\", content=TextField(label=\"Seed\", value=LoRA_dreambooth_prefs['seed'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'seed', ptype='int'), width = 160))\n",
        "    save_model = Checkbox(label=\"Save Model to HuggingFace   \", tooltip=\"\", value=LoRA_dreambooth_prefs['save_model'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'save_model'))\n",
        "    save_model = Tooltip(message=\"Requires WRITE access on API Key to Upload Checkpoint\", content=Switch(label=\"Save Model to HuggingFace    \", value=LoRA_dreambooth_prefs['save_model'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_save))\n",
        "    where_to_save_model = Dropdown(label=\"Where to Save Model\", width=250, options=[dropdown.Option(\"Public HuggingFace\"), dropdown.Option(\"Private HuggingFace\")], value=LoRA_dreambooth_prefs['where_to_save_model'], on_change=lambda e: changed(e, 'where_to_save_model'))\n",
        "    #class_data_dir = TextField(label=\"Prior Preservation Class Folder\", value=LoRA_dreambooth_prefs['class_data_dir'], on_change=lambda e:changed(e,'class_data_dir'))\n",
        "    readme_description = TextField(label=\"Extra README Description\", value=LoRA_dreambooth_prefs['readme_description'], on_change=lambda e:changed(e,'readme_description'))\n",
        "    max_row = SliderRow(label=\"Max Resolution Size\", min=256, max=1024, divisions=6, multiple=64, suffix=\"px\", pref=LoRA_dreambooth_prefs, key='resolution')\n",
        "    image_path = TextField(label=\"Image File or Folder Path or URL to Train\", value=LoRA_dreambooth_prefs['image_path'], on_change=lambda e:changed(e,'image_path'), suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_path), expand=1)\n",
        "    add_image_button = ElevatedButton(content=Text(\"Add File or Folder\"), on_click=add_image)\n",
        "    page.lora_dreambooth_file_list = Column([], tight=True, spacing=0)\n",
        "    load_images()\n",
        "    where_to_save_model.visible = LoRA_dreambooth_prefs['save_model']\n",
        "    readme_description.visible = LoRA_dreambooth_prefs['save_model']\n",
        "    #lambda_entropy = TextField(label=\"Lambda Entropy\", value=dreamfusLoRA_dreambooth_prefsion_prefs['lambda_entropy'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'lambda_entropy', ptype='float'), width = 160)\n",
        "    #max_steps = TextField(label=\"Max Steps\", value=LoRA_dreambooth_prefs['max_steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'max_steps', ptype='int'), width = 160)\n",
        "    page.LoRA_dreambooth_output = Column([])\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(page.LoRA_dreambooth_output.controls) > 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üåá  Training with Low-Rank Adaptation of Large Language Models (LoRA DreamBooth)\", \"Provide a collection of images to train. Adds on to the currently loaded Model Checkpoint...\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with LoRA DreamBooth Settings\", on_click=lora_dreambooth_help)]),\n",
        "        ResponsiveRow([instance_prompt, name_of_your_model]),\n",
        "        Row([num_class_images, sample_batch_size, train_batch_size, prior_loss_weight]),\n",
        "        Row([prior_preservation, gradient_checkpointing, lr_scheduler]),\n",
        "        Row([learning_rate, lr_warmup_steps, lr_num_cycles, lr_power]),\n",
        "        Row([max_train_steps, gradient_accumulation_steps, seed]),\n",
        "        Row([save_model, where_to_save_model]),\n",
        "        readme_description,\n",
        "        #Row([class_data_dir]),\n",
        "        max_row,\n",
        "        Row([image_path, add_image_button]),\n",
        "        page.lora_dreambooth_file_list,\n",
        "        Row([ElevatedButton(content=Text(\"üåÑ  Run LoRA DreamBooth\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_LoRA_dreambooth(page))]),\n",
        "        page.LoRA_dreambooth_output,\n",
        "        clear_button,\n",
        "      ]\n",
        "    ))], scroll=ScrollMode.AUTO)\n",
        "    return c\n",
        "\n",
        "\n",
        "def buildLoRA(page):\n",
        "    global prefs, LoRA_prefs\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "        if pref is not None:\n",
        "          try:\n",
        "            if ptype == \"int\":\n",
        "              LoRA_prefs[pref] = int(e.control.value)\n",
        "            elif ptype == \"float\":\n",
        "              LoRA_prefs[pref] = float(e.control.value)\n",
        "            else:\n",
        "              LoRA_prefs[pref] = e.control.value\n",
        "          except Exception:\n",
        "            alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "            pass\n",
        "    def add_to_LoRA_output(o):\n",
        "        page.LoRA_output.controls.append(o)\n",
        "        page.LoRA_output.update()\n",
        "    def clear_output(e):\n",
        "        if prefs['enable_sounds']: page.snd_delete.play()\n",
        "        page.LoRA_output.controls = []\n",
        "        page.LoRA_output.update()\n",
        "        clear_button.visible = False\n",
        "        clear_button.update()\n",
        "    def lora_help(e):\n",
        "        def close_lora_dlg(e):\n",
        "          nonlocal lora_help_dlg\n",
        "          lora_help_dlg.open = False\n",
        "          page.update()\n",
        "        lora_help_dlg = AlertDialog(title=Text(\"üíÅ   Help with LoRA DreamBooth\"), content=Column([\n",
        "            Text(\"First thing is to collect all your own images that you want to teach it to dream.  Feed it at least 5 square pictures of the object or style to learn, and it'll save your Custom Model Checkpoint.\"),\n",
        "            Markdown(\"\"\"Low-Rank Adaption of Large Language Models was first introduced by Microsoft in [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685) by *Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen*\n",
        "In a nutshell, LoRA allows to adapt pretrained models by adding pairs of rank-decomposition matrices to existing weights and **only** training those newly added weights. This has a couple of advantages:\n",
        "- Previous pretrained weights are kept frozen so that the model is not prone to [catastrophic forgetting](https://www.pnas.org/doi/10.1073/pnas.1611835114)\n",
        "- Rank-decomposition matrices have significantly fewer parameters than the original model, which means that trained LoRA weights are easily portable.\n",
        "- LoRA attention layers allow to control to which extent the model is adapted torwards new training images via a `scale` parameter.\"\"\", on_tap_link=lambda e: e.page.launch_url(e.data)),\n",
        "            Text(\"Fine-tune your perameters, but be aware that the training process takes a long time to run, so careful with the settings if you don't have the patience or processor. Dream at your own risk.\"),\n",
        "          ], scroll=ScrollMode.AUTO), actions=[TextButton(emojize(':sun_with_face:') + \"  Neato... \", on_click=close_lora_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "        page.dialog = lora_help_dlg\n",
        "        lora_help_dlg.open = True\n",
        "        page.update()\n",
        "    def delete_image(e):\n",
        "        f = e.control.data\n",
        "        if os.path.isfile(f):\n",
        "          os.remove(f)\n",
        "          for i, fl in enumerate(page.lora_file_list.controls):\n",
        "            if fl.title.value == f:\n",
        "              del page.lora_file_list.controls[i]\n",
        "              page.lora_file_list.update()\n",
        "              continue\n",
        "    def delete_all_images(e):\n",
        "        for fl in page.lora_file_list.controls:\n",
        "          f = fl.title.value\n",
        "          if os.path.isfile(f):\n",
        "            os.remove(f)\n",
        "        page.lora_file_list.controls.clear()\n",
        "        page.lora_file_list.update()\n",
        "    def image_details(e):\n",
        "        img = e.control.data\n",
        "        #TODO: Get file size & resolution\n",
        "        alert_msg(e.page, \"Image Details\", content=Column([Text(img), Img(src=img, gapless_playback=True)]), sound=False)\n",
        "    def add_file(fpath, update=True):\n",
        "        page.lora_file_list.controls.append(ListTile(title=Text(fpath), dense=True, trailing=PopupMenuButton(icon=icons.MORE_VERT,\n",
        "          items=[#TODO: View Image\n",
        "              PopupMenuItem(icon=icons.INFO, text=\"Image Details\", on_click=image_details, data=fpath),\n",
        "              PopupMenuItem(icon=icons.DELETE, text=\"Delete Image\", on_click=delete_image, data=fpath),\n",
        "              PopupMenuItem(icon=icons.DELETE_SWEEP, text=\"Delete All\", on_click=delete_all_images, data=fpath),\n",
        "          ]), subtitle=TextField(label=\"Caption Image Description\", height=55, filled=True, content_padding=padding.only(top=12, left=12)), data=fpath, on_click=image_details))\n",
        "        if update: page.lora_file_list.update()\n",
        "    def file_picker_result(e: FilePickerResultEvent):\n",
        "        if e.files != None:\n",
        "          upload_files(e)\n",
        "    save_dir = os.path.join(root_dir, 'my_model')\n",
        "    def on_upload_progress(e: FilePickerUploadEvent):\n",
        "        if e.progress == 1:\n",
        "          if not os.path.exists(save_dir):\n",
        "            os.mkdir(save_dir)\n",
        "          if not slash in e.file_name:\n",
        "            fname = os.path.join(root_dir, e.file_name)\n",
        "            fpath = os.path.join(save_dir, e.file_name)\n",
        "          else:\n",
        "            fname = e.file_name\n",
        "            fpath = os.path.join(save_dir, e.file_name.rpartition(slash)[2])\n",
        "          original_img = PILImage.open(fname)\n",
        "          width, height = original_img.size\n",
        "          width, height = scale_dimensions(width, height, LoRA_prefs['resolution'])\n",
        "          original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert(\"RGB\")\n",
        "          original_img.save(fpath)\n",
        "          if page.web: os.remove(fname)\n",
        "          #shutil.move(fname, fpath)\n",
        "          add_file(fpath)\n",
        "    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)\n",
        "    def pick_path(e):\n",
        "        file_picker.pick_files(allow_multiple=True, allowed_extensions=[\"png\", \"PNG\", \"jpg\", \"jpeg\"], dialog_title=\"Pick Image File to Enlarge\")\n",
        "    def upload_files(e):\n",
        "        uf = []\n",
        "        if file_picker.result != None and file_picker.result.files != None:\n",
        "            for f in file_picker.result.files:\n",
        "              if page.web:\n",
        "                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))\n",
        "              else:\n",
        "                on_upload_progress(FilePickerUploadEvent(f.path, 1, \"\"))\n",
        "            file_picker.upload(uf)\n",
        "    page.overlay.append(file_picker)\n",
        "    def add_image(e):\n",
        "        save_dir = os.path.join(root_dir, 'my_model')\n",
        "        if not os.path.exists(save_dir):\n",
        "          os.mkdir(save_dir)\n",
        "        if image_path.value.startswith('http'):\n",
        "          import requests\n",
        "          from io import BytesIO\n",
        "          response = requests.get(image_path.value)\n",
        "          fpath = os.path.join(save_dir, image_path.value.rpartition(slash)[2])\n",
        "          model_image = PILImage.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "          width, height = model_image.size\n",
        "          width, height = scale_dimensions(width, height, LoRA_prefs['resolution'])\n",
        "          model_image = model_image.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert(\"RGB\")\n",
        "          model_image.save(fpath)\n",
        "          add_file(fpath)\n",
        "        elif os.path.isfile(image_path.value):\n",
        "          fpath = os.path.join(save_dir, image_path.value.rpartition(slash)[2])\n",
        "          original_img = PILImage.open(image_path.value)\n",
        "          width, height = original_img.size\n",
        "          width, height = scale_dimensions(width, height, LoRA_prefs['resolution'])\n",
        "          original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert(\"RGB\")\n",
        "          original_img.save(fpath)\n",
        "          #shutil.copy(image_path.value, fpath)\n",
        "          add_file(fpath)\n",
        "        elif os.path.isdir(image_path.value):\n",
        "          for f in os.listdir(image_path.value):\n",
        "            file_path = os.path.join(image_path.value, f)\n",
        "            if os.path.isdir(file_path): continue\n",
        "            if f.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "              fpath = os.path.join(save_dir, f)\n",
        "              original_img = PILImage.open(file_path)\n",
        "              width, height = original_img.size\n",
        "              width, height = scale_dimensions(width, height, LoRA_prefs['resolution'])\n",
        "              original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert(\"RGB\")\n",
        "              original_img.save(fpath)\n",
        "              #shutil.copy(file_path, fpath)\n",
        "              add_file(fpath)\n",
        "        else:\n",
        "          if bool(image_path.value):\n",
        "            alert_msg(page, \"Couldn't find a valid File, Path or URL...\")\n",
        "          else:\n",
        "            pick_path(e)\n",
        "          return\n",
        "        image_path.value = \"\"\n",
        "        image_path.update()\n",
        "    def load_images():\n",
        "        if os.path.exists(save_dir):\n",
        "          for f in os.listdir(save_dir):\n",
        "            existing = os.path.join(save_dir, f)\n",
        "            if os.path.isdir(existing): continue\n",
        "            if f.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "              add_file(existing, update=False)\n",
        "    def toggle_save(e):\n",
        "        changed(e, 'save_model')\n",
        "        where_to_save_model.visible = LoRA_prefs['save_model']\n",
        "        where_to_save_model.update()\n",
        "        readme_description.visible = LoRA_prefs['save_model']\n",
        "        readme_description.update()\n",
        "    validation_prompt = Container(content=Tooltip(message=\"A prompt that is sampled during training for inference.\", content=TextField(label=\"Validation Prompt Text\", value=LoRA_prefs['validation_prompt'], on_change=lambda e:changed(e,'validation_prompt'))), col={'md':9})\n",
        "    name_of_your_model = TextField(label=\"Name of your Model\", value=LoRA_prefs['name_of_your_model'], on_change=lambda e:changed(e,'name_of_your_model'), col={'md':3})\n",
        "    #class_prompt = TextField(label=\"Class Prompt\", value=LoRA_prefs['class_prompt'], on_change=lambda e:changed(e,'class_prompt'))\n",
        "    #'num_validation_images': 4, #Number of images that should be generated during validation with `validation_prompt`.\n",
        "    #'validation_epochs': 1, #Run fine-tuning validation every X epochs. The validation process consists of running the prompt\n",
        "    num_validation_images = Tooltip(message=\"Number of images that should be generated during validation with `validation_prompt`\", content=TextField(label=\"# of Validation Images\", value=LoRA_prefs['num_validation_images'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'num_validation_images', ptype='int'), width = 160))\n",
        "    validation_epochs = Tooltip(message=\"Run fine-tuning validation every X epochs. The validation process consists of running the prompt\", content=TextField(label=\"Validation Epochs\", value=LoRA_prefs['validation_epochs'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'validation_epochs', ptype='int'), width = 160))\n",
        "    lr_scheduler = Dropdown(label=\"Learning Rate Scheduler\", width=250, options=[dropdown.Option(\"constant\"), dropdown.Option(\"constant_with_warmup\"), dropdown.Option(\"linear\"), dropdown.Option(\"cosine\"), dropdown.Option(\"cosine_with_restarts\"), dropdown.Option(\"polynomial\")], value=LoRA_prefs['lr_scheduler'], on_change=lambda e: changed(e, 'lr_scheduler'))\n",
        "    prior_preservation = Checkbox(label=\"Prior Preservation\", tooltip=\"If you'd like class of the model (e.g.: toy, dog, painting) is guaranteed to be preserved. This increases the quality and helps with generalization at the cost of training time\", value=LoRA_prefs['prior_preservation'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'prior_preservation'))\n",
        "    gradient_checkpointing = Checkbox(label=\"Gradient Checkpointing   \", tooltip=\"Whether or not to use gradient checkpointing to save memory at the expense of slower backward pass.\", value=LoRA_prefs['gradient_checkpointing'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'gradient_checkpointing'))\n",
        "    #num_class_images = Tooltip(message=\"Minimal class images for prior preservation loss. If there are not enough images already present in class_data_dir, additional images will be sampled with class_prompt.\", content=TextField(label=\"Number of Class Images\", value=LoRA_prefs['num_class_images'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'num_class_images', ptype='int'), width = 160))\n",
        "    #sample_batch_size = Tooltip(message=\"Batch size (per device) for sampling images.\", content=TextField(label=\"Sample Batch Size\", value=LoRA_prefs['sample_batch_size'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'sample_batch_size', ptype='int'), width = 160))\n",
        "    train_batch_size = Tooltip(message=\"Batch size (per device) for the training dataloader.\", content=TextField(label=\"Train Batch Size\", value=LoRA_prefs['train_batch_size'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'train_batch_size', ptype='int'), width = 160))\n",
        "    #prior_loss_weight = Tooltip(message=\"The weight of prior preservation loss.\", content=TextField(label=\"Prior Loss Weight\", value=LoRA_prefs['prior_loss_weight'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'prior_loss_weight', ptype='float'), width = 160))\n",
        "    max_train_steps = Tooltip(message=\"Total number of training steps to perform.  If provided, overrides num_train_epochs.\", content=TextField(label=\"Max Training Steps\", value=LoRA_prefs['max_train_steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'max_train_steps', ptype='int'), width = 160))\n",
        "    gradient_accumulation_steps = Tooltip(message=\"Number of updates steps to accumulate before performing a backward/update pass.\", content=TextField(label=\"Gradient Accumulation Steps\", value=LoRA_prefs['gradient_accumulation_steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'gradient_accumulation_steps', ptype='int'), width = 160))\n",
        "    learning_rate = Tooltip(message=\"Initial learning rate (after the potential warmup period) to use.\", content=TextField(label=\"Learning Rate\", value=LoRA_prefs['learning_rate'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'learning_rate', ptype='float'), width = 160))\n",
        "    lr_warmup_steps = Tooltip(message=\"Number of steps for the warmup in the lr scheduler.\", content=TextField(label=\"LR Warmup Steps\", value=LoRA_prefs['lr_warmup_steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'lr_warmup_steps', ptype='int'), width = 160))\n",
        "    #lr_num_cycles = Tooltip(message=\"Number of hard resets of the lr in cosine_with_restarts scheduler.\", content=TextField(label=\"LR Number of Cycles\", value=LoRA_prefs['lr_num_cycles'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'lr_num_cycles', ptype='int'), width = 160))\n",
        "    #lr_power = Tooltip(message=\"Power factor of the polynomial scheduler.\", content=TextField(label=\"LR Power\", value=LoRA_prefs['lr_power'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'lr_power', ptype='int'), width = 160))\n",
        "    seed = Tooltip(message=\"0 or -1 for Random. Pick any number.\", content=TextField(label=\"Seed\", value=LoRA_prefs['seed'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'seed', ptype='int'), width = 160))\n",
        "    save_model = Checkbox(label=\"Save Model to HuggingFace   \", tooltip=\"\", value=LoRA_prefs['save_model'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'save_model'))\n",
        "    save_model = Tooltip(message=\"Requires WRITE access on API Key to Upload Checkpoint\", content=Switch(label=\"Save Model to HuggingFace    \", value=LoRA_prefs['save_model'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_save))\n",
        "    where_to_save_model = Dropdown(label=\"Where to Save Model\", width=250, options=[dropdown.Option(\"Public HuggingFace\"), dropdown.Option(\"Private HuggingFace\")], value=LoRA_prefs['where_to_save_model'], on_change=lambda e: changed(e, 'where_to_save_model'))\n",
        "    #class_data_dir = TextField(label=\"Prior Preservation Class Folder\", value=LoRA_prefs['class_data_dir'], on_change=lambda e:changed(e,'class_data_dir'))\n",
        "    readme_description = TextField(label=\"Extra README Description\", value=LoRA_prefs['readme_description'], on_change=lambda e:changed(e,'readme_description'))\n",
        "    max_row = SliderRow(label=\"Max Resolution Size\", min=256, max=1024, divisions=6, multiple=64, suffix=\"px\", pref=LoRA_prefs, key='resolution')\n",
        "    image_path = TextField(label=\"Image File or Folder Path or URL to Train\", value=LoRA_prefs['image_path'], on_change=lambda e:changed(e,'image_path'), suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_path), expand=1)\n",
        "    add_image_button = ElevatedButton(content=Text(\"Add File or Folder\"), on_click=add_image)\n",
        "    page.lora_file_list = Column([], tight=True, spacing=0)\n",
        "    load_images()\n",
        "    where_to_save_model.visible = LoRA_prefs['save_model']\n",
        "    readme_description.visible = LoRA_prefs['save_model']\n",
        "    #lambda_entropy = TextField(label=\"Lambda Entropy\", value=dreamfusLoRA_prefsion_prefs['lambda_entropy'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'lambda_entropy', ptype='float'), width = 160)\n",
        "    #max_steps = TextField(label=\"Max Steps\", value=LoRA_prefs['max_steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'max_steps', ptype='int'), width = 160)\n",
        "    page.LoRA_output = Column([])\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(page.LoRA_output.controls) > 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üå´Ô∏è  Training text-to-image Low-Rank Adaptation of Large Language Models (LoRA)\", \"Provide a collection of images to train. Smaller sized. Adds on to the currently loaded Model Checkpoint...\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with LoRA DreamBooth Settings\", on_click=lora_help)]),\n",
        "        ResponsiveRow([validation_prompt, name_of_your_model]),\n",
        "        Row([num_validation_images, validation_epochs, train_batch_size]),\n",
        "        Row([prior_preservation, gradient_checkpointing]),\n",
        "        Row([learning_rate, lr_warmup_steps, lr_scheduler]),\n",
        "        Row([max_train_steps, gradient_accumulation_steps, seed]),\n",
        "        Row([save_model, where_to_save_model]),\n",
        "        readme_description,\n",
        "        #Row([class_data_dir]),\n",
        "        max_row,\n",
        "        Row([image_path, add_image_button]),\n",
        "        page.lora_file_list,\n",
        "        Row([ElevatedButton(content=Text(\"üèÑ  Run LoRA Training\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_LoRA(page))]),\n",
        "        page.LoRA_output,\n",
        "        clear_button,\n",
        "      ]\n",
        "    ))], scroll=ScrollMode.AUTO)\n",
        "    return c\n",
        "\n",
        "converter_prefs = {\n",
        "    'from_format': 'ckpt',\n",
        "    'to_format': 'pytorch',\n",
        "    'model_path': '',\n",
        "    'model_name': '',\n",
        "    'base_model': '',\n",
        "    'model_type': 'SD v1.x text2image',\n",
        "    'scheduler_type': 'pndm',\n",
        "    'half_percision': True,\n",
        "    'save_model': False,\n",
        "    'where_to_save_model': \"Public HuggingFace\",\n",
        "    'readme_description': '',\n",
        "    'load_custom_model': True,\n",
        "}\n",
        "\n",
        "def buildConverter(page):\n",
        "    global prefs, converter_prefs\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "        if pref is not None:\n",
        "          try:\n",
        "            if ptype == \"int\":\n",
        "              converter_prefs[pref] = int(e.control.value)\n",
        "            elif ptype == \"float\":\n",
        "              converter_prefs[pref] = float(e.control.value)\n",
        "            else:\n",
        "              converter_prefs[pref] = e.control.value\n",
        "          except Exception:\n",
        "            alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "            pass\n",
        "    def add_to_converter_output(o):\n",
        "        page.converter_output.controls.append(o)\n",
        "        page.converter_output.update()\n",
        "    def clear_output(e):\n",
        "        if prefs['enable_sounds']: page.snd_delete.play()\n",
        "        page.converter_output.controls = []\n",
        "        page.converter_output.update()\n",
        "        clear_button.visible = False\n",
        "        clear_button.update()\n",
        "    def converter_help(e):\n",
        "        def close_converter_dlg(e):\n",
        "          nonlocal converter_help_dlg\n",
        "          converter_help_dlg.open = False\n",
        "          page.update()\n",
        "        converter_help_dlg = AlertDialog(title=Text(\"üíÅ   Help with Converters\"), content=Column([\n",
        "            Text(\"Because there have been so many competing formats for Stable Diffusion models, we here have standardized with HuggingFace Diffusers, which is great but doesn't support all the Checkpoint Model types that are out there in the wild.  This should allow you to take other peoples custom trained model files and convert it to the better Diffusers PyTorch format, and then it'll save your Custom Model Checkpoint to HuggingFace (free) to reuse and/or share.\"),\n",
        "          ], scroll=ScrollMode.AUTO), actions=[TextButton(\"üéà  Handy... \", on_click=close_converter_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "        page.dialog = converter_help_dlg\n",
        "        converter_help_dlg.open = True\n",
        "        page.update()\n",
        "    def toggle_save(e):\n",
        "        changed(e, 'save_model')\n",
        "        where_to_save_model.visible = converter_prefs['save_model']\n",
        "        where_to_save_model.update()\n",
        "        readme_description.visible = converter_prefs['save_model']\n",
        "        readme_description.update()\n",
        "    def change_from_format(e):\n",
        "        changed(e, 'from_format')\n",
        "        base_model_row.visible = converter_prefs['from_format'] == \"lora_safetensors\"\n",
        "        base_model_row.update()\n",
        "        \n",
        "    from_format = Dropdown(label=\"From Format\", width=250, options=[dropdown.Option(\"ckpt\"), dropdown.Option(\"safetensors\"), dropdown.Option(\"lora_safetensors\"), dropdown.Option(\"controlnet\"), dropdown.Option(\"KerasCV\")], value=converter_prefs['from_format'], on_change=change_from_format, col={'lg':6})\n",
        "    to_format = Dropdown(label=\"To Format\", width=250, options=[dropdown.Option(\"pytorch\"), dropdown.Option(\"safetensors\"), dropdown.Option(\"dance_diffusion\")], value=converter_prefs['to_format'], on_change=lambda e: changed(e, 'to_format'), col={'lg':6})\n",
        "    #instance_prompt = Container(content=Tooltip(message=\"The prompt with identifier specifying the instance\", content=TextField(label=\"Instance Prompt Token Text\", value=converter_prefs['instance_prompt'], on_change=lambda e:changed(e,'instance_prompt'))), col={'md':9})\n",
        "    from_model_path = TextField(label=\"Model Path to HuggingFace or .ckpt or .safetensors file\", value=converter_prefs['model_path'], on_change=lambda e:changed(e,'model_path'), col={'md':6})\n",
        "    from_model_name = TextField(label=\"Name of your Model\", value=converter_prefs['model_name'], on_change=lambda e:changed(e,'model_name'), col={'md':6})\n",
        "    model_type = Dropdown(label=\"Model Type\", width=250, options=[dropdown.Option(\"SD v1.x text2image\"), dropdown.Option(\"SD v2.x text2image\")], value=converter_prefs['model_type'], on_change=lambda e: changed(e, 'model_type'), col={'lg':6})\n",
        "    base_model = TextField(label=\"Base Model Path to HuggingFace Diffusers\", value=converter_prefs['base_model'], on_change=lambda e:changed(e,'base_model'), col={'md':6})\n",
        "    base_model_row = ResponsiveRow([base_model])\n",
        "    base_model_row.visible = converter_prefs['from_format'] == \"lora_safetensors\"\n",
        "    #sd_version = Dropdown(label=\"Stable Diffusion Version\", width=250, options=[dropdown.Option(\"text2image\")], value=converter_prefs['model_type'], on_change=lambda e: changed(e, 'model_type'), col={'lg':6})\n",
        "    #class_prompt = TextField(label=\"Class Prompt\", value=converter_prefs['class_prompt'], on_change=lambda e:changed(e,'class_prompt'))\n",
        "    scheduler_type = Dropdown(label=\"Original Scheduler Mode\", hint_text=\"Hopefuly you know what Scheduler/Sampler they used in training\", width=200,\n",
        "            options=[\n",
        "                dropdown.Option(\"pndm\"),\n",
        "                dropdown.Option(\"lms\"),\n",
        "                dropdown.Option(\"ddim\"),\n",
        "                dropdown.Option(\"euler\"),\n",
        "                dropdown.Option(\"euler-ancestral\"),\n",
        "                dropdown.Option(\"dpm\"),\n",
        "            ], value=converter_prefs['scheduler_type'], autofocus=False, on_change=lambda e:changed(e, 'scheduler_type'), col={'lg':6},\n",
        "        )\n",
        "    #save_model = Checkbox(label=\"Save Model to HuggingFace   \", tooltip=\"\", value=converter_prefs['save_model'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'save_model'))\n",
        "    save_model = Tooltip(message=\"Requires WRITE access on API Key to Upload Checkpoint\", content=Switch(label=\"Save Model to HuggingFace    \", value=converter_prefs['save_model'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_save))\n",
        "    half_percision = Tooltip(message=\"Save weights in half precision.\", content=Switch(label=\"Save Half Percision Float16    \", value=converter_prefs['half_percision'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e: changed(e, 'half_percision')))\n",
        "    where_to_save_model = Dropdown(label=\"Where to Save Model\", width=250, options=[dropdown.Option(\"Public HuggingFace\"), dropdown.Option(\"Private HuggingFace\")], value=converter_prefs['where_to_save_model'], on_change=lambda e: changed(e, 'where_to_save_model'))\n",
        "    #class_data_dir = TextField(label=\"Prior Preservation Class Folder\", value=converter_prefs['class_data_dir'], on_change=lambda e:changed(e,'class_data_dir'))\n",
        "    readme_description = TextField(label=\"Extra README Description\", value=converter_prefs['readme_description'], on_change=lambda e:changed(e,'readme_description'))\n",
        "    load_custom_model = Checkbox(label=\"Load Custom Model\", tooltip=\"After conversion is done, will put it in your Custom Model setting, ready to test out\", value=converter_prefs['load_custom_model'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'load_custom_model'))\n",
        "    #resolution = Slider(min=256, max=1024, divisions=6, label=\"{value}px\", value=float(converter_prefs['resolution']), expand=True, on_change=lambda e:changed(e,'resolution', ptype='int'))\n",
        "    #max_row = Row([Text(\"Max Resolution Size: \"), resolution])\n",
        "    #image_path = TextField(label=\"Image File or Folder Path or URL to Train\", value=converter_prefs['image_path'], on_change=lambda e:changed(e,'image_path'), suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_path), expand=1)\n",
        "    #add_image_button = ElevatedButton(content=Text(\"Add File or Folder\"), on_click=add_image)\n",
        "    #page.converter_file_list = Column([], tight=True, spacing=0)\n",
        "    #load_images()\n",
        "    where_to_save_model.visible = converter_prefs['save_model']\n",
        "    readme_description.visible = converter_prefs['save_model']\n",
        "    #lambda_entropy = TextField(label=\"Lambda Entropy\", value=dreamfusconverter_prefsion_prefs['lambda_entropy'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'lambda_entropy', ptype='float'), width = 160)\n",
        "    #max_steps = TextField(label=\"Max Steps\", value=converter_prefs['max_steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'max_steps', ptype='int'), width = 160)\n",
        "    page.converter_output = Column([])\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(page.converter_output.controls) > 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üîÄ  Model Converter Tool\", \"Lets you Convert Format of Model Checkpoints to work with Diffusers...\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with Model Converters Settings\", on_click=converter_help)]),\n",
        "        ResponsiveRow([from_format, to_format]),\n",
        "        ResponsiveRow([from_model_path, from_model_name]),\n",
        "        base_model_row,\n",
        "        ResponsiveRow([model_type, scheduler_type]),\n",
        "        half_percision,\n",
        "        Row([save_model, where_to_save_model]),\n",
        "        readme_description,\n",
        "        load_custom_model,\n",
        "        #max_row,\n",
        "        #Row([image_path, add_image_button]),\n",
        "        #page.converter_file_list,\n",
        "        Row([ElevatedButton(content=Text(\"„ÄΩÔ∏è  Run Model Converter\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_converter(page))]),\n",
        "        page.converter_output,\n",
        "        clear_button,\n",
        "      ]\n",
        "    ))], scroll=ScrollMode.AUTO)\n",
        "    return c\n",
        "\n",
        "checkpoint_merger_prefs = {\n",
        "    'pretrained_model': '',\n",
        "    'selected_model': 'Stable Diffusion v1.5',\n",
        "    'pretrained_models': [], #A list of valid pretrained model names in the HuggingFace hub or paths to locally stored models in the HuggingFace format.\n",
        "    'alpha': 0.5, #The interpolation parameter. Ranges from 0 to 1.  It affects the ratio in which the checkpoints are merged. A 0.8 alpha would mean that the first model checkpoints would affect the final result far less than an alpha of 0.2\n",
        "    'interp': 'weighted_sum', #The interpolation method to use for the merging. Supports \"sigmoid\", \"inv_sigmoid\", \"add_difference\" and None. Passing None uses the default interpolation which is weighted sum interpolation. For merging three checkpoints, only \"add_difference\" is supported.\n",
        "    'force': False, #Whether to ignore mismatch in model_config.json for the current models. Defaults to False.\n",
        "    'validation_prompt': '',\n",
        "    'name_of_your_model': '',\n",
        "    'save_model': True,\n",
        "    'where_to_save_model': 'Public HuggingFace',\n",
        "    'readme_description': '',\n",
        "}\n",
        "\n",
        "def buildCheckpointMerger(page):\n",
        "    global prefs, checkpoint_merger_prefs\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "        if pref is not None:\n",
        "          try:\n",
        "            if ptype == \"int\":\n",
        "              checkpoint_merger_prefs[pref] = int(e.control.value)\n",
        "            elif ptype == \"float\":\n",
        "              checkpoint_merger_prefs[pref] = float(e.control.value)\n",
        "            else:\n",
        "              checkpoint_merger_prefs[pref] = e.control.value\n",
        "          except Exception:\n",
        "            alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "            pass\n",
        "    def checkpoint_merger_help(e):\n",
        "        def close_checkpoint_merger_dlg(e):\n",
        "          nonlocal checkpoint_merger_help_dlg\n",
        "          checkpoint_merger_help_dlg.open = False\n",
        "          page.update()\n",
        "        checkpoint_merger_help_dlg = AlertDialog(title=Text(\"üíÅ   Help with Checkpoint Merger\"), content=Column([\n",
        "            Text(\"Provide a list of valid pretrained model names in the HuggingFace hub or paths to locally stored models in the HuggingFace format.  Merges the Checkpoint Weights into a new model that you can save for free to HuggingFace to reuse.\"),\n",
        "          ], scroll=ScrollMode.AUTO), actions=[TextButton(\"üçª  Sure thing... \", on_click=close_checkpoint_merger_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "        page.dialog = checkpoint_merger_help_dlg\n",
        "        checkpoint_merger_help_dlg.open = True\n",
        "        page.update()\n",
        "    def toggle_save(e):\n",
        "        changed(e, 'save_model')\n",
        "        where_to_save_model.visible = checkpoint_merger_prefs['save_model']\n",
        "        where_to_save_model.update()\n",
        "        readme_description.visible = checkpoint_merger_prefs['save_model']\n",
        "        readme_description.update()\n",
        "    def remove_model(e):\n",
        "        f = e.control.data\n",
        "        for i, fl in enumerate(page.checkpoint_merger_file_list.controls):\n",
        "            if fl.title.value == f:\n",
        "                del page.checkpoint_merger_file_list.controls[i]\n",
        "                page.checkpoint_merger_file_list.update()\n",
        "                continue\n",
        "    def remove_all_models(e):\n",
        "        checkpoint_merger_prefs['pretrained_models'].clear()\n",
        "        page.checkpoint_merger_file_list.controls.clear()\n",
        "        page.checkpoint_merger_file_list.update()\n",
        "    def move_down(e):\n",
        "        idx = checkpoint_merger_prefs['pretrained_models'].index(e.control.data)\n",
        "        if idx < (len(checkpoint_merger_prefs['pretrained_models']) - 1):\n",
        "          d = checkpoint_merger_prefs['pretrained_models'].pop(idx)\n",
        "          checkpoint_merger_prefs['pretrained_models'].insert(idx+1, d)\n",
        "          dr = page.checkpoint_merger_file_list.controls.pop(idx)\n",
        "          page.checkpoint_merger_file_list.controls.insert(idx+1, dr)\n",
        "          page.checkpoint_merger_file_list.update()\n",
        "    def move_up(e):\n",
        "        idx = checkpoint_merger_prefs['pretrained_models'].index(e.control.data)\n",
        "        if idx > 0:\n",
        "          d = checkpoint_merger_prefs['pretrained_models'].pop(idx)\n",
        "          checkpoint_merger_prefs['pretrained_models'].insert(idx-1, d)\n",
        "          dr = page.checkpoint_merger_file_list.controls.pop(idx)\n",
        "          page.checkpoint_merger_file_list.controls.insert(idx-1, dr)\n",
        "          page.checkpoint_merger_file_list.update()\n",
        "    def add_selected_model(e):\n",
        "        name = checkpoint_merger_prefs['selected_model']\n",
        "        m = {'name':''}\n",
        "        if name == \"Stable Diffusion v2.1 x768\":\n",
        "            m = {'name':'Stable Diffusion v2.1 x768', 'path':'stabilityai/stable-diffusion-2-1'}\n",
        "        elif name == \"Stable Diffusion v2.1 x512\":\n",
        "            m = {'name':'Stable Diffusion v2.1 x512', 'path':'stabilityai/stable-diffusion-2-1-base'}\n",
        "        elif name == \"Stable Diffusion v2.0\":\n",
        "            m = {'name':'Stable Diffusion v2.0', 'path':'stabilityai/stable-diffusion-2'}\n",
        "        elif name == \"Stable Diffusion v2.0 x768\":\n",
        "            m = {'name':'Stable Diffusion v2.0 x768', 'path':'stabilityai/stable-diffusion-2'}\n",
        "        elif name == \"Stable Diffusion v2.0 x512\":\n",
        "            m = {'name':'Stable Diffusion v2.0 x512', 'path':'stabilityai/stable-diffusion-2-base'}\n",
        "        elif name == \"Stable Diffusion v1.5\":\n",
        "            m = {'name':'Stable Diffusion v1.5', 'path':'runwayml/stable-diffusion-v1-5'}\n",
        "        elif name == \"Stable Diffusion v1.4\":\n",
        "            m = {'name':'Stable Diffusion v1.4', 'path':'CompVis/stable-diffusion-v1-4'}\n",
        "        else:\n",
        "            m = get_finetuned_model(name)\n",
        "            if not bool(m['name']):\n",
        "                m = get_dreambooth_model(name)\n",
        "        if bool(m['path']):\n",
        "            add_model(m['path'])\n",
        "    def add_custom_model(e):\n",
        "        mpath = checkpoint_merger_prefs['pretrained_model']\n",
        "        add_model(mpath)\n",
        "    def add_model(mpath, update=True):\n",
        "        if mpath in checkpoint_merger_prefs['pretrained_models']:\n",
        "            alert_msg(page, \"That model path is already in your list...\")\n",
        "            return\n",
        "        page.checkpoint_merger_file_list.controls.append(ListTile(title=Text(mpath), dense=True, trailing=PopupMenuButton(icon=icons.MORE_VERT,\n",
        "          items=[#TODO: View Image\n",
        "              PopupMenuItem(icon=icons.DELETE, text=\"Remove Model\", on_click=remove_model, data=mpath),\n",
        "              PopupMenuItem(icon=icons.DELETE_SWEEP, text=\"Remove All\", on_click=remove_all_models, data=mpath),\n",
        "              PopupMenuItem(icon=icons.ARROW_UPWARD, text=\"Move Up\", on_click=move_up, data=mpath),\n",
        "              PopupMenuItem(icon=icons.ARROW_DOWNWARD, text=\"Move Down\", on_click=move_down, data=mpath),\n",
        "          ]), data=mpath))\n",
        "        checkpoint_merger_prefs['pretrained_models'].append(mpath)\n",
        "        if update: page.checkpoint_merger_file_list.update()\n",
        "    validation_prompt = Container(content=Tooltip(message=\"Optional prompt to test after the merger is finished.\", content=TextField(label=\"Validation Test Prompt\", value=checkpoint_merger_prefs['validation_prompt'], on_change=lambda e:changed(e,'validation_prompt'))), col={'md':9})\n",
        "    name_of_your_model = TextField(label=\"Name of New Model\", value=checkpoint_merger_prefs['name_of_your_model'], on_change=lambda e:changed(e,'name_of_your_model'), col={'md':3})\n",
        "    force = Checkbox(label=\"Force if Mismatch\", tooltip=\"Whether to ignore mismatch in model_config.json for the current models.\", value=checkpoint_merger_prefs['force'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'force'))\n",
        "    alpha_row = SliderRow(label=\"Alpha Interpolation\", min=0.0, max=1.0, divisions=20, round=2, pref=checkpoint_merger_prefs, key='alpha', tooltip=\"The interpolation parameter. Ranges from 0 to 1.  It affects the ratio in which the checkpoints are merged. A 0.8 alpha would mean that the first model checkpoints would affect the final result far less than an alpha of 0.2\")\n",
        "    interp = Dropdown(label=\"Interpolation Method\", width=250, options=[dropdown.Option(\"weighted_sum\"), dropdown.Option(\"sigmoid\"), dropdown.Option(\"inv_sigmoid\"), dropdown.Option(\"add_difference\")], value=checkpoint_merger_prefs['interp'], on_change=lambda e: changed(e, 'interp'))\n",
        "    #The interpolation method to use for the merging. Supports \"sigmoid\", \"inv_sigmoid\", \"add_difference\" and None. For merging three checkpoints, only \"add_difference\" is supported.\n",
        "    model_ckpt = Dropdown(label=\"Model Checkpoint\", options=[\n",
        "        dropdown.Option(\"Stable Diffusion v2.1 x768\"), dropdown.Option(\"Stable Diffusion v2.1 x512\"), \n",
        "        dropdown.Option(\"Stable Diffusion v2.0 x768\"), dropdown.Option(\"Stable Diffusion v2.0 x512\"), dropdown.Option(\"Stable Diffusion v1.5\"), dropdown.Option(\"Stable Diffusion v1.4\")], value=checkpoint_merger_prefs['selected_model'], on_change=lambda e: changed(e, 'selected_model'))\n",
        "    for mod in finetuned_models:\n",
        "        model_ckpt.options.append(dropdown.Option(mod[\"name\"]))\n",
        "    for db in dreambooth_models:\n",
        "        model_ckpt.options.append(dropdown.Option(db[\"name\"]))\n",
        "    add_selected_model_button = ElevatedButton(content=Text(\"Add Selected Model\"), on_click=add_selected_model)\n",
        "    pretrained_model = TextField(label=\"HuggingFace Path or Local Path to Merge\", value=checkpoint_merger_prefs['pretrained_model'], on_change=lambda e:changed(e,'pretrained_model'), expand=1)\n",
        "    add_model_button = ElevatedButton(content=Text(\"Add Model Path\"), on_click=add_custom_model)\n",
        "    save_model = Tooltip(message=\"Requires WRITE access on API Key to Upload Checkpoint\", content=Switch(label=\"Save Model to HuggingFace    \", value=checkpoint_merger_prefs['save_model'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_save))\n",
        "    where_to_save_model = Dropdown(label=\"Where to Save Model\", width=250, options=[dropdown.Option(\"Public HuggingFace\"), dropdown.Option(\"Private HuggingFace\")], value=checkpoint_merger_prefs['where_to_save_model'], on_change=lambda e: changed(e, 'where_to_save_model'))\n",
        "    readme_description = TextField(label=\"Extra README Description\", value=checkpoint_merger_prefs['readme_description'], on_change=lambda e:changed(e,'readme_description'))\n",
        "\n",
        "    page.checkpoint_merger_file_list = Column([], tight=True, spacing=0)\n",
        "    page.checkpoint_merger_output = Column([])\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üë•  Checkpoint Merger Tool\", \"Combine together two or more custom models to create a mixture of weights...\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with Checkpoint Merger Settings\", on_click=checkpoint_merger_help)]),\n",
        "        Row([model_ckpt, add_selected_model_button]),\n",
        "        Row([pretrained_model, add_model_button]),\n",
        "        page.checkpoint_merger_file_list,\n",
        "        Divider(thickness=3, height=6),\n",
        "        Row([interp, force]),\n",
        "        alpha_row,\n",
        "        ResponsiveRow([name_of_your_model, validation_prompt]),\n",
        "        Row([save_model, where_to_save_model]),\n",
        "        readme_description,\n",
        "        Row([ElevatedButton(content=Text(\"ü§ó  Run Checkpoint Merger\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_checkpoint_merger(page))]),\n",
        "        page.checkpoint_merger_output,\n",
        "        #clear_button,\n",
        "      ]\n",
        "    ))], scroll=ScrollMode.AUTO)\n",
        "    return c\n",
        "\n",
        "\n",
        "tortoise_prefs = {\n",
        "    'text': '',\n",
        "    'preset': 'standard', #\"ultra_fast\", \"fast\", \"standard\", \"high_quality\"\n",
        "    'voice': [],\n",
        "    'voices': ['angie', 'applejack', 'daniel', 'deniro', 'emma', 'freeman', 'geralt', 'halle', 'jlaw', 'lj', 'mol', 'myself', 'pat', 'pat2', 'rainbow', 'snakes', 'tim_reynolds', 'tom', 'train_atkins', 'train_daws', 'train_dotrice', 'train_dreams', 'train_empire', 'train_grace', 'train_kennard', 'train_lescault', 'train_mouse', 'weaver', 'william'],\n",
        "    'train_custom': False,\n",
        "    'custom_voice_name': '',\n",
        "    'custom_wavs': [],\n",
        "    'wav_path': '',\n",
        "    'batch_folder_name': '',\n",
        "    'file_prefix': 'tts-',\n",
        "}\n",
        "\n",
        "def buildTortoiseTTS(page):\n",
        "    global prefs, tortoise_prefs\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "        if pref is not None:\n",
        "          try:\n",
        "            if ptype == \"int\":\n",
        "              tortoise_prefs[pref] = int(e.control.value)\n",
        "            elif ptype == \"float\":\n",
        "              tortoise_prefs[pref] = float(e.control.value)\n",
        "            else:\n",
        "              tortoise_prefs[pref] = e.control.value\n",
        "          except Exception:\n",
        "            alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "            pass\n",
        "    def add_to_tortoise_output(o):\n",
        "        page.tortoise_output.controls.append(o)\n",
        "        page.tortoise_output.update()\n",
        "    def clear_output(e):\n",
        "        if prefs['enable_sounds']: page.snd_delete.play()\n",
        "        page.tortoise_output.controls = []\n",
        "        page.tortoise_output.update()\n",
        "        clear_button.visible = False\n",
        "        clear_button.update()\n",
        "    def tortoise_help(e):\n",
        "        def close_tortoise_dlg(e):\n",
        "          nonlocal tortoise_help_dlg\n",
        "          tortoise_help_dlg.open = False\n",
        "          page.update()\n",
        "        tortoise_help_dlg = AlertDialog(title=Text(\"üíÅ   Help with Tortoise-TTS\"), content=Column([\n",
        "            Text(\"Tortoise was specifically trained to be a multi-speaker model. It accomplishes this by consulting reference clips. These reference clips are recordings of a speaker that you provide to guide speech generation. These clips are used to determine many properties of the output, such as the pitch and tone of the voice, speaking speed, and even speaking defects like a lisp or stuttering. The reference clip is also used to determine non-voice related aspects of the audio output like volume, background noise, recording quality and reverb.\"),\n",
        "            Text(\"This comes with several pre-packaged voices. Voices prepended with 'train_' came from the training set and perform far better than the others. If your goal is high quality speech, we recommend you pick one of them. If you want to see what Tortoise can do for zero-shot mimicing, take a look at the others.\"),\n",
        "            Text(\"To add new voices to Tortoise, you will need to do the following: Gather audio clips of your speaker(s). Good sources are YouTube interviews (you can use youtube-dl to fetch the audio), audiobooks or podcasts. Guidelines for good clips are in the next section. Cut your clips into ~10 second segments. You want at least 3 clips. More is better, but I only experimented with up to 5 in my testing. Save the clips as a WAV file with floating point format and a 22,050 sample rate.\"),\n",
        "            Text(\"You can do prompt engineering with Tortoise to get the performance you want. For example, you can evoke emotion by including things like [I am really sad], before your text. It redacts the phrase in brackets, but keeps the context of the meaning to the voice reading. Experiment with grammar and spelling, and use an audio editor to perfect the vocals later.\"),\n",
        "          ], scroll=ScrollMode.AUTO), actions=[TextButton(\"üëÑ  What to say... \", on_click=close_tortoise_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "        page.dialog = tortoise_help_dlg\n",
        "        tortoise_help_dlg.open = True\n",
        "        page.update()\n",
        "    def delete_audio(e):\n",
        "        f = e.control.data\n",
        "        if os.path.isfile(f):\n",
        "          os.remove(f)\n",
        "          for i, fl in enumerate(page.tortoise_file_list.controls):\n",
        "            if fl.title.value == f:\n",
        "              del page.tortoise_file_list.controls[i]\n",
        "              page.tortoise_file_list.update()\n",
        "              if f in tortoise_prefs['custom_wavs']:\n",
        "                tortoise_prefs['custom_wavs'].remove(f)\n",
        "              continue\n",
        "    def delete_all_audios(e):\n",
        "        for fl in page.tortoise_file_list.controls:\n",
        "          f = fl.title.value\n",
        "          if os.path.isfile(f):\n",
        "            os.remove(f)\n",
        "        page.tortoise_file_list.controls.clear()\n",
        "        page.tortoise_file_list.update()\n",
        "        tortoise_prefs['custom_wavs'].clear()\n",
        "    def add_file(fpath, update=True):\n",
        "        page.tortoise_file_list.controls.append(ListTile(title=Text(fpath), dense=True, trailing=PopupMenuButton(icon=icons.MORE_VERT,\n",
        "          items=[#TODO: View Image\n",
        "              PopupMenuItem(icon=icons.DELETE, text=\"Delete Audio\", on_click=delete_audio, data=fpath),\n",
        "              PopupMenuItem(icon=icons.DELETE_SWEEP, text=\"Delete All\", on_click=delete_all_audios, data=fpath),\n",
        "          ])))\n",
        "        tortoise_prefs['custom_wavs'].append(fpath)\n",
        "        if update: page.tortoise_file_list.update()\n",
        "    def file_picker_result(e: FilePickerResultEvent):\n",
        "        if e.files != None:\n",
        "          upload_files(e)\n",
        "    save_dir = os.path.join(root_dir, 'tortoise-audio')\n",
        "    def on_upload_progress(e: FilePickerUploadEvent):\n",
        "        if e.progress == 1:\n",
        "          if not os.path.exists(save_dir):\n",
        "            os.mkdir(save_dir)\n",
        "          if not slash in e.file_name:\n",
        "            fname = os.path.join(root_dir, e.file_name)\n",
        "            fpath = os.path.join(save_dir, e.file_name)\n",
        "            shutil.move(fname, fpath)\n",
        "          else:\n",
        "            fname = e.file_name\n",
        "            fpath = os.path.join(save_dir, e.file_name.rpartition(slash)[2])\n",
        "            shutil.copy(fname, fpath)\n",
        "          add_file(fpath)\n",
        "    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)\n",
        "    def pick_path(e):\n",
        "        file_picker.pick_files(allow_multiple=True, allowed_extensions=[\"wav\", \"WAV\", \"mp3\", \"MP3\"], dialog_title=\"Pick Voice WAV or MP3 Files to Train\")\n",
        "    def upload_files(e):\n",
        "        uf = []\n",
        "        if file_picker.result != None and file_picker.result.files != None:\n",
        "            for f in file_picker.result.files:\n",
        "              if page.web:\n",
        "                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))\n",
        "              else:\n",
        "                on_upload_progress(FilePickerUploadEvent(f.path, 1, \"\"))\n",
        "            file_picker.upload(uf)\n",
        "    page.overlay.append(file_picker)\n",
        "    def add_wav(e):\n",
        "        if not os.path.exists(save_dir):\n",
        "            os.mkdir(save_dir)\n",
        "        if wav_path.value.startswith('http'):\n",
        "            import requests\n",
        "            from io import BytesIO\n",
        "            #response = requests.get(wav_path.value)\n",
        "            fpath = download_file(wav_path.value)\n",
        "            #fpath = os.path.join(save_dir, wav_path.value.rpartition(slash)[2])\n",
        "            add_file(fpath)\n",
        "        elif os.path.isfile(wav_path.value):\n",
        "          fpath = os.path.join(save_dir, wav_path.value.rpartition(slash)[2])\n",
        "          shutil.copy(wav_path.value, fpath)\n",
        "          add_file(fpath)\n",
        "        elif os.path.isdir(wav_path.value):\n",
        "          for f in os.listdir(wav_path.value):\n",
        "            file_path = os.path.join(wav_path.value, f)\n",
        "            if os.path.isdir(file_path): continue\n",
        "            if f.lower().endswith(('.wav', '.WAV', '.mp3', '.MP3')):\n",
        "              fpath = os.path.join(save_dir, f)\n",
        "              shutil.copy(file_path, fpath)\n",
        "              add_file(fpath)\n",
        "        else:\n",
        "          if bool(wav_path.value):\n",
        "            alert_msg(page, \"Couldn't find a valid File, Path or URL...\")\n",
        "          else:\n",
        "            pick_path(e)\n",
        "          return\n",
        "        wav_path.value = \"\"\n",
        "        wav_path.update()\n",
        "    def load_wavs():\n",
        "        if os.path.exists(save_dir):\n",
        "          for f in os.listdir(save_dir):\n",
        "            existing = os.path.join(save_dir, f)\n",
        "            if os.path.isdir(existing): continue\n",
        "            if f.lower().endswith(('.wav', '.WAV', '.mp3', '.MP3')):\n",
        "              add_file(existing, update=False)\n",
        "    def toggle_custom(e):\n",
        "        changed(e, 'train_custom')\n",
        "        custom_box.height = None if tortoise_prefs['train_custom'] else 0\n",
        "        custom_box.update()\n",
        "        custom_voice_name.visible = tortoise_prefs['train_custom']\n",
        "        custom_voice_name.update()\n",
        "    text = TextField(label=\"Text to Read\", value=tortoise_prefs['text'], multiline=True, min_lines=1, max_lines=8, on_change=lambda e:changed(e,'text'))\n",
        "    preset = Dropdown(label=\"Quality Preset\", width=250, options=[dropdown.Option(\"ultra_fast\"), dropdown.Option(\"fast\"), dropdown.Option(\"standard\"), dropdown.Option(\"high_quality\")], value=tortoise_prefs['preset'], on_change=lambda e: changed(e, 'preset'))\n",
        "    batch_folder_name = TextField(label=\"Batch Folder Name\", value=tortoise_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    file_prefix = TextField(label=\"Filename Prefix\", value=tortoise_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))\n",
        "    page.tortoise_voices = ResponsiveRow(controls=[])\n",
        "    for v in tortoise_prefs['voices']:\n",
        "      page.tortoise_voices.controls.append(Checkbox(label=v, fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, col={'xs':12, 'sm':6, 'md':3, 'lg':3, 'xl': 2}))\n",
        "    if len(prefs['tortoise_custom_voices']) > 0:\n",
        "      for custom in prefs['tortoise_custom_voices']:\n",
        "        page.tortoise_voices.controls.append(Checkbox(label=custom['name'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, col={'xs':12, 'sm':6, 'md':3, 'lg':3, 'xl': 2}))\n",
        "    custom_voice_name = TextField(label=\"Custom Voice Name\", value=tortoise_prefs['custom_voice_name'], on_change=lambda e:changed(e,'custom_voice_name'))\n",
        "    train_custom = Switch(label=\"Train Custom Voice  \", value=tortoise_prefs['train_custom'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_custom)\n",
        "    wav_path = TextField(label=\"Audio Files or Folder Path or URL to Train\", value=tortoise_prefs['wav_path'], on_change=lambda e:changed(e,'wav_path'), suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_path), expand=1)\n",
        "    add_wav_button = ElevatedButton(content=Text(\"Add Audio Files\"), on_click=add_wav)\n",
        "    page.tortoise_file_list = Column([], tight=True, spacing=0)\n",
        "    custom_box = Container(Column([Text(\"Provide 3 or more ~10 second clips of voice as mp3 or wav files with 22050 sample rate:\"),\n",
        "        Row([wav_path, add_wav_button]),\n",
        "        page.tortoise_file_list,]), animate_size=animation.Animation(1000, AnimationCurve.EASE_IN_CIRC), clip_behavior=ClipBehavior.HARD_EDGE)\n",
        "    custom_box.height = None if tortoise_prefs['train_custom'] else 0\n",
        "    custom_voice_name.visible = tortoise_prefs['train_custom']\n",
        "    load_wavs()\n",
        "    #seed = TextField(label=\"Seed\", value=tortoise_prefs['seed'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'seed', ptype='int'), width = 160)\n",
        "    #lambda_entropy = TextField(label=\"Lambda Entropy\", value=dreamfustortoise_prefsion_prefs['lambda_entropy'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'lambda_entropy', ptype='float'), width = 160)\n",
        "    #max_steps = TextField(label=\"Max Steps\", value=tortoise_prefs['max_steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'max_steps', ptype='int'), width = 160)\n",
        "    page.tortoise_output = Column([])\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(page.tortoise_output.controls) > 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üê¢  Tortoise Text-to-Speech Voice Modeling\", \"Reads your text in a realistic AI voice, train your own to mimic vocal performances...\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with Tortoise-TTS Settings\", on_click=tortoise_help)]),\n",
        "        text,\n",
        "        preset,\n",
        "        Row([batch_folder_name, file_prefix]),\n",
        "        Row([Text(\"Select one or more voices:\", weight=FontWeight.BOLD), Text(\"(none for random or custom)\")]),\n",
        "        page.tortoise_voices,\n",
        "        Row([train_custom, custom_voice_name], vertical_alignment=CrossAxisAlignment.START),\n",
        "        #Row([output_dir]),\n",
        "        custom_box,\n",
        "        ElevatedButton(content=Text(\"üó£Ô∏è  Run Tortoise-TTS\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_tortoise_tts(page)),\n",
        "        page.tortoise_output,\n",
        "        clear_button,\n",
        "      ]\n",
        "    ))], scroll=ScrollMode.AUTO)\n",
        "    return c\n",
        "\n",
        "audioLDM_prefs = {\n",
        "    'text': '',\n",
        "    'duration': 5.0,\n",
        "    'guidance_scale': 2.5,\n",
        "    'n_candidates': 3,#This number control the number of candidates (e.g., generate three audios and choose the best to show you). A Larger value usually lead to better quality with heavier computation\n",
        "    'seed': 0,\n",
        "    'wav_path': '',\n",
        "    'batch_folder_name': '',\n",
        "    'file_prefix': 'ldm-',\n",
        "}\n",
        "\n",
        "def buildAudioLDM(page):\n",
        "    global prefs, audioLDM_prefs\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "        if pref is not None:\n",
        "          try:\n",
        "            if ptype == \"int\":\n",
        "              audioLDM_prefs[pref] = int(e.control.value)\n",
        "            elif ptype == \"float\":\n",
        "              audioLDM_prefs[pref] = float(e.control.value)\n",
        "            else:\n",
        "              audioLDM_prefs[pref] = e.control.value\n",
        "          except Exception:\n",
        "            alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "            pass\n",
        "    def add_to_audioLDM_output(o):\n",
        "        page.audioLDM_output.controls.append(o)\n",
        "        page.audioLDM_output.update()\n",
        "    def clear_output(e):\n",
        "        if prefs['enable_sounds']: page.snd_delete.play()\n",
        "        page.audioLDM_output.controls = []\n",
        "        page.audioLDM_output.update()\n",
        "        clear_button.visible = False\n",
        "        clear_button.update()\n",
        "    def audioLDM_help(e):\n",
        "        def close_audioLDM_dlg(e):\n",
        "          nonlocal audioLDM_help_dlg\n",
        "          audioLDM_help_dlg.open = False\n",
        "          page.update()\n",
        "        audioLDM_help_dlg = AlertDialog(title=Text(\"üíÅ   Help with Audio-LDM\"), content=Column([\n",
        "            Text(\"AudioLDM is a TTA system that is built on a latent space to learn the continuous audio representations from contrastive language-audio pretraining (CLAP) latents. The pretrained CLAP models enable us to train LDMs with audio embedding while providing text embedding as a condition during sampling. By learning the latent representations of audio signals and their compositions without modeling the cross-modal relationship, AudioLDM is advantageous in both generation quality and computational efficiency. Trained on AudioCaps with a single GPU, AudioLDM achieves state-of-the-art TTA performance measured by both objective and subjective metrics (e.g., frechet distance). Moreover, AudioLDM is the first TTA system that enables various text-guided audio manipulations (e.g., style transfer) in a zero-shot fashion.\"),\n",
        "            Markdown(\"They built the model with data from [AudioSet](http://research.google.com/audioset/), [Freesound](https://freesound.org/) and [BBC Sound Effect library](https://sound-effects.bbcrewind.co.uk/). We share this demo based on the [UK copyright exception](https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/375954/Research.pdf) of data for academic research.\", on_tap_link=lambda e: e.page.launch_url(e.data)),\n",
        "          ], scroll=ScrollMode.AUTO), actions=[TextButton(\"üîî  Good to hear... \", on_click=close_audioLDM_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "        page.dialog = audioLDM_help_dlg\n",
        "        audioLDM_help_dlg.open = True\n",
        "        page.update()\n",
        "    duration_row = SliderRow(label=\"Duration\", min=1, max=20, divisions=38, round=1, suffix=\"s\", pref=audioLDM_prefs, key='duration')\n",
        "    guidance = SliderRow(label=\"Guidance Scale\", min=0, max=5, divisions=10, round=1, pref=audioLDM_prefs, key='guidance_scale', tooltip=\"Large => better quality and relavancy to text; Small => better diversity\")\n",
        "    text = TextField(label=\"Text Prompt to Auditorialize\", value=audioLDM_prefs['text'], multiline=True, min_lines=1, max_lines=8, on_change=lambda e:changed(e,'text'))\n",
        "    batch_folder_name = TextField(label=\"Batch Folder Name\", value=audioLDM_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    file_prefix = TextField(label=\"Filename Prefix\", value=audioLDM_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))\n",
        "    n_candidates = Tooltip(message=\"Automatic quality control. Generates candidates and choose the best. Larger value usually lead to better quality with heavier computation.\", content=NumberPicker(label=\"Number of Candidates:   \", min=1, max=5, value=audioLDM_prefs['n_candidates'], on_change=lambda e: changed(e, 'n_candidates')))\n",
        "    seed = TextField(label=\"Seed\", value=audioLDM_prefs['seed'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'seed', ptype='int'), width = 120)\n",
        "    page.audioLDM_output = Column([])\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(page.audioLDM_output.controls) > 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"ü¶ª  Audio LDM Modeling\", \"Text-to-Audio Generation with Latent Diffusion Model...\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with Audio LDM-TTS Settings\", on_click=audioLDM_help)]),\n",
        "        text,\n",
        "        duration_row,\n",
        "        guidance,\n",
        "        Row([n_candidates, seed]),\n",
        "        Row([batch_folder_name, file_prefix]),\n",
        "        ElevatedButton(content=Text(\"üëè  Run AudioLDM\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_audio_ldm(page)),\n",
        "        page.audioLDM_output,\n",
        "        clear_button,\n",
        "      ]\n",
        "    ))], scroll=ScrollMode.AUTO)\n",
        "    return c\n",
        "\n",
        "bark_prefs = {\n",
        "    'text': '',\n",
        "    'text_temp': 0.7,\n",
        "    'waveform_temp': 0.7,\n",
        "    'acoustic_prompt': 'Unconditional',\n",
        "    'n_iterations': 1,\n",
        "    'seed': 0,\n",
        "    'batch_folder_name': '',\n",
        "    'file_prefix': 'bark-',\n",
        "}\n",
        "\n",
        "def buildBark(page):\n",
        "    global prefs, bark_prefs\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "        if pref is not None:\n",
        "          try:\n",
        "            if ptype == \"int\":\n",
        "              bark_prefs[pref] = int(e.control.value)\n",
        "            elif ptype == \"float\":\n",
        "              bark_prefs[pref] = float(e.control.value)\n",
        "            else:\n",
        "              bark_prefs[pref] = e.control.value\n",
        "          except Exception:\n",
        "            alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "            pass\n",
        "    def add_to_bark_output(o):\n",
        "        page.bark_output.controls.append(o)\n",
        "        page.bark_output.update()\n",
        "    def clear_output(e):\n",
        "        if prefs['enable_sounds']: page.snd_delete.play()\n",
        "        page.bark_output.controls = []\n",
        "        page.bark_output.update()\n",
        "        clear_button.visible = False\n",
        "        clear_button.update()\n",
        "    def bark_help(e):\n",
        "        def close_bark_dlg(e):\n",
        "          nonlocal bark_help_dlg\n",
        "          bark_help_dlg.open = False\n",
        "          page.update()\n",
        "        bark_help_dlg = AlertDialog(title=Text(\"üíÅ   Help with Bark AI\"), content=Column([\n",
        "            Text(\"Bark is a transformer-based text-to-audio model created by Suno. Bark can generate highly realistic, multilingual speech as well as other audio - including music, background noise and simple sound effects. The model can also produce nonverbal communications like laughing, sighing and crying. To support the research community, we are providing access to pretrained model checkpoints ready for inference.\"),\n",
        "            Text(\"Below is a list of some known non-speech sounds, but we are finding more every day. Please let us know if you find patterns that work particularly well on Discord!\"),\n",
        "            Text(\"[laughter], [laughs], [sighs], [music], [gasps], [clears throat], ‚Äî or ‚Ä¶ for hesitations, ‚ô™ for song lyrics, capitalization for emphasis of a word, MAN/WOMAN: for bias towards speaker\"),\n",
        "            Markdown(\"Checkout their [GitHub Project](https://github.com/suno-ai/bark), [HuggingFace Space](https://huggingface.co/spaces/suno/bark) and [Suno Discord](https://discord.com/invite/J2B2vsjKuE).\", on_tap_link=lambda e: e.page.launch_url(e.data)),\n",
        "          ], scroll=ScrollMode.AUTO), actions=[TextButton(\"ü¶ä  Woof... \", on_click=close_bark_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "        page.dialog = bark_help_dlg\n",
        "        bark_help_dlg.open = True\n",
        "        page.update()\n",
        "    #duration_row = SliderRow(label=\"Duration\", min=1, max=20, divisions=38, round=1, suffix=\"s\", pref=bark_prefs, key='duration')\n",
        "    text_temp = SliderRow(label=\"Text Temperature\", min=0, max=1, divisions=20, round=2, pref=bark_prefs, key='text_temp', tooltip=\"1.0 more diverse, 0.0 more conservative\")\n",
        "    waveform_temp = SliderRow(label=\"Wave Temperature\", min=0, max=1, divisions=20, round=2, pref=bark_prefs, key='waveform_temp', tooltip=\"1.0 more diverse, 0.0 more conservative\")\n",
        "    text = TextField(label=\"Text Prompt to Vocalize\", value=bark_prefs['text'], multiline=True, min_lines=1, max_lines=8, on_change=lambda e:changed(e,'text'))\n",
        "    acoustic_prompt = Dropdown(label=\"Acoustic Prompt\", width=250, options=[dropdown.Option(\"Unconditional\"), dropdown.Option(\"Announcer\")], value=bark_prefs['acoustic_prompt'], on_change=lambda e: changed(e, 'acoustic_prompt'))\n",
        "    langs = [\"en\", \"de\", \"es\", \"fr\", \"hi\", \"it\", \"ja\", \"ko\", \"pl\", \"pt\", \"ru\", \"tr\", \"zh\"]\n",
        "    for lang in langs:\n",
        "        for n in range(10):\n",
        "            #label = f\"Speaker {n} ({lang})\"\n",
        "            acoustic_prompt.options.append(dropdown.Option(f\"{lang}_speaker_{n}\"))\n",
        "    batch_folder_name = TextField(label=\"Batch Folder Name\", value=bark_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    file_prefix = TextField(label=\"Filename Prefix\", value=bark_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))\n",
        "    n_iterations = Tooltip(message=\"Make multiple for quality control. Generates candidates and choose the best.\", content=NumberPicker(label=\"Number of Iterations:   \", min=1, max=10, value=bark_prefs['n_iterations'], on_change=lambda e: changed(e, 'n_iterations')))\n",
        "    seed = TextField(label=\"Seed\", value=bark_prefs['seed'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'seed', ptype='int'), width = 120)\n",
        "    page.bark_output = Column([])\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(page.bark_output.controls) > 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üê∂  Bark AI\", \"Text-to-Audio Generation for Multilingual Speech, Music and Sound Effects...\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with Audio LDM-TTS Settings\", on_click=bark_help)]),\n",
        "        text,\n",
        "        text_temp,\n",
        "        waveform_temp,\n",
        "        Row([acoustic_prompt, n_iterations]),\n",
        "        #Row([n_iterations, seed]),\n",
        "        Row([batch_folder_name, file_prefix]),\n",
        "        ElevatedButton(content=Text(\"üêï  Run Bark\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_bark(page)),\n",
        "        page.bark_output,\n",
        "        clear_button,\n",
        "      ]\n",
        "    ))], scroll=ScrollMode.AUTO)\n",
        "    return c\n",
        "\n",
        "  \n",
        "riffusion_prefs = {\n",
        "    'prompt': '',\n",
        "    'negative_prompt': '',\n",
        "    'audio_file': '',\n",
        "    'duration': 5.0,\n",
        "    'steps': 50,\n",
        "    'strength': 0.5,\n",
        "    'guidance_scale': 7.0,\n",
        "    'batch_size': 1,\n",
        "    'max_size': 768,\n",
        "    'seed': 0,\n",
        "    'wav_path': '',\n",
        "    'batch_folder_name': '',\n",
        "    'file_prefix': 'riff-',\n",
        "    'loaded_pipe': '',\n",
        "}\n",
        "\n",
        "def buildRiffusion(page):\n",
        "    global prefs, riffusion_prefs\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "        if pref is not None:\n",
        "          try:\n",
        "            if ptype == \"int\":\n",
        "              riffusion_prefs[pref] = int(e.control.value)\n",
        "            elif ptype == \"float\":\n",
        "              riffusion_prefs[pref] = float(e.control.value)\n",
        "            else:\n",
        "              riffusion_prefs[pref] = e.control.value\n",
        "          except Exception:\n",
        "            alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "            pass\n",
        "    def add_to_riffusion_output(o):\n",
        "        page.riffusion_output.controls.append(o)\n",
        "        page.riffusion_output.update()\n",
        "    def clear_output(e):\n",
        "        if prefs['enable_sounds']: page.snd_delete.play()\n",
        "        page.riffusion_output.controls = []\n",
        "        page.riffusion_output.update()\n",
        "        clear_button.visible = False\n",
        "        clear_button.update()\n",
        "    def riffusion_help(e):\n",
        "        def close_riffusion_dlg(e):\n",
        "          nonlocal riffusion_help_dlg\n",
        "          riffusion_help_dlg.open = False\n",
        "          page.update()\n",
        "        riffusion_help_dlg = AlertDialog(title=Text(\"üíÅ   Help with Audio-LDM\"), content=Column([\n",
        "            Text(\"This is the v1.5 stable diffusion model with no modifications, just fine-tuned on images of spectrograms paired with text. Audio processing happens downstream of the model. It can generate infinite variations of a prompt by varying the seed. All the same web UIs and techniques like img2img, inpainting, negative prompts, and interpolation work out of the box.\"),\n",
        "            Markdown(\"[Project Page](https://www.riffusion.com/about), [Codebase](https://github.com/riffusion/riffusion), [Discord](https://discord.gg/yu6SRwvX4v)\", on_tap_link=lambda e: e.page.launch_url(e.data)),\n",
        "          ], scroll=ScrollMode.AUTO), actions=[TextButton(\"üé∫  Let's Jam... \", on_click=close_riffusion_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "        page.dialog = riffusion_help_dlg\n",
        "        riffusion_help_dlg.open = True\n",
        "        page.update()\n",
        "    def file_picker_result(e: FilePickerResultEvent):\n",
        "        if e.files != None:\n",
        "            upload_files(e)\n",
        "    def on_upload_progress(e: FilePickerUploadEvent):\n",
        "        if e.progress == 1:\n",
        "            if not slash in e.file_name:\n",
        "              fname = os.path.join(root_dir, e.file_name)\n",
        "              riffusion_prefs['file_name'] = e.file_name.rpartition('.')[0]\n",
        "            else:\n",
        "              fname = e.file_name\n",
        "              riffusion_prefs['file_name'] = e.file_name.rpartition(slash)[2].rpartition('.')[0]\n",
        "            audio_file.value = fname\n",
        "            audio_file.update()\n",
        "            riffusion_prefs['audio_file'] = fname\n",
        "            page.update()\n",
        "    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)\n",
        "    def upload_files(e):\n",
        "        uf = []\n",
        "        if file_picker.result != None and file_picker.result.files != None:\n",
        "            for f in file_picker.result.files:\n",
        "              if page.web:\n",
        "                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))\n",
        "              else:\n",
        "                on_upload_progress(FilePickerUploadEvent(f.path, 1, \"\"))\n",
        "            file_picker.upload(uf)\n",
        "    page.overlay.append(file_picker)\n",
        "    def pick_audio(e):\n",
        "        file_picker.pick_files(allow_multiple=False, allowed_extensions=[\"mp3\", \"wav\"], dialog_title=\"Pick Init Audio File\")\n",
        "    audio_file = TextField(label=\"Input Audio File (optional)\", value=riffusion_prefs['audio_file'], on_change=lambda e:changed(e,'audio_file'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_audio))\n",
        "\n",
        "    def change_duration(e):\n",
        "        changed(e, 'duration', ptype=\"float\")\n",
        "        duration_value.value = f\" {riffusion_prefs['duration']}s\"\n",
        "        duration_value.update()\n",
        "    duration = Slider(min=1, max=20, divisions=38, label=\"{value}s\", value=float(riffusion_prefs['duration']), expand=True, on_change=change_duration)\n",
        "    duration_value = Text(f\" {float(riffusion_prefs['duration'])}s\", weight=FontWeight.BOLD)\n",
        "    duration_row = Row([Text(\"Duration: \"), duration_value, duration])\n",
        "    steps_row = SliderRow(label=\"Number of Steps\", min=1, max=100, divisions=99, pref=riffusion_prefs, key='steps', tooltip=\"The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.\")   \n",
        "    guidance = SliderRow(label=\"Guidance Scale\", min=0, max=10, divisions=20, round=1, pref=riffusion_prefs, key='guidance_scale', tooltip=\"Large => better quality and relavancy to text; Small => better diversity\")\n",
        "    prompt = TextField(label=\"Musical Text Prompt\", value=riffusion_prefs['prompt'], multiline=True, min_lines=1, max_lines=8, on_change=lambda e:changed(e,'prompt'), col={'md':9})\n",
        "    negative_prompt = TextField(label=\"Negative Prompt\", value=riffusion_prefs['negative_prompt'], multiline=True, min_lines=1, max_lines=8, on_change=lambda e:changed(e,'negative_prompt'), col={'md':3})\n",
        "    max_size_row = SliderRow(label=\"Max Size\", min=256, max=1024, divisions=12, multiple=32, suffix=\"px\", pref=riffusion_prefs, key='max_size')\n",
        "    batch_folder_name = TextField(label=\"Batch Folder Name\", value=riffusion_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    file_prefix = TextField(label=\"Filename Prefix\", value=riffusion_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))\n",
        "    batch_size = NumberPicker(label=\"Batch Size:   \", min=1, max=5, value=riffusion_prefs['batch_size'], on_change=lambda e: changed(e, 'batch_size'))\n",
        "    seed = TextField(label=\"Seed\", value=riffusion_prefs['seed'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'seed', ptype='int'), width = 120)\n",
        "    page.riffusion_output = Column([])\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(page.riffusion_output.controls) > 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üíΩ  Riffusion Spectrogram Sound Modeling\", \"Stable Diffusion for real-time music generation...\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with Audio LDM-TTS Settings\", on_click=riffusion_help)]),\n",
        "        ResponsiveRow([prompt, negative_prompt]),\n",
        "        #audio_file,\n",
        "        #duration_row,\n",
        "        guidance,\n",
        "        steps_row,\n",
        "        max_size_row,\n",
        "        Row([batch_size, seed]),\n",
        "        Row([batch_folder_name, file_prefix]),\n",
        "        ElevatedButton(content=Text(\"üë®‚Äçüé§Ô∏è  Run Riffusion\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_riffusion(page)),\n",
        "        page.riffusion_output,\n",
        "        clear_button,\n",
        "      ]\n",
        "    ))], scroll=ScrollMode.AUTO)\n",
        "    return c\n",
        "\n",
        "mubert_prefs = {\n",
        "    'prompt': '',\n",
        "    'duration': 30,\n",
        "    'is_loop': False,\n",
        "    'email': '',\n",
        "    'tags': '',\n",
        "    'tag_list': '',\n",
        "    'batch_folder_name': '',\n",
        "    'file_prefix': 'mu-'\n",
        "}\n",
        "\n",
        "def buildMubert(page):\n",
        "    global mubert_prefs\n",
        "    def changed(e, pref=None, ptype=\"str\"):\n",
        "        if pref is not None:\n",
        "          try:\n",
        "            if ptype == \"int\":\n",
        "              mubert_prefs[pref] = int(e.control.value)\n",
        "            elif ptype == \"float\":\n",
        "              mubert_prefs[pref] = float(e.control.value)\n",
        "            else:\n",
        "              mubert_prefs[pref] = e.control.value\n",
        "          except Exception:\n",
        "            alert_msg(page, \"Error updating field. Make sure your Numbers are numbers...\")\n",
        "            pass\n",
        "    def add_to_mubert_output(o):\n",
        "        page.mubert_output.controls.append(o)\n",
        "        page.mubert_output.update()\n",
        "    def clear_output(e):\n",
        "        if prefs['enable_sounds']: page.snd_delete.play()\n",
        "        page.mubert_output.controls = []\n",
        "        page.mubert_output.update()\n",
        "        clear_button.visible = False\n",
        "        clear_button.update()\n",
        "    def mubert_help(e):\n",
        "        def close_mubert_dlg(e):\n",
        "          nonlocal mubert_help_dlg\n",
        "          mubert_help_dlg.open = False\n",
        "          page.update()\n",
        "        mubert_help_dlg = AlertDialog(title=Text(\"üíÅ   Help with Mubert\"), content=Column([\n",
        "            Text(\"Mubert AI is a cutting-edge tool that allows you to create realistic and infinite music by learning from a large dataset of existing music. The result is a high-quality and original music stream that sounds like it was composed by a professional musician.\"),\n",
        "            Text(\"This uses the Mubert Developer API which is actually quite expensive to use, so don't be surprised if your generation fails because the API Key has used it's monthy quota. Sorry, it's not free/opensource...\"),\n",
        "          ], scroll=ScrollMode.AUTO), actions=[TextButton(\"ü•Å  Gimme a Beat... \", on_click=close_mubert_dlg)], actions_alignment=MainAxisAlignment.END)\n",
        "        page.dialog = mubert_help_dlg\n",
        "        mubert_help_dlg.open = True\n",
        "        page.update()\n",
        "    prompt = TextField(label=\"Prompt to generate a track (genre, theme, etc.)\", value=mubert_prefs['prompt'], multiline=True, min_lines=1, max_lines=8, on_change=lambda e:changed(e,'prompt'))\n",
        "    duration_row = SliderRow(label=\"Duration\", min=1, max=250, divisions=249, suffix=\"s\", pref=mubert_prefs, key='duration')\n",
        "    is_loop = Checkbox(label=\"Is Audio Loop   \", value=mubert_prefs['is_loop'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'is_loop'))\n",
        "    email = TextField(label=\"Email Address (for API use)\", keyboard_type=KeyboardType.EMAIL, value=mubert_prefs['email'], on_change=lambda e:changed(e,'email'))\n",
        "    batch_folder_name = TextField(label=\"Batch Folder Name\", value=mubert_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))\n",
        "    file_prefix = TextField(label=\"Filename Prefix\", value=mubert_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))\n",
        "    #page.mubert_songs = ResponsiveRow(controls=[])\n",
        "    #for v in mubert_prefs['tag_list']:\n",
        "    #  page.mubert_songs.controls.append(Checkbox(label=v, fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, col={'xs':12, 'sm':6, 'md':3, 'lg':3, 'xl': 2}))\n",
        "    page.mubert_output = Column([])\n",
        "    clear_button = Row([ElevatedButton(content=Text(\"‚ùå   Clear Output\"), on_click=clear_output)], alignment=MainAxisAlignment.END)\n",
        "    clear_button.visible = len(page.mubert_output.controls) > 0\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üéº  Mubert Music Generator\", \"AI music is generated by Mubert API. Pretty good grooves...\", actions=[IconButton(icon=icons.HELP, tooltip=\"Help with Mubert Settings\", on_click=mubert_help)]),\n",
        "        prompt,\n",
        "        duration_row,\n",
        "        is_loop,\n",
        "        email,\n",
        "        Row([batch_folder_name, file_prefix]),\n",
        "        #Row([Text(\"Select one or more tags:\", weight=FontWeight.BOLD), Text(\"(none for random or custom)\")]),\n",
        "        #page.mubert_songs,\n",
        "        ElevatedButton(content=Text(\"üéª  Run Mubert Music\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_mubert(page)),\n",
        "        page.mubert_output,\n",
        "        clear_button,\n",
        "      ]\n",
        "    ))], scroll=ScrollMode.AUTO)\n",
        "    return c\n",
        "\n",
        "def buildCustomModelManager(page):\n",
        "    global prefs\n",
        "    def title_header(title, type):\n",
        "        return Row([Text(title, style=TextThemeStyle.BODY_LARGE), \n",
        "                    ft.FilledButton(f\"Add {type} Model\", on_click=lambda e: add_model(type))], alignment=MainAxisAlignment.SPACE_BETWEEN)\n",
        "    def model_tile(name, path, token, type):\n",
        "        return ListTile(title=Row([Text(name, weight=FontWeight.BOLD), Text(path), Text(token)], alignment=MainAxisAlignment.SPACE_BETWEEN), data=type, dense=True, trailing=PopupMenuButton(icon=icons.MORE_VERT,\n",
        "          items=[PopupMenuItem(icon=icons.EDIT, text=\"Edit Custom Model\", on_click=lambda e: edit_model(e, name), data=type),\n",
        "                 PopupMenuItem(icon=icons.DELETE, text=\"Delete Custom Model\", on_click=lambda e: del_model(e, name), data=type)]), on_click=lambda e: edit_model(e, name))\n",
        "    def load_customs():\n",
        "        #custom_models.controls.append(title_header(\"Stable Diffusion Finetuned Models\", type=\"Finetuned\"))\n",
        "        for mod in prefs['custom_models']:\n",
        "            token = mod['prefix'] if 'prefix' in mod else \"\"\n",
        "            custom_models.controls.append(model_tile(mod['name'], mod['path'], token, \"Finetuned\"))\n",
        "            #page.custom_models.controls.append(ListTile(title=Row([Text(f\".{slash}{dir.rpartition(slash)[2]}\", weight=FontWeight.BOLD), Text(\"\")], alignment=MainAxisAlignment.SPACE_BETWEEN), data=dir, dense=True, trailing=PopupMenuButton(icon=icons.MORE_VERT,\n",
        "            #  items=[PopupMenuItem(icon=icons.DELETE, text=\"Delete Custom Model\", on_click=del_model, data=dir)])))\n",
        "        #custom_models.update()\n",
        "        for mod in prefs['custom_LoRA_models']:\n",
        "            token = mod['prefix'] if 'prefix' in mod else \"\"\n",
        "            custom_LoRA_models.controls.append(model_tile(mod['name'], mod['path'], token, \"LoRA\"))\n",
        "        #custom_LoRA_models.update()\n",
        "        for mod in prefs['custom_dance_diffusion_models']:\n",
        "            token = mod['prefix'] if 'prefix' in mod else \"\"\n",
        "            custom_dance_diffusion_models.controls.append(model_tile(mod['name'], mod['path'], token, \"DanceDiffusion\"))\n",
        "        #custom_dance_diffusion_models.update()\n",
        "        for mod in prefs['tortoise_custom_voices']:\n",
        "            token = mod['prefix'] if 'prefix' in mod else \"\"\n",
        "            tortoise_custom_voices.controls.append(model_tile(mod['name'], mod['folder'], token, \"Tortoise\"))\n",
        "        #tortoise_custom_voices.update()\n",
        "    def model_list(type):\n",
        "        if type == \"Finetuned\":\n",
        "            return prefs[\"custom_models\"]\n",
        "        elif type == \"LoRA\":\n",
        "            return prefs[\"custom_LoRA_models\"]\n",
        "        elif type == \"DanceDiffusion\":\n",
        "            return prefs[\"custom_dance_diffusion_models\"]\n",
        "        elif type == \"Tortoise\":\n",
        "            return prefs[\"tortoise_custom_voices\"]\n",
        "    def edit_model(e, name):\n",
        "        #name = e.control.title.controls[0].value\n",
        "        #path = e.control.title.controls[1].value\n",
        "        type = e.control.data\n",
        "        mod = None\n",
        "        for sub in model_list(type):\n",
        "            if sub['name'] == name:\n",
        "                mod = sub\n",
        "                break\n",
        "        if mod is None: return\n",
        "        path = mod['path' if type != \"Tortoise\" else 'folder']\n",
        "        #print(str(mod))\n",
        "        def close_dlg(e):\n",
        "            dlg_edit.open = False\n",
        "            page.update()\n",
        "        def save_model(e):\n",
        "            if type == \"Finetuned\":\n",
        "                for m in custom_models.controls:\n",
        "                    if m.title.controls[0].value == name:\n",
        "                        m.title.controls[0].value = model_name.value\n",
        "                        m.title.controls[1].value = model_path.value\n",
        "                        m.update()\n",
        "            elif type == \"LoRA\":\n",
        "                for m in custom_LoRA_models.controls:\n",
        "                    if m.title.controls[0].value == name:\n",
        "                        m.title.controls[0].value = model_name.value\n",
        "                        m.title.controls[1].value = model_path.value\n",
        "                        m.update()\n",
        "            elif type == \"DanceDiffusion\":\n",
        "                for m in custom_dance_diffusion_models.controls:\n",
        "                    if m.title.controls[0].value == name:\n",
        "                        m.title.controls[0].value = model_name.value\n",
        "                        m.title.controls[1].value = model_path.value\n",
        "                        m.update()\n",
        "            elif type == \"Tortoise\":\n",
        "                for m in tortoise_custom_voices.controls:\n",
        "                    if m.title.controls[0].value == name:\n",
        "                        m.title.controls[0].value = model_name.value\n",
        "                        m.title.controls[1].value = model_path.value\n",
        "                        m.update()\n",
        "            mod['name'] = model_name.value\n",
        "            mod['path' if type != \"Tortoise\" else 'folder'] = model_path.value\n",
        "            dlg_edit.open = False\n",
        "            e.control.update()\n",
        "            page.update()\n",
        "        model_name = TextField(label=\"Custom Model Name\", value=name)\n",
        "        model_path = TextField(label=\"Model Path\", value=path)\n",
        "        dlg_edit = AlertDialog(modal=False, title=Text(f\"üß≥ Edit {type} Model Info\"), content=Container(Column([model_name, model_path], alignment=MainAxisAlignment.START, tight=True, scroll=ScrollMode.AUTO)), actions=[TextButton(content=Text(\"Cancel\", size=18), on_click=close_dlg), ElevatedButton(content=Text(value=emojize(\":floppy_disk:\") + \"  Save Model \", size=19, weight=FontWeight.BOLD), on_click=save_model)], actions_alignment=MainAxisAlignment.END)\n",
        "        page.dialog = dlg_edit\n",
        "        dlg_edit.open = True\n",
        "        page.update()\n",
        "    def add_model(type):\n",
        "        mod_list = model_list(type)\n",
        "        def close_dlg(e):\n",
        "            dlg_edit.open = False\n",
        "            page.update()\n",
        "        def save_model(e):\n",
        "            mod = {'name': model_name.value, 'path': model_path.value}\n",
        "            mod_list.append(mod)\n",
        "            if type == \"Finetuned\":\n",
        "                custom_models.controls.append(model_tile(mod['name'], mod['path'], \"\", \"Finetuned\"))\n",
        "                custom_models.update()\n",
        "            elif type == \"LoRA\":\n",
        "                custom_LoRA_models.controls.append(model_tile(mod['name'], mod['path'], \"\", \"LoRA\"))\n",
        "                custom_LoRA_models.update()\n",
        "            elif type == \"DanceDiffusion\":\n",
        "                custom_dance_diffusion_models.controls.append(model_tile(mod['name'], mod['path'], \"\", \"DanceDiffusion\"))\n",
        "                custom_dance_diffusion_models.update()\n",
        "            elif type == \"Tortoise\":\n",
        "                tortoise_custom_voices.controls.append(model_tile(mod['name'], mod['folder'], \"\", \"Tortoise\"))\n",
        "                tortoise_custom_voices.update()\n",
        "            dlg_edit.open = False\n",
        "            e.control.update()\n",
        "            page.update()\n",
        "        model_name = TextField(label=\"Custom Model Name\")\n",
        "        model_path = TextField(label=\"Model Path\")\n",
        "        dlg_edit = AlertDialog(modal=False, title=Text(f\"üß≥ Add Custom {type} Model\"), content=Container(Column([model_name, model_path], alignment=MainAxisAlignment.START, tight=True, scroll=ScrollMode.AUTO)), actions=[TextButton(content=Text(\"Cancel\", size=18), on_click=close_dlg), ElevatedButton(content=Text(value=emojize(\":floppy_disk:\") + \"  Save Model \", size=19, weight=FontWeight.BOLD), on_click=save_model)], actions_alignment=MainAxisAlignment.END)\n",
        "        page.dialog = dlg_edit\n",
        "        dlg_edit.open = True\n",
        "        page.update()\n",
        "    def del_model(e, name):\n",
        "        type = e.control.data\n",
        "        mod_list = model_list(type)\n",
        "        for i, sub in enumerate(mod_list):\n",
        "            if sub['name'] == name:\n",
        "                mod = sub\n",
        "                del mod_list[i]\n",
        "                break\n",
        "        if type == \"Finetuned\":\n",
        "            for i, l in enumerate(custom_models.controls):\n",
        "                if l.title.controls[0].value == name:\n",
        "                    del custom_models.controls[i]\n",
        "                    custom_models.update()\n",
        "                    break\n",
        "        elif type == \"LoRA\":\n",
        "            for i, l in enumerate(custom_LoRA_models.controls):\n",
        "                if l.title.controls[0].value == name:\n",
        "                    del custom_LoRA_models.controls[i]\n",
        "                    custom_LoRA_models.update()\n",
        "                    break\n",
        "        elif type == \"DanceDiffusion\":\n",
        "            for i, l in enumerate(custom_dance_diffusion_models.controls):\n",
        "                if l.title.controls[0].value == name:\n",
        "                    del custom_dance_diffusion_models.controls[i]\n",
        "                    custom_dance_diffusion_models.update()\n",
        "                    break\n",
        "        elif type == \"Tortoise\":\n",
        "            for i, l in enumerate(tortoise_custom_voices.controls):\n",
        "                if l.title.controls[0].value == name:\n",
        "                    del tortoise_custom_voices.controls[i]\n",
        "                    tortoise_custom_voices.update()\n",
        "                    break\n",
        "        if prefs['enable_sounds']: e.page.snd_delete.play()\n",
        "    def update_list(e):\n",
        "        page.finetuned_model.options.clear()\n",
        "        for cust in model_list(\"Finetuned\"):\n",
        "            page.finetuned_model.options.append(dropdown.Option(cust[\"name\"]))\n",
        "        for mod in finetuned_models:\n",
        "            page.finetuned_model.options.append(dropdown.Option(mod[\"name\"]))\n",
        "        try: page.finetuned_model.update()\n",
        "        except: pass\n",
        "        page.LoRA_model.options.clear()\n",
        "        for cust in model_list(\"LoRA\"):\n",
        "            page.LoRA_model.options.append(dropdown.Option(cust[\"name\"]))\n",
        "        for mod in LoRA_models:\n",
        "            page.LoRA_model.options.append(dropdown.Option(mod[\"name\"]))\n",
        "        page.LoRA_model.options.append(dropdown.Option(\"Custom LoRA Path\"))\n",
        "        try: page.LoRA_model.update()\n",
        "        except: pass\n",
        "        page.community_dance_diffusion_model.options.clear()\n",
        "        for cust in model_list(\"DanceDiffusion\"):\n",
        "            page.community_dance_diffusion_model.options.append(dropdown.Option(cust[\"name\"]))\n",
        "        for mod in community_dance_diffusion_models:\n",
        "            page.community_dance_diffusion_model.options.append(dropdown.Option(mod[\"name\"]))\n",
        "        try: page.community_dance_diffusion_model.update()\n",
        "        except: pass\n",
        "        page.tortoise_voices.controls.clear()\n",
        "        for v in tortoise_prefs['voices']:\n",
        "            page.tortoise_voices.controls.append(Checkbox(label=v, fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, col={'xs':12, 'sm':6, 'md':3, 'lg':3, 'xl': 2}))\n",
        "        if len(prefs['tortoise_custom_voices']) > 0:\n",
        "            for custom in prefs['tortoise_custom_voices']:\n",
        "                page.tortoise_voices.controls.append(Checkbox(label=custom['name'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, col={'xs':12, 'sm':6, 'md':3, 'lg':3, 'xl': 2}))\n",
        "        try: page.tortoise_voices.update()\n",
        "        except: pass\n",
        "        save_settings_file(e.page)\n",
        "        if prefs['enable_sounds']: page.snd_drop.play()\n",
        "    custom_models = Column([], spacing=0)\n",
        "    custom_LoRA_models = Column([], spacing=0)\n",
        "    custom_dance_diffusion_models = Column([], spacing=0)\n",
        "    tortoise_custom_voices = Column([], spacing=0)\n",
        "    load_customs()\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üßß   Manage your Saved Custom Models\", \"Add or Edit your favorite models from HuggingFace, URL or Local Path\"),\n",
        "        title_header(\"Custom Finetuned Models\", \"Finetuned\"),\n",
        "        custom_models,\n",
        "        title_header(\"Custom LoRA Models\", \"LoRA\"),\n",
        "        custom_LoRA_models,\n",
        "        title_header(\"Custom Tortoise Voice Models\", \"Tortoise\"),\n",
        "        tortoise_custom_voices,\n",
        "        title_header(\"Custom Dance Diffusion Models\", \"DanceDiffusion\"),\n",
        "        custom_dance_diffusion_models,\n",
        "        ElevatedButton(content=Text(\"üõÑ  Update Custom Dropdowns\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=update_list),\n",
        "      ]\n",
        "    ))], scroll=ScrollMode.AUTO)\n",
        "    return c\n",
        "\n",
        "def get_directory_size(directory):\n",
        "    total = 0\n",
        "    for entry in os.scandir(directory):\n",
        "        if entry.is_file():\n",
        "            total += entry.stat().st_size\n",
        "        elif entry.is_dir():\n",
        "            try:\n",
        "                total += get_directory_size(entry.path)\n",
        "            except FileNotFoundError:\n",
        "                pass\n",
        "    return total\n",
        "def convert_bytes(num):\n",
        "    step_unit = 1000.0 #1024 bad the size\n",
        "    for x in ['bytes', 'KB', 'MB', 'GB', 'TB']:\n",
        "        if num < step_unit:\n",
        "            return \"%3.1f %s\" % (num, x)\n",
        "        num /= step_unit\n",
        "\n",
        "def buildCachedModelManager(page):\n",
        "    global prefs\n",
        "    def scan_cache(e):\n",
        "      if not bool(prefs['cache_dir']):\n",
        "        alert_msg(page, \"You haven't set a Cache Directory in your Settings...\")\n",
        "        return\n",
        "      elif not os.path.isdir(prefs['cache_dir']):\n",
        "        alert_msg(page, \"The Cache Directory in your Settings can't be found...\")\n",
        "        return\n",
        "      if len(page.cached_folders.controls) > 1:\n",
        "        page.cached_folders.controls.clear()\n",
        "        page.cached_folders.update()\n",
        "      page.cached_folders.controls.append(Installing(f\"Scanning {prefs['cache_dir']}\"))\n",
        "      dirs = [f.path for f in os.scandir(prefs['cache_dir']) if f.is_dir()]\n",
        "      del page.cached_folders.controls[-1]\n",
        "      page.cached_folders.update()\n",
        "      for dir in dirs:\n",
        "        page.cached_folders.controls.append(ListTile(title=Row([Text(f\".{slash}{dir.rpartition(slash)[2]}\", weight=FontWeight.BOLD), Text(\"\")], alignment=MainAxisAlignment.SPACE_BETWEEN), data=dir, dense=True, trailing=PopupMenuButton(icon=icons.MORE_VERT,\n",
        "          items=[PopupMenuItem(icon=icons.DELETE, text=\"Delete Model Directory\", on_click=del_dir, data=dir)])))\n",
        "        page.cached_folders.update()\n",
        "      for l in page.cached_folders.controls:\n",
        "        size = convert_bytes(get_directory_size(l.data))\n",
        "        l.title.controls[1].value = size\n",
        "        l.title.controls[1].update()\n",
        "        page.cached_folders.update()\n",
        "    def del_dir(e):\n",
        "      dir = e.control.data\n",
        "      shutil.rmtree(dir, ignore_errors=True)\n",
        "      for i, l in enumerate(page.cached_folders.controls):\n",
        "        if l.data == dir:\n",
        "          del page.cached_folders.controls[i]\n",
        "          page.cached_folders.update()\n",
        "          break\n",
        "      if prefs['enable_sounds']: e.page.snd_delete.play()\n",
        "    page.cached_folders = Column([])\n",
        "    c = Column([Container(\n",
        "      padding=padding.only(18, 14, 20, 10),\n",
        "      content=Column([\n",
        "        Header(\"üóÇÔ∏è   Manage your Cache Directory Saved Models\", \"If you're cacheing your model files, it can fill up your drive space quickly, so you can trim the fat as needed... Redownloads when used.\"),\n",
        "        page.cached_folders,\n",
        "        ElevatedButton(content=Text(\"üîç  Scan Cache Dirctory\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=scan_cache),\n",
        "      ]\n",
        "    ))], scroll=ScrollMode.AUTO)\n",
        "    return c\n",
        "\n",
        "if 'pipe' in locals():\n",
        "  clear_pipes()\n",
        "use_custom_scheduler = False\n",
        "retry_attempts_if_NSFW = 3\n",
        "unet = None\n",
        "pipe = None\n",
        "pipe_img2img = None\n",
        "pipe_interpolation = None\n",
        "pipe_clip_guided = None\n",
        "pipe_conceptualizer = None\n",
        "pipe_repaint = None\n",
        "pipe_imagic = None\n",
        "pipe_composable = None\n",
        "pipe_safe = None\n",
        "pipe_versatile = None\n",
        "pipe_versatile_text2img = None\n",
        "pipe_versatile_variation = None\n",
        "pipe_versatile_dualguided = None\n",
        "pipe_upscale = None\n",
        "pipe_depth = None\n",
        "pipe_image_variation = None\n",
        "pipe_semantic = None\n",
        "pipe_DiffEdit = None\n",
        "pipe_EDICT = None\n",
        "text_encoder_EDICT = None\n",
        "pipe_unCLIP = None\n",
        "pipe_unCLIP_image_variation = None\n",
        "pipe_unCLIP_interpolation = None\n",
        "pipe_unCLIP_image_interpolation = None\n",
        "pipe_magic_mix = None\n",
        "pipe_paint_by_example = None\n",
        "pipe_instruct_pix2pix = None\n",
        "pipe_alt_diffusion = None\n",
        "pipe_alt_diffusion_img2img = None\n",
        "pipe_SAG = None\n",
        "pipe_attend_and_excite = None\n",
        "pipe_panorama = None\n",
        "pipe_DiT = None\n",
        "pipe_dance = None\n",
        "pipe_kandinsky = None\n",
        "pipe_tortoise_tts = None\n",
        "pipe_audio_ldm = None\n",
        "pipe_riffusion = None\n",
        "pipe_audio_diffusion = None\n",
        "pipe_text_to_video = None\n",
        "pipe_text_to_video_zero = None\n",
        "pipe_gpt2 = None\n",
        "pipe_distil_gpt2 = None\n",
        "pipe_stable_lm = None\n",
        "tokenizer_stable_lm = None\n",
        "pipe_controlnet = None\n",
        "controlnet = None\n",
        "controlnet_models = {\"Canny Map Edge\":None, \"Scribble\":None, \"OpenPose\":None, \"Depth\":None, \"HED\":None, \"M-LSD\":None, \"Normal Map\":None, \"Segmented\":None, \"LineArt\":None, \"Shuffle\":None, \"Instruct Pix2Pix\":None}\n",
        "stability_api = None\n",
        "\n",
        "model_path = \"CompVis/stable-diffusion-v1-4\"\n",
        "inpaint_model = \"stabilityai/stable-diffusion-2-inpainting\"\n",
        "#\"runwayml/stable-diffusion-inpainting\"\n",
        "scheduler = None\n",
        "scheduler_clip = None\n",
        "if is_Colab:\n",
        "  from google.colab import output\n",
        "  output.enable_custom_widget_manager()\n",
        "\n",
        "finetuned_models = [\n",
        "    #{\"name\": \"Stable Diffusion v1.5\", \"path\": \"runwayml/stable-diffusion-v1-5\", \"prefix\": \"\", \"revision\": \"fp16\"},\n",
        "    #{\"name\": \"Stable Diffusion v1.4\", \"path\": \"CompVis/stable-diffusion-v1-4\", \"prefix\": \"\", \"revision\": \"fp16\"},\n",
        "    {\"name\": \"Midjourney v4 style\", \"path\": \"prompthero/midjourney-v4-diffusion\", \"prefix\": \"mdjrny-v4 style \"},\n",
        "    {\"name\": \"Openjourney\", \"path\": \"prompthero/openjourney\", \"prefix\": \"mdjrny-v4 style \"},\n",
        "    {\"name\": \"Openjourney LoRA\", \"path\": \"prompthero/openjourney-lora\", \"prefix\": \"\"},\n",
        "    {\"name\": \"Openjourney v2\", \"path\": \"prompthero/openjourney-v2\", \"prefix\": \"\"},\n",
        "    {\"name\": \"Future Diffusion\", \"path\": \"nitrosocke/Future-Diffusion\", \"prefix\": \"future style \"},\n",
        "    #{\"name\": \"Anything v3.0\", \"path\": \"Linaqruf/anything-v3.0\", \"prefix\": \"\"},\n",
        "    {\"name\": \"Anything v3.0\", \"path\": \"ckpt/anything-v3.0\", \"prefix\": \"\"},\n",
        "    #{\"name\": \"Anything v4.0\", \"path\": \"andite/anything-v4.0\", \"prefix\": \"\"},\n",
        "    {\"name\": \"Anything v4.5\", \"path\": \"ckpt/anything-v4.5\", \"prefix\": \"\"},\n",
        "    {\"name\": \"Analog Diffusion\", \"path\": \"wavymulder/Analog-Diffusion\", \"prefix\": \"analog style \"},\n",
        "    {\"name\": \"Architecture Diffusers\", \"path\": \"rrustom/stable-architecture-diffusers\", \"prefix\": \"\"},\n",
        "    {\"name\": \"Arcane\", \"path\":\"nitrosocke/Arcane-Diffusion\", \"prefix\":\"arcane style \"},\n",
        "    {\"name\": \"Archer Diffusion\", \"path\":\"nitrosocke/archer-diffusion\", \"prefix\":\"archer style \"},\n",
        "    {\"name\": \"Nitro Diffusion\", \"path\":\"nitrosocke/nitro-diffusion\", \"prefix\":\"archer style, arcane style, modern disney style \"},\n",
        "    {\"name\": \"Beeple Diffusion\", \"path\": \"riccardogiorato/beeple-diffusion\", \"prefix\": \"beeple style \"},\n",
        "    {\"name\": \"Protogen v5.8\", \"path\": \"darkstorm2150/Protogen_x5.8_Official_Release\", \"prefix\": \"\"},\n",
        "    {\"name\": \"Protogen Eclipse\", \"path\": \"darkstorm2150/Protogen_Eclipse_Official_Release\", \"prefix\": \"\"},\n",
        "    #{\"name\": \"Protogen Infinity\", \"path\": \"darkstorm2150/Protogen_Infinity_Official_Release\", \"prefix\": \"\"},\n",
        "    #{\"name\": \"Protogen Nova\", \"path\": \"darkstorm2150/Protogen_Nova_Official_Release\", \"prefix\": \"\"},\n",
        "    {\"name\": \"Protogen Dragon\", \"path\": \"darkstorm2150/Protogen_Dragon_Official_Release\", \"prefix\": \"\"},\n",
        "    {\"name\": \"Deliberate\", \"path\": \"XpucT/Deliberate\", \"prefix\":\"\"},\n",
        "    {\"name\": \"Deliberate 2\", \"path\": \"SdValar/deliberate2\", \"prefix\":\"\"},\n",
        "    {\"name\": \"Elden Ring\", \"path\": \"nitrosocke/elden-ring-diffusion\", \"prefix\":\"elden ring style \"},\n",
        "    {\"name\": \"Modern Disney\", \"path\": \"nitrosocke/mo-di-diffusion\", \"prefix\": \"modern disney style \"},\n",
        "    {\"name\": \"Classic Disney\", \"path\": \"nitrosocke/classic-anim-diffusion\", \"prefix\": \"classic disney style \"},\n",
        "    {\"name\": \"Loving Vincent (Van Gogh)\", \"path\": \"dallinmackay/Van-Gogh-diffusion\", \"prefix\": \"lvngvncnt \"},\n",
        "    {\"name\": \"Realistic Vision v1.4\", \"path\": \"SG161222/Realistic_Vision_V1.4\", \"prefix\": \"\"},\n",
        "    {\"name\": \"Redshift Renderer (Cinema4D)\", \"path\": \"nitrosocke/redshift-diffusion\", \"prefix\": \"redshift style \"},\n",
        "    {\"name\": \"Waifu Diffusion\", \"path\": \"hakurei/waifu-diffusion\", \"prefix\": \"\", \"revision\": \"fp16\"},\n",
        "    {\"name\": \"Ultima Waifu Diffusion\", \"path\": \"AdamOswald1/Ultima-Waifu-Diffusion\", \"prefix\": \"\"},\n",
        "    #{\"name\": \"TrinArt Waifu 50-50\", \"path\": \"doohickey/trinart-waifu-diffusion-50-50\", \"prefix\": \"\"},\n",
        "    {\"name\": \"WikiArt v2\", \"path\": \"valhalla/sd-wikiart-v2\", \"prefix\": \"\"},\n",
        "    {\"name\": \"Jak's Woolitize\", \"path\": \"plasmo/woolitize\", \"prefix\": \"woolitize \"},\n",
        "    {\"name\": \"Inkpunk Diffusion\", \"path\": \"Envvi/Inkpunk-Diffusion\", \"prefix\": \"nvinkpunk \"},\n",
        "    {\"name\": \"Simpsons Model\", \"path\": \"Norod78/sd-simpsons-model\", \"prefix\":\"\"},\n",
        "    {\"name\": \"Spider-Verse\", \"path\": \"nitrosocke/spider-verse-diffusion\", \"prefix\":\"spiderverse style \"},\n",
        "    {\"name\": \"Pok√©mon\", \"path\": \"lambdalabs/sd-pokemon-diffusers\", \"prefix\": \"\"},\n",
        "    {\"name\": \"Pony Diffusion\", \"path\": \"AstraliteHeart/pony-diffusion\", \"prefix\": \"\"},\n",
        "    {\"name\": \"Robo Diffusion\", \"path\": \"nousr/robo-diffusion\", \"prefix\": \"\"},\n",
        "    {\"name\": \"Dungeons & Diffusion\", \"path\": \"0xJustin/Dungeons-and-Diffusion\", \"prefix\": \"\"},\n",
        "    {\"name\": \"Cyberpunk Anime\", \"path\": \"DGSpitzer/Cyberpunk-Anime-Diffusion\", \"prefix\": \"dgs illustration style \"},\n",
        "    {\"name\": \"Tron Legacy\", \"path\": \"dallinmackay/Tron-Legacy-diffusion\", \"prefix\": \"trnlgcy \"},\n",
        "    {\"name\": \"Guohua Diffusion\", \"path\": \"Langboat/Guohua-Diffusion\", \"prefix\": \"guohua style \"},\n",
        "    {\"name\": \"Trin-sama TrinArt\", \"path\": \"naclbit/trinart_stable_diffusion_v2\", \"prefix\": \"\", \"revision\": \"diffusers-115k\"},\n",
        "    {\"name\": \"Naruto Diffusers\", \"path\": \"lambdalabs/sd-naruto-diffusers\", \"prefix\": \"\"},\n",
        "    {\"name\": \"Zelda: Breath of The Wild\", \"path\": \"s3nh/zelda-botw-stable-diffusion\", \"prefix\": \"botw style \"},\n",
        "    {\"name\": \"JWST Deep Space Diffusion\", \"path\": \"dallinmackay/JWST-Deep-Space-diffusion\", \"prefix\": \"JWST \"},\n",
        "    #{\"name\": \"LinkedIn-diffusion\", \"path\": \"prompthero/linkedin-diffusion\", \"prefix\": \"lnkdn photography \"},\n",
        "    {\"name\": \"Bloodborne Diffusion\", \"path\": \"Guizmus/BloodborneDiffusion\", \"prefix\": \"Bloodborne Style \"},\n",
        "    {\"name\": \"Cats the Musical\", \"path\": \"dallinmackay/Cats-Musical-diffusion\", \"prefix\": \"ctsmscl \"},\n",
        "    {\"name\": \"Anon v1\", \"path\": \"TheMindExpansionNetwork/anonv1\", \"prefix\": \"AnonV1 \"},\n",
        "    {\"name\": \"Avatar\", \"path\": \"Jersonm89/Avatar\", \"prefix\": \"avatar style \"},\n",
        "    {\"name\": \"Dreamlike Diffusion v1\", \"path\": \"dreamlike-art/dreamlike-diffusion-1.0\", \"prefix\": \"dreamlikeart \"},\n",
        "    {\"name\": \"Dreamlike Photoreal 2\", \"path\": \"dreamlike-art/dreamlike-photoreal-2.0\", \"prefix\": \"\"},\n",
        "    {\"name\": \"Glitch\", \"path\": \"BakkerHenk/glitch\", \"prefix\": \"a photo in sks glitched style \"},\n",
        "    {\"name\": \"Knollingcase\", \"path\": \"Aybeeceedee/knollingcase\", \"prefix\": \"knollingcase \"},\n",
        "    {\"name\": \"Wavy Diffusion\", \"path\": \"wavymulder/wavyfusion\", \"prefix\": \"wa-vy style \"},\n",
        "    {\"name\": \"TARDISfusion Classic Tardis\", \"path\": \"Guizmus/Tardisfusion\", \"prefix\": \"Classic Tardis style \"},\n",
        "    {\"name\": \"TARDISfusion Modern Tardis\", \"path\": \"Guizmus/Tardisfusion\", \"prefix\": \"Modern Tardis style \"},\n",
        "    {\"name\": \"TARDISfusion Tardis Box\", \"path\": \"Guizmus/Tardisfusion\", \"prefix\": \"Tardis Box style \"},\n",
        "    {\"name\": \"Rick-Roll Style\", \"path\": \"TheLastBen/rick-roll-style\", \"prefix\": \"rckrll \"},\n",
        "    {\"name\": \"Filmation MOTU\", \"path\": \"zuleo/filmation-motu\", \"prefix\": \"\"},\n",
        "    {\"name\": \"Char Helper\", \"path\": \"ManglerFTW/CharHelper\", \"prefix\": \"\"},\n",
        "    {\"name\": \"Maxwell the Cat\", \"path\": \"kabachuha/maxwell-the-cat-diffusion\", \"prefix\": \"\"},\n",
        "    {\"name\": \"Glitch Embedding\", \"path\": \"joachimsallstrom/Glitch-Embedding\", \"prefix\": \"glitch \"},\n",
        "    {\"name\": \"Pokemon 3D\", \"path\": \"Timmahw/SD2.1_Pokemon3D\", \"prefix\": \"\"},\n",
        "    {\"name\": \"Nephos\", \"path\": \"RomeroRZ/Nephos\", \"prefix\": \"\"},\n",
        "    {\"name\": \"effeffIX Concept\", \"path\": \"zuleo/effeffIX-concept-diffusion\", \"prefix\": \"effeff9 \"},\n",
        "    {\"name\": \"effeffIX Woman\", \"path\": \"zuleo/effeffIX-concept-diffusion\", \"prefix\": \"effeff9 woman \"},\n",
        "    {\"name\": \"effeffIX Man\", \"path\": \"zuleo/effeffIX-concept-diffusion\", \"prefix\": \"effeff9 man \"},\n",
        "    {\"name\": \"effeffIX Creature\", \"path\": \"zuleo/effeffIX-concept-diffusion\", \"prefix\": \"effeff9 creature \"},\n",
        "    {\"name\": \"effeffIX Architecture\", \"path\": \"zuleo/effeffIX-concept-diffusion\", \"prefix\": \"effeff9 architecture \"},\n",
        "    {\"name\": \"Double-Exposure-Diffusion\", \"path\": \"joachimsallstrom/Double-Exposure-Diffusion\", \"prefix\": \"dublex style \"},\n",
        "    #{\"name\": \"Illuminati Diffusion\", \"path\": \"IlluminatiAI/Illuminati_Diffusion_v1.0\", \"prefix\": \"\"},\n",
        "    {\"name\": \"ChillOutMix\", \"path\": \"windwhinny/chilloutmix\", \"prefix\": \"\"},\n",
        "    {\"name\": \"Colorful-v4.5\", \"path\": \"Manseo/Colorful-v4.5\", \"prefix\": \"\"},\n",
        "    {\"name\": \"Cool Japan Diffusion\", \"path\": \"aipicasso/cool-japan-diffusion-2-1-2-beta\", \"prefix\": \"\"},\n",
        "    {\"name\": \"Fantasy Mix\", \"path\": \"theintuitiveye/FantasyMix-v1\", \"prefix\": \"\"},\n",
        "    {\"name\": \"Roughness Painter\", \"path\": \"AIARTCHAN/roughnessPainter_v1.0\", \"prefix\": \"\"},\n",
        "    {\"name\": \"Isometric Dreams\", \"path\": \"Duskfallcrew/isometric-dreams-sd-1-5\", \"prefix\": \"\"},\n",
        "    {\"name\": \"Photography & Landscapes\", \"path\": \"Duskfallcrew/photography-and-landscapes\", \"prefix\": \"phtdzk1 \"},\n",
        "    {\"name\": \"Sygil Diffusion\", \"path\": \"Sygil/Sygil-Diffusion\", \"prefix\": \"\"},\n",
        "    {\"name\": \"Flat Icons\", \"path\": \"viba98/flat-icons\", \"prefix\": \"\"},\n",
        "    {\"name\": \"No Branch Repo\", \"path\": \"huggingface/the-no-branch-repo\", \"prefix\": \"\"},\n",
        "    {\"name\": \"Isometric Floating Icons\", \"path\": \"viba98/isometric-floating-icons\", \"prefix\": \"\"},\n",
        "    {\"name\": \"Dilbert Diffusion\", \"path\": \"CSAle/DilbertDiffusion2\", \"prefix\": \"dilbert \"},\n",
        "    {\"name\": \"Sketchstyle\", \"path\": \"Cosk/sketchstyle-cutesexyrobutts\", \"prefix\": \"sketchstyle \"},\n",
        "    {\"name\": \"Midjourney Shatter\", \"path\": \"ShadoWxShinigamI/Midjourney-Shatter\", \"prefix\": \"mdjrny-shttr \"},\n",
        "    {\"name\": \"Midjourney PaperCut\", \"path\": \"ShadoWxShinigamI/MidJourney-PaperCut\", \"prefix\": \"mdjrny-pprct eagle \"},\n",
        "    {\"name\": \"Midjourney Graffiti\", \"path\": \"ShadoWxShinigamI/midjourney-graffiti\", \"prefix\": \"in the style of mdjrny-grfft \"},\n",
        "    {\"name\": \"MJStyle\", \"path\": \"ShadoWxShinigamI/mjstyle\", \"prefix\": \"mjstyle \"},\n",
        "    {\"name\": \"Xpero End1ess\", \"path\": \"sakistriker/XperoEnd1essModel\", \"prefix\": \"\"},\n",
        "    {\"name\": \"Pepe Diffuser\", \"path\": \"Dipl0/pepe-diffuser\", \"prefix\": \"\"},\n",
        "    {\"name\": \"Pastel Mix\", \"path\": \"andite/pastel-mix\", \"prefix\": \"\"},\n",
        "    {\"name\": \"Counterfeit v2.5\", \"path\": \"gsdf/Counterfeit-V2.5\", \"prefix\": \"\"},\n",
        "    {\"name\": \"Basil Mix\", \"path\": \"nuigurumi/basil_mix\", \"prefix\": \"\"},\n",
        "    {\"name\": \"Inkpunk Diffusion\", \"path\": \"Envvi/Inkpunk-Diffusion\", \"prefix\": \"nvinkpunk \"},\n",
        "    {\"name\": \"Ghibli Diffusion\", \"path\": \"nitrosocke/Ghibli-Diffusion\", \"prefix\": \"ghibli style \"},\n",
        "    {\"name\": \"7th Layer\", \"path\": \"syaimu/7th_Layer\", \"prefix\": \"\"},\n",
        "    {\"name\": \"Comic-Diffusion\", \"path\": \"ogkalu/Comic-Diffusion\", \"prefix\": \"\"},\n",
        "    {\"name\": \"Vintedois Diffusion\", \"path\": \"22h/vintedois-diffusion-v0-1\", \"prefix\": \"\"},\n",
        "    {\"name\": \"PaperCut\", \"path\": \"Fictiverse/Stable_Diffusion_PaperCut_Model\", \"prefix\": \"PaperCut \"},\n",
        "    {\"name\": \"Complex Lineart\", \"path\": \"Conflictx/Complex-Lineart\", \"prefix\": \"ComplexLA style \"},\n",
        "    {\"name\": \"GuoFeng3\", \"path\": \"xiaolxl/GuoFeng3\", \"prefix\": \"\"},\n",
        "    {\"name\": \"Portrait+\", \"path\": \"wavymulder/portraitplus\", \"prefix\": \"portrait+ style\"},\n",
        "    {\"name\": \"ACertainThing\", \"path\": \"JosephusCheung/ACertainThing\", \"prefix\": \"\"},\n",
        "    {\"name\": \"Hassan Blend\", \"path\": \"hassanblend/HassanBlend1.5.1.2\", \"prefix\": \"\"},\n",
        "    #{\"name\": \"\", \"path\": \"\", \"prefix\": \"\"},\n",
        "    #{\"name\": \"Latent Labs 360\", \"path\": \"AlanB/LatentLabs360\", \"prefix\": \"\"},\n",
        "    #{\"name\": \"Rodent Diffusion 1.5\", \"path\": \"NerdyRodent/rodent-diffusion-1-5\", \"prefix\": \"\"},\n",
        "    #{\"name\": \"Laxpeint\", \"path\": \"EldritchAdam/laxpeint\", \"prefix\": \"\"},\n",
        "    #{\"name\": \"HeartArt\", \"path\": \"spaablauw/HeartArt\", \"prefix\": \"\"},\n",
        "    #{\"name\": \"ConceptArt\", \"path\": \"SatyamSSJ10/ConceptArt\", \"prefix\": \"\"},\n",
        "    #{\"name\": \"Modern Buildings\", \"path\": \"smereces/2.1-SD-Modern-Buildings-Style-MD\", \"prefix\": \"\"},\n",
        "    #{\"name\": \"Floral Marbles\", \"path\": \"N75242/FloralMarbles_Model\", \"prefix\": \"\"},\n",
        "    #{\"name\": \"Gemini_Anime\", \"path\": \"Cryonicus/Gemini_Anime\", \"prefix\": \"\"},\n",
        "    #{\"name\": \"Disco Difland\", \"path\": \"DarkBeam/discodifland\", \"prefix\": \"\"},\n",
        "    #{\"name\": \"Princess Jai Lee\", \"path\": \"zuleo/princess-jai-lee\", \"prefix\": \"\"},\n",
        "    #{\"name\": \"Style Goblinmode\", \"path\": \"TheAllyPrompts/Style-Goblinmode\", \"prefix\": \"\"},\n",
        "    #{\"name\": \"Joe87-Vibe\", \"path\": \"Joe87/joe87-vibe\", \"prefix\": \"joe87-vibe \"},\n",
        "    #{\"name\": \"Microwaist\", \"path\": \"SweetTalk/Microwaist\", \"prefix\": \"\"},\n",
        "    #{\"name\": \"Sci-Fi Diffusion\", \"path\": \"Corruptlake/Sci-Fi-Diffusion\", \"prefix\": \"\"},\n",
        "    #{\"name\": \"ParchArt\", \"path\": \"EldritchAdam/ParchArt\", \"prefix\": \"\"},\n",
        "    #{\"name\": \"Classipeint\", \"path\": \"EldritchAdam/classipeint\", \"prefix\": \"\"},\n",
        "    #{\"name\": \"Cyberpunked\", \"path\": \"GeneralAwareness/Cyberpunked\", \"prefix\": \"\"},\n",
        "    #{\"name\": \"Mangaka Boichi\", \"path\": \"Akumetsu971/SD_Boichi_Art_Style\", \"prefix\": \"\"},\n",
        "    #{\"name\": \"Samurai Anime\", \"path\": \"Akumetsu971/SD_Samurai_Anime_Style\", \"prefix\": \"\"},\n",
        "    #{\"name\": \"Cmodel\", \"path\": \"jinofcoolnes/CmodelSDV2\", \"prefix\": \"cmodel \"},\n",
        "    #{\"name\": \"Cyberware\", \"path\": \"Eppinette/Cyberware\", \"prefix\": \"-cyberware style \"},\n",
        "    #{\"name\": \"OldJourney\", \"path\": \"StarwingDigital/Oldjourney\", \"prefix\": \"\"},\n",
        "    #{\"name\": \"Hyper Smoke\", \"path\": \"spaablauw/HyperSmoke\", \"prefix\": \"\"},\n",
        "    #{\"name\": \"Fantasy Diffusion\", \"path\": \"IceChes/fantasydiffusionembedding\", \"prefix\": \"\"},\n",
        "    #{\"name\": \"Double-Exposure\", \"path\": \"joachimsallstrom/Double-Exposure-Embedding\", \"prefix\": \"dblx \"},\n",
        "    #{\"name\": \"Studio Ghibli\", \"path\": \"flax/StudioGhibli\", \"prefix\": \"\", \"vae\": True},\n",
        "    #{\"name\": \"Picture of the Week\", \"path\": \"Guizmus/SD_PoW_Collection\", \"prefix\": \"PoW Style \", \"vae\": True},\n",
        "    #{\"name\": \"PoW Bendstract \", \"path\": \"Guizmus/SD_PoW_Collection\", \"prefix\": \"Bendstract Style \", \"vae\": True},\n",
        "    #{\"name\": \"PoW BendingReality\", \"path\": \"Guizmus/SD_PoW_Collection\", \"prefix\": \"BendingReality Style \", \"vae\": True},\n",
        "    #{\"name\": \"3d Illustration\", \"path\": \"aidystark/3Dillustration-stable-diffusion\", \"prefix\": \"3d illustration style \", \"vae\": True},\n",
        "    #{\"name\": \"megaPals Vintage\", \"path\": \"elRivx/megaPals\", \"prefix\": \"megaPals style \"},\n",
        "    #{\"name\": \"Epic Space Machine\", \"path\": \"rabidgremlin/sd-db-epic-space-machine\", \"prefix\": \"EpicSpaceMachine \"},\n",
        "    #{\"name\": \"Ouroboros\", \"path\": \"Eppinette/Ouroboros\", \"prefix\": \"m_ouroboros style \"},\n",
        "    #{\"name\": \"Neko Girls\", \"path\": \"Nerfgun3/NekoModel\", \"prefix\": \"neko \"},\n",
        "    #{\"name\": \"New Horror Fantasy\", \"path\": \"elRivx/sd-newhorrorfantasy_style\", \"prefix\": \"newhorrorfantasy_style \"},\n",
        "    #{\"name\": \"DCAU Batman\", \"path\": \"IShallRiseAgain/DCAU\", \"prefix\": \"Batman_the_animated_series \"},\n",
        "    #{\"name\": \"Smoke Diffusion\", \"path\": \"guumaster/smoke-diffusion\", \"prefix\": \"ssmoky \"},\n",
        "    #{\"name\": \"reasonableDrink Dreams\", \"path\": \"elRivx/reasonableDrink\", \"prefix\": \"reasonableDrink \"},\n",
        "]\n",
        "dreambooth_models = [{'name': 'disco-diffusion-style', 'token': 'a photo of ddfusion style'}, {'name': 'cat-toy', 'token': 'a photo of sks toy'}, {'name': 'herge-style', 'token': 'a photo of sks herge_style'}, {'name': 'alberto-pablo', 'token': 'a photo of sks Alberto'}, {'name': 'noggles-sd15-800-4e6', 'token': 'someone wearing sks glasses'}, {'name': 'spacecat', 'token': 'a photo of sks spacecat'}, {'name': 'pikachu', 'token': 'pikachu'}, {'name': 'kaltsit', 'token': 'kaltsit'}, {'name': 'robeez-baby-girl-water-shoes', 'token': 'a photo of sks  shoes'}, {'name': 'mertgunhan', 'token': 'mertgunhan'}, {'name': 'soydavidtapia', 'token': 'a photo of david tapia'}, {'name': 'spacecat0001', 'token': 'a photo of sks spacecat'}, {'name': 'noggles-glasses-600', 'token': 'a photo of a person wearing sks glasses'}, {'name': 'mario-action-figure', 'token': 'a photo of sks action figure'}, {'name': 'tattoo-design', 'token': 'line art sks tattoo design'}, {'name': 'danielveneco2', 'token': 'danielveneco'}, {'name': 'scarlet-witch-two', 'token': 'a photo of scarletwi person'}, {'name': 'angus-mcbride-style', 'token': 'angus mcbride style'}, {'name': 'mirtha-legrand', 'token': 'a photo of sks mirtha legrand'}, {'name': 'kiril', 'token': 'kiril'}, {'name': 'mr-potato-head', 'token': 'a photo of sks mr potato head'}, {'name': 'homelander', 'token': 'a photo of homelander guy'}, {'name': 'king-dog-sculpture', 'token': 'a photo of sks king dog sculpture'}, {'name': 'pedrocastillodonkey', 'token': 'a photo of PedroCastilloDonkey'}, {'name': 'xogren', 'token': 'a photo of xogren'}, {'name': 'emily-carroll-style', 'token': 'a detailed digital matte illustration by sks'}, {'name': 'sneaker', 'token': 'a photo of sks sneaker'}, {'name': 'rajj', 'token': 'a photo of sks man face'}, {'name': 'puuung', 'token': 'Puuung'}, {'name': 'partis', 'token': 'a photo of sks partis'}, {'name': 'alien-coral', 'token': 'a photo of sks alien coral'}, {'name': 'hensley-art-style', 'token': 'a painting in style of sks'}, {'name': 'tails-from-sonic', 'token': 'tails'}, {'name': 'ba-shiroko', 'token': 'a photo of sks shiroko'}, {'name': 'marina', 'token': 'marina'}, {'name': 'noggles-glasses-1200', 'token': 'a photo of a person wearing sks glasses'}, {'name': 'a-hat-in-time-girl', 'token': 'a render of sks'}, {'name': 'axolotee', 'token': 'a photo of sks Axolote'}, {'name': 'transparent-90s-console', 'token': 'a photo of sks handheld gaming console'}, {'name': 'andynsane', 'token': 'a photo of sks andynsane'}, {'name': 'tanidareal-v1', 'token': 'tanidareal'}, {'name': 'adventure-time-style', 'token': 'advtime style'}, {'name': 'sks-rv', 'token': 'a photo of sks rv'}, {'name': 'neff-voice-amp-2', 'token': 'a photo of sks neff voice amp #1'}, {'name': '27-from-mayonnaise-salesmen', 'token': 'a drawing of 27 from Mayonnaise SalesMen'}, {'name': 'baracus', 'token': 'b.a. baracus mr t'}, {'name': 'tahdig-rice', 'token': 'tahmricdig'}, {'name': 'angus-mcbride-style-v4', 'token': 'mcbride_style'}, {'name': 'the-witcher-game-ciri', 'token': 'a photo of a sks woman with white hair'}, {'name': 'paolo-bonolis', 'token': 'a photo of sks paolo bonolis'}, {'name': 'the-child', 'token': 'a photo of a mini australian shepherd with a slight underbite sks'}, {'name': 'gomber', 'token': 'a photo of sks toy'}, {'name': 'backpack', 'token': 'a photo of sks backpack'}, {'name': 'ricky-fort', 'token': 'a photo of sks ricky fort'}, {'name': 'mate', 'token': 'a photo of sks mate'}, {'name': 'zombie-head', 'token': 'a photo of sks zombie'}, {'name': 'leone-from-akame-ga-kill-v2', 'token': 'an anime woman character of sks'}, {'name': 'face2contra', 'token': 'a photo of sks face2contra'}, {'name': 'yakuza-0-kiryu-kazuma', 'token': 'photo of sks kiryu'}, {'name': 'gemba-cat', 'token': 'a photo of sks cat'}, {'name': 'angus-mcbride-v-3', 'token': 'angus mcbride style'}, {'name': 'california-gurls-music-video', 'token': 'caligurls'}, {'name': 'solo-levelling-art-style', 'token': 'sololeveling'}, {'name': 'blue-lightsaber-toy', 'token': 'a photo of sks toy'}, {'name': 'dmt-entity', 'token': 'a photo of sks DMT Entity'}, {'name': 'yingdream', 'token': 'a photo of an anime girl'}, {'name': 'kamenridergeats', 'token': 'a photo of kamenridergeats'}, {'name': 'quino', 'token': 'a photo of sks quino'}, {'name': 'digimon-adventure-anime-background-style', 'token': 'a landscape in sks style'}, {'name': 'evangelion-mech-unit-01', 'token': 'rendering of sks evangelion mech'}, {'name': 'elvis', 'token': 'elvis'}, {'name': 'musical-isotope', 'token': 'mi'}, {'name': 'tempa', 'token': 'a photo of sks Tempa'}, {'name': 'tempa2', 'token': 'a photo of sks Tempa'}, {'name': 'froggewut', 'token': 'a painting in the style of sks'}, {'name': 'smiling-friends-cartoon-style', 'token': 'a photo in style of sks'}, {'name': 'smario-world-map', 'token': 'a map in style of sks'}, {'name': 'edd', 'token': 'sks boy smiles'}, {'name': 'fang-yuan-002', 'token': 'an anime art of sks Fang_Yuan'}, {'name': 'langel', 'token': 'Langel'}, {'name': 'arthur-leywin', 'token': 'a photo of sks guy'}, {'name': 'kid-chameleon-character', 'token': 'kid-chameleon-character'}, {'name': 'road-to-ruin', 'token': 'starry night. sks themed level design. tiki ruins, stone statues, night sky and black silhouettes'}, {'name': 'vaporfades', 'token': 'an image in the style of sks'}, {'name': 'beard-oil-big-sur', 'token': 'a photo of sks beard oil'}, {'name': 'monero', 'token': 'a logo of sks'}, {'name': 'yagami-taichi-from-digimon-adventure-1999', 'token': 'an anime boy character of sks'}, {'name': 'duregar', 'token': 'a painting of sks character'}, {'name': 'pathfinder-iconics', 'token': 'drawing in the style of sks'}, {'name': 'tyxxxszv', 'token': 'tyxxxszv'}, {'name': 'Origtron', 'token': 'Entry not found'}, {'name': 'oleg-kog', 'token': 'oleg'}, {'name': 'mau-cat', 'token': 'a photo of sks cat'}, {'name': 'justinkrane-artwork', 'token': 'art by sks JustinKrane'}, {'name': 'little-mario-jumping', 'token': 'a screenshot of tiny sks character'}, {'name': 'blue-moo-moo', 'token': 'an image of sks creature'}, {'name': 'noggles-render-1k', 'token': 'a render of sks'}, {'name': 'metahuman-rkr', 'token': 'a photo of sks rkr'}, {'name': 'taras', 'token': 'photo of sks taras'}, {'name': 'rollerbeetle', 'token': 'a photo of rollerbeetle mount'}, {'name': 'joseph-russel-ammen', 'token': 'Joseph Russel Ammen'}, {'name': 'manybearsx', 'token': 'a photo of sks drawing'}, {'name': 'mexican-concha', 'token': 'a photo of sks Mexican Concha'}, {'name': 'angus-mcbride-style-v2', 'token': 'angus mcbride style'}, {'name': 'magikarp-pokemon', 'token': 'a photo of sks pokemon'}, {'name': 'seraphm', 'token': 'serphm'}, {'name': 'estelle-sims-style', 'token': '3D render from a videogame in sks style'}, {'name': 'iman-maleki-morteza-koutzian', 'token': 'imamk'}, {'name': 'abstract-patterns-in-nature', 'token': 'abnapa'}, {'name': 'retro3d', 'token': 'trsldamrl'}, {'name': 'glitched', 'token': 'trsldamrl'}, {'name': 'dulls', 'token': '<dulls-avatar> face'}, {'name': 'nasa-space-v2-768', 'token': 'Nasa style'}, {'name': 'avocado-toy', 'token': '<avocado-toy> toy'}, {'name': 'crisimsestelle', 'token': '3d render in <cri-sims> style'}, {'name': 'sally-whitemanev', 'token': 'whitemanedb'}, {'name': 'taylorswift', 'token': 'indexaa.png'}, {'name': 'house-emblem', 'token': 'a photo of sks house-emblem'}, {'name': 'skshikakinotonoderugomi', 'token': 'sksHikakinotonoderugomi'}, {'name': 'sksbinjousoudayo', 'token': 'sksBinjouSoudayo'}, {'name': 'sksseisupusyamuzero', 'token': '„ÉÄ„Ç¶„É≥„É≠„Éº„Éâ'}, {'name': 'sksuminaoshishimabu', 'token': '„ÉÄ„Ç¶„É≥„É≠„Éº„Éâ'}, {'name': 'hog-rider', 'token': 'a photo of sks character'}, {'name': 'harvard-beating-yale-ii', 'token': 'a photo of sks Harvard beating Yale'}, {'name': 'hockey-player', 'token': 'a photo of sks hockey'}, {'name': 'christiano-ronaldo', 'token': 'a photo of sks'}, {'name': 'colorful-ball', 'token': 'a photo of sks ball'}, {'name': 'american-flag-cowboy-hat', 'token': 'a photo of sks hat'}, {'name': 'pranav', 'token': 'a photo of sks person'}, {'name': 'top-gun-jacket-stable-diffusion', 'token': 'a photo of sks jacket'}, {'name': 'english-bulldog-1', 'token': 'a photo of sks an english bulldog'}, {'name': 'danreynolds', 'token': 'a photo of sks dan reynolds'}, {'name': 'persona-5-shigenori-style', 'token': 'descarga'}, {'name': 'original-character-cyclps', 'token': 'cyclps'}, {'name': 'zlnsky', 'token': 'zlnsky'}, {'name': 'true-guweiz-style', 'token': 'descarga'}, {'name': 'noggles-widescreen-4e6-800', 'token': 'noggles'}, {'name': 'conf', 'token': 'AWCDJG'}, {'name': 'dtv-pkmn-monster-style', 'token': 'image'}, {'name': 'xmasvibes', 'token': 'xmasvibes'}, {'name': 'blue-lightsaber-toy', 'token': 'a photo of sks toy'}, {'name': 'adventure-time-style', 'token': 'advtime style'}, {'name': 'brime', 'token': 'prplbrime'}, {'name': 'angus-mcbride-style-v4', 'token': 'mcbride_style'}, {'name': 'oleg-kog', 'token': 'oleg'}, {'name': 'tanidareal-v1', 'token': 'tanidareal'}, {'name': 'mertgunhan', 'token': 'mertgunhan'}, {'name': 'solo-levelling-art-style', 'token': 'sololeveling'}, {'name': 'tyxxxszv', 'token': 'tyxxxszv'}, {'name': 'california-gurls-music-video', 'token': 'caligurls'}, {'name': 'mario-action-figure', 'token': 'a photo of sks action figure'}, {'name': 'tahdig-rice', 'token': 'tahmricdig'}, {'name': 'pathfinder-iconics', 'token': 'drawing in the style of sks'}, {'name': 'angus-mcbride-v-3', 'token': 'angus mcbride style'}, {'name': 'angus-mcbride-style-v2', 'token': 'angus mcbride style'}, {'name': 'angus-mcbride-style', 'token': 'angus mcbride style'}, {'name': 'danielveneco2', 'token': 'danielveneco'}, {'name': 'emily-carroll-style', 'token': 'a detailed digital matte illustration by sks'}, {'name': 'noggles-sd15-800-4e6', 'token': 'someone wearing sks glasses'}, {'name': 'alberto-pablo', 'token': 'a photo of sks Alberto'}, {'name': 'marina', 'token': 'marina'}, {'name': 'kiril', 'token': 'kiril'}, {'name': 'spacecat0001', 'token': 'a photo of sks spacecat'}, {'name': 'baracus', 'token': 'b.a. baracus mr t'}, {'name': 'gemba-cat', 'token': 'a photo of sks cat'}, {'name': 'xogren', 'token': 'a photo of xogren'}, {'name': 'musical-isotope', 'token': 'mi'}, {'name': 'spacecat', 'token': 'a photo of sks spacecat'}, {'name': 'soydavidtapia', 'token': 'a photo of david tapia'}, {'name': 'yakuza-0-kiryu-kazuma', 'token': 'photo of sks kiryu'}, {'name': 'pedrocastillodonkey', 'token': 'a photo of PedroCastilloDonkey'}, {'name': 'rajj', 'token': 'a photo of sks man face'}, {'name': 'tails-from-sonic', 'token': 'tails'}, {'name': 'pikachu', 'token': 'pikachu'}, {'name': '27-from-mayonnaise-salesmen', 'token': 'a drawing of 27 from Mayonnaise SalesMen'}, {'name': 'vaporfades', 'token': 'an image in the style of sks'}, {'name': 'sally-whitemanev', 'token': 'whitemanedb'} ]\n",
        "\n",
        "def get_model(name):\n",
        "  #dropdown.Option(\"Stable Diffusion v1.5\"), dropdown.Option(\"Stable Diffusion v1.4\", dropdown.Option(\"Community Finetuned Model\", dropdown.Option(\"DreamBooth Library Model\"), dropdown.Option(\"Custom Model Path\")\n",
        "  if name == \"Stable Diffusion v2.1 x768\":\n",
        "    return {'name':'Stable Diffusion v2.1 x768', 'path':'stabilityai/stable-diffusion-2-1', 'prefix':'', 'revision': 'fp16'}\n",
        "  elif name == \"Stable Diffusion v2.1 x512\":\n",
        "    return {'name':'Stable Diffusion v2.1 x512', 'path':'stabilityai/stable-diffusion-2-1-base', 'prefix':''}\n",
        "  elif name == \"Stable Diffusion v2.0\":\n",
        "    return {'name':'Stable Diffusion v2.0', 'path':'stabilityai/stable-diffusion-2', 'prefix':'', 'revision': 'fp16'}\n",
        "  elif name == \"Stable Diffusion v2.0 x768\":\n",
        "    return {'name':'Stable Diffusion v2.0 x768', 'path':'stabilityai/stable-diffusion-2', 'prefix':'', 'revision': 'fp16'}\n",
        "  elif name == \"Stable Diffusion v2.0 x512\":\n",
        "    return {'name':'Stable Diffusion v2.0 x512', 'path':'stabilityai/stable-diffusion-2-base', 'prefix':'', 'revision': 'fp16'}\n",
        "  elif name == \"Stable Diffusion v1.5\":\n",
        "    return {'name':'Stable Diffusion v1.5', 'path':'runwayml/stable-diffusion-v1-5', 'prefix':'', 'revision': 'fp16'}\n",
        "  elif name == \"Stable Diffusion v1.4\":\n",
        "    return {'name':'Stable Diffusion v1.4', 'path':'CompVis/stable-diffusion-v1-4', 'prefix':'', 'revision': 'fp16'}\n",
        "  elif name == \"Community Finetuned Model\":\n",
        "    return get_finetuned_model(prefs['finetuned_model'])\n",
        "  elif name == \"DreamBooth Library Model\":\n",
        "    return get_dreambooth_model(prefs['dreambooth_model'])\n",
        "  elif name == \"Custom Model Path\":\n",
        "    return {'name':'Custom Model', 'path':prefs['custom_model'], 'prefix':''}\n",
        "  else:\n",
        "    return {'name':'', 'path':'', 'prefix':''}\n",
        "\n",
        "def get_finetuned_model(name):\n",
        "  for mod in finetuned_models:\n",
        "      if mod['name'] == name:\n",
        "        return mod\n",
        "  for mod in prefs['custom_models']:\n",
        "      if mod['name'] == name:\n",
        "        return mod\n",
        "  return {'name':'', 'path':'', 'prefix':''}\n",
        "def get_dreambooth_model(name):\n",
        "  for mod in dreambooth_models:\n",
        "      if mod['name'] == name:\n",
        "        return {'name':mod['name'], 'path':f'sd-dreambooth-library/{mod[\"name\"]}', 'prefix':mod['token']}\n",
        "  return {'name':'', 'path':'', 'prefix':''}\n",
        "def get_LoRA_model(name):\n",
        "  if name == \"Custom LoRA Path\":\n",
        "      return {'name':\"Custom LoRA Model\", 'path':prefs['custom_LoRA_model']}\n",
        "  for mod in LoRA_models:\n",
        "      if mod['name'] == name:\n",
        "        return {'name':mod['name'], 'path':mod['path']}\n",
        "  if len(prefs['custom_LoRA_models']) > 0:\n",
        "    for mod in prefs['custom_LoRA_models']:\n",
        "      if mod['name'] == name:\n",
        "        return {'name':mod['name'], 'path':mod['path']}\n",
        "  return {'name':'', 'path':''}\n",
        "\n",
        "def get_diffusers(page):\n",
        "    global scheduler, model_path, prefs, status\n",
        "    torch_installed = False\n",
        "    try:\n",
        "        import torch\n",
        "        torch_installed = True\n",
        "    except:\n",
        "        pass\n",
        "    if torch_installed:\n",
        "        if version.parse(torch.__version__) < version.parse(\"2.0.0\"):\n",
        "            torch_installed = False\n",
        "    if not torch_installed:\n",
        "        page.console_msg(\"Upgrading Torch 2.0 Packages...\")\n",
        "        run_process(\"pip uninstall --yes torch torchaudio torchvision torchtext torchdata\", page=page)\n",
        "        if is_Colab:\n",
        "            run_process(\"pip install torch torchaudio torchvision torchtext torchdata\", page=page)\n",
        "        else: #TODO: Check OS and run platform specific\n",
        "            run_process(\"pip install torch torchvision torchaudio torchtext torchdata --index-url https://download.pytorch.org/whl/cu118\", page=page)\n",
        "    if prefs['memory_optimization'] == 'Xformers Mem Efficient Attention':\n",
        "        try:\n",
        "            import xformers\n",
        "            if force_updates: raise ModuleNotFoundError(\"Forcing update\")\n",
        "        except ModuleNotFoundError:\n",
        "            page.console_msg(\"Installing FaceBook's Xformers Memory Efficient Package...\")\n",
        "            run_process(\"pip install --pre -U triton\", page=page)\n",
        "            run_process(\"pip install -U xformers==0.0.18\", page=page)\n",
        "            import xformers\n",
        "            page.console_msg(\"Installing Hugging Face Diffusers Pipeline...\")\n",
        "            pass\n",
        "        #run_process(\"pip install pyre-extensions==0.0.23\", page=page)\n",
        "        #run_process(\"pip install -i https://test.pypi.org/simple/ formers==0.0.15.dev376\", page=page)\n",
        "        #run_process(\"pip install -q https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/T4/xformers-0.0.13.dev0-py3-none-any.whl\", page=page)\n",
        "        #run_process(\"pip install https://github.com/metrolobo/xformers_wheels/releases/download/1d31a3ac/xformers-0.0.14.dev0-cp37-cp37m-linux_x86_64.whl\", page=page)\n",
        "        #if install_xformers(page):\n",
        "        status['installed_xformers'] = True\n",
        "    try:\n",
        "        import accelerate\n",
        "    except ModuleNotFoundError:\n",
        "        page.console_msg(\"Installing Hugging Face Accelerate Package...\")\n",
        "        run_process(\"pip install --upgrade accelerate~=0.18 -q\", page=page)\n",
        "        #run_process(\"pip install --upgrade git+https://github.com/huggingface/accelerate.git -q\", page=page)\n",
        "        #run_sp(\"python -c from accelerate.utils import write_basic_config; write_basic_config(mixed_precision='fp16')\")\n",
        "        from accelerate.utils import write_basic_config\n",
        "        #write_basic_config(mixed_precision='fp16')\n",
        "        pass\n",
        "    page.console_msg(\"Installing Hugging Face Diffusers Pipeline...\")\n",
        "    try:\n",
        "        import transformers\n",
        "        #print(f\"transformers=={transformers.__version__}\")\n",
        "        if transformers.__version__ == \"4.21.3\": #Workaround because CLIP-Interrogator required other version\n",
        "          run_process(\"pip uninstall -y git+https://github.com/pharmapsychotic/BLIP.git@lib#egg=blip\", realtime=False)\n",
        "          run_process(\"pip uninstall -y clip-interrogator\", realtime=False)\n",
        "          run_process(\"pip uninstall -y transformers\", realtime=False)\n",
        "          #run_process(\"pip uninstall -q transformers==4.21.3\", page=page, realtime=False)\n",
        "        if transformers.__version__ == \"4.23.1\": # Kandinsky conflict\n",
        "          run_process(\"pip uninstall -y transformers\", realtime=False)\n",
        "    except ModuleNotFoundError:\n",
        "        pass\n",
        "    #run_process(\"pip install -q huggingface_hub\", page=page)\n",
        "    '''try:\n",
        "      from huggingface_hub import notebook_login, HfApi, HfFolder, login\n",
        "      from diffusers import StableDiffusionPipeline, logging\n",
        "      import transformers\n",
        "    except ModuleNotFoundError as e:#ModuleNotFoundError as e:'''\n",
        "    try:\n",
        "        import diffusers\n",
        "        if force_updates: raise ModuleNotFoundError(\"Forcing update\")\n",
        "    except ModuleNotFoundError:\n",
        "        #run_process(\"pip install --upgrade git+https://github.com/Skquark/diffusers.git\", page=page)\n",
        "        run_process(\"pip install --upgrade git+https://github.com/Skquark/diffusers.git@main#egg=diffusers[torch]\", page=page)\n",
        "        pass\n",
        "    try:\n",
        "        import transformers\n",
        "        if force_updates: raise ModuleNotFoundError(\"Forcing update\")\n",
        "    except ModuleNotFoundError:\n",
        "        #run_process(\"pip install -qq --upgrade git+https://github.com/huggingface/transformers\", page=page)\n",
        "        run_process(\"pip install --upgrade transformers~=4.28\", page=page)\n",
        "        pass\n",
        "    try:\n",
        "        import scipy\n",
        "    except ModuleNotFoundError:\n",
        "        run_process(\"pip install -upgrade scipy\", page=page)\n",
        "        pass\n",
        "    try:\n",
        "        import ftfy\n",
        "    except ModuleNotFoundError:\n",
        "        run_process(\"pip install --upgrade ftfy\", page=page)\n",
        "        pass\n",
        "    try:\n",
        "        import safetensors\n",
        "    except ModuleNotFoundError:\n",
        "        run_process(\"pip install --upgrade safetensors~=0.3\", page=page)\n",
        "        import safetensors\n",
        "        from safetensors import safe_open\n",
        "        pass\n",
        "    try:\n",
        "        import ipywidgets\n",
        "    except ModuleNotFoundError:\n",
        "        run_process('pip install -qq \"ipywidgets>=7,<8\"', page=page)\n",
        "        pass\n",
        "    run_process(\"git config --global credential.helper store\", page=page)\n",
        "    \n",
        "    from huggingface_hub import notebook_login, HfApi, HfFolder, login\n",
        "    #from diffusers import StableDiffusionPipeline, logging\n",
        "    from diffusers import logging\n",
        "    logging.set_verbosity_error()\n",
        "    if not os.path.exists(HfFolder.path_token):\n",
        "        #from huggingface_hub.commands.user import _login\n",
        "        #_login(HfApi(), token=prefs['HuggingFace_api_key'])\n",
        "        try:\n",
        "          login(token=prefs['HuggingFace_api_key'], add_to_git_credential=True)\n",
        "        except Exception:\n",
        "          alert_msg(page, \"ERROR Logging into HuggingFace... Check your API Key or Internet conenction.\")\n",
        "          return\n",
        "    # TODO: Get Username to prefs\n",
        "    api = HfApi()\n",
        "    prefs['HuggingFace_username'] = api.whoami()[\"name\"]\n",
        "    #if prefs['model_ckpt'] == \"Stable Diffusion v1.5\": model_path =  \"runwayml/stable-diffusion-v1-5\"\n",
        "    #elif prefs['model_ckpt'] == \"Stable Diffusion v1.4\": model_path =  \"CompVis/stable-diffusion-v1-4\"\n",
        "    model = get_model(prefs['model_ckpt'])\n",
        "    model_path = model['path']\n",
        "    #try:\n",
        "    #  scheduler = model_scheduler(model_path)\n",
        "    #except Exception as e:\n",
        "    #  alert_msg(page, f\"ERROR: {prefs['scheduler_mode']} Scheduler couldn't load for {model_path}\", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip())]))\n",
        "    #  pass\n",
        "    status['finetuned_model'] = False if model['name'].startswith(\"Stable\") else True\n",
        "    \n",
        "\n",
        "def model_scheduler(model, big3=False):\n",
        "    scheduler_mode = prefs['scheduler_mode']\n",
        "    if scheduler_mode == \"LMS Discrete\":\n",
        "      from diffusers import LMSDiscreteScheduler\n",
        "      s = LMSDiscreteScheduler.from_pretrained(model, subfolder=\"scheduler\")\n",
        "    elif scheduler_mode == \"PNDM\":\n",
        "      from diffusers import PNDMScheduler\n",
        "      s = PNDMScheduler.from_pretrained(model, subfolder=\"scheduler\")\n",
        "    elif scheduler_mode == \"DDIM\":\n",
        "      from diffusers import DDIMScheduler\n",
        "      s = DDIMScheduler.from_pretrained(model, subfolder=\"scheduler\")\n",
        "    elif big3:\n",
        "      from diffusers import DDIMScheduler\n",
        "      s = DDIMScheduler.from_pretrained(model, subfolder=\"scheduler\")\n",
        "    elif scheduler_mode == \"DPM Solver\":\n",
        "      from diffusers import DPMSolverMultistepScheduler #\"hf-internal-testing/tiny-stable-diffusion-torch\"\n",
        "      s = DPMSolverMultistepScheduler.from_pretrained(model, subfolder=\"scheduler\")\n",
        "    elif scheduler_mode == \"DPM Solver Singlestep\":\n",
        "      from diffusers import DPMSolverSinglestepScheduler\n",
        "      s = DPMSolverSinglestepScheduler.from_pretrained(model, subfolder=\"scheduler\")\n",
        "    elif scheduler_mode == \"K-Euler Discrete\":\n",
        "      from diffusers import EulerDiscreteScheduler\n",
        "      s = EulerDiscreteScheduler.from_pretrained(model, subfolder=\"scheduler\")\n",
        "    elif scheduler_mode == \"K-Euler Ancestral\":\n",
        "      from diffusers import EulerAncestralDiscreteScheduler\n",
        "      s = EulerAncestralDiscreteScheduler.from_pretrained(model, subfolder=\"scheduler\")\n",
        "    elif scheduler_mode == \"Karras-LMS\":\n",
        "      from diffusers import LMSDiscreteScheduler\n",
        "      s = LMSDiscreteScheduler.from_pretrained(model, subfolder=\"scheduler\")\n",
        "      scheduler_config = s.get_scheduler_config()\n",
        "      s = LMSDiscreteScheduler(**scheduler_config, use_karras_sigmas=True)\n",
        "    elif scheduler_mode == \"DPM Stochastic\":\n",
        "      from diffusers import DPMSolverSDEScheduler\n",
        "      s = DPMSolverSDEScheduler.from_pretrained(model, subfolder=\"scheduler\")\n",
        "    elif scheduler_mode == \"SDE-DPM Solver++\":\n",
        "      from diffusers import DPMSolverMultistepScheduler\n",
        "      s = DPMSolverMultistepScheduler.from_pretrained(model, subfolder=\"scheduler\")\n",
        "      s.config.algorithm_type = 'sde-dpmsolver++'\n",
        "    elif scheduler_mode == \"DPM Solver++\":\n",
        "      from diffusers import DPMSolverMultistepScheduler\n",
        "      s = DPMSolverMultistepScheduler.from_pretrained(model, subfolder=\"scheduler\",\n",
        "        beta_start=0.00085,\n",
        "        beta_end=0.012,\n",
        "        beta_schedule=\"scaled_linear\",\n",
        "        num_train_timesteps=1000,\n",
        "        trained_betas=None,\n",
        "        #predict_epsilon=True,\n",
        "        prediction_type=\"v_prediction\" if model.startswith('stabilityai') else \"epsilon\",\n",
        "        thresholding=False,\n",
        "        algorithm_type=\"dpmsolver++\",\n",
        "        solver_type=\"midpoint\",\n",
        "        solver_order=2,\n",
        "        #denoise_final=True,\n",
        "        lower_order_final=True,\n",
        "      )\n",
        "    elif scheduler_mode == \"Heun Discrete\":\n",
        "      from diffusers import HeunDiscreteScheduler\n",
        "      s = HeunDiscreteScheduler.from_pretrained(model, subfolder=\"scheduler\")\n",
        "    elif scheduler_mode == \"Karras Heun Discrete\":\n",
        "      from diffusers import HeunDiscreteScheduler\n",
        "      s = HeunDiscreteScheduler.from_pretrained(model, subfolder=\"scheduler\", use_karras_sigmas=True)\n",
        "    elif scheduler_mode == \"K-DPM2 Ancestral\":\n",
        "      from diffusers import KDPM2AncestralDiscreteScheduler\n",
        "      s = KDPM2AncestralDiscreteScheduler.from_pretrained(model, subfolder=\"scheduler\")\n",
        "    elif scheduler_mode == \"K-DPM2 Discrete\":\n",
        "      from diffusers import KDPM2DiscreteScheduler\n",
        "      s = KDPM2DiscreteScheduler.from_pretrained(model, subfolder=\"scheduler\")\n",
        "    elif scheduler_mode == \"IPNDM\":\n",
        "      from diffusers import IPNDMScheduler\n",
        "      s = IPNDMScheduler.from_pretrained(model, subfolder=\"scheduler\")\n",
        "    elif scheduler_mode == \"DEIS Multistep\":\n",
        "      from diffusers import DEISMultistepScheduler\n",
        "      s = DEISMultistepScheduler.from_pretrained(model, subfolder=\"scheduler\")\n",
        "    elif scheduler_mode == \"UniPC Multistep\":\n",
        "      from diffusers import UniPCMultistepScheduler\n",
        "      s = UniPCMultistepScheduler.from_pretrained(model, subfolder=\"scheduler\")\n",
        "    elif scheduler_mode == \"Score-SDE-Vp\":\n",
        "      from diffusers import ScoreSdeVpScheduler\n",
        "      s = ScoreSdeVpScheduler() #(num_train_timesteps=2000, beta_min=0.1, beta_max=20, sampling_eps=1e-3, tensor_format=\"np\")\n",
        "      use_custom_scheduler = True\n",
        "    elif scheduler_mode == \"Score-SDE-Ve\":\n",
        "      from diffusers import ScoreSdeVeScheduler\n",
        "      s = ScoreSdeVeScheduler() #(num_train_timesteps=2000, snr=0.15, sigma_min=0.01, sigma_max=1348, sampling_eps=1e-5, correct_steps=1, tensor_format=\"pt\"\n",
        "      use_custom_scheduler = True\n",
        "    elif scheduler_mode == \"DDPM\":\n",
        "      from diffusers import DDPMScheduler\n",
        "      s = DDPMScheduler(num_train_timesteps=1000, beta_start=0.0001, beta_end=0.02, beta_schedule=\"linear\", trained_betas=None, variance_type=\"fixed_small\", clip_sample=True, tensor_format=\"pt\")\n",
        "      use_custom_scheduler = True\n",
        "    elif scheduler_mode == \"Karras-Ve\":\n",
        "      from diffusers import KarrasVeScheduler\n",
        "      s = KarrasVeScheduler() #(sigma_min=0.02, sigma_max=100, s_noise=1.007, s_churn=80, s_min=0.05, s_max=50, tensor_format=\"pt\")\n",
        "      use_custom_scheduler = True\n",
        "    elif scheduler_mode == \"LMS\": #no more\n",
        "      from diffusers import LMSScheduler\n",
        "      s = LMSScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\")\n",
        "      #(num_train_timesteps=1000, beta_start=0.0001, beta_end=0.02, beta_schedule=\"linear\", trained_betas=None, timestep_values=None, tensor_format=\"pt\")\n",
        "      use_custom_scheduler = True\n",
        "    #print(f\"Loaded Schedueler {scheduler_mode} {type(scheduler)}\")\n",
        "    else:\n",
        "      print(f\"Unknown scheduler request {scheduler_mode} - Using LMS Discrete\")\n",
        "      from diffusers import LMSDiscreteScheduler\n",
        "      s = LMSDiscreteScheduler.from_pretrained(model, subfolder=\"scheduler\")\n",
        "    return s\n",
        "\n",
        "def pipeline_scheduler(p, big3=False, from_scheduler = True):\n",
        "    scheduler_mode = prefs['scheduler_mode']\n",
        "    if scheduler_mode == \"LMS Discrete\":\n",
        "      from diffusers import LMSDiscreteScheduler\n",
        "      s = LMSDiscreteScheduler.from_config(p.scheduler.config if from_scheduler else p.config)\n",
        "    elif scheduler_mode == \"PNDM\":\n",
        "      from diffusers import PNDMScheduler\n",
        "      s = PNDMScheduler.from_config(p.scheduler.config if from_scheduler else p.config)\n",
        "    elif scheduler_mode == \"DDIM\":\n",
        "      from diffusers import DDIMScheduler\n",
        "      s = DDIMScheduler.from_config(p.scheduler.config if from_scheduler else p.config)\n",
        "    elif big3:\n",
        "      from diffusers import DDIMScheduler\n",
        "      s = DDIMScheduler.from_config(p.scheduler.config if from_scheduler else p.config)\n",
        "    elif scheduler_mode == \"DPM Solver\":\n",
        "      from diffusers import DPMSolverMultistepScheduler #\"hf-internal-testing/tiny-stable-diffusion-torch\"\n",
        "      s = DPMSolverMultistepScheduler.from_config(p.scheduler.config if from_scheduler else p.config)\n",
        "    elif scheduler_mode == \"DPM Solver Singlestep\":\n",
        "      from diffusers import DPMSolverSinglestepScheduler\n",
        "      s = DPMSolverSinglestepScheduler.from_config(p.scheduler.config if from_scheduler else p.config)\n",
        "    elif scheduler_mode == \"DPM Stochastic\":\n",
        "      from diffusers import DPMSolverSDEScheduler\n",
        "      s = DPMSolverSDEScheduler.from_config(p.scheduler.config if from_scheduler else p.config)\n",
        "    elif scheduler_mode == \"SDE-DPM Solver++\":\n",
        "      from diffusers import DPMSolverMultistepScheduler #\"hf-internal-testing/tiny-stable-diffusion-torch\"\n",
        "      s = DPMSolverMultistepScheduler.from_config(p.scheduler.config if from_scheduler else p.config)\n",
        "      s.config.algorithm_type = 'sde-dpmsolver++'\n",
        "    elif scheduler_mode == \"K-Euler Discrete\":\n",
        "      from diffusers import EulerDiscreteScheduler\n",
        "      s = EulerDiscreteScheduler.from_config(p.scheduler.config if from_scheduler else p.config)\n",
        "    elif scheduler_mode == \"K-Euler Ancestral\":\n",
        "      from diffusers import EulerAncestralDiscreteScheduler\n",
        "      s = EulerAncestralDiscreteScheduler.from_config(p.scheduler.config if from_scheduler else p.config)\n",
        "    elif scheduler_mode == \"Karras-LMS\":\n",
        "      from diffusers import LMSDiscreteScheduler\n",
        "      s = LMSDiscreteScheduler.from_config(p.scheduler.config if from_scheduler else p.config)\n",
        "      scheduler_config = s.get_scheduler_config()\n",
        "      s = LMSDiscreteScheduler(**scheduler_config, use_karras_sigmas=True)\n",
        "    elif scheduler_mode == \"DPM Solver++\":\n",
        "      from diffusers import DPMSolverMultistepScheduler\n",
        "      s = DPMSolverMultistepScheduler.from_config(p.scheduler.config if from_scheduler else p.config,\n",
        "        beta_start=0.00085,\n",
        "        beta_end=0.012,\n",
        "        beta_schedule=\"scaled_linear\",\n",
        "        num_train_timesteps=1000,\n",
        "        trained_betas=None,\n",
        "        #predict_epsilon=True,\n",
        "        prediction_type=\"v_prediction\" if p.model.startswith('stabilityai') else \"epsilon\",\n",
        "        thresholding=False,\n",
        "        algorithm_type=\"dpmsolver++\",\n",
        "        solver_type=\"midpoint\",\n",
        "        solver_order=2,\n",
        "        #denoise_final=True,\n",
        "        lower_order_final=True,\n",
        "      )\n",
        "    elif scheduler_mode == \"Heun Discrete\":\n",
        "      from diffusers import HeunDiscreteScheduler\n",
        "      s = HeunDiscreteScheduler.from_config(p.scheduler.config if from_scheduler else p.config)\n",
        "    elif scheduler_mode == \"Karras Heun Discrete\":\n",
        "      from diffusers import HeunDiscreteScheduler\n",
        "      s = HeunDiscreteScheduler.from_config(p.scheduler.config if from_scheduler else p.config, use_karras_sigmas=True)\n",
        "    elif scheduler_mode == \"K-DPM2 Ancestral\":\n",
        "      from diffusers import KDPM2AncestralDiscreteScheduler\n",
        "      s = KDPM2AncestralDiscreteScheduler.from_config(p.scheduler.config if from_scheduler else p.config)\n",
        "    elif scheduler_mode == \"K-DPM2 Discrete\":\n",
        "      from diffusers import KDPM2DiscreteScheduler\n",
        "      s = KDPM2DiscreteScheduler.from_config(p.scheduler.config if from_scheduler else p.config)\n",
        "    elif scheduler_mode == \"IPNDM\":\n",
        "      from diffusers import IPNDMScheduler\n",
        "      s = IPNDMScheduler.from_config(p.scheduler.config if from_scheduler else p.config)\n",
        "    elif scheduler_mode == \"DEIS Multistep\":\n",
        "      from diffusers import DEISMultistepScheduler\n",
        "      s = DEISMultistepScheduler.from_config(p.scheduler.config if from_scheduler else p.config)\n",
        "    elif scheduler_mode == \"UniPC Multistep\":\n",
        "      from diffusers import UniPCMultistepScheduler\n",
        "      s = UniPCMultistepScheduler.from_config(p.scheduler.config if from_scheduler else p.config)\n",
        "    elif scheduler_mode == \"Score-SDE-Vp\":\n",
        "      from diffusers import ScoreSdeVpScheduler\n",
        "      s = ScoreSdeVpScheduler() #(num_train_timesteps=2000, beta_min=0.1, beta_max=20, sampling_eps=1e-3, tensor_format=\"np\")\n",
        "      use_custom_scheduler = True\n",
        "    elif scheduler_mode == \"Score-SDE-Ve\":\n",
        "      from diffusers import ScoreSdeVeScheduler\n",
        "      s = ScoreSdeVeScheduler() #(num_train_timesteps=2000, snr=0.15, sigma_min=0.01, sigma_max=1348, sampling_eps=1e-5, correct_steps=1, tensor_format=\"pt\"\n",
        "      use_custom_scheduler = True\n",
        "    elif scheduler_mode == \"DDPM\":\n",
        "      from diffusers import DDPMScheduler\n",
        "      s = DDPMScheduler(num_train_timesteps=1000, beta_start=0.0001, beta_end=0.02, beta_schedule=\"linear\", trained_betas=None, variance_type=\"fixed_small\", clip_sample=True, tensor_format=\"pt\")\n",
        "      use_custom_scheduler = True\n",
        "    elif scheduler_mode == \"Karras-Ve\":\n",
        "      from diffusers import KarrasVeScheduler\n",
        "      s = KarrasVeScheduler() #(sigma_min=0.02, sigma_max=100, s_noise=1.007, s_churn=80, s_min=0.05, s_max=50, tensor_format=\"pt\")\n",
        "      use_custom_scheduler = True\n",
        "    elif scheduler_mode == \"LMS\": #no more\n",
        "      from diffusers import LMSScheduler\n",
        "      s = LMSScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\")\n",
        "      #(num_train_timesteps=1000, beta_start=0.0001, beta_end=0.02, beta_schedule=\"linear\", trained_betas=None, timestep_values=None, tensor_format=\"pt\")\n",
        "      use_custom_scheduler = True\n",
        "    #print(f\"Loaded Schedueler {scheduler_mode} {type(scheduler)}\")\n",
        "    else:\n",
        "      print(f\"Unknown scheduler request {scheduler_mode} - Using LMS Discrete\")\n",
        "      from diffusers import LMSDiscreteScheduler\n",
        "      s = LMSDiscreteScheduler.from_config(p.scheduler.config)\n",
        "    p.scheduler = s\n",
        "    return p\n",
        "#if is_Colab:\n",
        "#    os.remove(\"/usr/local/lib/python3.8/dist-packages/torch/lib/libcudnn.so.8\")\n",
        "#    download_file(\"https://github.com/Skquark/diffusers/blob/main/utils/libcudnn.so.8?raw=true\", to=\"/usr/local/lib/python3.8/dist-packages/torch/lib/\")\n",
        "torch_device = \"cuda\"\n",
        "try:\n",
        "    import torch\n",
        "except ModuleNotFoundError:\n",
        "    #page.console_msg(\"Installing PyTorch with CUDA 1.17\")\n",
        "    print(\"Installing PyTorch with CUDA 1.18\")\n",
        "    run_sp(\"pip install -U --force-reinstall torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu118\", realtime=False)\n",
        "    #run_sp(\"pip install --pre torch torchvision --index-url https://download.pytorch.org/whl/nightly/cu117\", realtime=False)\n",
        "    try:\n",
        "      import torch\n",
        "    except ModuleNotFoundError:\n",
        "      run_sp(\"pip install -q torch\")\n",
        "      import torch\n",
        "      pass\n",
        "    pass\n",
        "finally:\n",
        "    torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    if torch_device == \"cpu\": print(\"WARNING: CUDA is only available with CPU, so GPU tasks are limited. Can use Stability-API & OpenAI, but not Diffusers...\")\n",
        "\n",
        "import gc\n",
        "#from torch.amp.autocast_mode import autocast\n",
        "from random import random\n",
        "import time\n",
        "\n",
        "pb = ProgressBar(width=420, bar_height=8)\n",
        "total_steps = args['steps']\n",
        "def callback_fn(step: int, timestep: int, latents: torch.FloatTensor) -> None:\n",
        "    callback_fn.has_been_called = True\n",
        "    global total_steps, pb\n",
        "    if total_steps is None: total_steps = timestep\n",
        "    if total_steps == 0: total_steps = len(latents)\n",
        "    multiplier = 1\n",
        "    if prefs['scheduler_mode'].startswith(\"Heun\") or prefs['scheduler_mode'].startswith(\"K-DPM\"):\n",
        "      multiplier = 2\n",
        "    percent = (step +1)/ (total_steps * multiplier)\n",
        "    pb.value = percent\n",
        "    pb.tooltip = f\"[{step +1} / {total_steps * multiplier}] (Timestep: {timestep})\"\n",
        "    #print(f\"step: {step}, total: {total_steps}, latent: {len(latents)}\")\n",
        "    #if step == 0:\n",
        "        #latents = latents.detach().cpu().numpy()\n",
        "        #assert latents.shape == (1, 4, 64, 64)\n",
        "        #latents_slice = latents[0, -3:, -3:, -1]\n",
        "        #expected_slice = np.array([1.8285, 1.2857, -0.1024, 1.2406, -2.3068, 1.0747, -0.0818, -0.6520, -2.9506])\n",
        "        #assert np.abs(latents_slice.flatten() - expected_slice).max() < 1e-3\n",
        "    pb.update()\n",
        "\n",
        "def optimize_pipe(p, vae=False, unet=False, no_cpu=False, vae_tiling=False, to_gpu=True, tome=True, torch_compile=True):\n",
        "    global prefs, status\n",
        "    if prefs['memory_optimization'] == 'Attention Slicing':\n",
        "      #if not model['name'].startswith('Stable Diffusion v2'): #TEMP hack until it updates my git with fix\n",
        "      if prefs['sequential_cpu_offload'] and not no_cpu:\n",
        "        p.enable_attention_slicing(1)\n",
        "      else:\n",
        "        p.enable_attention_slicing()\n",
        "    elif prefs['memory_optimization'] == 'Xformers Mem Efficient Attention' and status['installed_xformers']:\n",
        "      #p.set_use_memory_efficient_attention_xformers(True)\n",
        "      p.enable_xformers_memory_efficient_attention()\n",
        "    elif prefs['memory_optimization'] == 'Xformers Mem Efficient Attention':\n",
        "      p.enable_attention_slicing()\n",
        "    if prefs['vae_slicing'] and vae:\n",
        "      p.enable_vae_slicing()\n",
        "    if prefs['vae_tiling'] and vae_tiling:\n",
        "      p.enable_vae_tiling()\n",
        "    if unet:\n",
        "      p.unet = torch.compile(p.unet)\n",
        "    if prefs['use_LoRA_model']:\n",
        "      lora = get_LoRA_model(prefs['LoRA_model'])\n",
        "      p.unet.load_attn_procs(lora['path'])\n",
        "    if prefs['sequential_cpu_offload'] and not no_cpu:\n",
        "      p.enable_sequential_cpu_offload()\n",
        "    else:\n",
        "      if to_gpu:\n",
        "        p.to(torch_device)\n",
        "    if prefs['enable_torch_compile'] and torch_compile:\n",
        "      p.unet.to(memory_format=torch.channels_last)\n",
        "      p.unet = torch.compile(p.unet, mode=\"reduce-overhead\", fullgraph=True)\n",
        "    if prefs['enable_tome'] and tome:\n",
        "      try:\n",
        "        import tomesd\n",
        "      except Exception:\n",
        "        run_sp(\"pip install tomesd\", realtime=False)\n",
        "        import tomesd\n",
        "        pass\n",
        "      tomesd.apply_patch(p, ratio=prefs['tome_ratio'])\n",
        "    status['loaded_scheduler'] = prefs['scheduler_mode']\n",
        "    status['loaded_model'] = get_model(prefs['model_ckpt'])['path']\n",
        "    return p\n",
        "\n",
        "def install_xformers(page):\n",
        "    ''' No longer needed, they finally updated to make it easier'''\n",
        "    run_process(\"pip install -U --pre trito\", page=page)\n",
        "    from subprocess import getoutput\n",
        "\n",
        "    s = getoutput('nvidia-smi')\n",
        "    if 'T4' in s:\n",
        "      gpu = 'T4'\n",
        "    elif 'P100' in s:\n",
        "      gpu = 'P100'\n",
        "    elif 'V100' in s:\n",
        "      gpu = 'V100'\n",
        "    elif 'A100' in s:\n",
        "      gpu = 'A100'\n",
        "    if not (gpu=='T4'or gpu=='P100'or gpu=='V100'or gpu=='A100'):\n",
        "      alert_msg(page, \"Xformers Error: It seems that your GPU is not supported at the moment\")\n",
        "      return False\n",
        "    print(f\"Installing Xformers for {gpu}\")\n",
        "    if (gpu=='T4'):\n",
        "      run_process(\"pip install -q https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/T4/xformers-0.0.13.dev0-py3-none-any.whl\", page=page, show=True)\n",
        "    elif (gpu=='P100'):\n",
        "      run_process(\"pip install -q https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/P100/xformers-0.0.13.dev0-py3-none-any.whl\", page=page)\n",
        "    elif (gpu=='V100'):\n",
        "      run_process(\"pip install -q https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/V100/xformers-0.0.13.dev0-py3-none-any.whl\", page=page)\n",
        "    elif (gpu=='A100'):\n",
        "      run_process(\"pip install -q https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/A100/xformers-0.0.13.dev0-py3-none-any.whl\", page=page)\n",
        "    return True\n",
        "\n",
        "def get_text2image(page):\n",
        "    os.chdir(root_dir)\n",
        "    global pipe, unet, scheduler, prefs, model\n",
        "    def open_url(e):\n",
        "      page.launch_url(e.data)\n",
        "    '''if pipe is not None:\n",
        "        #print(\"Clearing the ol' pipe first...\")\n",
        "        del pipe\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "        pipe = None'''\n",
        "    try:\n",
        "      if use_custom_scheduler: # Not really using anymore, maybe later\n",
        "        from transformers import CLIPTextModel, CLIPTokenizer\n",
        "        from diffusers import AutoencoderKL, UNet2DConditionModel\n",
        "        # 1. Load the autoencoder model which will be used to decode the latents into image space. \n",
        "        vae = AutoencoderKL.from_pretrained(model_path, subfolder=\"vae\", use_auth_token=True)\n",
        "        # 2. Load the tokenizer and text encoder to tokenize and encode the text. \n",
        "        tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "        text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "        if prefs['higher_vram_mode']:\n",
        "          unet = UNet2DConditionModel.from_pretrained(model_path, subfolder=\"unet\", use_auth_token=True, device_map=\"auto\")\n",
        "        else:\n",
        "          unet = UNet2DConditionModel.from_pretrained(model_path, revision=\"fp16\", torch_dtype=torch.float16, subfolder=\"unet\", use_auth_token=True, device_map=\"auto\")\n",
        "        vae = vae.to(torch_device)\n",
        "        text_encoder = text_encoder.to(torch_device)\n",
        "        #if enable_attention_slicing:\n",
        "        #  unet.enable_attention_slicing() #slice_size\n",
        "        unet = unet.to(torch_device)\n",
        "      else:\n",
        "        #if status['finetuned_model']: pipe = get_txt2img_pipe()\n",
        "        #else:\n",
        "        pipe = get_lpw_pipe()\n",
        "    except EnvironmentError as e:\n",
        "      model = get_model(prefs['model_ckpt'])\n",
        "      model_url = f\"https://huggingface.co/{model['path']}\"\n",
        "      alert_msg(page, f'ERROR: Looks like you need to accept the HuggingFace {model[\"name\"]} Model Cards to use Checkpoint',\n",
        "                content=Column([Markdown(f'[{model_url}]({model_url})', on_tap_link=open_url), Text(str(e)), Text(str(traceback.format_exc()).strip(), selectable=True)]))\n",
        "\n",
        "# I thought it's what I wanted, but current implementation does same as mine but doesn't clear memory between\n",
        "def get_mega_pipe():\n",
        "  global pipe, scheduler, model_path, prefs\n",
        "  from diffusers import DiffusionPipeline\n",
        "  from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker\n",
        "  \n",
        "  if prefs['higher_vram_mode']:\n",
        "    pipe = DiffusionPipeline.from_pretrained(model_path, custom_pipeline=\"stable_diffusion_mega\", safety_checker=None if prefs['disable_nsfw_filter'] else StableDiffusionSafetyChecker.from_pretrained(\"CompVis/stable-diffusion-safety-checker\"))\n",
        "    #pipe = StableDiffusionPipeline.from_pretrained(model_path, scheduler=scheduler, safety_checker=None if prefs['disable_nsfw_filter'] else StableDiffusionSafetyChecker.from_pretrained(\"CompVis/stable-diffusion-safety-checker\"))\n",
        "  else:\n",
        "    pipe = DiffusionPipeline.from_pretrained(model_path, custom_pipeline=\"stable_diffusion_mega\", revision=\"fp16\", torch_dtype=torch.float16, safety_checker=None if prefs['disable_nsfw_filter'] else StableDiffusionSafetyChecker.from_pretrained(\"CompVis/stable-diffusion-safety-checker\"))\n",
        "    #pipe = StableDiffusionPipeline.from_pretrained(model_path, scheduler=scheduler, revision=\"fp16\", torch_dtype=torch.float16, safety_checker=None if prefs['disable_nsfw_filter'] else StableDiffusionSafetyChecker.from_pretrained(\"CompVis/stable-diffusion-safety-checker\"))\n",
        "  #pipe = pipe.to(torch_device)\n",
        "  pipe = pipeline_scheduler(pipe)\n",
        "  pipe = optimize_pipe(pipe)\n",
        "  pipe.set_progress_bar_config(disable=True)\n",
        "  return pipe\n",
        "\n",
        "def get_lpw_pipe():\n",
        "  global pipe, scheduler, model_path, prefs\n",
        "  from diffusers import DiffusionPipeline\n",
        "  from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker\n",
        "  from diffusers import AutoencoderKL, UNet2DConditionModel\n",
        "  model = get_model(prefs['model_ckpt'])\n",
        "  os.chdir(root_dir)\n",
        "  #if not os.path.isfile(os.path.join(root_dir, 'lpw_stable_diffusion.py')):\n",
        "  #  run_sp(\"wget -q --show-progress --no-cache --backups=1 https://raw.githubusercontent.com/Skquark/diffusers/main/examples/community/lpw_stable_diffusion.py\")\n",
        "  #from lpw_stable_diffusion import StableDiffusionLongPromptWeightingPipeline\n",
        "  if pipe is not None:\n",
        "    if model['path'] != status['loaded_model']:\n",
        "      #clear_txt2img_pipe()\n",
        "      clear_pipes()\n",
        "    elif prefs['scheduler_mode'] != status['loaded_scheduler']:\n",
        "      pipe = pipeline_scheduler(pipe)\n",
        "      return pipe\n",
        "    else:\n",
        "      return pipe\n",
        "  if 'revision' in model:\n",
        "    pipe = DiffusionPipeline.from_pretrained(model_path, custom_pipeline=\"AlanB/lpw_stable_diffusion_update\", cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, revision=model['revision'], torch_dtype=torch.float16, safety_checker=None if prefs['disable_nsfw_filter'] else StableDiffusionSafetyChecker.from_pretrained(\"CompVis/stable-diffusion-safety-checker\").to(torch_device), requires_safety_checker=not prefs['disable_nsfw_filter'])\n",
        "  else:\n",
        "    if 'vae' in model:\n",
        "      from diffusers import AutoencoderKL, UNet2DConditionModel\n",
        "      vae = AutoencoderKL.from_pretrained(model_path, subfolder=\"vae\", torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32)\n",
        "      unet = UNet2DConditionModel.from_pretrained(model_path, subfolder=\"unet\", torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32)\n",
        "      pipe = DiffusionPipeline.from_pretrained(model_path, custom_pipeline=\"AlanB/lpw_stable_diffusion_update\", vae=vae, unet=unet, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32, safety_checker=None if prefs['disable_nsfw_filter'] else StableDiffusionSafetyChecker.from_pretrained(\"CompVis/stable-diffusion-safety-checker\").to(torch_device), requires_safety_checker=not prefs['disable_nsfw_filter'])\n",
        "    else:\n",
        "      if prefs['disable_nsfw_filter']:\n",
        "        if 'from_ckpt' in model:\n",
        "          pipe = DiffusionPipeline.from_ckpt(model_path, custom_pipeline=\"AlanB/lpw_stable_diffusion_update\", cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32, safety_checker=None, requires_safety_checker=False, feature_extractor=None)\n",
        "        else:  \n",
        "          pipe = DiffusionPipeline.from_pretrained(model_path, custom_pipeline=\"AlanB/lpw_stable_diffusion_update\", cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32, safety_checker=None, requires_safety_checker=False, feature_extractor=None)\n",
        "      else:\n",
        "        if 'from_ckpt' in model:\n",
        "          pipe = DiffusionPipeline.from_ckpt(model_path, custom_pipeline=\"AlanB/lpw_stable_diffusion_update\", cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32, requires_safety_checker=True)\n",
        "        else:\n",
        "          pipe = DiffusionPipeline.from_pretrained(model_path, custom_pipeline=\"AlanB/lpw_stable_diffusion_update\", cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32, requires_safety_checker=True)\n",
        "    #pipe = DiffusionPipeline.from_pretrained(model_path, community=\"lpw_stable_diffusion\", scheduler=scheduler, revision=\"fp16\", torch_dtype=torch.float16, safety_checker=None if prefs['disable_nsfw_filter'] else StableDiffusionSafetyChecker.from_pretrained(\"CompVis/stable-diffusion-safety-checker\"))\n",
        "  #if prefs['enable_attention_slicing']: pipe.enable_attention_slicing()\n",
        "  #pipe = pipe.to(torch_device)\n",
        "  pipe = pipeline_scheduler(pipe)\n",
        "  pipe = optimize_pipe(pipe, vae=True)\n",
        "  pipe.set_progress_bar_config(disable=True)\n",
        "  return pipe\n",
        "\n",
        "def get_txt2img_pipe():\n",
        "  global pipe, scheduler, model_path, prefs, status\n",
        "  from diffusers import StableDiffusionPipeline\n",
        "  from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker\n",
        "  #from diffusers import AutoencoderKL, UNet2DConditionModel\n",
        "  #if status['finetuned_model']:\n",
        "  #  vae = AutoencoderKL.from_pretrained(model_path, subfolder=\"vae\", torch_dtype=torch.float16)\n",
        "  #  unet = UNet2DConditionModel.from_pretrained(model_path, subfolder=\"unet\", torch_dtype=torch.float16)\n",
        "  #pipe = optimize_pipe(pipe, vae=True)\n",
        "  #pipe.set_progress_bar_config(disable=True)\n",
        "  #pipe = pipe.to(torch_device)\n",
        "  pipe = get_lpw_pipe()\n",
        "  return pipe\n",
        "\n",
        "def get_unet_pipe():\n",
        "  global unet, scheduler, model_path, prefs\n",
        "  from transformers import CLIPTextModel, CLIPTokenizer\n",
        "  from diffusers import AutoencoderKL, UNet2DConditionModel\n",
        "  from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker\n",
        "  # 1. Load the autoencoder model which will be used to decode the latents into image space. \n",
        "  vae = AutoencoderKL.from_pretrained(model_path, subfolder=\"vae\")\n",
        "  # 2. Load the tokenizer and text encoder to tokenize and encode the text. \n",
        "  tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "  text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "  if prefs['higher_vram_mode']:\n",
        "    unet = UNet2DConditionModel.from_pretrained(model_path, subfolder=\"unet\", feature_extractor=None, safety_checker=None if prefs['disable_nsfw_filter'] else StableDiffusionSafetyChecker.from_pretrained(\"CompVis/stable-diffusion-safety-checker\"), device_map=\"auto\")\n",
        "  else:\n",
        "    unet = UNet2DConditionModel.from_pretrained(model_path, revision=\"fp16\", feature_extractor=None, torch_dtype=torch.float16, subfolder=\"unet\", safety_checker=None if prefs['disable_nsfw_filter'] else StableDiffusionSafetyChecker.from_pretrained(\"CompVis/stable-diffusion-safety-checker\"), device_map=\"auto\")\n",
        "  vae = vae.to(torch_device)\n",
        "  text_encoder = text_encoder.to(torch_device)\n",
        "  #if enable_attention_slicing:\n",
        "  #  unet.enable_attention_slicing() #slice_size\n",
        "  unet = unet.to(torch_device)\n",
        "  return unet\n",
        "\n",
        "def get_interpolation(page):\n",
        "    from diffusers import DDIMScheduler, PNDMScheduler, LMSDiscreteScheduler\n",
        "    import torch, gc\n",
        "    global pipe_interpolation\n",
        "    torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    if pipe_interpolation is not None:\n",
        "      #print(\"Clearing the ol' pipe first...\")\n",
        "      del pipe_interpolation\n",
        "      gc.collect()\n",
        "      torch.cuda.empty_cache()\n",
        "      pipe_interpolation = None\n",
        "\n",
        "    pipe_interpolation = get_interpolation_pipe()\n",
        "    run_process(\"pip install watchdog -q\", page=page, realtime=False)\n",
        "    status['loaded_interpolation'] = True\n",
        "\n",
        "def get_interpolation_pipe():\n",
        "    global pipe_interpolation, scheduler, model_path, prefs\n",
        "    from diffusers import StableDiffusionPipeline\n",
        "    from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker\n",
        "    os.chdir(root_dir)\n",
        "    if not os.path.isfile(os.path.join(root_dir, 'clip_guided_stable_diffusion.py')):\n",
        "      run_sp(\"wget -q --show-progress --no-cache --backups=1 https://raw.githubusercontent.com/Skquark/diffusers/main/examples/community/interpolate_stable_diffusion.py\")\n",
        "    from interpolate_stable_diffusion import StableDiffusionWalkPipeline\n",
        "    model = get_model(prefs['model_ckpt'])\n",
        "    if pipe_interpolation is not None:\n",
        "      if model['path'] != status['loaded_model']:\n",
        "        clear_interpolation_pipe()\n",
        "      elif prefs['scheduler_mode'] != status['loaded_scheduler']:\n",
        "        pipe_interpolation = pipeline_scheduler(pipe_interpolation)\n",
        "        return pipe_interpolation\n",
        "      else:\n",
        "        return pipe_interpolation\n",
        "    if 'revision' in model:\n",
        "      pipe_interpolation = StableDiffusionWalkPipeline.from_pretrained(model_path, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, revision=model['revision'], torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32, safety_checker=None if prefs['disable_nsfw_filter'] else StableDiffusionSafetyChecker.from_pretrained(\"CompVis/stable-diffusion-safety-checker\"), feature_extractor=None)\n",
        "    else:\n",
        "      pipe_interpolation = StableDiffusionWalkPipeline.from_pretrained(model_path, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32, safety_checker=None if prefs['disable_nsfw_filter'] else StableDiffusionSafetyChecker.from_pretrained(\"CompVis/stable-diffusion-safety-checker\"), feature_extractor=None)\n",
        "    #pipe = StableDiffusionPipeline.from_pretrained(model_path, scheduler=scheduler, revision=\"fp16\", torch_dtype=torch.float16, safety_checker=None if prefs['disable_nsfw_filter'] else StableDiffusionSafetyChecker.from_pretrained(\"CompVis/stable-diffusion-safety-checker\"))\n",
        "    #pipe_interpolation = pipe_interpolation.to(torch_device)\n",
        "    pipe_interpolation = pipeline_scheduler(pipe_interpolation)\n",
        "    pipe_interpolation = optimize_pipe(pipe_interpolation)\n",
        "    pipe_interpolation.set_progress_bar_config(disable=True)\n",
        "    return pipe_interpolation\n",
        "\n",
        "def get_image2image(page):\n",
        "    from diffusers import StableDiffusionInpaintPipeline, DDIMScheduler, PNDMScheduler, LMSDiscreteScheduler\n",
        "    import torch, gc\n",
        "    global pipe_img2img\n",
        "    def open_url(e):\n",
        "      page.launch_url(e.data)\n",
        "    torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    if pipe_img2img is not None:\n",
        "      if model['path'] != status['loaded_model']:\n",
        "        clear_img2img_pipe()\n",
        "      elif prefs['scheduler_mode'] != status['loaded_scheduler']:\n",
        "        pipe_img2img = pipeline_scheduler(pipe_img2img)\n",
        "        return pipe_img2img\n",
        "      else:\n",
        "        return pipe_img2img\n",
        "    try:\n",
        "      pipe_img2img = get_img2img_pipe()\n",
        "    except EnvironmentError:\n",
        "      model_url = f\"https://huggingface.co/{inpaint_model}\"\n",
        "      alert_msg(page, f'ERROR: Looks like you need to accept the HuggingFace Inpainting Model Card to use Checkpoint',\n",
        "                content=Markdown(f'[{model_url}]({model_url})', on_tap_link=open_url))\n",
        "    loaded_img2img = True\n",
        "\n",
        "def get_img2img_pipe():\n",
        "  global pipe_img2img, scheduler, model_path, inpaint_model, prefs, callback_fn\n",
        "  from diffusers import DiffusionPipeline\n",
        "  from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker\n",
        "  if pipe_img2img is not None:\n",
        "      if prefs['scheduler_mode'] != status['loaded_scheduler']:\n",
        "        pipe_img2img = pipeline_scheduler(pipe_img2img)\n",
        "        return pipe_img2img\n",
        "      else:\n",
        "        return pipe_img2img\n",
        "  if prefs['higher_vram_mode']:\n",
        "    pipe_img2img = DiffusionPipeline.from_pretrained(\n",
        "        inpaint_model,\n",
        "        custom_pipeline=\"img2img_inpainting\",\n",
        "        #scheduler=model_scheduler(inpaint_model),\n",
        "        cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,\n",
        "        safety_checker=None if prefs['disable_nsfw_filter'] else StableDiffusionSafetyChecker.from_pretrained(\"CompVis/stable-diffusion-safety-checker\"), feature_extractor=None\n",
        "    )\n",
        "  else:\n",
        "      pipe_img2img = DiffusionPipeline.from_pretrained(\n",
        "      inpaint_model,\n",
        "      custom_pipeline=\"img2img_inpainting\",\n",
        "      #scheduler=model_scheduler(inpaint_model),\n",
        "      cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,\n",
        "      revision=\"fp16\", \n",
        "      torch_dtype=torch.float16,\n",
        "      safety_checker=None if prefs['disable_nsfw_filter'] else StableDiffusionSafetyChecker.from_pretrained(\"CompVis/stable-diffusion-safety-checker\"), feature_extractor=None)\n",
        "  #pipe_img2img.to(torch_device)\n",
        "  #if prefs['enable_attention_slicing']: pipe_img2img.enable_attention_slicing() #slice_size\n",
        "  pipe_img2img = pipeline_scheduler(pipe_img2img)\n",
        "  pipe_img2img = optimize_pipe(pipe_img2img, vae=True)\n",
        "  pipe_img2img.set_progress_bar_config(disable=True)\n",
        "  #def dummy(images, **kwargs): return images, False\n",
        "  #pipe_img2img.safety_checker = dummy\n",
        "  return pipe_img2img\n",
        "\n",
        "def get_imagic(page):\n",
        "    global pipe_imagic\n",
        "    pipe_imagic = get_imagic_pipe()\n",
        "\n",
        "def get_imagic_pipe():\n",
        "  global pipe_imagic, scheduler, model_path, prefs\n",
        "  from diffusers import DiffusionPipeline#, DDIMScheduler\n",
        "  from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker\n",
        "  #ddim = DDIMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", clip_sample=False, set_alpha_to_one=False)\n",
        "  #if prefs['higher_vram_mode']:\n",
        "  if pipe_imagic is not None:\n",
        "      if model_path != status['loaded_model']:\n",
        "        clear_imagic_pipe()\n",
        "      elif prefs['scheduler_mode'] != status['loaded_scheduler']:\n",
        "        pipe_imagic = pipeline_scheduler(pipe_imagic)\n",
        "        return pipe_imagic\n",
        "      else:\n",
        "        return pipe_imagic\n",
        "  if True:\n",
        "    pipe_imagic = DiffusionPipeline.from_pretrained(model_path, custom_pipeline=\"AlanB/imagic_stable_diffusion_mod\", use_auth_token=True, safety_checker=None if prefs['disable_nsfw_filter'] else StableDiffusionSafetyChecker.from_pretrained(\"CompVis/stable-diffusion-safety-checker\"), feature_extractor=None)\n",
        "  else:\n",
        "    pipe_imagic = DiffusionPipeline.from_pretrained(model_path, custom_pipeline=\"AlanB/imagic_stable_diffusion_mod\", revision=\"fp16\", torch_dtype=torch.float16, safety_checker=None if prefs['disable_nsfw_filter'] else StableDiffusionSafetyChecker.from_pretrained(\"CompVis/stable-diffusion-safety-checker\"), feature_extractor=None)\n",
        "  #pipe_imagic = pipe_imagic.to(torch_device)\n",
        "  def dummy(images, **kwargs):\n",
        "    return images, False\n",
        "  if prefs['disable_nsfw_filter']:\n",
        "    pipe_imagic.safety_checker = dummy\n",
        "  pipe_imagic = pipeline_scheduler(pipe_imagic, big3=True)\n",
        "  pipe_imagic = optimize_pipe(pipe_imagic)\n",
        "  #pipe_imagic.set_progress_bar_config(disable=True)\n",
        "  return pipe_imagic\n",
        "\n",
        "def get_composable(page):\n",
        "    global pipe_composable\n",
        "    pipe_composable = get_composable_pipe()\n",
        "\n",
        "def get_composable_pipe():\n",
        "  global pipe_composable, scheduler, model_path, prefs\n",
        "  from diffusers import DiffusionPipeline\n",
        "  from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker\n",
        "  if pipe_composable is not None:\n",
        "      if model_path != status['loaded_model']:\n",
        "        clear_composable_pipe()\n",
        "      elif prefs['scheduler_mode'] != status['loaded_scheduler']:\n",
        "        pipe_composable = pipeline_scheduler(pipe_composable, big3=True)\n",
        "        return pipe_composable\n",
        "      else:\n",
        "        return pipe_composable\n",
        "  #if prefs['higher_vram_mode']:\n",
        "  if True:\n",
        "    pipe_composable = DiffusionPipeline.from_pretrained(model_path, custom_pipeline=\"AlanB/composable_stable_diffusion_mod\", use_auth_token=True, feature_extractor=None, safety_checker=None)\n",
        "  else:\n",
        "    pipe_composable = DiffusionPipeline.from_pretrained(model_path, custom_pipeline=\"AlanB/composable_stable_diffusion_mod\", revision=\"fp16\", torch_dtype=torch.float16, feature_extractor=None, safety_checker=None)\n",
        "  #pipe_composable = pipe_composable.to(torch_device)\n",
        "  def dummy(images, **kwargs):\n",
        "    return images, False\n",
        "  if prefs['disable_nsfw_filter']:\n",
        "    pipe_composable.safety_checker = dummy\n",
        "  pipe_composable = pipeline_scheduler(pipe_composable, big3=True)\n",
        "  pipe_composable = optimize_pipe(pipe_composable)\n",
        "  #pipe_composable.set_progress_bar_config(disable=True)\n",
        "  return pipe_composable\n",
        "\n",
        "def get_versatile(page):\n",
        "    import torch, gc\n",
        "    global pipe_versatile_text2img\n",
        "    def open_url(e):\n",
        "      page.launch_url(e.data)\n",
        "    try:\n",
        "      pipe_versatile_text2img = get_versatile_text2img_pipe()\n",
        "    except Exception as er:\n",
        "      model_url = f\"https://huggingface.co/shi-labs/versatile-diffusion\"\n",
        "      alert_msg(page, f'ERROR: Looks like you need to accept the HuggingFace Versatile Diffusion Model Card to use Checkpoint',\n",
        "                content=Markdown(f'[{model_url}]({model_url})<br>{er}', on_tap_link=open_url))\n",
        "\n",
        "def get_versatile_pipe(): # Mega was taking up too much vram and crashing the system\n",
        "  global pipe_versatile, scheduler, model_path, prefs\n",
        "  from diffusers import VersatileDiffusionPipeline\n",
        "  from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker\n",
        "  model_id = \"shi-labs/versatile-diffusion\"\n",
        "  if pipe_composable is not None:\n",
        "      if prefs['scheduler_mode'] != status['loaded_scheduler']:\n",
        "        pipe_composable = pipeline_scheduler(pipe_composable)\n",
        "        return pipe_composable\n",
        "      else:\n",
        "        return pipe_composable\n",
        "  pipe_versatile = VersatileDiffusionPipeline.from_pretrained(\n",
        "      model_id,\n",
        "      #scheduler=model_scheduler(model_id),\n",
        "      cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,\n",
        "      #revision=\"fp16\", \n",
        "      torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32,\n",
        "      safety_checker=None if prefs['disable_nsfw_filter'] else StableDiffusionSafetyChecker.from_pretrained(\"CompVis/stable-diffusion-safety-checker\"), feature_extractor=None\n",
        "  )\n",
        "  #pipe_versatile.to(torch_device)\n",
        "  pipe_versatile = pipeline_scheduler(pipe_versatile)\n",
        "  pipe_versatile = optimize_pipe(pipe_versatile)\n",
        "  pipe_versatile.set_progress_bar_config(disable=True)\n",
        "  return pipe_versatile\n",
        "\n",
        "def get_versatile_text2img_pipe():\n",
        "  global pipe_versatile_text2img, scheduler, model_path, prefs\n",
        "  from diffusers import VersatileDiffusionTextToImagePipeline\n",
        "  from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker\n",
        "  model_id = \"shi-labs/versatile-diffusion\"\n",
        "  if pipe_versatile_text2img is not None:\n",
        "      if prefs['scheduler_mode'] != status['loaded_scheduler']:\n",
        "        pipe_versatile_text2img = pipeline_scheduler(pipe_versatile_text2img)\n",
        "        return pipe_versatile_text2img\n",
        "      else:\n",
        "        return pipe_versatile_text2img\n",
        "  pipe_versatile_text2img = VersatileDiffusionTextToImagePipeline.from_pretrained(\n",
        "      model_id,\n",
        "      #scheduler=model_scheduler(model_id),\n",
        "      cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,\n",
        "      #revision=\"fp16\", \n",
        "      torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32,\n",
        "      safety_checker=None if prefs['disable_nsfw_filter'] else StableDiffusionSafetyChecker.from_pretrained(\"CompVis/stable-diffusion-safety-checker\"), feature_extractor=None\n",
        "  )\n",
        "  #pipe_versatile_text2img.to(torch_device)\n",
        "  pipe_versatile_text2img = pipeline_scheduler(pipe_versatile_text2img)\n",
        "  pipe_versatile_text2img = optimize_pipe(pipe_versatile_text2img)\n",
        "  pipe_versatile_text2img.set_progress_bar_config(disable=True)\n",
        "  return pipe_versatile_text2img\n",
        "\n",
        "def get_versatile_variation_pipe():\n",
        "  global pipe_versatile_variation, scheduler, model_path, prefs\n",
        "  from diffusers import VersatileDiffusionImageVariationPipeline\n",
        "  from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker\n",
        "  model_id = \"shi-labs/versatile-diffusion\"\n",
        "  if pipe_versatile_variation is not None:\n",
        "      if prefs['scheduler_mode'] != status['loaded_scheduler']:\n",
        "        pipe_versatile_variation = pipeline_scheduler(pipe_versatile_variation)\n",
        "        return pipe_versatile_variation\n",
        "      else:\n",
        "        return pipe_versatile_variation\n",
        "  if prefs['higher_vram_mode']:\n",
        "    pipe_versatile_variation = VersatileDiffusionImageVariationPipeline.from_pretrained(\n",
        "        model_id,\n",
        "        #scheduler=model_scheduler(model_id),\n",
        "        cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,\n",
        "        safety_checker=None if prefs['disable_nsfw_filter'] else StableDiffusionSafetyChecker.from_pretrained(\"CompVis/stable-diffusion-safety-checker\"), feature_extractor=None\n",
        "    )\n",
        "  else:\n",
        "    pipe_versatile_variation = VersatileDiffusionImageVariationPipeline.from_pretrained(\n",
        "        model_id,\n",
        "        #scheduler=model_scheduler(model_id),\n",
        "        cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,\n",
        "        #revision=\"fp16\", \n",
        "        torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32,\n",
        "        safety_checker=None if prefs['disable_nsfw_filter'] else StableDiffusionSafetyChecker.from_pretrained(\"CompVis/stable-diffusion-safety-checker\"), feature_extractor=None\n",
        "    )\n",
        "  #pipe_versatile_variation.to(torch_device)\n",
        "  pipe_versatile_variation = pipeline_scheduler(pipe_versatile_variation)\n",
        "  pipe_versatile_variation = optimize_pipe(pipe_versatile_variation)\n",
        "  pipe_versatile_variation.set_progress_bar_config(disable=True)\n",
        "  return pipe_versatile_variation\n",
        "\n",
        "def get_versatile_dualguided_pipe():\n",
        "  global pipe_versatile_dualguided, scheduler, model_path, prefs\n",
        "  from diffusers import VersatileDiffusionDualGuidedPipeline\n",
        "  from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker\n",
        "  model_id = \"shi-labs/versatile-diffusion\"\n",
        "  if pipe_versatile_dualguided is not None:\n",
        "      if prefs['scheduler_mode'] != status['loaded_scheduler']:\n",
        "        pipe_versatile_dualguided = pipeline_scheduler(pipe_versatile_dualguided)\n",
        "        return pipe_versatile_dualguided\n",
        "      else:\n",
        "        return pipe_versatile_dualguided\n",
        "  if prefs['higher_vram_mode']:\n",
        "    pipe_versatile_dualguided = VersatileDiffusionDualGuidedPipeline.from_pretrained(\n",
        "        model_id,\n",
        "        #scheduler=model_scheduler(model_id),\n",
        "        cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,\n",
        "        safety_checker=None if prefs['disable_nsfw_filter'] else StableDiffusionSafetyChecker.from_pretrained(\"CompVis/stable-diffusion-safety-checker\"), feature_extractor=None\n",
        "    )\n",
        "  else:\n",
        "    pipe_versatile_dualguided = VersatileDiffusionDualGuidedPipeline.from_pretrained(\n",
        "        model_id,\n",
        "        #scheduler=model_scheduler(model_id),\n",
        "        cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,\n",
        "        #revision=\"fp16\", \n",
        "        torch_dtype=torch.float16,\n",
        "        safety_checker=None if prefs['disable_nsfw_filter'] else StableDiffusionSafetyChecker.from_pretrained(\"CompVis/stable-diffusion-safety-checker\"), feature_extractor=None\n",
        "    )\n",
        "  #pipe_versatile_dualguided.to(torch_device)\n",
        "  pipe_versatile_dualguided = pipeline_scheduler(pipe_versatile_dualguided)\n",
        "  pipe_versatile_dualguided = optimize_pipe(pipe_versatile_dualguided)\n",
        "  pipe_versatile_dualguided.set_progress_bar_config(disable=True)\n",
        "  return pipe_versatile_dualguided\n",
        "\n",
        "def get_safe(page):\n",
        "    import torch, gc\n",
        "    global pipe_safe\n",
        "    def open_url(e):\n",
        "      page.launch_url(e.data)\n",
        "    try:\n",
        "      pipe_safe = get_safe_pipe()\n",
        "    except Exception as er:\n",
        "      model_url = f\"https://huggingface.co/AIML-TUDA/stable-diffusion-safe\"\n",
        "      alert_msg(page, f'ERROR: Looks like you need to accept the HuggingFace Safe Model Card to use Checkpoint. Reinstall after accepting TOS.',\n",
        "                content=Markdown(f'[{model_url}]({model_url})<br>{er}', on_tap_link=open_url))\n",
        "\n",
        "def get_safe_pipe():\n",
        "  global pipe_safe, scheduler, model_path, prefs, callback_fn\n",
        "  from diffusers import StableDiffusionPipelineSafe\n",
        "  from diffusers.pipelines.stable_diffusion_safe import StableDiffusionPipelineSafe\n",
        "  #from diffusers.pipelines.safety_checker import SafeStableDiffusionPipelineSafe\n",
        "  #from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker\n",
        "  model_id = \"AIML-TUDA/stable-diffusion-safe\"\n",
        "  if pipe_safe is not None:\n",
        "      if prefs['scheduler_mode'] != status['loaded_scheduler']:\n",
        "        pipe_safe = pipeline_scheduler(pipe_safe)\n",
        "        return pipe_safe\n",
        "      else:\n",
        "        return pipe_safe\n",
        "  #if prefs['higher_vram_mode']:\n",
        "  if True:\n",
        "    pipe_safe = StableDiffusionPipelineSafe.from_pretrained(\n",
        "        model_id,\n",
        "        #scheduler=model_scheduler(model_id),\n",
        "        cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,\n",
        "        safety_checker=None# if prefs['disable_nsfw_filter'] else SafeStableDiffusionSafetyChecker.from_pretrained(\"CompVis/stable-diffusion-safety-checker\"),\n",
        "    )\n",
        "  else:\n",
        "      pipe_safe = StableDiffusionPipelineSafe.from_pretrained(\n",
        "        model_id,\n",
        "        #scheduler=model_scheduler(model_id),\n",
        "        cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,\n",
        "        revision=\"fp16\", \n",
        "        torch_dtype=torch.float16,\n",
        "        safety_checker=None# if prefs['disable_nsfw_filter'] else SafeStableDiffusionSafetyChecker.from_pretrained(\"CompVis/stable-diffusion-safety-checker\")\n",
        "      )\n",
        "  #pipe_safe.to(torch_device)\n",
        "  pipe_safe = pipeline_scheduler(pipe_safe)\n",
        "  pipe_safe = optimize_pipe(pipe_safe)\n",
        "  pipe_safe.set_progress_bar_config(disable=True)\n",
        "  return pipe_safe\n",
        "\n",
        "def get_SAG(page):\n",
        "  global pipe_SAG\n",
        "  #clear_SAG_pipe()\n",
        "  pipe_SAG = get_SAG_pipe()\n",
        "\n",
        "def get_SAG_pipe():\n",
        "  global pipe_SAG, scheduler, model_path, prefs\n",
        "  from diffusers import StableDiffusionSAGPipeline\n",
        "  from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker\n",
        "  if pipe_SAG is not None:\n",
        "      if model_path != status['loaded_model']:\n",
        "        clear_SAG_pipe()\n",
        "      elif prefs['scheduler_mode'] != status['loaded_scheduler']:\n",
        "        pipe_SAG = pipeline_scheduler(pipe_SAG, big3=True)\n",
        "        return pipe_SAG\n",
        "      else:\n",
        "        return pipe_SAG\n",
        "  if prefs['higher_vram_mode']:\n",
        "    pipe_SAG = StableDiffusionSAGPipeline.from_pretrained(\n",
        "        model_path,\n",
        "        #scheduler=model_scheduler(model_path, big3=True),\n",
        "        cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,\n",
        "        safety_checker=None if prefs['disable_nsfw_filter'] else StableDiffusionSafetyChecker.from_pretrained(\"CompVis/stable-diffusion-safety-checker\"), feature_extractor=None\n",
        "    )\n",
        "  else:\n",
        "      pipe_SAG = StableDiffusionSAGPipeline.from_pretrained(\n",
        "      model_path,\n",
        "      #scheduler=model_scheduler(model_path, big3=True),\n",
        "      cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,\n",
        "      torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32,\n",
        "      safety_checker=None if prefs['disable_nsfw_filter'] else StableDiffusionSafetyChecker.from_pretrained(\"CompVis/stable-diffusion-safety-checker\"), feature_extractor=None)\n",
        "  pipe_SAG = pipeline_scheduler(pipe_SAG, big3=True)\n",
        "  pipe_SAG = optimize_pipe(pipe_SAG, vae=False)\n",
        "  pipe_SAG.set_progress_bar_config(disable=True)\n",
        "  return pipe_SAG\n",
        "\n",
        "def get_attend_and_excite(page):\n",
        "  global pipe_attend_and_excite\n",
        "  #clear_attend_and_excite_pipe()\n",
        "  pipe_attend_and_excite = get_attend_and_excite_pipe()\n",
        "\n",
        "def get_attend_and_excite_pipe():\n",
        "  global pipe_attend_and_excite, scheduler, model_path, prefs\n",
        "  from diffusers import StableDiffusionAttendAndExcitePipeline\n",
        "  from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker\n",
        "  if pipe_attend_and_excite is not None:\n",
        "      if model_path != status['loaded_model']:\n",
        "        clear_attend_and_excite_pipe()\n",
        "      elif prefs['scheduler_mode'] != status['loaded_scheduler']:\n",
        "        pipe_attend_and_excite = pipeline_scheduler(pipe_attend_and_excite)\n",
        "        return pipe_attend_and_excite\n",
        "      else:\n",
        "        return pipe_attend_and_excite\n",
        "  if prefs['higher_vram_mode']:\n",
        "    pipe_attend_and_excite = StableDiffusionAttendAndExcitePipeline.from_pretrained(\n",
        "        model_path,\n",
        "        #scheduler=model_scheduler(model_path),\n",
        "        cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,\n",
        "        safety_checker=None if prefs['disable_nsfw_filter'] else StableDiffusionSafetyChecker.from_pretrained(\"CompVis/stable-diffusion-safety-checker\")\n",
        "    )\n",
        "  else:\n",
        "      pipe_attend_and_excite = StableDiffusionAttendAndExcitePipeline.from_pretrained(\n",
        "      model_path,\n",
        "      #scheduler=model_scheduler(model_path),\n",
        "      cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,\n",
        "      torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32,\n",
        "      safety_checker=None if prefs['disable_nsfw_filter'] else StableDiffusionSafetyChecker.from_pretrained(\"CompVis/stable-diffusion-safety-checker\"))\n",
        "  pipe_attend_and_excite = pipeline_scheduler(pipe_attend_and_excite)\n",
        "  pipe_attend_and_excite = optimize_pipe(pipe_attend_and_excite, vae=True)\n",
        "  pipe_attend_and_excite.set_progress_bar_config(disable=True)\n",
        "  return pipe_attend_and_excite\n",
        "\n",
        "def get_panorama(page):\n",
        "  global pipe_panorama\n",
        "  #clear_panorama_pipe()\n",
        "  pipe_panorama = get_panorama_pipe()\n",
        "\n",
        "def get_panorama_pipe():\n",
        "  global pipe_panorama, scheduler, model_path, prefs\n",
        "  from diffusers import StableDiffusionPanoramaPipeline\n",
        "  from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker\n",
        "  if pipe_panorama is not None:\n",
        "      if model_path != status['loaded_model']:\n",
        "        clear_panorama_pipe()\n",
        "      elif prefs['scheduler_mode'] != status['loaded_scheduler']:\n",
        "        pipe_panorama = pipeline_scheduler(pipe_panorama)\n",
        "        return pipe_panorama\n",
        "      else:\n",
        "        return pipe_panorama\n",
        "  if prefs['higher_vram_mode']:\n",
        "    pipe_panorama = StableDiffusionPanoramaPipeline.from_pretrained(\n",
        "        model_path,\n",
        "        #scheduler=model_scheduler(model_path, big3=True),\n",
        "        cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,\n",
        "        safety_checker=None if prefs['disable_nsfw_filter'] else StableDiffusionSafetyChecker.from_pretrained(\"CompVis/stable-diffusion-safety-checker\")\n",
        "    )\n",
        "  else:\n",
        "      pipe_panorama = StableDiffusionPanoramaPipeline.from_pretrained(\n",
        "      model_path,\n",
        "      #scheduler=model_scheduler(model_path, big3=True),\n",
        "      cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,\n",
        "      torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32,\n",
        "      safety_checker=None if prefs['disable_nsfw_filter'] else StableDiffusionSafetyChecker.from_pretrained(\"CompVis/stable-diffusion-safety-checker\"))\n",
        "  pipe_panorama = pipeline_scheduler(pipe_panorama, big3=True)\n",
        "  pipe_panorama = optimize_pipe(pipe_panorama, vae=True)\n",
        "  pipe_panorama.set_progress_bar_config(disable=True)\n",
        "  return pipe_panorama\n",
        "\n",
        "def get_upscale(page):\n",
        "    import torch, gc\n",
        "    global pipe_upscale\n",
        "    def open_url(e):\n",
        "      page.launch_url(e.data)\n",
        "    if pipe_upscale is None:\n",
        "      try:\n",
        "        pipe_upscale = get_upscale_pipe()\n",
        "      except Exception as er:\n",
        "        model_url = f\"https://huggingface.co/{model_path}\"\n",
        "        alert_msg(page, f'ERROR: Looks like you need to accept the HuggingFace Upscale Model Card to use Checkpoint',\n",
        "                  content=Markdown(f'[{model_url}]({model_url})<br>{er}', on_tap_link=open_url))\n",
        "\n",
        "def get_upscale_pipe():\n",
        "  global pipe_upscale, scheduler, prefs\n",
        "  from diffusers import StableDiffusionUpscalePipeline\n",
        "  model_id = \"stabilityai/stable-diffusion-x4-upscaler\"\n",
        "  if pipe_upscale is not None:\n",
        "      if prefs['scheduler_mode'] != status['loaded_scheduler']:\n",
        "        pipe_upscale = pipeline_scheduler(pipe_upscale, big3=True)\n",
        "        return pipe_upscale\n",
        "      else:\n",
        "        return pipe_upscale\n",
        "  if prefs['higher_vram_mode']:\n",
        "    pipe_upscale = StableDiffusionUpscalePipeline.from_pretrained(\n",
        "        model_id,\n",
        "        #scheduler=model_scheduler(model_id, big3=True),\n",
        "        cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,\n",
        "        #safety_checker=None if prefs['disable_nsfw_filter'] else StableDiffusionSafetyChecker.from_pretrained(\"CompVis/stable-diffusion-safety-checker\"),\n",
        "    )\n",
        "  else:\n",
        "    pipe_upscale = StableDiffusionUpscalePipeline.from_pretrained(\n",
        "      model_id,\n",
        "      #scheduler=model_scheduler(model_id, big3=True),\n",
        "      cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,\n",
        "      revision=\"fp16\", \n",
        "      torch_dtype=torch.float16,\n",
        "      #safety_checker=None if prefs['disable_nsfw_filter'] else StableDiffusionSafetyChecker.from_pretrained(\"CompVis/stable-diffusion-safety-checker\")\n",
        "    )\n",
        "  #pipe_upscale.to(torch_device)\n",
        "  pipe_upscale = pipeline_scheduler(pipe_upscale, big3=True)\n",
        "  pipe_upscale = optimize_pipe(pipe_upscale)\n",
        "  pipe_upscale.set_progress_bar_config(disable=True)\n",
        "  return pipe_upscale\n",
        "\n",
        "def get_clip(page):\n",
        "    global pipe_clip_guided, model_path\n",
        "    pipe_clip_guided = get_clip_guided_pipe()\n",
        "\n",
        "def get_clip_guided_pipe():\n",
        "    global pipe_clip_guided, scheduler_clip, prefs, model_path\n",
        "    from diffusers import DiffusionPipeline\n",
        "    from diffusers import LMSDiscreteScheduler, PNDMScheduler, StableDiffusionPipeline\n",
        "    from transformers import CLIPModel, CLIPFeatureExtractor #, CLIPGuidedStableDiffusion\n",
        "    if pipe_clip_guided is not None:\n",
        "      if model_path != status['loaded_model']:\n",
        "        clear_clip_guided_pipe()\n",
        "      elif prefs['scheduler_mode'] != status['loaded_scheduler']:\n",
        "        pipe_clip_guided = pipeline_scheduler(pipe_clip_guided, big3=True)\n",
        "        return pipe_clip_guided\n",
        "      else:\n",
        "        return pipe_clip_guided\n",
        "    #if isinstance(scheduler, LMSDiscreteScheduler) or isinstance(scheduler, PNDMScheduler):\n",
        "    #  scheduler_clip = scheduler\n",
        "    #else:\n",
        "    #  scheduler_clip = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\")\n",
        "    model = get_model(prefs['model_ckpt'])\n",
        "\n",
        "    clip_model = CLIPModel.from_pretrained(prefs['clip_model_id'], torch_dtype=torch.float16)\n",
        "    feature_extractor = CLIPFeatureExtractor.from_pretrained(prefs['clip_model_id'])\n",
        "\n",
        "    if 'revision' in model:\n",
        "      pipe_clip_guided = DiffusionPipeline.from_pretrained(\n",
        "              model_path,\n",
        "              custom_pipeline=\"AlanB/clip_guided_stable_diffusion_mod\",\n",
        "              clip_model=clip_model,\n",
        "              feature_extractor=feature_extractor,\n",
        "              #scheduler=model_scheduler(model_path, big3=True),\n",
        "              cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,\n",
        "              safety_checker=None,\n",
        "              torch_dtype=torch.float16,\n",
        "              revision=model['revision'],\n",
        "              #device_map=\"auto\",\n",
        "          )\n",
        "    else:\n",
        "      pipe_clip_guided = DiffusionPipeline.from_pretrained(model_path, custom_pipeline=\"AlanB/clip_guided_stable_diffusion_mod\", clip_model=clip_model, feature_extractor=feature_extractor, safety_checker=None, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, torch_dtype=torch.float16)\n",
        "    #pipe_clip_guided = pipe_clip_guided.to(torch_device)\n",
        "    '''\n",
        "    pipe_clip_guided = CLIPGuidedStableDiffusion(\n",
        "        unet=pipeline.unet,\n",
        "        vae=pipeline.vae,\n",
        "        tokenizer=pipeline.tokenizer,\n",
        "        text_encoder=pipeline.text_encoder,\n",
        "        scheduler=scheduler_clip,\n",
        "        clip_model=clip_model,\n",
        "        feature_extractor=feature_extractor,\n",
        "    )'''\n",
        "    pipe_clip_guided = pipeline_scheduler(pipe_clip_guided, big3=True)\n",
        "    pipe_clip_guided = optimize_pipe(pipe_clip_guided)\n",
        "    return pipe_clip_guided\n",
        "\n",
        "def get_clip_guided_img2img_pipe():\n",
        "    global pipe_clip_guided, scheduler_clip, prefs, model_path\n",
        "    from diffusers import DiffusionPipeline\n",
        "    from diffusers import LMSDiscreteScheduler, PNDMScheduler, StableDiffusionPipeline\n",
        "    from transformers import CLIPModel, CLIPFeatureExtractor #, CLIPGuidedStableDiffusion\n",
        "    if pipe_clip_guided is not None:\n",
        "      if model_path != status['loaded_model']:\n",
        "        clear_clip_guided_pipe()\n",
        "      elif prefs['scheduler_mode'] != status['loaded_scheduler']:\n",
        "        pipe_clip_guided = pipeline_scheduler(pipe_clip_guided, big3=True)\n",
        "        return pipe_clip_guided\n",
        "      else:\n",
        "        return pipe_clip_guided\n",
        "    #if isinstance(scheduler, LMSDiscreteScheduler) or isinstance(scheduler, PNDMScheduler):\n",
        "    #  scheduler_clip = scheduler\n",
        "    #else:\n",
        "    #  scheduler_clip = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\")\n",
        "    model = get_model(prefs['model_ckpt'])\n",
        "\n",
        "    clip_model = CLIPModel.from_pretrained(prefs['clip_model_id'], torch_dtype=torch.float16)\n",
        "    feature_extractor = CLIPFeatureExtractor.from_pretrained(prefs['clip_model_id'])\n",
        "\n",
        "    if 'revision' in model:\n",
        "      pipe_clip_guided = DiffusionPipeline.from_pretrained(\n",
        "              model_path,\n",
        "              custom_pipeline=\"AlanB/clip_guided_stable_diffusion_mod\",\n",
        "              clip_model=clip_model,\n",
        "              feature_extractor=feature_extractor,\n",
        "              #scheduler=model_scheduler(model_path, big3=True),\n",
        "              cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,\n",
        "              safety_checker=None,\n",
        "              torch_dtype=torch.float16,\n",
        "              revision=model['revision'],\n",
        "              #device_map=\"auto\",\n",
        "          )\n",
        "    else:\n",
        "      pipe_clip_guided = DiffusionPipeline.from_pretrained(model_path, custom_pipeline=\"AlanB/clip_guided_stable_diffusion_mod\", clip_model=clip_model, feature_extractor=feature_extractor, safety_checker=None, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, torch_dtype=torch.float16)\n",
        "    #pipe_clip_guided = pipe_clip_guided.to(torch_device)\n",
        "    '''\n",
        "    pipe_clip_guided = CLIPGuidedStableDiffusion(\n",
        "        unet=pipeline.unet,\n",
        "        vae=pipeline.vae,\n",
        "        tokenizer=pipeline.tokenizer,\n",
        "        text_encoder=pipeline.text_encoder,\n",
        "        scheduler=scheduler_clip,\n",
        "        clip_model=clip_model,\n",
        "        feature_extractor=feature_extractor,\n",
        "    )'''\n",
        "    pipe_clip_guided = pipeline_scheduler(pipe_clip_guided, big3=True)\n",
        "    pipe_clip_guided = optimize_pipe(pipe_clip_guided)\n",
        "    return pipe_clip_guided\n",
        "\n",
        "def get_repaint(page):\n",
        "    global pipe_repaint\n",
        "    pipe_repaint = get_repaint_pipe()\n",
        "\n",
        "def get_repaint_pipe():\n",
        "    global pipe_repaint\n",
        "    from diffusers import UNet2DModel, RePaintScheduler, RePaintPipeline\n",
        "    #model = get_model(prefs['model_ckpt'])\n",
        "    #model_path = model['path']\n",
        "    model_id = \"google/ddpm-ema-celebahq-256\"\n",
        "    unet = UNet2DModel.from_pretrained(model_id)\n",
        "    repaint_scheduler = RePaintScheduler.from_pretrained(model_id)\n",
        "    pipe_repaint = RePaintPipeline(unet=unet, scheduler=repaint_scheduler).to(torch_device)\n",
        "    return pipe_repaint\n",
        "\n",
        "def get_depth2img(page):\n",
        "  global pipe_depth\n",
        "  pipe_depth = get_depth_pipe()\n",
        "\n",
        "def get_depth_pipe():\n",
        "  global pipe_depth, prefs\n",
        "  from diffusers import StableDiffusionDepth2ImgPipeline\n",
        "  model_id = \"stabilityai/stable-diffusion-2-depth\"\n",
        "  if pipe_depth is not None:\n",
        "      if prefs['scheduler_mode'] != status['loaded_scheduler']:\n",
        "        pipe_depth = pipeline_scheduler(pipe_depth)\n",
        "        return pipe_depth\n",
        "      else:\n",
        "        return pipe_depth\n",
        "  if prefs['higher_vram_mode']:\n",
        "    pipe_depth = StableDiffusionDepth2ImgPipeline.from_pretrained(\n",
        "        model_id,\n",
        "        #scheduler=model_scheduler(model_id),\n",
        "        cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,\n",
        "    )\n",
        "  else:\n",
        "    pipe_depth = StableDiffusionDepth2ImgPipeline.from_pretrained(\n",
        "        model_id,\n",
        "        #scheduler=model_scheduler(model_id),\n",
        "        cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,\n",
        "        revision=\"fp16\", \n",
        "        torch_dtype=torch.float16,\n",
        "    )\n",
        "  #pipe_depth.to(torch_device)\n",
        "  pipe_depth = pipeline_scheduler(pipe_depth)\n",
        "  pipe_depth = optimize_pipe(pipe_depth)\n",
        "  pipe_depth.set_progress_bar_config(disable=True)\n",
        "  return pipe_depth\n",
        "\n",
        "def get_alt_diffusion(page):\n",
        "    global pipe_alt_diffusion\n",
        "    run_process(\"pip install -q sentencepiece\", page=page)\n",
        "    pipe_alt_diffusion = get_alt_diffusion_pipe()\n",
        "\n",
        "def get_alt_diffusion_pipe():\n",
        "    global pipe_alt_diffusion\n",
        "    from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker\n",
        "    from diffusers import AltDiffusionPipeline, StableDiffusionPipeline\n",
        "    #from diffusers.pipelines.alt_diffusion.modeling_roberta_series import (RobertaSeriesConfig, RobertaSeriesModelWithTransformation)\n",
        "    model_id = \"BAAI/AltDiffusion-m9\"\n",
        "    if pipe_alt_diffusion is not None:\n",
        "      if prefs['scheduler_mode'] != status['loaded_scheduler']:\n",
        "        pipe_alt_diffusion = pipeline_scheduler(pipe_alt_diffusion)\n",
        "        return pipe_alt_diffusion\n",
        "      else:\n",
        "        return pipe_alt_diffusion\n",
        "    if prefs['higher_vram_mode']:\n",
        "      pipe_alt_diffusion = StableDiffusionPipeline.from_pretrained(\n",
        "          model_id,\n",
        "          #scheduler=model_scheduler(model_id),\n",
        "          cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,\n",
        "          requires_safety_checker = not prefs['disable_nsfw_filter'],\n",
        "          safety_checker=None if prefs['disable_nsfw_filter'] else StableDiffusionSafetyChecker.from_pretrained(\"CompVis/stable-diffusion-safety-checker\")\n",
        "      )\n",
        "    else:\n",
        "      pipe_alt_diffusion = StableDiffusionPipeline.from_pretrained(\n",
        "          model_id,\n",
        "          #scheduler=model_scheduler(model_id),\n",
        "          cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, \n",
        "          torch_dtype=torch.float16,\n",
        "          requires_safety_checker = not prefs['disable_nsfw_filter'],\n",
        "          safety_checker=None if prefs['disable_nsfw_filter'] else StableDiffusionSafetyChecker.from_pretrained(\"CompVis/stable-diffusion-safety-checker\")\n",
        "      )\n",
        "    #pipe_alt_diffusion.to(torch_device)\n",
        "    pipe_alt_diffusion = pipeline_scheduler(pipe_alt_diffusion)\n",
        "    pipe_alt_diffusion = optimize_pipe(pipe_alt_diffusion)\n",
        "    pipe_alt_diffusion.set_progress_bar_config(disable=True)\n",
        "    return pipe_alt_diffusion\n",
        "\n",
        "def get_alt_diffusion_img2img(page):\n",
        "    global pipe_alt_diffusion_img2img\n",
        "    pipe_alt_diffusion_img2img = get_alt_diffusion_img2img_pipe()\n",
        "\n",
        "def get_alt_diffusion_img2img_pipe():\n",
        "    global pipe_alt_diffusion_img2img\n",
        "    from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker\n",
        "    from diffusers import AltDiffusionImg2ImgPipeline, StableDiffusionImg2ImgPipeline\n",
        "    model_id = \"BAAI/AltDiffusion-m9\"\n",
        "    if pipe_alt_diffusion_img2img is not None:\n",
        "      if prefs['scheduler_mode'] != status['loaded_scheduler']:\n",
        "        pipe_alt_diffusion_img2img = pipeline_scheduler(pipe_alt_diffusion_img2img)\n",
        "        return pipe_alt_diffusion_img2img\n",
        "      else:\n",
        "        return pipe_alt_diffusion_img2img\n",
        "    if prefs['higher_vram_mode']:\n",
        "      pipe_alt_diffusion_img2img = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
        "          model_id,\n",
        "          #scheduler=model_scheduler(model_id),\n",
        "          cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,\n",
        "          requires_safety_checker = not prefs['disable_nsfw_filter'],\n",
        "          safety_checker=None if prefs['disable_nsfw_filter'] else StableDiffusionSafetyChecker.from_pretrained(\"CompVis/stable-diffusion-safety-checker\"), feature_extractor=None\n",
        "      )\n",
        "    else:\n",
        "      pipe_alt_diffusion_img2img = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
        "          model_id,\n",
        "          #scheduler=model_scheduler(model_id),\n",
        "          cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, \n",
        "          torch_dtype=torch.float16,\n",
        "          requires_safety_checker = not prefs['disable_nsfw_filter'],\n",
        "          safety_checker=None if prefs['disable_nsfw_filter'] else StableDiffusionSafetyChecker.from_pretrained(\"CompVis/stable-diffusion-safety-checker\"), feature_extractor=None\n",
        "      )\n",
        "    #pipe_alt_diffusion_img2img.to(torch_device)\n",
        "    pipe_alt_diffusion_img2im = pipeline_scheduler(pipe_alt_diffusion_img2im)\n",
        "    pipe_alt_diffusion_img2img = optimize_pipe(pipe_alt_diffusion_img2img)\n",
        "    pipe_alt_diffusion_img2img.set_progress_bar_config(disable=True)\n",
        "    return pipe_alt_diffusion_img2img\n",
        "\n",
        "SD_sampler = None\n",
        "def get_stability(page):\n",
        "    global prefs, SD_sampler#, stability_api\n",
        "    '''try:\n",
        "      from stability_sdk import client\n",
        "      import stability_sdk.interfaces.gooseai.generation.generation_pb2 as generation\n",
        "    except ImportError as e:\n",
        "      run_process(\"pip install stability-sdk -q\", page=page)\n",
        "      from stability_sdk import client\n",
        "      import stability_sdk.interfaces.gooseai.generation.generation_pb2 as generation\n",
        "      pass\n",
        "    stability_api = client.StabilityInference(\n",
        "        key=prefs['Stability_api_key'], \n",
        "        verbose=True,\n",
        "        engine=prefs['model_checkpoint']# if prefs['model_checkpoint'] == \"stable-diffusion-v1-5\" else \"stable-diffusion-v1\",\n",
        "    )\n",
        "    SD_sampler = client.get_sampler_from_str(prefs['generation_sampler'].lower())'''\n",
        "    # New way, other is obsolete\n",
        "    import requests\n",
        "    api_host = os.getenv('API_HOST', 'https://api.stability.ai')\n",
        "    stability_url = f\"{api_host}/v1/engines/list\" #user/account\"\n",
        "    response = requests.get(stability_url, headers={\"Authorization\": prefs['Stability_api_key']})\n",
        "    if response.status_code != 200:\n",
        "      alert_msg(page, \"ERROR with Stability-ai: \" + str(response.text))\n",
        "      return\n",
        "    payload = response.json()\n",
        "    print(str(payload))\n",
        "    status['installed_stability'] = True\n",
        "\n",
        "'''\n",
        "def update_stability():\n",
        "    global SD_sampler, stability_api\n",
        "    from stability_sdk import client\n",
        "    stability_api = client.StabilityInference(\n",
        "        key=prefs['Stability_api_key'], \n",
        "        verbose=True,\n",
        "        engine=prefs['model_checkpoint']\n",
        "    )\n",
        "    SD_sampler = client.get_sampler_from_str(prefs['generation_sampler'].lower())\n",
        "'''\n",
        "def get_AIHorde(page):\n",
        "    global prefs, status\n",
        "    import requests\n",
        "    api_host = os.getenv('API_HOST', 'https://stablehorde.net/api/')\n",
        "    horde_url = f\"{api_host}/v2/find_user\" #user/account\"\n",
        "    response = requests.get(horde_url, headers={\"apikey\": prefs['AIHorde_api_key'], 'accept': 'application/json'})\n",
        "    if response.status_code != 200:\n",
        "      alert_msg(page, \"ERROR {response.status_code} with AIHorde Authentication\", content=Text(str(response.text)))\n",
        "      return\n",
        "    payload = response.json()\n",
        "    print(str(payload))\n",
        "    AI_Horde = os.path.join(dist_dir, \"AI-Horde-CLI\")\n",
        "    if not os.path.exists(AI_Horde) or force_updates:\n",
        "      run_sp(\"git clone https://github.com/db0/AI-Horde-CLI.git\", cwd=dist_dir, realtime=False)\n",
        "      run_sp(\"pip install -r cli_requirements.txt --user\", cwd=AI_Horde, realtime=False)\n",
        "    try:\n",
        "      import yaml\n",
        "    except ModuleNotFoundError:\n",
        "      run_sp(\"pip install pyyaml\", realtime=False)\n",
        "      pass\n",
        "    status['installed_AIHorde'] = True\n",
        "    \n",
        "def get_ESRGAN(page):\n",
        "    os.chdir(dist_dir)\n",
        "    run_process(f\"git clone https://github.com/xinntao/Real-ESRGAN.git -q\", page=page, cwd=dist_dir)\n",
        "    os.chdir(os.path.join(dist_dir, 'Real-ESRGAN'))\n",
        "    run_process(\"pip install basicsr --quiet\", page=page, cwd=os.path.join(dist_dir, 'Real-ESRGAN'))\n",
        "    run_process(\"pip install facexlib --quiet\", page=page, cwd=os.path.join(dist_dir, 'Real-ESRGAN'))\n",
        "    run_process(\"pip install gfpgan --quiet\", page=page, cwd=os.path.join(dist_dir, 'Real-ESRGAN'))\n",
        "    run_process(f\"pip install -r requirements.txt --quiet\", page=page, realtime=False, cwd=os.path.join(dist_dir, 'Real-ESRGAN'))\n",
        "    run_process(f\"python setup.py develop --quiet\", page=page, realtime=False, cwd=os.path.join(dist_dir, 'Real-ESRGAN'))\n",
        "    run_process(f\"wget https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.5.0/realesr-general-x4v3.pth -P experiments/pretrained_models --quiet\", page=page, cwd=os.path.join(dist_dir, 'Real-ESRGAN'))\n",
        "    #run_process(f\"wget https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/RealESRGAN_x4plus.pth -P experiments/pretrained_models --quiet\", page=page, cwd=os.path.join(dist_dir, 'Real-ESRGAN'))\n",
        "    os.chdir(root_dir)\n",
        "\n",
        "\n",
        "concepts = [{'name': 'cat-toy', 'token': 'cat-toy'}, {'name': 'madhubani-art', 'token': 'madhubani-art'}, {'name': 'birb-style', 'token': 'birb-style'}, {'name': 'indian-watercolor-portraits', 'token': 'watercolor-portrait'}, {'name': 'xyz', 'token': 'xyz'}, {'name': 'poolrooms', 'token': 'poolrooms'}, {'name': 'cheburashka', 'token': 'cheburashka'}, {'name': 'hours-style', 'token': 'hours'}, {'name': 'turtlepics', 'token': 'henry-leonardi'}, {'name': 'karl-s-lzx-1', 'token': 'lzx'}, {'name': 'canary-cap', 'token': 'canary-cap'}, {'name': 'ti-junglepunk-v0', 'token': 'jungle-punk'}, {'name': 'mafalda-character', 'token': 'mafalda-quino'}, {'name': 'magic-pengel', 'token': 'magic-pengel'}, {'name': 'schloss-mosigkau', 'token': 'ralph'}, {'name': 'cubex', 'token': 'cube'}, {'name': 'covid-19-rapid-test', 'token': 'covid-test'}, {'name': 'character-pingu', 'token': 'character-pingu'}, {'name': '2814-roth', 'token': '2814Roth'}, {'name': 'vkuoo1', 'token': 'style-vkuoo1'}, {'name': 'ina-art', 'token': ''}, {'name': 'monte-novo', 'token': 'monte novo cutting board'}, {'name': 'interchanges', 'token': 'xchg'}, {'name': 'walter-wick-photography', 'token': 'walter-wick'}, {'name': 'arcane-style-jv', 'token': 'arcane-style-jv'}, {'name': 'w3u', 'token': 'w3u'}, {'name': 'smiling-friend-style', 'token': 'smilingfriends-cartoon'}, {'name': 'dr-livesey', 'token': 'dr-livesey'}, {'name': 'monster-girl', 'token': 'monster-girl'}, {'name': 'abstract-concepts', 'token': 'art-style'}, {'name': 'reeducation-camp', 'token': 'reeducation-camp'}, {'name': 'miko-3-robot', 'token': 'miko-3'}, {'name': 'party-girl', 'token': 'party-girl'}, {'name': 'dicoo', 'token': 'Dicoo'}, {'name': 'kuvshinov', 'token': 'kuvshinov'}, {'name': 'mass', 'token': 'mass'}, {'name': 'ldr', 'token': 'ldr'}, {'name': 'hub-city', 'token': 'HubCity'}, {'name': 'masyunya', 'token': 'masyunya'}, {'name': 'david-moreno-architecture', 'token': 'dm-arch'}, {'name': 'lolo', 'token': 'lolo'}, {'name': 'apulian-rooster-v0-1', 'token': 'apulian-rooster-v0.1'}, {'name': 'fractal', 'token': 'fractal'}, {'name': 'nebula', 'token': 'nebula'}, {'name': 'ldrs', 'token': 'ldrs'}, {'name': 'art-brut', 'token': 'art-brut'}, {'name': 'malika-favre-art-style', 'token': 'malika-favre'}, {'name': 'line-art', 'token': 'line-art'}, {'name': 'shrunken-head', 'token': 'shrunken-head'}, {'name': 'bonzi-monkey', 'token': 'bonzi'}, {'name': 'herge-style', 'token': 'herge'}, {'name': 'johnny-silverhand', 'token': 'johnny-silverhand'}, {'name': 'linnopoke', 'token': 'linnopoke'}, {'name': 'koko-dog', 'token': 'koko-dog'}, {'name': 'stuffed-penguin-toy', 'token': 'pengu-toy'}, {'name': 'monster-toy', 'token': 'monster-toy'}, {'name': 'dong-ho', 'token': 'dong-ho'}, {'name': 'orangejacket', 'token': 'orangejacket'}, {'name': 'fergal-cat', 'token': 'fergal-cat'}, {'name': 'summie-style', 'token': 'summie-style'}, {'name': 'chonkfrog', 'token': 'chonkfrog'}, {'name': 'alberto-mielgo', 'token': 'street'}, {'name': 'lucky-luke', 'token': 'lucky-luke'}, {'name': 'zdenek-art', 'token': 'zdenek-artwork'}, {'name': 'star-tours-posters', 'token': 'star-tours'}, {'name': 'huang-guang-jian', 'token': 'huang-guang-jian'}, {'name': 'painting', 'token': 'will'}, {'name': 'line-style', 'token': 'line-style'}, {'name': 'venice', 'token': 'venice'}, {'name': 'russian', 'token': 'Russian'}, {'name': 'tony-diterlizzi-s-planescape-art', 'token': 'tony-diterlizzi-planescape'}, {'name': 'moeb-style', 'token': 'moe-bius'}, {'name': 'amine', 'token': 'ayna'}, {'name': 'kojima-ayami', 'token': 'KOJIMA'}, {'name': 'dong-ho2', 'token': 'dong-ho-2'}, {'name': 'ruan-jia', 'token': 'ruan-jia'}, {'name': 'purplefishli', 'token': 'purplefishli'}, {'name': 'cry-baby-style', 'token': 'cry-baby'}, {'name': 'between2-mt-fade', 'token': 'b2MTfade'}, {'name': 'mtl-longsky', 'token': 'mtl-longsky'}, {'name': 'scrap-style', 'token': 'style-chewie'}, {'name': 'tela-lenca', 'token': 'tela-lenca'}, {'name': 'zillertal-can', 'token': 'zillertal-ipa'}, {'name': 'shu-doll', 'token': 'shu-doll'}, {'name': 'eastward', 'token': 'eastward'}, {'name': 'chuck-walton', 'token': 'Chuck_Walton'}, {'name': 'chucky', 'token': 'merc'}, {'name': 'smw-map', 'token': 'smw-map'}, {'name': 'erwin-olaf-style', 'token': 'erwin-olaf'}, {'name': 'maurice-quentin-de-la-tour-style', 'token': 'maurice'}, {'name': 'dan-seagrave-art-style', 'token': 'dan-seagrave'}, {'name': 'drive-scorpion-jacket', 'token': 'drive-scorpion-jacket'}, {'name': 'dark-penguin-pinguinanimations', 'token': 'darkpenguin-robot'}, {'name': 'rd-paintings', 'token': 'rd-painting'}, {'name': 'borderlands', 'token': 'borderlands'}, {'name': 'depthmap', 'token': 'depthmap'}, {'name': 'lego-astronaut', 'token': 'lego-astronaut'}, {'name': 'transmutation-circles', 'token': 'tcircle'}, {'name': 'mycat', 'token': 'mycat'}, {'name': 'ilya-shkipin', 'token': 'ilya-shkipin-style'}, {'name': 'moxxi', 'token': 'moxxi'}, {'name': 'riker-doll', 'token': 'rikerdoll'}, {'name': 'apex-wingman', 'token': 'wingman-apex'}, {'name': 'naf', 'token': 'nal'}, {'name': 'handstand', 'token': 'handstand'}, {'name': 'vb-mox', 'token': 'vb-mox'}, {'name': 'pixel-toy', 'token': 'pixel-toy'}, {'name': 'olli-olli', 'token': 'olli-olli'}, {'name': 'floral', 'token': 'ntry not foun'}, {'name': 'minecraft-concept-art', 'token': 'concept'}, {'name': 'yb-anime', 'token': 'anime-character'}, {'name': 'ditko', 'token': 'cat-toy'}, {'name': 'disquieting-muses', 'token': 'muses'}, {'name': 'ned-flanders', 'token': 'flanders'}, {'name': 'fluid-acrylic-jellyfish-creatures-style-of-carl-ingram-art', 'token': 'jelly-core'}, {'name': 'ic0n', 'token': 'ic0n'}, {'name': 'pyramidheadcosplay', 'token': 'Cos-Pyramid'}, {'name': 'phc', 'token': 'Cos-Pyramid'}, {'name': 'og-mox-style', 'token': 'og-mox-style'}, {'name': 'klance', 'token': 'klance'}, {'name': 'john-blanche', 'token': 'john-blanche'}, {'name': 'cowboy', 'token': 'cowboyStyle'}, {'name': 'darkpenguinanimatronic', 'token': 'penguin-robot'}, {'name': 'doener-red-line-art', 'token': 'dnr'}, {'name': 'style-of-marc-allante', 'token': 'Marc_Allante'}, {'name': 'crybaby-style-2-0', 'token': 'crybaby2'}, {'name': 'werebloops', 'token': 'werebloops'}, {'name': 'xbh', 'token': 'xbh'}, {'name': 'unfinished-building', 'token': 'unfinished-building'}, {'name': 'teelip-ir-landscape', 'token': 'teelip-ir-landscape'}, {'name': 'road-to-ruin', 'token': 'RtoR'}, {'name': 'piotr-jablonski', 'token': 'piotr-jablonski'}, {'name': 'jamiels', 'token': 'jamiels'}, {'name': 'tomcat', 'token': 'tom-cat'}, {'name': 'meyoco', 'token': 'meyoco'}, {'name': 'nixeu', 'token': 'nixeu'}, {'name': 'tnj', 'token': 'tnj'}, {'name': 'cute-bear', 'token': 'cute-bear'}, {'name': 'leica', 'token': 'leica'}, {'name': 'anime-boy', 'token': 'myAItestShota'}, {'name': 'garfield-pizza-plush', 'token': 'garfield-plushy'}, {'name': 'design', 'token': 'design'}, {'name': 'mikako-method', 'token': 'm-m'}, {'name': 'cornell-box', 'token': 'cornell-box'}, {'name': 'sculptural-style', 'token': 'diaosu'}, {'name': 'aavegotchi', 'token': 'aave-gotchi'}, {'name': 'swamp-choe-2', 'token': 'cat-toy'}, {'name': 'super-nintendo-cartridge', 'token': 'snesfita-object'}, {'name': 'garfield-pizza-plush-v2', 'token': 'garfield-plushy'}, {'name': 'rickyart', 'token': 'RickyArt'}, {'name': 'eye-of-agamotto', 'token': 'eye-aga'}, {'name': 'freddy-fazbear', 'token': 'freddy-fazbear'}, {'name': 'glass-pipe', 'token': 'glass-sherlock'}, {'name': 'black-waifu', 'token': 'black-waifu'}, {'name': 'roy-lichtenstein', 'token': 'roy-lichtenstein'}, {'name': 'ugly-sonic', 'token': 'ugly-sonic'}, {'name': 'glow-forest', 'token': 'dark-forest'}, {'name': 'painted-student', 'token': 'painted_student'}, {'name': 'salmonid', 'token': 'salmonid'}, {'name': 'huayecai820-greyscale', 'token': 'huayecaigreyscale-style'}, {'name': 'arthur1', 'token': 'arthur1'}, {'name': 'huckleberry', 'token': 'huckleberry'}, {'name': 'collage3', 'token': 'Collage3'}, {'name': 'spritual-monsters', 'token': 'spritual-monsters'}, {'name': 'baldi', 'token': 'baldi'}, {'name': 'tcirle', 'token': 'tcircle'}, {'name': 'pantone-milk', 'token': 'pantone-milk'}, {'name': 'retropixelart-pinguin', 'token': 'retropixelart-style'}, {'name': 'doose-s-realistic-art-style', 'token': 'doose-realistic'}, {'name': 'grit-toy', 'token': 'grit-toy'}, {'name': 'pink-beast-pastelae-style', 'token': 'pinkbeast'}, {'name': 'mikako-methodi2i', 'token': 'm-mi2i'}, {'name': 'aj-fosik', 'token': 'AJ-Fosik'}, {'name': 'collage-cutouts', 'token': 'collage-cutouts'}, {'name': 'cute-cat', 'token': 'cute-bear'}, {'name': 'kaleido', 'token': 'kaleido'}, {'name': 'xatu', 'token': 'xatu-pokemon'}, {'name': 'a-female-hero-from-the-legend-of-mir', 'token': ' <female-hero> from The Legend of Mi'}, {'name': 'cologne', 'token': 'cologne-dom'}, {'name': 'wlop-style', 'token': 'wlop-style'}, {'name': 'larrette', 'token': 'larrette'}, {'name': 'bert-muppet', 'token': 'bert-muppet'}, {'name': 'my-hero-academia-style', 'token': 'MHA style'}, {'name': 'vcr-classique', 'token': 'vcr_c'}, {'name': 'xatu2', 'token': 'xatu-test'}, {'name': 'tela-lenca2', 'token': 'tela-lenca'}, {'name': 'dragonborn', 'token': 'dragonborn'}, {'name': 'mate', 'token': 'mate'}, {'name': 'alien-avatar', 'token': 'alien-avatar'}, {'name': 'pastelartstyle', 'token': 'Arzy'}, {'name': 'kings-quest-agd', 'token': 'ings-quest-ag'}, {'name': 'doge-pound', 'token': 'doge-pound'}, {'name': 'type', 'token': 'typeface'}, {'name': 'fileteado-porteno', 'token': 'fileteado-porteno'}, {'name': 'bullvbear', 'token': 'bullVBear'}, {'name': 'freefonix-style', 'token': 'Freefonix'}, {'name': 'garcon-the-cat', 'token': 'garcon-the-cat'}, {'name': 'better-collage3', 'token': 'C3'}, {'name': 'metagabe', 'token': 'metagabe'}, {'name': 'ggplot2', 'token': 'ggplot2'}, {'name': 'yoshi', 'token': 'yoshi'}, {'name': 'illustration-style', 'token': 'illustration-style'}, {'name': 'centaur', 'token': 'centaur'}, {'name': 'zoroark', 'token': 'zoroark'}, {'name': 'bad_Hub_Hugh', 'token': 'HubHugh'}, {'name': 'irasutoya', 'token': 'irasutoya'}, {'name': 'liquid-light', 'token': 'lls'}, {'name': 'zaneypixelz', 'token': 'zaneypixelz'}, {'name': 'tubby', 'token': 'tubby'}, {'name': 'atm-ant', 'token': 'atm-ant'}, {'name': 'fang-yuan-001', 'token': 'fang-yuan'}, {'name': 'dullboy-caricature', 'token': 'dullboy-cari'}, {'name': 'bada-club', 'token': 'bada-club'}, {'name': 'zaney', 'token': 'zaney'}, {'name': 'a-tale-of-two-empires', 'token': 'two-empires'}, {'name': 'dabotap', 'token': 'dabotap'}, {'name': 'harley-quinn', 'token': 'harley-quinn'}, {'name': 'vespertine', 'token': 'Vesp'}, {'name': 'ricar', 'token': 'ricard'}, {'name': 'conner-fawcett-style', 'token': 'badbucket'}, {'name': 'ingmar-bergman', 'token': 'ingmar-bergman'}, {'name': 'poutine-dish', 'token': 'poutine-qc'}, {'name': 'shev-linocut', 'token': 'shev-linocut'}, {'name': 'grifter', 'token': 'grifter'}, {'name': 'dog', 'token': 'Winston'}, {'name': 'tangles', 'token': 'cora-tangle'}, {'name': 'lost-rapper', 'token': 'lost-rapper'}, {'name': 'eddie', 'token': 'ddi'}, {'name': 'thunderdome-covers', 'token': 'thunderdome'}, {'name': 'she-mask', 'token': 'she-mask'}, {'name': 'chillpill', 'token': 'Chillpill'}, {'name': 'robertnava', 'token': 'robert-nava'}, {'name': 'looney-anime', 'token': 'looney-anime'}, {'name': 'axe-tattoo', 'token': 'axe-tattoo'}, {'name': 'fireworks-over-water', 'token': 'firework'}, {'name': 'collage14', 'token': 'C14'}, {'name': 'green-tent', 'token': 'green-tent'}, {'name': 'dtv-pkmn', 'token': 'dtv-pkm2'}, {'name': 'crinos-form-garou', 'token': 'crinos'}, {'name': '8bit', 'token': '8bit'}, {'name': 'tubby-cats', 'token': 'tubby'}, {'name': 'travis-bedel', 'token': 'bedelgeuse2'}, {'name': 'uma', 'token': 'uma'}, {'name': 'ie-gravestone', 'token': 'internet-explorer-gravestone'}, {'name': 'colossus', 'token': 'colossus'}, {'name': 'uma-style-classic', 'token': 'uma'}, {'name': 'collage3-hubcity', 'token': 'C3Hub'}, {'name': 'goku', 'token': 'goku'}, {'name': 'galaxy-explorer', 'token': 'galaxy-explorer'}, {'name': 'rl-pkmn-test', 'token': 'rl-pkmn'}, {'name': 'naval-portrait', 'token': 'naval-portrait'}, {'name': 'daycare-attendant-sun-fnaf', 'token': 'biblic-sun-fnaf'}, {'name': 'reksio-dog', 'token': 'reksio-dog'}, {'name': 'breakcore', 'token': 'reakcor'}, {'name': 'junji-ito-artstyle', 'token': 'junji-ito-style'}, {'name': 'gram-tops', 'token': 'gram-tops'}, {'name': 'henjo-techno-show', 'token': 'HENJOTECHNOSHOW'}, {'name': 'trash-polka-artstyle', 'token': 'trash-polka-style'}, {'name': 'faraon-love-shady', 'token': ''}, {'name': 'trigger-studio', 'token': 'Trigger Studio'}, {'name': 'tb303', 'token': '\"tb303'}, {'name': 'neon-pastel', 'token': 'neon-pastel'}, {'name': 'fursona', 'token': 'fursona-2'}, {'name': 'sterling-archer', 'token': 'archer-style'}, {'name': 'captain-haddock', 'token': 'captain-haddock'}, {'name': 'my-mug', 'token': 'my-mug'}, {'name': 'joe-whiteford-art-style', 'token': 'joe-whiteford-artstyle'}, {'name': 'on-kawara', 'token': 'on-kawara'}, {'name': 'hours-sentry-fade', 'token': 'Hours_Sentry'}, {'name': 'rektguy', 'token': 'rektguy'}, {'name': 'dyoudim-style', 'token': 'DyoudiM-style'}, {'name': 'kaneoya-sachiko', 'token': 'Kaneoya'}, {'name': 'retro-girl', 'token': 'retro-girl'}, {'name': 'buddha-statue', 'token': 'buddha-statue'}, {'name': 'hitokomoru-style-nao', 'token': 'hitokomoru-style'}, {'name': 'plant-style', 'token': 'plant'}, {'name': 'cham', 'token': 'cham'}, {'name': 'mayor-richard-irvin', 'token': 'Richard_Irvin'}, {'name': 'sd-concepts-library-uma-meme', 'token': 'uma-object-full'}, {'name': 'uma-meme', 'token': 'uma-object-full'}, {'name': 'thunderdome-cover', 'token': 'thunderdome-cover'}, {'name': 'sem-mac2n', 'token': 'SEM_Mac2N'}, {'name': 'hoi4', 'token': 'hoi4'}, {'name': 'hd-emoji', 'token': 'HDemoji-object'}, {'name': 'lumio', 'token': 'lumio'}, {'name': 't-skrang', 'token': 'tskrang'}, {'name': 'agm-style-nao', 'token': 'agm-style'}, {'name': 'uma-meme-style', 'token': 'uma-meme-style'}, {'name': 'retro-mecha-rangers', 'token': 'aesthetic'}, {'name': 'babushork', 'token': 'babushork'}, {'name': 'qpt-atrium', 'token': 'QPT_ATRIUM'}, {'name': 'sushi-pixel', 'token': 'sushi-pixel'}, {'name': 'osrsmini2', 'token': ''}, {'name': 'ttte', 'token': 'ttte-2'}, {'name': 'atm-ant-2', 'token': 'atm-ant'}, {'name': 'dan-mumford', 'token': 'dan-mumford'}, {'name': 'renalla', 'token': 'enall'}, {'name': 'cow-uwu', 'token': 'cow-uwu'}, {'name': 'one-line-drawing', 'token': 'lineart'}, {'name': 'inuyama-muneto-style-nao', 'token': 'inuyama-muneto-style'}, {'name': 'altvent', 'token': 'AltVent'}, {'name': 'accurate-angel', 'token': 'accurate-angel'}, {'name': 'mtg-card', 'token': 'mtg-card'}, {'name': 'ddattender', 'token': 'ddattender'}, {'name': 'thalasin', 'token': 'thalasin-plus'}, {'name': 'moebius', 'token': 'moebius'}, {'name': 'liqwid-aquafarmer', 'token': 'aquafarmer'}, {'name': 'onepunchman', 'token': 'OnePunch'}, {'name': 'kawaii-colors', 'token': 'kawaii-colors-style'}, {'name': 'naruto', 'token': 'Naruto'}, {'name': 'backrooms', 'token': 'Backrooms'}, {'name': 'a-hat-kid', 'token': 'hatintime-kid'}, {'name': 'furrpopasthetic', 'token': 'furpop'}, {'name': 'RINGAO', 'token': ''}, {'name': 'csgo-awp-texture-map', 'token': 'csgo_awp_texture'}, {'name': 'luinv2', 'token': 'luin-waifu'}, {'name': 'hydrasuit', 'token': 'hydrasuit'}, {'name': 'milady', 'token': 'milady'}, {'name': 'ganyu-genshin-impact', 'token': 'ganyu'}, {'name': 'wayne-reynolds-character', 'token': 'warcharport'}, {'name': 'david-firth-artstyle', 'token': 'david-firth-artstyle'}, {'name': 'seraphimmoonshadow-art', 'token': 'seraphimmoonshadow-art'}, {'name': 'osrstiny', 'token': 'osrstiny'}, {'name': 'lugal-ki-en', 'token': 'lugal-ki-en'}, {'name': 'seamless-ground', 'token': 'seamless-ground'}, {'name': 'sewerslvt', 'token': 'ewerslv'}, {'name': 'diaosu-toy', 'token': 'diaosu-toy'}, {'name': 'sakimi-style', 'token': 'sakimi'}, {'name': 'rj-palmer', 'token': 'rj-palmer'}, {'name': 'harmless-ai-house-style-1', 'token': 'bee-style'}, {'name': 'harmless-ai-1', 'token': 'bee-style'}, {'name': 'yerba-mate', 'token': 'yerba-mate'}, {'name': 'bella-goth', 'token': 'bella-goth'}, {'name': 'bobs-burgers', 'token': 'bobs-burgers'}, {'name': 'jamie-hewlett-style', 'token': 'hewlett'}, {'name': 'belen', 'token': 'belen'}, {'name': 'shvoren-style', 'token': 'shvoren-style'}, {'name': 'gymnastics-leotard-v2', 'token': 'gymnastics-leotard2'}, {'name': 'rd-chaos', 'token': 'rd-chaos'}, {'name': 'armor-concept', 'token': 'armor-concept'}, {'name': 'ouroboros', 'token': 'ouroboros'}, {'name': 'm-geo', 'token': 'm-geo'}, {'name': 'Akitsuki', 'token': ''}, {'name': 'uzumaki', 'token': 'NARUTO'}, {'name': 'sorami-style', 'token': 'sorami-style'}, {'name': 'lxj-o4', 'token': 'csp'}, {'name': 'she-hulk-law-art', 'token': 'shehulk-style'}, {'name': 'led-toy', 'token': 'led-toy'}, {'name': 'durer-style', 'token': 'drr-style'}, {'name': 'hiten-style-nao', 'token': 'hiten-style-nao'}, {'name': 'mechasoulall', 'token': 'mechasoulall'}, {'name': 'wish-artist-stile', 'token': 'wish-style'}, {'name': 'max-foley', 'token': 'max-foley'}, {'name': 'loab-style', 'token': 'loab-style'}, {'name': '3d-female-cyborgs', 'token': 'A female cyborg'}, {'name': 'r-crumb-style', 'token': 'rcrumb'}, {'name': 'paul-noir', 'token': 'paul-noir'}, {'name': 'cgdonny1', 'token': 'donny1'}, {'name': 'valorantstyle', 'token': 'valorant'}, {'name': 'loab-character', 'token': 'loab-character'}, {'name': 'Atako', 'token': ''}, {'name': 'threestooges', 'token': 'threestooges'}, {'name': 'dsmuses', 'token': 'DSmuses'}, {'name': 'fish', 'token': 'fish'}, {'name': 'glass-prism-cube', 'token': 'glass-prism-cube'}, {'name': 'elegant-flower', 'token': 'elegant-flower'}, {'name': 'hanfu-anime-style', 'token': 'hanfu-anime-style'}, {'name': 'green-blue-shanshui', 'token': 'green-blue shanshui'}, {'name': 'lizardman', 'token': 'laceholderTokenLizardma'}, {'name': 'rail-scene', 'token': 'rail-pov'}, {'name': 'lula-13', 'token': 'lula-13'}, {'name': 'laala-character', 'token': 'laala'}, {'name': 'margo', 'token': 'dog-margo'}, {'name': 'carrascharacter', 'token': 'Carras'}, {'name': 'vietstoneking', 'token': 'vietstoneking'}, {'name': 'rhizomuse-machine-bionic-sculpture', 'token': ''}, {'name': 'rcrumb-portraits-style', 'token': 'rcrumb-portraits'}, {'name': 'mu-sadr', 'token': '783463b'}, {'name': 'bozo-22', 'token': 'bozo-22'}, {'name': 'skyfalls', 'token': 'SkyFalls'}, {'name': 'zk', 'token': ''}, {'name': 'tudisco', 'token': 'cat-toy'}, {'name': 'kogecha', 'token': 'kogecha'}, {'name': 'ori-toor', 'token': 'ori-toor'}, {'name': 'isabell-schulte-pviii-style', 'token': 'isabell-schulte-p8-style'}, {'name': 'rilakkuma', 'token': 'rilakkuma'}, {'name': 'indiana', 'token': 'indiana'}, {'name': 'black-and-white-design', 'token': 'PM_style'}, {'name': 'isabell-schulte-pviii-1024px-1500-steps-style', 'token': 'isabell-schulte-p8-style-1024p-1500s'}, {'name': 'fold-structure', 'token': 'fold-geo'}, {'name': 'brunnya', 'token': 'Brunnya'}, {'name': 'jos-de-kat', 'token': 'kat-jos'}, {'name': 'singsing-doll', 'token': 'singsing'}, {'name': 'singsing', 'token': 'singsing'}, {'name': 'isabell-schulte-pviii-12tiles-3000steps-style', 'token': 'isabell-schulte-p8-style-12tiles-3000s'}, {'name': 'f-22', 'token': 'f-22'}, {'name': 'jin-kisaragi', 'token': 'jin-kisaragi'}, {'name': 'depthmap-style', 'token': 'depthmap'}, {'name': 'crested-gecko', 'token': 'crested-gecko'}, {'name': 'grisstyle', 'token': 'gris'}, {'name': 'ikea-fabler', 'token': 'ikea-fabler'}, {'name': 'joe-mad', 'token': 'joe-mad'}, {'name': 'boissonnard', 'token': 'boissonnard'}, {'name': 'overprettified', 'token': 'overprettified'}, {'name': 'all-rings-albuns', 'token': 'rings-all-albuns'}, {'name': 'shiny-polyman', 'token': 'shiny-polyman'}, {'name': 'scarlet-witch', 'token': 'sw-mom'}, {'name': 'wojaks-now', 'token': 'red-wojak'}, {'name': 'carasibana', 'token': 'carasibana'}, {'name': 'towerplace', 'token': 'TowerPlace'}, {'name': 'cumbia-peruana', 'token': 'cumbia-peru'}, {'name': 'bloo', 'token': 'owl-guy'}, {'name': 'dog-django', 'token': 'dog-django'}, {'name': 'facadeplace', 'token': 'FacadePlace'}, {'name': 'blue-zombie', 'token': 'blue-zombie'}, {'name': 'blue-zombiee', 'token': 'blue-zombie'}, {'name': 'jinjoon-lee-they', 'token': 'jinjoon_lee_they'}, {'name': 'ralph-mcquarrie', 'token': 'ralph-mcquarrie'}, {'name': 'hiyuki-chan', 'token': 'hiyuki-chan'}, {'name': 'isabell-schulte-pviii-4tiles-6000steps', 'token': 'isabell-schulte-p8-style-4tiles-6000s'}, {'name': 'liliana', 'token': 'liliana'}, {'name': 'morino-hon-style', 'token': 'morino-hon'}, {'name': 'artist-yukiko-kanagai', 'token': 'Yukiko Kanagai '}, {'name': 'wheatland', 'token': ''}, {'name': 'm-geoo', 'token': 'm-geo'}, {'name': 'wheatland-arknight', 'token': 'golden-wheats-fields'}, {'name': 'mokoko', 'token': 'mokoko'}, {'name': '001glitch-core', 'token': '01glitch_cor'}, {'name': 'stardew-valley-pixel-art', 'token': 'pixelart-stardew'}, {'name': 'isabell-schulte-pviii-4tiles-500steps', 'token': 'isabell-schulte-p8-style-4tiles-500s'}, {'name': 'anime-girl', 'token': 'anime-girl'}, {'name': 'heather', 'token': 'eather'}, {'name': 'rail-scene-style', 'token': 'rail-pov'}, {'name': 'quiesel', 'token': 'quiesel'}, {'name': 'matthew-stone', 'token': 'atthew-ston'}, {'name': 'dreamcore', 'token': 'dreamcore'}, {'name': 'pokemon-conquest-sprites', 'token': 'poke-conquest'}, {'name': 'tili-concept', 'token': 'tili'}, {'name': 'nouns-glasses', 'token': 'nouns glasses'}, {'name': 'shigure-ui-style', 'token': 'shigure-ui'}, {'name': 'pen-ink-portraits-bennorthen', 'token': 'ink-portrait-by-BenNorthern'}, {'name': 'nikodim', 'token': 'nikodim'}, {'name': 'ori', 'token': 'Ori'}, {'name': 'anya-forger', 'token': 'anya-forger'}, {'name': 'lavko', 'token': 'lavko'}, {'name': 'fasina', 'token': 'Fasina'}, {'name': 'uma-clean-object', 'token': 'uma-clean-object'}, {'name': 'wojaks-now-now-now', 'token': 'red-wojak'}, {'name': 'memnarch-mtg', 'token': 'mtg-memnarch'}, {'name': 'tonal1', 'token': 'Tonal'}, {'name': 'tesla-bot', 'token': 'tesla-bot'}, {'name': 'red-glasses', 'token': 'red-glasses'}, {'name': 'csgo-awp-object', 'token': 'csgo_awp'}, {'name': 'stretch-re1-robot', 'token': 'stretch'}, {'name': 'isabell-schulte-pv-pvii-3000steps', 'token': 'isabell-schulte-p5-p7-style-3000s'}, {'name': 'insidewhale', 'token': 'InsideWhale'}, {'name': 'noggles', 'token': 'noggles'}, {'name': 'isometric-tile-test', 'token': 'iso-tile'}, {'name': 'bamse-og-kylling', 'token': 'bamse-kylling'}, {'name': 'marbling-art', 'token': 'marbling-art'}, {'name': 'joemad', 'token': 'joemad'}, {'name': 'bamse', 'token': 'bamse'}, {'name': 'dq10-anrushia', 'token': 'anrushia'}, {'name': 'test', 'token': 'AIO'}, {'name': 'naoki-saito', 'token': 'naoki_saito'}, {'name': 'raichu', 'token': 'raichu'}, {'name': 'child-zombie', 'token': 'child-zombie'}, {'name': 'yf21', 'token': 'YF21'}, {'name': 'titan-robot', 'token': 'titan'}, {'name': 'cyberpunk-lucy', 'token': 'cyberpunk-lucy'}, {'name': 'giygas', 'token': 'giygas'}, {'name': 'david-martinez-cyberpunk', 'token': 'david-martinez-cyberpunk'}, {'name': 'phan-s-collage', 'token': 'pcollage'}, {'name': 'jojo-bizzare-adventure-manga-lineart', 'token': 'JoJo_lineart'}, {'name': 'homestuck-sprite', 'token': 'homestuck-sprite'}, {'name': 'kogatan-shiny', 'token': 'ogata'}, {'name': 'moo-moo', 'token': 'moomoo'}, {'name': 'detectivedinosaur1', 'token': 'dd1'}, {'name': 'arcane-face', 'token': 'arcane-face'}, {'name': 'sherhook-painting', 'token': 'sherhook'}, {'name': 'isabell-schulte-pviii-1-image-style', 'token': 'isabell-schulte-p8-1-style'}, {'name': 'dicoo2', 'token': 'dicoo'}, {'name': 'hrgiger-drmacabre', 'token': 'barba'}, {'name': 'babau', 'token': 'babau'}, {'name': 'darkplane', 'token': 'DarkPlane'}, {'name': 'wildkat', 'token': 'wildkat'}, {'name': 'half-life-2-dog', 'token': 'hl-dog'}, {'name': 'outfit-items', 'token': 'outfit-items'}, {'name': 'midjourney-style', 'token': 'midjourney-style'}, {'name': 'puerquis-toy', 'token': 'puerquis'}, {'name': 'maus', 'token': 'Maus'}, {'name': 'jetsetdreamcastcovers', 'token': 'jet'}, {'name': 'karan-gloomy', 'token': 'karan'}, {'name': 'yoji-shinkawa-style', 'token': 'yoji-shinkawa'}, {'name': 'million-live-akane-15k', 'token': 'akane'}, {'name': 'million-live-akane-3k', 'token': 'akane'}, {'name': 'sherhook-painting-v2', 'token': 'sherhook'}, {'name': 'gba-pokemon-sprites', 'token': 'GBA-Poke-Sprites'}, {'name': 'gim', 'token': 'grimes-album-style'}, {'name': 'char-con', 'token': 'char-con'}, {'name': 'bluebey', 'token': 'bluebey'}, {'name': 'homestuck-troll', 'token': 'homestuck-troll'}, {'name': 'million-live-akane-shifuku-3k', 'token': 'akane'}, {'name': 'thegeneral', 'token': 'bobknight'}, {'name': 'million-live-spade-q-object-3k', 'token': 'spade_q'}, {'name': 'million-live-spade-q-style-3k', 'token': 'spade_q'}, {'name': 'ibere-thenorio', 'token': 'ibere-thenorio'}, {'name': 'yinit', 'token': 'init-dropca'}, {'name': 'bee', 'token': 'b-e-e'}, {'name': 'pixel-mania', 'token': 'pixel-mania'}, {'name': 'sunfish', 'token': 'SunFish'}, {'name': 'test2', 'token': 'AIOCARD'}, {'name': 'pool-test', 'token': 'pool_test'}, {'name': 'mokoko-seed', 'token': 'mokoko-seed'}, {'name': 'isabell-schulte-pviii-4-tiles-1-lr-3000-steps-style', 'token': 'isabell-schulte-p8-4tiles-1lr-300s-style'}, {'name': 'ghostproject-men', 'token': 'ghostsproject-style'}, {'name': 'phan', 'token': 'phan'}, {'name': 'chen-1', 'token': 'chen-1'}, {'name': 'bluebey-2', 'token': 'bluebey'}, {'name': 'waterfallshadow', 'token': 'WaterfallShadow'}, {'name': 'chop', 'token': 'Le Petit Prince'}, {'name': 'sintez-ico', 'token': 'sintez-ico'}, {'name': 'carlitos-el-mago', 'token': 'carloscarbonell'}, {'name': 'david-martinez-edgerunners', 'token': 'david-martinez-edgerunners'}, {'name': 'isabell-schulte-pviii-4-tiles-3-lr-5000-steps-style', 'token': 'isabell-schulte-p8-4tiles-3lr-5000s-style'}, {'name': 'guttestreker', 'token': 'guttestreker'}, {'name': 'ransom', 'token': 'ransom'}, {'name': 'museum-by-coop-himmelblau', 'token': 'coop himmelblau museum'}, {'name': 'coop-himmelblau', 'token': 'coop himmelblau'}, {'name': 'yesdelete', 'token': 'yesdelete'}, {'name': 'conway-pirate', 'token': 'conway'}, {'name': 'ilo-kunst', 'token': 'ilo-kunst'}, {'name': 'yilanov2', 'token': 'yilanov'}, {'name': 'dr-strange', 'token': 'dr-strange'}, {'name': 'hubris-oshri', 'token': 'Hubris'}, {'name': 'osaka-jyo', 'token': 'osaka-jyo'}, {'name': 'paolo-bonolis', 'token': 'paolo-bonolis'}, {'name': 'repeat', 'token': 'repeat'}, {'name': 'geggin', 'token': 'geggin'}, {'name': 'lex', 'token': 'lex'}, {'name': 'osaka-jyo2', 'token': 'osaka-jyo2'}, {'name': 'owl-house', 'token': 'owl-house'}, {'name': 'nazuna', 'token': 'nazuna'}, {'name': 'thorneworks', 'token': 'Thorneworks'}, {'name': 'kysa-v-style', 'token': 'kysa-v-style'}, {'name': 'senneca', 'token': 'Senneca'}, {'name': 'zero-suit-samus', 'token': 'zero-suit-samus'}, {'name': 'kanv1', 'token': 'KAN'}, {'name': 'dlooak', 'token': 'dlooak'}, {'name': 'wire-angels', 'token': 'wire-angels'}, {'name': 'mizkif', 'token': 'mizkif'}, {'name': 'brittney-williams-art', 'token': 'Brittney_Williams'}, {'name': 'wheelchair', 'token': 'wheelchair'}, {'name': 'yuji-himukai-style', 'token': 'Yuji Himukai-Style'}, {'name': 'cindlop', 'token': 'cindlop'}, {'name': 'sas-style', 'token': 'smooth-aesthetic-style'}, {'name': 'remert', 'token': 'Remert'}, {'name': 'alex-portugal', 'token': 'alejandro-portugal'}, {'name': 'explosions-cat', 'token': 'explosions-cat'}, {'name': 'onzpo', 'token': 'onzpo'}, {'name': 'eru-chitanda-casual', 'token': 'c-eru-chitanda'}, {'name': 'poring-ragnarok-online', 'token': 'poring-ro'}, {'name': 'cg-bearded-man', 'token': 'LH-Keeper'}, {'name': 'ba-shiroko', 'token': 'shiroko'}, {'name': 'at-wolf-boy-object', 'token': 'AT-Wolf-Boy-Object'}, {'name': 'fairytale', 'token': 'fAIrytale'}, {'name': 'kira-sensei', 'token': 'kira-sensei'}, {'name': 'kawaii-girl-plus-style', 'token': 'kawaii_girl'}, {'name': 'kawaii-girl-plus-object', 'token': 'kawaii_girl'}, {'name': 'boris-anderson', 'token': 'boris-anderson'}, {'name': 'medazzaland', 'token': 'edazzalan'}, {'name': 'duranduran', 'token': 'uranDura'}, {'name': 'crbart', 'token': 'crbart'}, {'name': 'happy-person12345', 'token': 'Happy-Person12345'}, {'name': 'fzk', 'token': 'fzk'}, {'name': 'rishusei-style', 'token': 'crishusei-style'}, {'name': 'felps', 'token': 'Felps'}, {'name': 'plen-ki-mun', 'token': 'plen-ki-mun'}, {'name': 'babs-bunny', 'token': 'babs_bunny'}, {'name': 'james-web-space-telescope', 'token': 'James-Web-Telescope'}, {'name': 'blue-haired-boy', 'token': 'Blue-Haired-Boy'}, {'name': '80s-anime-ai', 'token': '80s-anime-AI'}, {'name': 'spider-gwen', 'token': 'spider-gwen'}, {'name': 'takuji-kawano', 'token': 'takuji-kawano'}, {'name': 'fractal-temple-style', 'token': 'fractal-temple'}, {'name': 'sanguo-guanyu', 'token': 'sanguo-guanyu'}, {'name': 's1m-naoto-ohshima', 'token': 's1m-naoto-ohshima'}, {'name': 'kawaii-girl-plus-style-v1-1', 'token': 'kawaii'}, {'name': 'nathan-wyatt', 'token': 'Nathan-Wyatt'}, {'name': 'kasumin', 'token': 'kasumin'}, {'name': 'happy-person12345-assets', 'token': 'Happy-Person12345-assets'}, {'name': 'oleg-kuvaev', 'token': 'oleg-kuvaev'}, {'name': 'kanovt', 'token': 'anov'}, {'name': 'lphr-style', 'token': 'lphr-style'}, {'name': 'concept-art', 'token': 'concept-art'}, {'name': 'trust-support', 'token': 'trust'}, {'name': 'altyn-helmet', 'token': 'Altyn'}, {'name': '80s-anime-ai-being', 'token': 'anime-AI-being'}, {'name': 'baluchitherian', 'token': 'baluchiter'}, {'name': 'pineda-david', 'token': 'pineda-david'}, {'name': 'ohisashiburi-style', 'token': 'ohishashiburi-style'}, {'name': 'crb-portraits', 'token': 'crbportrait'}, {'name': 'i-love-chaos', 'token': 'chaos'}, {'name': 'alex-thumbnail-object-2000-steps', 'token': 'alex'}, {'name': '852style-girl', 'token': '852style-girl'}, {'name': 'nomad', 'token': 'nomad'}, {'name': 'new-priests', 'token': 'new-priest'}, {'name': 'liminalspaces', 'token': 'liminal image'}, {'name': 'aadhav-face', 'token': 'aadhav-face'}, {'name': 'jang-sung-rak-style', 'token': 'Jang-Sung-Rak-style'}, {'name': 'mattvidpro', 'token': 'mattvidpro'}, {'name': 'chungus-poodl-pet', 'token': 'poodl-chungus-big'}, {'name': 'liminal-spaces-2-0', 'token': 'iminal imag'}, {'name': 'crb-surrealz', 'token': 'crbsurreal'}, {'name': 'final-fantasy-logo', 'token': 'final-fantasy-logo'}, {'name': 'canadian-goose', 'token': 'canadian-goose'}, {'name': 'scratch-project', 'token': 'scratch-project'}, {'name': 'lazytown-stephanie', 'token': 'azytown-stephani'}, {'name': 'female-kpop-singer', 'token': 'female-kpop-star'}, {'name': 'aleyna-tilki', 'token': 'aleyna-tilki'}, {'name': 'other-mother', 'token': 'ther-mothe'}, {'name': 'beldam', 'token': 'elda'}, {'name': 'button-eyes', 'token': 'utton-eye'}, {'name': 'alisa', 'token': 'alisa-selezneva'}, {'name': 'im-poppy', 'token': 'm-popp'}, {'name': 'fractal-flame', 'token': 'fractal-flame'}, {'name': 'Exodus-Styling', 'token': 'Exouds-Style'}, {'name': '8sconception', 'token': '80s-car'}, {'name': 'christo-person', 'token': 'christo'}, {'name': 'slm', 'token': 'c-w388'}, {'name': 'meze-audio-elite-headphones', 'token': 'meze-elite'}, {'name': 'fox-purple', 'token': 'foxi-purple'}, {'name': 'roblox-avatar', 'token': 'roblox-avatar'}, {'name': 'toy-bonnie-plush', 'token': 'toy-bonnie-plush'}, {'name': 'alf', 'token': 'alf'}, {'name': 'wojak', 'token': 'oja'}, {'name': 'animalve3-1500seq', 'token': 'diodio'}, {'name': 'muxoyara', 'token': 'muxoyara'}, {'name': 'selezneva-alisa', 'token': 'selezneva-alisa'}, {'name': 'ayush-spider-spr', 'token': 'spr-mn'}, {'name': 'natasha-johnston', 'token': 'natasha-johnston'}, {'name': 'nard-style', 'token': 'nard'}, {'name': 'kirby', 'token': 'kirby'}, {'name': 'el-salvador-style-style', 'token': 'el-salvador-style'}, {'name': 'rahkshi-bionicle', 'token': 'rahkshi-bionicle'}, {'name': 'masyanya', 'token': 'masyanya'}, {'name': 'command-and-conquer-remastered-cameos', 'token': 'command_and_conquer_remastered_cameos'}, {'name': 'lucario', 'token': 'lucario'}, {'name': 'bruma', 'token': 'Bruma-the-cat'}, {'name': 'nissa-revane', 'token': 'nissa-revane'}, {'name': 'tamiyo', 'token': 'tamiyo'}, {'name': 'pascalsibertin', 'token': 'pascalsibertin'}, {'name': 'chandra-nalaar', 'token': 'chandra-nalaar'}, {'name': 'sam-yang', 'token': 'sam-yang'}, {'name': 'kiora', 'token': 'kiora'}, {'name': 'wedding', 'token': 'wedding1'}, {'name': 'arwijn', 'token': 'rwij'}, {'name': 'gba-fe-class-cards', 'token': 'lasscar'}, {'name': 'painted-by-silver-of-999', 'token': 'cat-toy'}, {'name': 'painted-by-silver-of-999-2', 'token': 'girl-painted-by-silver-of-999'}, {'name': 'toyota-sera', 'token': 'toyota-sera'}, {'name': 'vraska', 'token': 'vraska'}, {'name': 'mystical-nature', 'token': ''}, {'name': 'cartoona-animals', 'token': 'cartoona-animals'}, {'name': 'amogus', 'token': 'amogus'}, {'name': 'kinda-sus', 'token': 'amogus'}, {'name': 'xuna', 'token': 'Xuna'}, {'name': 'pion-by-august-semionov', 'token': 'pion'}, {'name': 'rikiart', 'token': 'rick-art'}, {'name': 'jacqueline-the-unicorn', 'token': 'jacqueline'}, {'name': 'flaticon-lineal-color', 'token': 'flaticon-lineal-color'}, {'name': 'test-epson', 'token': 'epson-branch'}, {'name': 'orientalist-art', 'token': 'orientalist-art'}, {'name': 'ki', 'token': 'ki-mars'}, {'name': 'fnf-boyfriend', 'token': 'fnf-boyfriend'}, {'name': 'phoenix-01', 'token': 'phoenix-style'}, {'name': 'society-finch', 'token': 'society-finch'}, {'name': 'rikiboy-art', 'token': 'Rikiboy-Art'}, {'name': 'flatic', 'token': 'flat-ct'}, {'name': 'logo-with-face-on-shield', 'token': 'logo-huizhang'}, {'name': 'elspeth-tirel', 'token': 'elspeth-tirel'}, {'name': 'zero', 'token': 'zero'}, {'name': 'willy-hd', 'token': 'willy_character'}, {'name': 'kaya-ghost-assasin', 'token': 'kaya-ghost-assasin'}, {'name': 'starhavenmachinegods', 'token': 'StarhavenMachineGods'}, {'name': 'namine-ritsu', 'token': 'namine-ritsu'}, {'name': 'mildemelwe-style', 'token': 'mildemelwe'}, {'name': 'nahiri', 'token': 'nahiri'}, {'name': 'ghost-style', 'token': 'ghost'}, {'name': 'arq-render', 'token': 'arq-style'}, {'name': 'saheeli-rai', 'token': 'saheeli-rai'}, {'name': 'youpi2', 'token': 'youpi'}, {'name': 'youtooz-candy', 'token': 'youtooz-candy'}, {'name': 'beholder', 'token': 'beholder'}, {'name': 'progress-chip', 'token': 'progress-chip'}, {'name': 'lofa', 'token': 'lofa'}, {'name': 'huatli', 'token': 'huatli'}, {'name': 'vivien-reid', 'token': 'vivien-reid'}, {'name': 'wedding-HandPainted', 'token': ''}, {'name': 'sims-2-portrait', 'token': 'sims2-portrait'}, {'name': 'flag-ussr', 'token': 'flag-ussr'}, {'name': 'cortana', 'token': 'cortana'}, {'name': 'azura-from-vibrant-venture', 'token': 'azura'}, {'name': 'liliana-vess', 'token': 'liliana-vess'}, {'name': 'dreamy-painting', 'token': 'dreamy-painting'}, {'name': 'munch-leaks-style', 'token': 'munch-leaks-style'}, {'name': 'gta5-artwork', 'token': 'gta5-artwork'}, {'name': 'xioboma', 'token': 'xi-obama'}, {'name': 'ashiok', 'token': 'ashiok'}, {'name': 'Aflac-duck', 'token': 'aflac duck'}, {'name': 'toho-pixel', 'token': 'toho-pixel'}, {'name': 'alicebeta', 'token': 'Alice-style'}, {'name': 'cute-game-style', 'token': 'cute-game-style'}, {'name': 'a-yakimova', 'token': 'a-yakimova'}, {'name': 'anime-background-style', 'token': 'anime-background-style'}, {'name': 'uliana-kudinova', 'token': 'liana-kudinov'}, {'name': 'msg', 'token': 'MSG69'}, {'name': 'gio', 'token': 'gio-single'}, {'name': 'smooth-pencils', 'token': ''}, {'name': 'pintu', 'token': 'pintu-dog'}, {'name': 'marty6', 'token': 'marty6'}, {'name': 'marty', 'token': 'marty'}, {'name': 'xi', 'token': 'JinpingXi'}, {'name': 'captainkirb', 'token': 'captainkirb'}, {'name': 'urivoldemort', 'token': 'uriboldemort'}, {'name': 'anime-background-style-v2', 'token': 'anime-background-style-v2'}, {'name': 'hk-peach', 'token': 'hk-peach'}, {'name': 'hk-goldbuddha', 'token': 'hk-goldbuddha'}, {'name': 'edgerunners-style', 'token': 'edgerunners-style-av'}, {'name': 'warhammer-40k-drawing-style', 'token': 'warhammer40k-drawing-style'}, {'name': 'hk-opencamera', 'token': 'hk-opencamera'}, {'name': 'hk-breakfast', 'token': 'hk-breakfast'}, {'name': 'iridescent-illustration-style', 'token': 'iridescent-illustration-style'}, {'name': 'edgerunners-style-v2', 'token': 'edgerunners-style-av-v2'}, {'name': 'leif-jones', 'token': 'leif-jones'}, {'name': 'hk-buses', 'token': 'hk-buses'}, {'name': 'hk-goldenlantern', 'token': 'hk-goldenlantern'}, {'name': 'hk-hkisland', 'token': 'hk-hkisland'}, {'name': 'hk-leaves', 'token': ''}, {'name': 'hk-oldcamera', 'token': 'hk-oldcamera'}, {'name': 'frank-frazetta', 'token': 'rank franzett'}, {'name': 'obama-based-on-xi', 'token': 'obama> <JinpingXi'}, {'name': 'hk-vintage', 'token': ''}, {'name': 'degods', 'token': 'degods'}, {'name': 'dishonored-portrait-styles', 'token': 'portrait-style-dishonored'}, {'name': 'manga-style', 'token': 'manga'}, {'name': 'degodsheavy', 'token': 'degods-heavy'}, {'name': 'teferi', 'token': 'teferi'}, {'name': 'car-toy-rk', 'token': 'car-toy'}, {'name': 'anders-zorn', 'token': 'anders-zorn'}, {'name': 'rayne-weynolds', 'token': 'rayne-weynolds'}, {'name': 'hk-bamboo', 'token': 'hk-bamboo'}, {'name': 'hk-betweenislands', 'token': 'hk-betweenislands'}, {'name': 'hk-bicycle', 'token': 'hk-bicycle'}, {'name': 'hk-blackandwhite', 'token': 'hk-blackandwhite'}, {'name': 'pjablonski-style', 'token': 'pjablonski-style'}, {'name': 'hk-market', 'token': 'hk-market'}, {'name': 'hk-phonevax', 'token': 'hk-phonevax'}, {'name': 'hk-clouds', 'token': 'hk-cloud'}, {'name': 'hk-streetpeople', 'token': 'hk-streetpeople'}, {'name': 'iridescent-photo-style', 'token': 'iridescent-photo-style'}, {'name': 'color-page', 'token': 'coloring-page'}, {'name': 'hoi4-leaders', 'token': 'HOI4-Leader'}, {'name': 'franz-unterberger', 'token': 'franz-unterberger'}, {'name': 'angus-mcbride-style', 'token': 'angus-mcbride-style'}, {'name': 'happy-chaos', 'token': 'happychaos'}, {'name': 'gt-color-paint-2', 'token': 'my-color-paint-GT'}, {'name': 'smurf-style', 'token': 'smurfy'}, {'name': 'coraline', 'token': 'coraline'}, {'name': 'terraria-style', 'token': 'terr-sty'}, {'name': 'ettblackteapot', 'token': 'my-teapot'}, {'name': 'gibasachan-v0.1', 'token': 'gibasachan'}, {'name': 'kodakvision500t', 'token': 'kodakvision_500T'}, {'name': 'obama-based-on-xi', 'token': 'obama'}, {'name': 'obama-self-2', 'token': 'Obama'}, {'name': 'bob-dobbs', 'token': 'bob'}, {'name': 'ahx-model-1', 'token': 'ivan-stripes'}, {'name': 'ahx-model-2', 'token': 'artist'}, {'name': 'beetlejuice-cartoon-style', 'token': 'beetlejuice-cartoon'}, {'name': 'pokemon-modern-artwork', 'token': 'pkmn-modern'}, {'name': 'pokemon-classic-artwork', 'token': 'pkmn-classic'}, {'name': 'pokemon-gens-1-to-8', 'token': 'pkmn-galar'}, {'name': 'pokemon-rgby-sprite', 'token': 'pkmn-rgby'}, {'name': 'max-twain', 'token': 'max-twain'}, {'name': 'ihylc', 'token': 'ihylc'}, {'name': 'test-man', 'token': 'Test-man'}, {'name': 'tron-style', 'token': 'tron-style>'}, {'name': 'dulls', 'token': 'dulls-avatar'}, {'name': 'vie-proceres', 'token': 'vie-proceres'}, {'name': 'dovin-baan', 'token': 'dovin-baan'}, {'name': 'polki-jewellery', 'token': 'ccess to model sd-concepts-library/polki-jewellery is restricted and you are not in the authorized list. Visit https://huggingface.co/sd-concepts-library/polki-jewellery to ask for access'}, {'name': 'dog2', 'token': 'ccess to model sd-concepts-library/dog2 is restricted and you are not in the authorized list. Visit https://huggingface.co/sd-concepts-library/dog2 to ask for access'}, {'name': 'caitlin-fairchild-character-gen13-comics', 'token': 'Caitlin-Fairchild'}, {'name': 'ugly-sonic', 'token': 'ugly-sonic'}, {'name': 'utopia-beer-mat', 'token': 'utopia-beer-mat'}, {'name': 'old-brno', 'token': 'old-brno'}, {'name': 'moka-pot', 'token': 'moka-pot'}, {'name': 'brno-trenck', 'token': 'brno-trenck'}, {'name': 'brno-tram', 'token': 'brno-tram'}, {'name': 'brno-obasa', 'token': 'brno-obasa'}, {'name': 'brno-night', 'token': 'brno-night'}, {'name': 'brno-dorm', 'token': 'brno-dorm'}, {'name': 'brno-busstop', 'token': 'brno-busstop'}, {'name': 'twitch-league-of-legends', 'token': 'twitch-lol'}, {'name': 'fp-shop2', 'token': 'fp-shop2'}, {'name': 'fp-shop1', 'token': 'fp-shop1'}, {'name': 'fp-content-b', 'token': 'fp-content-b'}, {'name': 'fp-content-a', 'token': 'fp-content-a'}, {'name': 'fp-city', 'token': 'fp-city'}, {'name': 'brno-city-results', 'token': 'brno-city-results'}, {'name': 'brno-city', 'token': 'brno-city'}, {'name': 'brno-chair-results', 'token': 'brno-chair-results'}, {'name': 'brno-chair', 'token': 'brno-chair'}, {'name': 'manga-char-nov-23', 'token': 'char-nov23'}, {'name': 'manga-nov-23', 'token': 'manga-characters-nov23'}, {'name': 'yellow-cockatiel-parrot', 'token': 'rosa-popugai'}, {'name': 'dreams', 'token': 'meeg'}, {'name': 'alberto-montt', 'token': 'AlbertoMontt'}, {'name': 'tooth-wu', 'token': 'tooth-wu'}, {'name': 'filename-2', 'token': 'filename2'}, {'name': 'iridescent-photo-style', 'token': 'iridescent-photo-style'}, {'name': 'bored-ape-textual-inversion', 'token': 'bored_ape'}, {'name': 'ghibli-face', 'token': 'ghibli-face'}, {'name': 'yoshimurachi', 'token': 'yoshi-san'}, {'name': 'jm-bergling-monogram', 'token': 'JM-Bergling-monogram'}, {'name': '4tnght', 'token': '4tNGHT'}, {'name': 'dancing-cactus', 'token': 'dancing-cactus'}, {'name': 'yolandi-visser', 'token': 'olandi-visse'}, {'name': 'zizigooloo', 'token': 'zizigooloo'}, {'name': 'princess-knight-art', 'token': 'princess-knight'}, {'name': 'belle-delphine', 'token': 'elle-delphin'}, {'name': 'cancer_style', 'token': 'cancer_style'}, {'name': 'trypophobia', 'token': 'rypophobi'}, {'name': 'incendegris-grey', 'token': 'incendegris-grey'}, {'name': 'fairy-tale-painting-style', 'token': 'fairy-tale-painting-style'}, {'name': 'arcimboldo-style', 'token': 'arcimboldo-style'}, {'name': 'xidiversity', 'token': 'JinpingXi'}, {'name': 'obama-based-on-xi', 'token': 'obama'}, {'name': 'zero-bottle', 'token': 'zero-bottle'}, {'name': 'victor-narm', 'token': 'victor-narm'}, {'name': 'supitcha-mask', 'token': 'supitcha-mask'}, {'name': 'smarties', 'token': 'smarties'}, {'name': 'rico-face', 'token': 'rico-face'}, {'name': 'rex-deno', 'token': 'rex-deno'}, {'name': 'abby-face', 'token': 'abby-face'}, {'name': 'nic-papercuts', 'token': 'nic-papercuts'}]\n",
        "\n",
        "\n",
        "def get_concept(name):\n",
        "  for con in concepts:\n",
        "      if con['name'] == name:\n",
        "        return con\n",
        "  return {'name':'', 'token':''}\n",
        "\n",
        "def get_conceptualizer(page):\n",
        "    from huggingface_hub import hf_hub_download\n",
        "    from diffusers import StableDiffusionPipeline\n",
        "    from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker\n",
        "    from transformers import CLIPFeatureExtractor, CLIPTextModel, CLIPTokenizer\n",
        "    global pipe_conceptualizer\n",
        "    repo_id_embeds = f\"sd-concepts-library/{prefs['concepts_model']}\"\n",
        "    embeds_url = \"\" #Add the URL or path to a learned_embeds.bin file in case you have one\n",
        "    placeholder_token_string = \"\" #Add what is the token string in case you are uploading your own embed\n",
        "\n",
        "    downloaded_embedding_folder = os.path.join(root_dir, \"downloaded_embedding\")\n",
        "    if not os.path.exists(downloaded_embedding_folder):\n",
        "      os.mkdir(downloaded_embedding_folder)\n",
        "    try:\n",
        "      if(not embeds_url):\n",
        "        embeds_path = hf_hub_download(repo_id=repo_id_embeds, filename=\"learned_embeds.bin\")\n",
        "        token_path = hf_hub_download(repo_id=repo_id_embeds, filename=\"token_identifier.txt\")\n",
        "        shutil.copy(embeds_path, downloaded_embedding_folder)\n",
        "        shutil.copy(token_path, downloaded_embedding_folder)\n",
        "        with open(f'{downloaded_embedding_folder}/token_identifier.txt', 'r') as file:\n",
        "          placeholder_token_string = file.read()\n",
        "      else:\n",
        "        run_sp(f\"wget -q -O {downloaded_embedding_folder}/learned_embeds.bin {embeds_url}\")\n",
        "        #!wget -q -O $downloaded_embedding_folder/learned_embeds.bin $embeds_url\n",
        "    except Exception as e:\n",
        "      alert_msg(page, f\"Error getting concept. May need to accept model at https://huggingface.co/sd-concepts-library/{prefs['concepts_model']}\", content=Text(e))\n",
        "      return\n",
        "    learned_embeds_path = f\"{downloaded_embedding_folder}/learned_embeds.bin\"\n",
        "    tokenizer = CLIPTokenizer.from_pretrained(model_path, subfolder=\"tokenizer\")\n",
        "    text_encoder = CLIPTextModel.from_pretrained(model_path, subfolder=\"text_encoder\", torch_dtype=torch.float16)\n",
        "    def load_learned_embed_in_clip(learned_embeds_path, text_encoder, tokenizer, token=None):\n",
        "      loaded_learned_embeds = torch.load(learned_embeds_path, map_location=\"cpu\")\n",
        "      trained_token = list(loaded_learned_embeds.keys())[0]\n",
        "      embeds = loaded_learned_embeds[trained_token]\n",
        "      dtype = text_encoder.get_input_embeddings().weight.dtype\n",
        "      embeds.to(dtype)\n",
        "      token = token if token is not None else trained_token\n",
        "      num_added_tokens = tokenizer.add_tokens(token)\n",
        "      if num_added_tokens == 0:\n",
        "        alert_msg(page, f\"The tokenizer already contains the token {token}. Please pass a different `token` that is not already in the tokenizer.\")\n",
        "        return\n",
        "      text_encoder.resize_token_embeddings(len(tokenizer))\n",
        "      token_id = tokenizer.convert_tokens_to_ids(token)\n",
        "      text_encoder.get_input_embeddings().weight.data[token_id] = embeds\n",
        "    try:\n",
        "      load_learned_embed_in_clip(learned_embeds_path, text_encoder, tokenizer)\n",
        "    except Exception as e:\n",
        "      alert_msg(page, f\"Error Loading Concept\", content=Text(e))\n",
        "      return\n",
        "    pipe_conceptualizer = StableDiffusionPipeline.from_pretrained(\n",
        "        model_path,\n",
        "        revision=\"fp16\",\n",
        "        torch_dtype=torch.float16,\n",
        "        text_encoder=text_encoder,\n",
        "        tokenizer=tokenizer,\n",
        "        cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,\n",
        "        safety_checker=None if prefs['disable_nsfw_filter'] else StableDiffusionSafetyChecker.from_pretrained(\"CompVis/stable-diffusion-safety-checker\"),\n",
        "    )\n",
        "    pipe_conceptualizer = pipeline_scheduler(pipe_conceptualizer)\n",
        "    pipe_conceptualizer = optimize_pipe(pipe_conceptualizer)\n",
        "    pipe_conceptualizer.set_progress_bar_config(disable=True)\n",
        "    #pipe_conceptualizer = pipe_conceptualizer.to(torch_device)\n",
        "    return pipe_conceptualizer\n",
        "\n",
        "\n",
        "def clear_img2img_pipe():\n",
        "  global pipe_img2img\n",
        "  if pipe_img2img is not None:\n",
        "    #print(\"Clearing out img2img pipeline for more VRAM\")\n",
        "    del pipe_img2img\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    pipe_img2img = None\n",
        "def clear_txt2img_pipe():\n",
        "  global pipe\n",
        "  if pipe is not None:\n",
        "    #print(\"Clearing out text2img pipeline for more VRAM\")\n",
        "    del pipe\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    pipe = None\n",
        "def clear_unet_pipe():\n",
        "  global unet\n",
        "  if unet is not None:\n",
        "    #print(\"Clearing out unet custom pipeline for more VRAM\")\n",
        "    del unet\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    unet = None\n",
        "def clear_clip_guided_pipe():\n",
        "  global pipe_clip_guided\n",
        "  if pipe_clip_guided is not None:\n",
        "    #print(\"Clearing out CLIP Guided pipeline for more VRAM\")\n",
        "    del pipe_clip_guided\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    pipe_clip_guided = None\n",
        "def clear_conceptualizer_pipe():\n",
        "  global pipe_conceptualizer\n",
        "  if pipe_conceptualizer is not None:\n",
        "    #print(\"Clearing out CLIP Guided pipeline for more VRAM\")\n",
        "    del pipe_conceptualizer\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    pipe_conceptualizer = None\n",
        "def clear_repaint_pipe():\n",
        "  global pipe_repaint\n",
        "  if pipe_repaint is not None:\n",
        "    del pipe_repaint\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    pipe_repaint = None\n",
        "def clear_imagic_pipe():\n",
        "  global pipe_imagic\n",
        "  if pipe_imagic is not None:\n",
        "    del pipe_imagic\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    pipe_imagic = None\n",
        "def clear_composable_pipe():\n",
        "  global pipe_composable\n",
        "  if pipe_composable is not None:\n",
        "    del pipe_composable\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    pipe_composable = None\n",
        "def clear_versatile_pipe():\n",
        "  global pipe_versatile\n",
        "  if pipe_versatile is not None:\n",
        "    del pipe_versatile\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    pipe_versatile = None\n",
        "def clear_versatile_text2img_pipe():\n",
        "  global pipe_versatile_text2img\n",
        "  if pipe_versatile_text2img is not None:\n",
        "    del pipe_versatile_text2img\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    pipe_versatile_text2img = None\n",
        "def clear_versatile_variation_pipe():\n",
        "  global pipe_versatile_variation\n",
        "  if pipe_versatile_variation is not None:\n",
        "    del pipe_versatile_variation\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    pipe_versatile_variation = None\n",
        "def clear_versatile_dualguided_pipe():\n",
        "  global pipe_versatile_dualguided\n",
        "  if pipe_versatile_dualguided is not None:\n",
        "    del pipe_versatile_dualguided\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    pipe_versatile_dualguided = None\n",
        "def clear_depth_pipe():\n",
        "  global pipe_depth\n",
        "  if pipe_depth is not None:\n",
        "    del pipe_depth\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    pipe_depth = None\n",
        "def clear_interpolation_pipe():\n",
        "  global pipe_interpolation\n",
        "  if pipe_interpolation is not None:\n",
        "    del pipe_interpolation\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    pipe_interpolation = None\n",
        "def clear_safe_pipe():\n",
        "  global pipe_safe\n",
        "  if pipe_safe is not None:\n",
        "    del pipe_safe\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    pipe_safe = None\n",
        "def clear_upscale_pipe():\n",
        "  global pipe_upscale\n",
        "  if pipe_upscale is not None:\n",
        "    del pipe_upscale\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    pipe_upscale = None\n",
        "def clear_image_variation_pipe():\n",
        "  global pipe_image_variation\n",
        "  if pipe_image_variation is not None:\n",
        "    del pipe_image_variation\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    pipe_image_variation = None\n",
        "def clear_semantic_pipe():\n",
        "  global pipe_semantic\n",
        "  if pipe_semantic is not None:\n",
        "    del pipe_semantic\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    pipe_semantic = None\n",
        "def clear_EDICT_pipe():\n",
        "  global pipe_EDICT, text_encoder_EDICT\n",
        "  if pipe_EDICT is not None:\n",
        "    del pipe_EDICT\n",
        "    del text_encoder_EDICT\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    pipe_EDICT = None\n",
        "    text_encoder_EDICT = None\n",
        "def clear_DiffEdit_pipe():\n",
        "  global pipe_DiffEdit\n",
        "  if pipe_DiffEdit is not None:\n",
        "    del pipe_DiffEdit\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    pipe_DiffEdit = None\n",
        "def clear_unCLIP_pipe():\n",
        "  global pipe_unCLIP\n",
        "  if pipe_unCLIP is not None:\n",
        "    del pipe_unCLIP\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    pipe_unCLIP = None\n",
        "def clear_unCLIP_image_variation_pipe():\n",
        "  global pipe_unCLIP_image_variation\n",
        "  if pipe_unCLIP_image_variation is not None:\n",
        "    del pipe_unCLIP_image_variation\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    pipe_unCLIP_image_variation = None\n",
        "def clear_unCLIP_interpolation_pipe():\n",
        "  global pipe_unCLIP_interpolation\n",
        "  if pipe_unCLIP_interpolation is not None:\n",
        "    del pipe_unCLIP_interpolation\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    pipe_unCLIP_interpolation = None\n",
        "def clear_unCLIP_image_interpolation_pipe():\n",
        "  global pipe_unCLIP_image_interpolation\n",
        "  if pipe_unCLIP_image_interpolation is not None:\n",
        "    del pipe_unCLIP_image_interpolation\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    pipe_unCLIP_image_interpolation = None\n",
        "def clear_magic_mix_pipe():\n",
        "  global pipe_magic_mix\n",
        "  if pipe_magic_mix is not None:\n",
        "    del pipe_magic_mix\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    pipe_magic_mix = None\n",
        "def clear_paint_by_example_pipe():\n",
        "  global pipe_paint_by_example\n",
        "  if pipe_paint_by_example is not None:\n",
        "    del pipe_paint_by_example\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    pipe_paint_by_example = None\n",
        "def clear_instruct_pix2pix_pipe():\n",
        "  global pipe_instruct_pix2pix\n",
        "  if pipe_instruct_pix2pix is not None:\n",
        "    del pipe_instruct_pix2pix\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    pipe_instruct_pix2pix = None\n",
        "def clear_alt_diffusion_pipe():\n",
        "  global pipe_alt_diffusion\n",
        "  if pipe_alt_diffusion is not None:\n",
        "    del pipe_alt_diffusion\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    pipe_alt_diffusion = None\n",
        "def clear_alt_diffusion_img2img_pipe():\n",
        "  global pipe_alt_diffusion_img2img\n",
        "  if pipe_alt_diffusion_img2img is not None:\n",
        "    del pipe_alt_diffusion_img2img\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    pipe_alt_diffusion_img2img = None\n",
        "def clear_SAG_pipe():\n",
        "  global pipe_SAG\n",
        "  if pipe_SAG is not None:\n",
        "    del pipe_SAG\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    pipe_SAG = None\n",
        "def clear_attend_and_excite_pipe():\n",
        "  global pipe_attend_and_excite\n",
        "  if pipe_attend_and_excite is not None:\n",
        "    del pipe_attend_and_excite\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    pipe_attend_and_excite = None\n",
        "def clear_panorama_pipe():\n",
        "  global pipe_panorama\n",
        "  if pipe_panorama is not None:\n",
        "    del pipe_panorama\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    pipe_panorama = None\n",
        "def clear_dance_pipe():\n",
        "  global pipe_dance\n",
        "  if pipe_dance is not None:\n",
        "    del pipe_dance\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    pipe_dance = None\n",
        "def clear_audio_diffusion_pipe():\n",
        "  global pipe_audio_diffusion\n",
        "  if pipe_audio_diffusion is not None:\n",
        "    del pipe_audio_diffusion\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    pipe_audio_diffusion = None\n",
        "def clear_riffusion_pipe():\n",
        "  global pipe_riffusion\n",
        "  if pipe_riffusion is not None:\n",
        "    del pipe_riffusion\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    pipe_riffusion = None\n",
        "def clear_text_to_video_pipe():\n",
        "  global pipe_text_to_video\n",
        "  if pipe_text_to_video is not None:\n",
        "    del pipe_text_to_video\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    pipe_text_to_video = None\n",
        "def clear_text_to_video_zero_pipe():\n",
        "  global pipe_text_to_video_zero\n",
        "  if pipe_text_to_video_zero is not None:\n",
        "    del pipe_text_to_video_zero\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.ipc_collect()\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "    pipe_text_to_video_zero = None\n",
        "def clear_DiT_pipe():\n",
        "  global pipe_DiT\n",
        "  if pipe_DiT is not None:\n",
        "    del pipe_DiT\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    pipe_DiT = None\n",
        "def clear_kandinsky_pipe():\n",
        "  global pipe_kandinsky, loaded_kandinsky_task\n",
        "  if pipe_kandinsky is not None:\n",
        "    del pipe_kandinsky\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    pipe_kandinsky = None\n",
        "    loaded_kandinsky_task = \"\"\n",
        "def clear_tortoise_tts_pipe():\n",
        "  global pipe_tortoise_tts\n",
        "  if pipe_tortoise_tts is not None:\n",
        "    del pipe_tortoise_tts\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    pipe_tortoise_tts = None\n",
        "def clear_audio_ldm_pipe():\n",
        "  global pipe_audio_ldm\n",
        "  if pipe_audio_ldm is not None:\n",
        "    del pipe_audio_ldm\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    pipe_audio_ldm = None\n",
        "def clear_gpt2_pipe():\n",
        "  global pipe_gpt2\n",
        "  if pipe_gpt2 is not None:\n",
        "    del pipe_gpt2\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    pipe_gpt2 = None\n",
        "def clear_distil_gpt2_pipe():\n",
        "  global pipe_distil_gpt2\n",
        "  if pipe_distil_gpt2 is not None:\n",
        "    del pipe_distil_gpt2\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    pipe_distil_gpt2 = None\n",
        "def clear_controlnet_pipe():\n",
        "  global pipe_controlnet, controlnet, controlnet_models, status\n",
        "  if pipe_controlnet is not None:\n",
        "    del pipe_controlnet\n",
        "    del controlnet\n",
        "    for k, v in controlnet_models.items():\n",
        "      if v != None:\n",
        "        del v\n",
        "        controlnet_models[k] = None\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    pipe_controlnet = None\n",
        "    controlnet = None\n",
        "    status['loaded_controlnet'] = None\n",
        "def clear_stable_lm_pipe():\n",
        "  global pipe_stable_lm, tokenizer_stable_lm\n",
        "  if pipe_stable_lm is not None:\n",
        "    del pipe_stable_lm\n",
        "    del tokenizer_stable_lm\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    pipe_stable_lm = None\n",
        "    tokenizer_stable_lm = None\n",
        "   \n",
        "def clear_pipes(allbut=None):\n",
        "    if torch_device == \"cpu\": return\n",
        "    but = [] if allbut == None else [allbut] if type(allbut) is str else allbut\n",
        "    if not 'txt2img' in but: clear_txt2img_pipe()\n",
        "    if not 'img2img' in but: clear_img2img_pipe()\n",
        "    if not 'unet' in but: clear_unet_pipe()\n",
        "    if not 'clip_guided' in but: clear_clip_guided_pipe()\n",
        "    if not 'conceptualizer' in but: clear_conceptualizer_pipe()\n",
        "    if not 'repaint' in but: clear_repaint_pipe()\n",
        "    if not 'imagic' in but: clear_imagic_pipe()\n",
        "    if not 'composable': clear_composable_pipe()\n",
        "    if not 'versatile_text2img' in but: clear_versatile_text2img_pipe()\n",
        "    if not 'versatile_variation' in but: clear_versatile_variation_pipe()\n",
        "    if not 'versatile_dualguided' in but: clear_versatile_dualguided_pipe()\n",
        "    if not 'depth' in but: clear_depth_pipe()\n",
        "    if not 'interpolation' in but: clear_interpolation_pipe()\n",
        "    if not 'safe' in but: clear_safe_pipe()\n",
        "    if not 'upscale' in but: clear_upscale_pipe()\n",
        "    if not 'unCLIP' in but: clear_unCLIP_pipe()\n",
        "    if not 'EDICT' in but: clear_EDICT_pipe()\n",
        "    if not 'DiffEdit' in but: clear_DiffEdit_pipe()\n",
        "    if not 'unCLIP_image_variation' in but: clear_unCLIP_image_variation_pipe()\n",
        "    if not 'unCLIP_interpolation' in but: clear_unCLIP_interpolation_pipe()\n",
        "    if not 'image_variation' in but: clear_image_variation_pipe()\n",
        "    if not 'semantic' in but: clear_semantic_pipe()\n",
        "    if not 'magic_mix' in but: clear_magic_mix_pipe()\n",
        "    if not 'alt_diffusion' in but: clear_alt_diffusion_pipe()\n",
        "    if not 'alt_diffusion_img2img' in but: clear_alt_diffusion_img2img_pipe()\n",
        "    if not 'paint_by_example' in but: clear_paint_by_example_pipe()\n",
        "    if not 'instruct_pix2pix' in but: clear_instruct_pix2pix_pipe()\n",
        "    if not 'SAG' in but: clear_SAG_pipe()\n",
        "    if not 'attend_and_excite' in but: clear_attend_and_excite_pipe()\n",
        "    if not 'DiT' in but: clear_DiT_pipe()\n",
        "    if not 'controlnet' in but: clear_controlnet_pipe()\n",
        "    if not 'panorama' in but: clear_panorama_pipe()\n",
        "    if not 'kandinsky' in but: clear_kandinsky_pipe()\n",
        "    if not 'dance' in but: clear_dance_pipe()\n",
        "    if not 'riffusion' in but: clear_riffusion_pipe()\n",
        "    if not 'audio_diffusion' in but: clear_audio_diffusion_pipe()\n",
        "    if not 'text_to_video' in but: clear_text_to_video_pipe()\n",
        "    if not 'text_to_video_zero' in but: clear_text_to_video_zero_pipe()\n",
        "    if not 'tortoise_tts' in but: clear_tortoise_tts_pipe()\n",
        "    if not 'audio_ldm' in but: clear_audio_ldm_pipe()\n",
        "    if not 'gpt2' in but: clear_gpt2_pipe()\n",
        "    if not 'distil_gpt2' in but: clear_distil_gpt2_pipe()\n",
        "    if not 'controlnet' in but: clear_controlnet_pipe()\n",
        "    if not 'stable_lm' in but: clear_stable_lm_pipe()\n",
        "    try:\n",
        "        torch.cuda.ipc_collect()\n",
        "        torch.cuda.reset_peak_memory_stats()\n",
        "    except Exception:\n",
        "        pass\n",
        "import base64\n",
        "def get_base64(image_path):\n",
        "    with open(image_path, \"rb\") as img_file:\n",
        "        my_string = base64.b64encode(img_file.read()).decode('utf-8')\n",
        "        return my_string\n",
        "\n",
        "def available_file(folder, name, idx, ext='png', no_num=False):\n",
        "  available = False\n",
        "  while not available:\n",
        "    # Todo, check if using PyDrive2\n",
        "    if no_num:\n",
        "      if os.path.isfile(os.path.join(folder, f'{name}.{ext}')):\n",
        "        return os.path.join(folder, f'{name}.{ext}')\n",
        "    if os.path.isfile(os.path.join(folder, f'{name}-{idx}.{ext}')):\n",
        "      idx += 1\n",
        "    else: available = True\n",
        "  return os.path.join(folder, f'{name}-{idx}.{ext}')\n",
        "\n",
        "def available_folder(folder, name, idx):\n",
        "  available = False\n",
        "  while not available:\n",
        "    if os.path.isdir(os.path.join(folder, f'{name}-{idx}')):\n",
        "      idx += 1\n",
        "    else: available = True\n",
        "  return os.path.join(folder, f'{name}-{idx}')\n",
        "\n",
        "#import asyncio\n",
        "#async \n",
        "def start_diffusion(page):\n",
        "  global pipe, unet, pipe_img2img, pipe_clip_guided, pipe_interpolation, pipe_conceptualizer, pipe_imagic, pipe_depth, pipe_composable, pipe_versatile_text2img, pipe_versatile_variation, pipe_versatile_dualguided, pipe_SAG, pipe_attend_and_excite, pipe_alt_diffusion, pipe_alt_diffusion_img2img, pipe_panorama, pipe_safe, pipe_upscale\n",
        "  global SD_sampler, stability_api, total_steps, pb, prefs, args, total_steps\n",
        "  def prt(line, update=True):\n",
        "    if type(line) == str:\n",
        "      line = Text(line)\n",
        "    try:\n",
        "      page.imageColumn.controls.append(line)\n",
        "      if update:\n",
        "        page.imageColumn.update()\n",
        "    except Exception:\n",
        "      clear_image_output()\n",
        "      pass\n",
        "    if update:\n",
        "      page.Images.update()\n",
        "  def clear_last(update=True):\n",
        "    del page.imageColumn.controls[-1]\n",
        "    if update:\n",
        "      page.imageColumn.update()\n",
        "      page.Images.update()\n",
        "  abort_run = False\n",
        "  def abort_diffusion(e):\n",
        "    nonlocal abort_run\n",
        "    abort_run = True\n",
        "    page.snd_error.play()\n",
        "    page.snd_delete.play()\n",
        "  def callback_cancel(cancel) -> None:\n",
        "    callback_cancel.has_been_called = True\n",
        "    if abort_run:\n",
        "      return True\n",
        "  def download_image(e):\n",
        "    if is_Colab:\n",
        "      print(f\"{type(e.control.data)} {e.control.data}\")\n",
        "      from google.colab import files\n",
        "      if os.path.isfile(e.control.data):\n",
        "        files.download(e.control.data)\n",
        "      else:\n",
        "        time.sleep(5)\n",
        "        files.download(e.control.data)\n",
        "  def clear_image_output():\n",
        "    for co in reversed(page.imageColumn.controls):\n",
        "      del co\n",
        "    page.imageColumn.controls.clear()\n",
        "    try:\n",
        "      page.imageColumn.update()\n",
        "    except Exception as e:\n",
        "      try:\n",
        "        page.imageColumn = Column([], auto_scroll=True, scroll=ScrollMode.AUTO)\n",
        "      except Exception as er:\n",
        "        alert_msg(page, f\"ERROR: Problem Clearing Image Output List. May need to stop script and restart app to recover, sorry...\", content=Text(f'{e}\\n{er}'))\n",
        "        page.Images = buildImages(page)\n",
        "        pass\n",
        "      page.update()\n",
        "      pass\n",
        "# Why getting Exception: control with ID '_3607' not found when re-running after error\n",
        "  #page.Images.content.controls = []\n",
        "  clear_image_output()\n",
        "  #pb = ProgressBar(bar_height=8)\n",
        "  pb.width=(page.window_width or page.width) - 50\n",
        "  #prt(Row([Text(\"‚ñ∂Ô∏è   Running Stable Diffusion on Batch Prompts List\", style=TextThemeStyle.TITLE_LARGE, color=colors.SECONDARY, weight=FontWeight.BOLD), IconButton(icon=icons.CANCEL, tooltip=\"Abort Current Diffusion Run\", on_click=abort_diffusion)], alignment=MainAxisAlignment.SPACE_BETWEEN))\n",
        "  prt(Header(\"‚ñ∂Ô∏è   Running Stable Diffusion on Batch Prompts List\", actions=[IconButton(icon=icons.CANCEL, tooltip=\"Abort Current Diffusion Run\", on_click=abort_diffusion)]))\n",
        "  import string, shutil, random, gc, io, json\n",
        "  from collections import ChainMap\n",
        "  from PIL.PngImagePlugin import PngInfo\n",
        "  from contextlib import contextmanager, nullcontext\n",
        "  import copy\n",
        "\n",
        "  if status['installed_diffusers']:\n",
        "    from diffusers import StableDiffusionPipeline\n",
        "  os.chdir(stable_dir)\n",
        "  generator = None\n",
        "  clear_repaint_pipe()\n",
        "  output_files = []\n",
        "  pipe_used = \"\"\n",
        "  retry_attempts_if_NSFW = prefs['retry_attempts']\n",
        "  #if (prefs['use_Stability_api'] and status['installed_stability']) or bool(not status['installed_diffusers'] and status['installed_stability']):\n",
        "  #  update_stability()\n",
        "  last_seed = args['seed']\n",
        "  if args['seed'] < 1 or args['seed'] is None:\n",
        "    rand_seed = random.randint(0,2147483647)\n",
        "    if (not (prefs['use_Stability_api'] or (not status['installed_diffusers'] and status['installed_stability']))) and (not (prefs['use_AIHorde_api'] or (not status['installed_diffusers'] and status['installed_AIHorde']))):\n",
        "      if use_custom_scheduler:\n",
        "        generator = torch.manual_seed(rand_seed)\n",
        "      else:\n",
        "        generator = torch.Generator(\"cuda\").manual_seed(rand_seed)\n",
        "    last_seed = rand_seed\n",
        "  else:\n",
        "    if not (prefs['use_Stability_api'] or (not status['installed_diffusers'] and status['installed_stability'])) and (not (prefs['use_AIHorde_api'] or (not status['installed_diffusers'] and status['installed_AIHorde']))):\n",
        "      if use_custom_scheduler:\n",
        "        generator = torch.manual_seed(args['seed'])\n",
        "      else:\n",
        "        generator = torch.Generator(\"cuda\").manual_seed(args['seed'])\n",
        "  strikes = 0\n",
        "  p_idx = 0\n",
        "  if prefs['centipede_prompts_as_init_images']:\n",
        "    os.makedirs(os.path.join(root_dir, 'init_images'), exist_ok=True)\n",
        "  last_image = None\n",
        "  updated_prompts = []\n",
        "  model = get_model(prefs['model_ckpt'])\n",
        "  if not (prefs[\"use_interpolation\"] and status['installed_interpolation']):\n",
        "    for p in prompts:\n",
        "      pr = None\n",
        "      arg = {}\n",
        "      if type(p) == list or type(p) == str:\n",
        "        pr = p\n",
        "        arg = args.copy()\n",
        "      elif isinstance(p, Dream):\n",
        "        pr = p.prompt\n",
        "        arg = merge_dict(args, p.arg)\n",
        "      else: prt(f'Unknown item in list of type {type(p)}')\n",
        "      #print(str(arg))\n",
        "      arg['width'] = int(arg['width'])\n",
        "      arg['height'] = int(arg['height'])\n",
        "      arg['seed'] = int(arg['seed'])\n",
        "      arg['guidance_scale'] = float(arg['guidance_scale'])\n",
        "      arg['batch_size'] = int(arg['batch_size'])\n",
        "      arg['n_iterations'] = int(arg['n_iterations'])\n",
        "      arg['steps'] = int(arg['steps'])\n",
        "      arg['eta'] = float(arg['eta'])\n",
        "      arg['init_image_strength'] = float(arg['init_image_strength'])\n",
        "      p.arg = arg\n",
        "      iterations = arg['n_iterations']\n",
        "      updated_prompts.append(p)\n",
        "      if iterations > 1:\n",
        "        #print(f\"Iterating {iterations} times - {pr}\")\n",
        "        for d in range(iterations - 1):\n",
        "          new_dream = None\n",
        "          if isinstance(p, Dream):\n",
        "            new_dream = copy.copy(p)\n",
        "            new_dream.prompt = pr[0] if type(pr) == list else pr\n",
        "            new_arg = new_dream.arg.copy()\n",
        "            new_arg['seed'] = random.randint(0,2147483647)\n",
        "            new_arg['n_iterations'] = 1\n",
        "            new_dream.arg = new_arg\n",
        "            #new_dream.arg['seed'] = random.randint(0,4294967295)\n",
        "          else:\n",
        "            new_dream = Dream(p, seed=random.randint(0,2147483647), n_iterations=1)\n",
        "          new_dream.arg['n_iterations'] = 1\n",
        "          #prompts.insert(p_idx+1, new_dream)\n",
        "          updated_prompts.append(new_dream)\n",
        "\n",
        "    if bool(model['prefix']):\n",
        "      if model['prefix'][-1] != ' ':\n",
        "        model['prefix'] = model['prefix'] + ' '\n",
        "    for p in updated_prompts:\n",
        "      pr = \"\"\n",
        "      images = None\n",
        "      usable_image = True\n",
        "      arg = {}\n",
        "      if type(p) == list or type(p) == str:\n",
        "        pr = model['prefix'] + p\n",
        "        arg = args.copy()\n",
        "      elif isinstance(p, Dream):\n",
        "        pr = model['prefix'] + p.prompt\n",
        "        arg = merge_dict(args, p.arg)\n",
        "      else: prt(f\"Unknown object {type(p)} in the prompt list\")\n",
        "      if arg['batch_size'] > 1:\n",
        "        pr = [pr] * arg['batch_size']\n",
        "        if bool(arg['negative_prompt']):\n",
        "          arg['negative_prompt'] = [arg['negative_prompt']] * arg['batch_size']\n",
        "      if last_seed != arg['seed']:\n",
        "        if arg['seed'] < 1 or arg['seed'] is None:\n",
        "          rand_seed = random.randint(0,2147483647)\n",
        "          if (not (prefs['use_Stability_api'] or (not status['installed_diffusers'] and status['installed_stability']))) and (not (prefs['use_AIHorde_api'] or (not status['installed_diffusers'] and status['installed_AIHorde']))):\n",
        "            if use_custom_scheduler:\n",
        "              generator = torch.manual_seed(rand_seed)\n",
        "            else:\n",
        "              generator = torch.Generator(\"cuda\").manual_seed(rand_seed)\n",
        "          arg['seed'] = rand_seed\n",
        "        else:\n",
        "          if (not(prefs['use_Stability_api'] or (not status['installed_diffusers'] and status['installed_stability']))) and (not (prefs['use_AIHorde_api'] or (not status['installed_diffusers'] and status['installed_AIHorde']))):\n",
        "            if use_custom_scheduler:\n",
        "              generator = torch.manual_seed(arg['seed'])\n",
        "            else:\n",
        "              generator = torch.Generator(\"cuda\").manual_seed(arg['seed'])\n",
        "        last_seed = arg['seed']\n",
        "      if prefs['centipede_prompts_as_init_images'] and last_image is not None:\n",
        "        arg['init_image'] = last_image\n",
        "      p_count = f'[{p_idx + 1} of {len(updated_prompts)}]  '\n",
        "      #if p_idx % 30 == 0 and p_idx > 1:\n",
        "      #  clear_output()\n",
        "      #  print(f\"{Color.BEIGE2}Cleared console display due to memory limit in console logging.  Images still saving.{Color.END}\")\n",
        "      #prt(Divider(height=6, thickness=2), update=False)\n",
        "      prt(Row([Text(p_count), Text(pr[0] if type(pr) == list else pr, expand=True, weight=FontWeight.BOLD), Text(f'seed: {arg[\"seed\"]}     ')]))\n",
        "      time.sleep(0.1)\n",
        "      page.auto_scrolling(False)\n",
        "      #prt(p_count + ('‚îÄ' * 90))\n",
        "      #prt(f'{pr[0] if type(pr) == list else pr} - seed:{arg[\"seed\"]}')\n",
        "      total_steps = arg['steps']\n",
        "      #if prefs['use_Stability_api'] or bool(arg['use_Stability'] or (not status['installed_diffusers'] and status['installed_stability'])):\n",
        "      if status['installed_stability'] and (not status['installed_diffusers'] or prefs['use_Stability_api']) and not (status['installed_AIHorde'] and prefs['use_AIHorde_api']):\n",
        "        print('use_Stability_api')\n",
        "        if not status['installed_stability']:\n",
        "          alert_msg(page, f\"ERROR: To use Stability-API, you must run the install it first and have proper API key\")\n",
        "          return\n",
        "        else:\n",
        "          prt('Stability API Diffusion ')# + ('‚îÄ' * 100))\n",
        "          #print(f'\"{SD_prompt}\", height={SD_height}, width={SD_width}, steps={SD_steps}, cfg_scale={SD_guidance_scale}, seed={SD_seed}, sampler={generation_sampler}')\n",
        "          #strikes = 0\n",
        "          images = []\n",
        "          arg['width'] = multiple_of_64(arg['width'])\n",
        "          arg['height'] = multiple_of_64(arg['height'])\n",
        "          prt(pb)\n",
        "          import stability_sdk.interfaces.gooseai.generation.generation_pb2 as generation\n",
        "          answers = response = None\n",
        "          \n",
        "          import requests\n",
        "          from io import BytesIO\n",
        "          import base64\n",
        "          api_host = os.getenv('API_HOST', 'https://api.stability.ai')\n",
        "          engine_id = prefs['model_checkpoint']# if prefs['model_checkpoint'] == \"stable-diffusion-v1-5\" else \"stable-diffusion-v1\"\n",
        "          url = f\"{api_host}/v1/generation/{engine_id}/\"#image-to-image\"\n",
        "          headers = {\n",
        "              'Content-Type': 'application/json',\n",
        "              'Accept': 'application/json',#'image/png',\n",
        "              'Authorization': prefs['Stability_api_key'],\n",
        "          }\n",
        "          payload = {\n",
        "              \"cfg_scale\": arg['guidance_scale'],\n",
        "              \"clip_guidance_preset\": prefs['clip_guidance_preset'],\n",
        "              \"height\": arg['height'],\n",
        "              \"width\": arg['width'],\n",
        "              \"sampler\": prefs['generation_sampler'],\n",
        "              \"samples\": arg['batch_size'],\n",
        "              \"seed\": arg['seed'],\n",
        "              \"steps\": arg['steps'],\n",
        "              \"text_prompts\": [\n",
        "                  {\n",
        "                      \"text\": pr[0] if type(pr) == list else pr,\n",
        "                      \"weight\": 1\n",
        "                  }\n",
        "              ],\n",
        "          }\n",
        "          if bool(arg['negative_prompt']):\n",
        "            payload['text_prompts'].append({\"text\": arg['negative_prompt'][0] if type(arg['negative_prompt']) == list else arg['negative_prompt'], \"weight\": -10})\n",
        "          if bool(arg['mask_image']) or (bool(arg['init_image']) and arg['alpha_mask']):\n",
        "            if not bool(arg['init_image']):\n",
        "              clear_last()\n",
        "              prt(f\"ERROR: You have not selected an init_image to go with your image mask..\")\n",
        "              continue\n",
        "            if arg['init_image'].startswith('http'):\n",
        "              i_response = requests.get(arg['init_image'])\n",
        "              init_img = PILImage.open(BytesIO(i_response.content)).convert(\"RGB\")\n",
        "            else:\n",
        "              if os.path.isfile(arg['init_image']):\n",
        "                init_img = PILImage.open(arg['init_image'])\n",
        "              else: \n",
        "                clear_last()\n",
        "                prt(f\"ERROR: Couldn't find your init_image {arg['init_image']}\")\n",
        "            init_img = init_img.resize((arg['width'], arg['height']))\n",
        "            buff = BytesIO()\n",
        "            init_img.save(buff, format=\"PNG\")\n",
        "            buff.seek(0)\n",
        "            img_str = io.BufferedReader(buff).read()\n",
        "            #init_image = preprocess(init_img)\n",
        "            if not arg['alpha_mask']:\n",
        "              if arg['mask_image'].startswith('http'):\n",
        "                i_response = requests.get(arg['mask_image'])\n",
        "                mask_img = PILImage.open(BytesIO(i_response.content)).convert(\"RGB\")\n",
        "              else:\n",
        "                if os.path.isfile(arg['mask_image']):\n",
        "                  mask_img = PILImage.open(arg['mask_image'])\n",
        "                else:\n",
        "                  clear_last()\n",
        "                  prt(f\"ERROR: Couldn't find your mask_image {arg['mask_image']}\")\n",
        "              mask = mask_img.resize((arg['width'], arg['height']))\n",
        "\n",
        "              buff = BytesIO()\n",
        "              mask.save(buff, format=\"PNG\")\n",
        "              buff.seek(0)\n",
        "              mask_str = io.BufferedReader(buff).read()\n",
        "            #payload[\"step_schedule_end\"] = 0.01\n",
        "            payload[\"step_schedule_start\"] = 1# - arg['init_image_strength']\n",
        "            files = {\n",
        "                'init_image': img_str,#base64.b64encode(init_img.tobytes()).decode(),#open(init_img, 'rb'),\n",
        "                #'mask_image': mask_str,\n",
        "                'mask_source': \"INIT_IMAGE_ALPHA\" if arg['alpha_mask'] else \"MASK_IMAGE_BLACK\" if arg['invert_mask'] else \"MASK_IMAGE_WHITE\",\n",
        "                'options': (None, json.dumps(payload)),\n",
        "            }\n",
        "            if not arg['alpha_mask']:\n",
        "              files['mask_image'] = mask_str\n",
        "            pipe_used = \"Stability-API Inpainting\"\n",
        "            #engine_id = prefs['model_checkpoint'] if prefs['model_checkpoint'] == \"stable-diffusion-v1-5\" else \"stable-diffusion-v1\"\n",
        "            response = requests.post(url+\"image-to-image/masking\", headers=headers, files=files)\n",
        "            #answers = stability_api.generate(prompt=pr, height=arg['height'], width=arg['width'], mask_image=mask, init_image=init_img, start_schedule= 1 - arg['init_image_strength'], steps=arg['steps'], cfg_scale=arg['guidance_scale'], samples=arg['batch_size'], safety=not prefs[\"disable_nsfw_filter\"], seed=arg['seed'], sampler=SD_sampler)\n",
        "          elif bool(arg['init_image']):\n",
        "            if arg['init_image'].startswith('http'):\n",
        "              i_response = requests.get(arg['init_image'])\n",
        "              init_img = PILImage.open(BytesIO(i_response.content)).convert(\"RGB\")\n",
        "            else:\n",
        "              if os.path.isfile(arg['init_image']):\n",
        "                init_img = PILImage.open(arg['init_image']).convert(\"RGB\")\n",
        "              else:\n",
        "                clear_last()\n",
        "                prt(f\"ERROR: Couldn't find your init_image {arg['init_image']}\")\n",
        "            init_img = init_img.resize((arg['width'], arg['height']))\n",
        "            \n",
        "            buff = BytesIO()\n",
        "            init_img.save(buff, format=\"PNG\")\n",
        "            buff.seek(0)\n",
        "            img_str = io.BufferedReader(buff).read()\n",
        "            #img_str = open(buff.read(), 'rb') #base64.b64encode(buff.getvalue())  init_img.tobytes(\"raw\")\n",
        "            payload[\"step_schedule_end\"] = 0.01\n",
        "            payload[\"step_schedule_start\"] = 1 - arg['init_image_strength']\n",
        "            files = {\n",
        "                'init_image': img_str,#base64.b64encode(init_img.tobytes()).decode(),#open(init_img, 'rb'),\n",
        "                'options': (None, json.dumps(payload)),\n",
        "            }\n",
        "            pipe_used = \"Stability-API Image-to-Image\"\n",
        "            response = requests.post(url+\"image-to-image\", headers=headers, files=files)\n",
        "            #answers = stability_api.generate(prompt=pr, height=arg['height'], width=arg['width'], init_image=init_img, start_schedule= 1 - arg['init_image_strength'], steps=arg['steps'], cfg_scale=arg['guidance_scale'], samples=arg['batch_size'], safety=not prefs[\"disable_nsfw_filter\"], seed=arg['seed'], sampler=SD_sampler)\n",
        "          else:\n",
        "            pipe_used = \"Stability-API Text-to-Image\"\n",
        "            response = requests.post(url+\"text-to-image\", headers=headers, json=payload)\n",
        "            #answers = stability_api.generate(prompt=pr, height=arg['height'], width=arg['width'], steps=arg['steps'], cfg_scale=arg['guidance_scale'], seed=arg['seed'], samples=arg['batch_size'], safety=False, sampler=SD_sampler)\n",
        "          clear_last(update=False)\n",
        "          clear_last()\n",
        "          if response != None:\n",
        "            if response.status_code != 200:\n",
        "              if response.status_code == 402:\n",
        "                alert_msg(page, \"Stability-API ERROR: Insufficient Credit Balance. Reload at DreamStudio.com...\", content=Text(str(response.text)))\n",
        "                return\n",
        "              else:\n",
        "                prt(Text(f\"Stability-API ERROR {response.status_code}: \" + str(response.text), selectable=True))\n",
        "                print(payload)\n",
        "                continue\n",
        "            #with open(output_file, \"wb\") as f:\n",
        "            #  f.write(response.content)\n",
        "            artifacts = json.loads(response.content)\n",
        "            for resp in artifacts['artifacts']:\n",
        "              #print(f'{type(resp)} - {resp[\"seed\"]}')\n",
        "              if resp == None: continue\n",
        "              images.append(PILImage.open(io.BytesIO(base64.b64decode(resp['base64']))))\n",
        "            #print(f'{type(response.content)} {response.content}')\n",
        "          if answers != None:\n",
        "            for resp in answers:\n",
        "              for artifact in resp.artifacts:\n",
        "                #print(\"Artifact reason: \" + str(artifact.finish_reason))\n",
        "                if artifact.finish_reason == generation.FILTER:         \n",
        "                  usable_image = False\n",
        "                if artifact.finish_reason == generation.ARTIFACT_TEXT:         \n",
        "                  usable_image = False\n",
        "                  prt(f\"Couldn't process NSFW text in prompt.  Can't retry so change your request.\")\n",
        "                if artifact.type == generation.ARTIFACT_IMAGE:\n",
        "                  images.append(PILImage.open(io.BytesIO(artifact.binary)))\n",
        "      elif prefs['use_AIHorde_api'] and status['installed_AIHorde']:# or bool(prefs['use_AIHorde_api'] or (not status['installed_diffusers'] and status['installed_AIHorde'])):\n",
        "        if not status['installed_AIHorde']:\n",
        "          alert_msg(page, f\"ERROR: To use AIHorde-API, you must run the install it first and have proper API key\")\n",
        "          return\n",
        "        stats = Text(\"Stable Horde API Diffusion \")\n",
        "        prt(stats)\n",
        "        #prt('Stable Horde API Diffusion ')# + ('‚îÄ' * 100))\n",
        "        #print(f'\"{SD_prompt}\", height={SD_height}, width={SD_width}, steps={SD_steps}, cfg_scale={SD_guidance_scale}, seed={SD_seed}, sampler={generation_sampler}')\n",
        "        #strikes = 0\n",
        "        images = []\n",
        "        arg['width'] = multiple_of_64(arg['width'])\n",
        "        arg['height'] = multiple_of_64(arg['height'])\n",
        "        prt(pb)\n",
        "        answers = None\n",
        "        response = None\n",
        "        \n",
        "        import requests\n",
        "        from io import BytesIO\n",
        "        import base64\n",
        "        try:\n",
        "            import cv2\n",
        "        except ModuleNotFoundError:\n",
        "            run_sp(\"pip install opencv-python\", realtime=False)\n",
        "            import cv2\n",
        "            pass\n",
        "        api_host = 'https://stablehorde.net/api'\n",
        "        engine_id = prefs['AIHorde_model']\n",
        "        api_check_url = f\"{api_host}/v2/generate/check/\"\n",
        "        api_get_result_url = f\"{api_host}/v2/generate/status/\"\n",
        "        url = f\"{api_host}/v2/generate/async\"\n",
        "        headers = {\n",
        "            #'Content-Type': 'application/json',\n",
        "            #'Accept': 'application/json',\n",
        "            'apikey': prefs['AIHorde_api_key'],\n",
        "        }\n",
        "        text_prompt = pr[0] if type(pr) == list else pr\n",
        "        if bool(arg['negative_prompt']):\n",
        "          text_prompt += \"###\" +arg['negative_prompt'][0] if type(arg['negative_prompt']) == list else arg['negative_prompt']\n",
        "        payload = {\n",
        "          \"prompt\": text_prompt,\n",
        "          \"nsfw\": prefs[\"disable_nsfw_filter\"],\n",
        "          \"models\": [prefs[\"AIHorde_model\"]]\n",
        "        }\n",
        "        params = {\n",
        "          \"cfg_scale\": arg['guidance_scale'],\n",
        "          \"denoising_strength\": arg['init_image_strength'],\n",
        "          \"width\": arg['width'],\n",
        "          \"height\": arg['height'],\n",
        "          \"sampler_name\": prefs['AIHorde_sampler'],\n",
        "          \"n\": arg['batch_size'],\n",
        "          \"seed\": str(arg['seed']),\n",
        "          \"steps\": arg['steps'],\n",
        "        }\n",
        "        if prefs['AIHorde_post_processing'] != \"None\":\n",
        "          params['post_processing'] = [prefs['AIHorde_post_processing']]\n",
        "        if bool(arg['mask_image']) or (bool(arg['init_image']) and arg['alpha_mask']):\n",
        "          if not bool(arg['init_image']):\n",
        "            clear_last()\n",
        "            prt(f\"ERROR: You have not selected an init_image to go with your image mask..\")\n",
        "            continue\n",
        "          if arg['init_image'].startswith('http'):\n",
        "            i_response = requests.get(arg['init_image'])\n",
        "            init_img = PILImage.open(BytesIO(i_response.content)).convert(\"RGB\")\n",
        "          else:\n",
        "            if os.path.isfile(arg['init_image']):\n",
        "              init_img = PILImage.open(arg['init_image'])\n",
        "            else: \n",
        "              clear_last()\n",
        "              prt(f\"ERROR: Couldn't find your init_image {arg['init_image']}\")\n",
        "          init_img = init_img.resize((arg['width'], arg['height']))\n",
        "          buff = BytesIO()\n",
        "          init_img.save(buff, format=\"PNG\")\n",
        "          buff.seek(0)\n",
        "          img_str = io.BufferedReader(buff).read()\n",
        "          #init_image = preprocess(init_img)\n",
        "          if not arg['alpha_mask']:\n",
        "            if arg['mask_image'].startswith('http'):\n",
        "              i_response = requests.get(arg['mask_image'])\n",
        "              mask_img = PILImage.open(BytesIO(i_response.content)).convert(\"RGB\")\n",
        "            else:\n",
        "              if os.path.isfile(arg['mask_image']):\n",
        "                mask_img = PILImage.open(arg['mask_image'])\n",
        "              else:\n",
        "                clear_last()\n",
        "                prt(f\"ERROR: Couldn't find your mask_image {arg['mask_image']}\")\n",
        "            mask = mask_img.resize((arg['width'], arg['height']))\n",
        "\n",
        "            buff = BytesIO()\n",
        "            mask.save(buff, format=\"PNG\")\n",
        "            buff.seek(0)\n",
        "            mask_str = io.BufferedReader(buff).read()\n",
        "          payload['source_image'] = img_str\n",
        "          if not arg['alpha_mask']:\n",
        "            payload['source_mask'] = mask_str\n",
        "          pipe_used = \"Stable Horde-API Inpainting\"\n",
        "          payload['source_processing'] = \"inpainting\"\n",
        "          #engine_id = prefs['model_checkpoint'] if prefs['model_checkpoint'] == \"stable-diffusion-v1-5\" else \"stable-diffusion-v1\"\n",
        "          #response = requests.post(url+\"image-to-image/masking\", headers=headers, files=files)\n",
        "          #answers = stability_api.generate(prompt=pr, height=arg['height'], width=arg['width'], mask_image=mask, init_image=init_img, start_schedule= 1 - arg['init_image_strength'], steps=arg['steps'], cfg_scale=arg['guidance_scale'], samples=arg['batch_size'], safety=not prefs[\"disable_nsfw_filter\"], seed=arg['seed'], sampler=SD_sampler)\n",
        "        elif bool(arg['init_image']):\n",
        "          if arg['init_image'].startswith('http'):\n",
        "            i_response = requests.get(arg['init_image'])\n",
        "            init_img = PILImage.open(BytesIO(i_response.content)).convert(\"RGB\")\n",
        "          else:\n",
        "            if os.path.isfile(arg['init_image']):\n",
        "              init_img = PILImage.open(arg['init_image']).convert(\"RGB\")\n",
        "            else:\n",
        "              clear_last()\n",
        "              prt(f\"ERROR: Couldn't find your init_image {arg['init_image']}\")\n",
        "          init_img = init_img.resize((arg['width'], arg['height']))\n",
        "          \n",
        "          buff = BytesIO()\n",
        "          init_img.save(buff, format=\"PNG\")\n",
        "          buff.seek(0)\n",
        "          img_str = io.BufferedReader(buff).read()\n",
        "          #img_str = open(buff.read(), 'rb') #base64.b64encode(buff.getvalue())  init_img.tobytes(\"raw\")\n",
        "          payload['source_image'] = img_str\n",
        "          pipe_used = \"Stable Horde-API Image-to-Image\"\n",
        "          payload['source_processing'] = \"img2img\"\n",
        "          #answers = stability_api.generate(prompt=pr, height=arg['height'], width=arg['width'], init_image=init_img, start_schedule= 1 - arg['init_image_strength'], steps=arg['steps'], cfg_scale=arg['guidance_scale'], samples=arg['batch_size'], safety=not prefs[\"disable_nsfw_filter\"], seed=arg['seed'], sampler=SD_sampler)\n",
        "        else:\n",
        "          pipe_used = \"Stable Horde-API Text-to-Image\"\n",
        "          #response = requests.post(url+\"text-to-image\", headers=headers, json=payload)\n",
        "          #answers = stability_api.generate(prompt=pr, height=arg['height'], width=arg['width'], steps=arg['steps'], cfg_scale=arg['guidance_scale'], seed=arg['seed'], samples=arg['batch_size'], safety=False, sampler=SD_sampler)\n",
        "        payload[\"params\"] = params\n",
        "        #print(params)\n",
        "        response = requests.post(url, headers=headers, json=payload)\n",
        "        \n",
        "        if response != None:\n",
        "          if response.status_code != 202:\n",
        "            if response.status_code == 400:\n",
        "              alert_msg(page, \"Stable Horde-API ERROR: Validation Error...\", content=Text(str(response.text)))\n",
        "              return\n",
        "            else:\n",
        "              prt(Text(f\"Stable Horde-API ERROR {response.status_code}: \" + str(response.text), selectable=True))\n",
        "              print(payload)\n",
        "              continue\n",
        "          #with open(output_file, \"wb\") as f:\n",
        "          #  f.write(response.content)\n",
        "        \n",
        "        artifacts = json.loads(response.content)\n",
        "        q_id = artifacts['id']\n",
        "        #print(str(artifacts))\n",
        "        #stats = Text(\"\")\n",
        "        #prt(stats)\n",
        "        elapsed_seconds = 0\n",
        "        try:\n",
        "          while True:\n",
        "            check_response = requests.get(api_check_url + q_id)\n",
        "            check = json.loads(check_response.content)\n",
        "            div = check['wait_time'] + elapsed_seconds\n",
        "            if div == 0: continue\n",
        "            try:\n",
        "              percentage = (1 - check['wait_time'] / div)\n",
        "            except Exception:\n",
        "              continue\n",
        "              pass\n",
        "            pb.value = percentage\n",
        "            pb.update()\n",
        "            status_txt = f\"Stable Horde API Diffusion - Queued Position: {check['queue_position']} - Waiting: {check['waiting']} - Wait Time: {check['wait_time']}\"\n",
        "            stats.value = status_txt #str(check)\n",
        "            stats.update()\n",
        "            if bool(check['done']):\n",
        "              break\n",
        "            time.sleep(1)\n",
        "            elapsed_seconds += 1\n",
        "        except Exception as e:\n",
        "          alert_msg(page, f\"EXCEPTION ERROR: Unknown error processing image. Check parameters and try again. Restart app if persists.\", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip(), selectable=True)]))\n",
        "          return\n",
        "        get_response = requests.get(api_get_result_url + q_id)\n",
        "        final_results = json.loads(get_response.content)\n",
        "        clear_last(update=False)\n",
        "        clear_last()\n",
        "        for gen in final_results[\"generations\"]:\n",
        "          #print(f'{type(resp)} - {resp[\"seed\"]}')\n",
        "          if gen[\"censored\"]:\n",
        "            prt(f\"Couldn't process NSFW text in prompt.  Can't retry so change your request.\")\n",
        "            continue\n",
        "          img_response = requests.get(gen['img'])\n",
        "          webp_file = io.BytesIO(img_response.content)\n",
        "          cv_img = cv2.imdecode(np.frombuffer(webp_file.read(), np.uint8), cv2.IMREAD_UNCHANGED)\n",
        "          images.append(PILImage.fromarray(cv_img))\n",
        "          #cv_img = cv2.imdecode(io.BytesIO(base64.b64decode(gen['img'])), cv2.IMREAD_COLOR)\n",
        "          #cv_img = cv2.imdecode(np.frombuffer(base64.b64decode(gen['img']), dtype=np.uint8), cv2.IMREAD_UNCHANGED)\n",
        "          #decoded_string = base64.b64decode(gen['img'])\n",
        "          #nparr = np.frombuffer(decoded_string, np.uint8)\n",
        "          #cv_img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n",
        "          #print(gen['img'])\n",
        "          #print(str(type(gen['img'])))\n",
        "          #img_bytes = io.BytesIO(decoded_string)\n",
        "          #img = Image.open(io.BytesIO(img_response.content))\n",
        "          #pil_image = PILImage.open(webp_file)\n",
        "          #images.append(PILImage.open(io.BytesIO(img_response.content)))\n",
        "          #images.append(PILImage.open(pil_image))\n",
        "          #images.append(PILImage.open(img_response.content))\n",
        "          #images.append(PILImage.open(io.BytesIO(base64.b64decode(gen['img']))).convert('RGB'))\n",
        "          #print(f'{type(response.content)} {response.content}')\n",
        "          '''if answers != None:\n",
        "            for resp in answers:\n",
        "              for artifact in resp.artifacts:\n",
        "                #print(\"Artifact reason: \" + str(artifact.finish_reason))\n",
        "                if artifact.finish_reason == generation.FILTER:         \n",
        "                  usable_image = False\n",
        "                if artifact.finish_reason == generation.ARTIFACT_TEXT:         \n",
        "                  usable_image = False\n",
        "                  prt(f\"Couldn't process NSFW text in prompt.  Can't retry so change your request.\")\n",
        "                if artifact.type == generation.ARTIFACT_IMAGE:\n",
        "                  images.append(PILImage.open(io.BytesIO(artifact.binary)))'''\n",
        "      else:\n",
        "        #from torch.amp.autocast_mode import autocast\n",
        "        #precision_scope = autocast if prefs['precision']==\"autocast\" else nullcontext\n",
        "        try:\n",
        "          if use_custom_scheduler and not bool(arg['init_image']) and not bool(arg['mask_image']) and not bool(arg['prompt2']):\n",
        "            # Not implemented correctly anymore, old code but might reuse custom\n",
        "            text_input = tokenizer(pr[0] if type(pr) == list else pr, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
        "            with torch.no_grad():\n",
        "              text_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]# We'll also get the unconditional text embeddings for classifier-free guidance, which are just the embeddings for the padding token (empty text). They need to have the same shape as the conditional text_embeddings (batch_size and seq_length)\n",
        "            max_length = text_input.input_ids.shape[-1]\n",
        "            uncond_input = tokenizer([\"\"] * arg['batch_size'], padding=\"max_length\", max_length=max_length, return_tensors=\"pt\")\n",
        "            with torch.no_grad():\n",
        "              uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0]   #For classifier-free guidance, we need to do two forward passes. One with the conditioned input (`text_embeddings`), and another with the unconditional embeddings (`uncond_embeddings`). In practice, we can concatenate both into a single batch to avoid doing two forward passes.\n",
        "\n",
        "            text_embeddings = torch.cat([uncond_embeddings, text_embeddings])#Generate the intial random noise.\n",
        "            #if generator:\n",
        "            #latents = torch.randn((arg['batch_size'], unet.in_channels, arg['height'], arg['width']), generator=generator)\n",
        "            latents = torch.randn((arg['batch_size'], unet.in_channels, arg['height'] // 8,  arg['width'] // 8), generator=generator)\n",
        "            #else:\n",
        "            #  latents = torch.randn((batch_size, unet.in_channels, arg['height'] // 8, arg['width'] // 8))\n",
        "            latents = latents.to(torch_device)\n",
        "            latents.shape\n",
        "            #Cool  64√ó64  is expected. The model will transform this latent representation (pure noise) into a 512 √ó 512 image later on.\n",
        "            #Next, we initialize the scheduler with our chosen num_inference_steps. This will compute the sigmas and exact time step values to be used during the denoising process.\n",
        "            scheduler.set_timesteps(arg['steps'])#The LMS Discrete scheduler needs to multiple the `latents` by its `sigma` values. Let's do this here\n",
        "            if prefs['scheduler_mode'] == \"LMS Discrete\" or prefs['scheduler_mode'] == \"Score-SDE-Vp\":\n",
        "              latents = latents * scheduler.sigmas[0]#We are ready to write the denoising loop.\n",
        "            from tqdm.auto import tqdm\n",
        "            clear_pipes(\"unet\")\n",
        "            if unet is None:\n",
        "              unet = get_unet_pipe()\n",
        "            #with precision_scope(\"cuda\"):\n",
        "            #with autocast(\"cuda\"):\n",
        "            for i, t in tqdm(enumerate(scheduler.timesteps)):\n",
        "              # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
        "              latent_model_input = torch.cat([latents] * 2)\n",
        "              if prefs['scheduler_mode'] == \"LMS Discrete\" or prefs['scheduler_mode'] == \"Score-SDE-Vp\":\n",
        "                sigma = scheduler.sigmas[i]\n",
        "                latent_model_input = latent_model_input / ((sigma**2 + 1) ** 0.5)\n",
        "              # predict the noise residual\n",
        "              if prefs['scheduler_mode'] == \"DDPM\":\n",
        "                #TODO: Work in progress, still not perfect\n",
        "                noisy_sample = torch.randn(1, unet.config.in_channels, unet.config.sample_size, unet.config.sample_size)\n",
        "                noisy_residual = unet(sample=noisy_sample, timestep=2)[\"sample\"]\n",
        "                less_noisy_sample = scheduler.step(model_output=noisy_residual, timestep=2, sample=noisy_sample)[\"prev_sample\"]\n",
        "                less_noisy_sample.shape\n",
        "              with torch.no_grad():\n",
        "                noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings).images\n",
        "              # perform guidance\n",
        "              noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
        "              noise_pred = noise_pred_uncond + arg['guidance_scale'] * (noise_pred_text - noise_pred_uncond)\n",
        "              # compute the previous noisy sample x_t -> x_t-1\n",
        "              latents = scheduler.step(noise_pred, i, latents)[\"prev_sample\"]#We now use the vae to decode the generated latents back into the image.\n",
        "            # scale and decode the image latents with vae\n",
        "            latents = 1 / 0.18215 * latents\n",
        "            with torch.no_grad():\n",
        "              image = vae.decode(latents)\n",
        "            image = (image / 2 + 0.5).clip(0, 1)\n",
        "            image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
        "            uint8_images = (image * 255).round().astype(\"uint8\")\n",
        "            #for img in uint8_images: images.append(Image.fromarray(img))\n",
        "            images = [PILImage.fromarray(img) for img in uint8_images]\n",
        "          else:\n",
        "            if bool(arg['use_clip_guided_model']) and status['installed_clip']:\n",
        "              if bool(arg['init_image']) or bool(arg['mask_image']):\n",
        "                #raise ValueError(\"Cannot use CLIP Guided Model with init or mask image yet.\")\n",
        "                alert_msg(page, \"Cannot use CLIP Guided Model with init or mask image yet.\")\n",
        "                return\n",
        "              clear_pipes(\"clip_guided\")\n",
        "              if pipe_clip_guided is None:\n",
        "                prt(Installing(\"Initializing CLIP-Guided Pipeline...\"))\n",
        "                pipe_clip_guided = get_clip_guided_pipe()\n",
        "                clear_last()\n",
        "              clip_prompt = arg[\"clip_prompt\"] if arg[\"clip_prompt\"].strip() != \"\" else None\n",
        "              if bool(arg[\"unfreeze_unet\"]):\n",
        "                pipe_clip_guided.unfreeze_unet()\n",
        "              else:\n",
        "                pipe_clip_guided.freeze_unet()\n",
        "              if bool(arg[\"unfreeze_vae\"]):\n",
        "                pipe_clip_guided.unfreeze_vae()\n",
        "              else:\n",
        "                pipe_clip_guided.freeze_vae()\n",
        "              # TODO: Figure out why it's broken with use_cutouts=False and doesn't generate, hacking it True for now\n",
        "              arg[\"use_cutouts\"] = True \n",
        "              prt(pb)\n",
        "              page.auto_scrolling(False)\n",
        "              pipe_used = \"CLIP Guided\"\n",
        "              images = pipe_clip_guided(pr, height=arg['height'], width=arg['width'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], clip_prompt=clip_prompt, clip_guidance_scale=arg[\"clip_guidance_scale\"], num_cutouts=int(arg[\"num_cutouts\"]) if arg[\"use_cutouts\"] else None, use_cutouts=arg[\"use_cutouts\"], generator=generator).images\n",
        "              clear_last()\n",
        "              page.auto_scrolling(True)\n",
        "            elif bool(prefs['use_conceptualizer']) and status['installed_conceptualizer']:\n",
        "              clear_pipes(\"conceptualizer\")\n",
        "              if pipe_conceptualizer is None:\n",
        "                prt(Installing(\"Initializing Conceptualizer Pipeline...\"))\n",
        "                pipe_conceptualizer = get_conceptualizer(page)\n",
        "                clear_last()\n",
        "              total_steps = arg['steps']\n",
        "              prt(pb)\n",
        "              page.auto_scrolling(False)\n",
        "              pipe_used = f\"Conceptualizer {prefs['concepts_model']}\"\n",
        "              images = pipe_conceptualizer(prompt=pr, negative_prompt=arg['negative_prompt'], height=arg['height'], width=arg['width'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback=callback_fn, callback_steps=1).images\n",
        "              clear_last()\n",
        "              page.auto_scrolling(True)\n",
        "            elif bool(arg['mask_image']) or (not bool(arg['mask_image']) and bool(arg['init_image']) and bool(arg['alpha_mask'])):\n",
        "              if not bool(arg['init_image']):\n",
        "                alert_msg(page, f\"ERROR: You have not selected an init_image to go with your image mask..\")\n",
        "                return\n",
        "              #if pipe_inpainting is None:\n",
        "              #  pipe_inpainting = get_inpainting_pipe()\n",
        "              if prefs['use_inpaint_model'] and status['installed_img2img']:\n",
        "                clear_pipes(\"img2img\")\n",
        "                if pipe_img2img is None:\n",
        "                  prt(Installing(\"Initializing Inpaint Pipeline...\"))\n",
        "                  pipe_img2img = get_img2img_pipe()\n",
        "                  clear_last()\n",
        "              else:\n",
        "                clear_pipes(\"txt2img\")\n",
        "                if pipe is None:\n",
        "                  prt(Installing(\"Initializing Long Prompt Weighting Inpaint Pipeline...\"))\n",
        "                  pipe = get_txt2img_pipe()\n",
        "                  clear_last()\n",
        "              '''if pipe_img2img is None:\n",
        "                try:\n",
        "                  pipe_img2img = get_img2img_pipe()\n",
        "                except NameError:\n",
        "                  prt(f\"{Color.RED}You must install the image2image Pipeline above.{Color.END}\")\n",
        "                finally:\n",
        "                  raise NameError(\"You must install the image2image Pipeline above\")'''\n",
        "              import requests\n",
        "              from io import BytesIO\n",
        "              if arg['init_image'].startswith('http'):\n",
        "                response = requests.get(arg['init_image'])\n",
        "                init_img = PILImage.open(BytesIO(response.content))\n",
        "              else:\n",
        "                if os.path.isfile(arg['init_image']):\n",
        "                  init_img = PILImage.open(arg['init_image'])\n",
        "                else: prt(f\"ERROR: Couldn't find your init_image {arg['init_image']}\")\n",
        "              if bool(arg['alpha_mask']):\n",
        "                init_img = init_img.convert(\"RGBA\")\n",
        "              else:\n",
        "                init_img = init_img.convert(\"RGB\")\n",
        "              init_img = init_img.resize((arg['width'], arg['height']))\n",
        "              #init_image = preprocess(init_img)\n",
        "              mask_img = None\n",
        "              if not bool(arg['mask_image']) and bool(arg['alpha_mask']):\n",
        "                mask_img = init_img.convert('RGBA')\n",
        "                red, green, blue, alpha = PILImage.Image.split(init_img)\n",
        "                mask_img = alpha.convert('L')\n",
        "              else:\n",
        "                if arg['mask_image'].startswith('http'):\n",
        "                  response = requests.get(arg['mask_image'])\n",
        "                  mask_img = PILImage.open(BytesIO(response.content))\n",
        "                else:\n",
        "                  if os.path.isfile(arg['mask_image']):\n",
        "                    mask_img = PILImage.open(arg['mask_image'])\n",
        "                  else: prt(f\"ERROR: Couldn't find your mask_image {arg['mask_image']}\")\n",
        "              if arg['invert_mask'] and not arg['alpha_mask']:\n",
        "                from PIL import ImageOps\n",
        "                mask_img = ImageOps.invert(mask_img.convert('RGB'))\n",
        "              mask_img = mask_img.convert(\"L\")\n",
        "              mask_img = mask_img.resize((arg['width'], arg['height']), resample=PILImage.Resampling.LANCZOS).convert(\"RGB\")\n",
        "              #mask = mask_img.resize((arg['width'], arg['height']))\n",
        "              #mask = np.array(mask).astype(np.float32) / 255.0\n",
        "              #mask = np.tile(mask,(4,1,1))\n",
        "              #mask = mask[None].transpose(0, 1, 2, 3)\n",
        "              #mask[np.where(mask != 0.0 )] = 1.0 #make sure mask is actually valid\n",
        "              #mask_img = torch.from_numpy(mask)\n",
        "              prt(pb)\n",
        "              page.auto_scrolling(False)\n",
        "              #with autocast(\"cuda\"):\n",
        "              if prefs['use_inpaint_model'] and status['installed_img2img']:\n",
        "                pipe_used = \"Diffusers Inpaint\"\n",
        "                images = pipe_img2img(prompt=pr, negative_prompt=arg['negative_prompt'], mask_image=mask_img, image=init_img, strength= 1 - arg['init_image_strength'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback=callback_fn, callback_steps=1).images\n",
        "              else:\n",
        "                pipe_used = \"Long Prompt Weight Inpaint\"\n",
        "                images = pipe.inpaint(prompt=pr, negative_prompt=arg['negative_prompt'], mask_image=mask_img, image=init_img, strength= 1 - arg['init_image_strength'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback=callback_fn, callback_steps=1).images\n",
        "              clear_last()\n",
        "              page.auto_scrolling(True)\n",
        "            elif bool(arg['init_image']):\n",
        "              if not status['installed_txt2img'] and not (prefs['use_imagic'] and status['installed_imagic']) and not (prefs['use_depth2img'] and status['installed_depth2img']) and not (prefs['use_alt_diffusion'] and status['installed_alt_diffusion']):\n",
        "                alert_msg(page, f\"CRITICAL ERROR: You have not installed a image2image pipeline yet.  Run in the Installer..\")\n",
        "                continue\n",
        "              if prefs['use_versatile'] and status['installed_versatile']:\n",
        "                if len(pr.strip()) > 2: # Find another way to know the difference\n",
        "                  clear_pipes(\"versatile_dualguided\")\n",
        "                  if pipe_versatile_dualguided is None:\n",
        "                    prt(Installing(\"Initializing Versatile Dual-Guided Pipeline...\"))\n",
        "                    pipe_versatile_dualguided = get_versatile_dualguided_pipe()\n",
        "                    clear_last()\n",
        "                else:\n",
        "                  clear_pipes(\"versatile_variation\")\n",
        "                  if pipe_versatile_variation is None:\n",
        "                    prt(Installing(\"Initializing Versatile Image Variation Pipeline...\"))\n",
        "                    pipe_versatile_variation = get_versatile_variation_pipe()\n",
        "                    clear_last()\n",
        "              elif prefs['use_alt_diffusion'] and status['installed_alt_diffusion']:\n",
        "                clear_pipes(\"alt_diffusion_img2img\")\n",
        "                if pipe_alt_diffusion_img2img is None:\n",
        "                  prt(Installing(\"Initializing AltDiffusion Image2Image Pipeline...\"))\n",
        "                  pipe_alt_diffusion_img2img = get_alt_diffusion_img2img_pipe()\n",
        "                  clear_last()\n",
        "              elif prefs['use_depth2img'] and status['installed_depth2img']:\n",
        "                clear_pipes(\"depth\")\n",
        "                if pipe_depth is None:\n",
        "                  prt(Installing(\"Initializing SD2 Depth2Image Pipeline...\"))\n",
        "                  pipe_depth = get_depth_pipe()\n",
        "                  clear_last()\n",
        "              elif prefs['use_inpaint_model'] and status['installed_img2img']:\n",
        "                clear_pipes(\"img2img\")\n",
        "                if pipe_img2img is None:\n",
        "                  prt(Installing(\"Initializing Inpaint Pipeline...\"))\n",
        "                  pipe_img2img = get_img2img_pipe()\n",
        "                  clear_last()\n",
        "              elif prefs['use_imagic'] and status['installed_imagic']:\n",
        "                clear_pipes(\"imagic\")\n",
        "                if pipe_imagic is None:\n",
        "                  prt(Installing(\"Initializing iMagic Image2Image Pipeline...\"))\n",
        "                  pipe_imagic = get_imagic_pipe()\n",
        "                  clear_last()\n",
        "              else:\n",
        "                clear_pipes(\"txt2img\")\n",
        "                if pipe is None:\n",
        "                  prt(Installing(\"Initializing Long Prompt Weighting Image2Image Pipeline...\"))\n",
        "                  pipe = get_txt2img_pipe()\n",
        "                  clear_last()\n",
        "              '''if pipe_img2img is None:\n",
        "                try:\n",
        "                  pipe_img2img = get_img2img_pipe()\n",
        "                except NameError:\n",
        "                  prt(f\"{Color.RED}You must install the image2image Pipeline above.{Color.END}\")\n",
        "                  raise NameError(\"You must install the image2image Pipeline above\")'''\n",
        "                #finally:\n",
        "              import requests\n",
        "              from io import BytesIO\n",
        "              if arg['init_image'].startswith('http'):\n",
        "                response = requests.get(arg['init_image'])\n",
        "                init_img = PILImage.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "              else:\n",
        "                if os.path.isfile(arg['init_image']):\n",
        "                  init_img = PILImage.open(arg['init_image']).convert(\"RGB\")\n",
        "                else: alert_msg(page, f\"ERROR: Couldn't find your init_image {arg['init_image']}\")\n",
        "              init_img = init_img.resize((arg['width'], arg['height']))\n",
        "              #init_image = preprocess(init_img)\n",
        "              #white_mask = PILImage.new(\"RGB\", (arg['width'], arg['height']), (255, 255, 255))\n",
        "              prt(pb)\n",
        "              page.auto_scrolling(False)\n",
        "              #with autocast(\"cuda\"):\n",
        "              #images = pipe_img2img(prompt=pr, negative_prompt=arg['negative_prompt'], init_image=init_img, mask_image=white_mask, strength= 1 - arg['init_image_strength'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback=callback_fn, callback_steps=1).images\n",
        "              if prefs['use_versatile'] and status['installed_versatile']:\n",
        "                if len(pr.strip()) > 2:\n",
        "                  pipe_used = \"Versatile Dual-Guided\"\n",
        "                  images = pipe_versatile_dualguided(prompt=pr, negative_prompt=arg['negative_prompt'], image=init_img, text_to_image_strength= arg['init_image_strength'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback=callback_fn, callback_steps=1).images\n",
        "                else:\n",
        "                  pipe_used = \"Versatile Variation\"\n",
        "                  images = pipe_versatile_variation(negative_prompt=arg['negative_prompt'], image=init_img, num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback=callback_fn, callback_steps=1).images\n",
        "              elif prefs['use_alt_diffusion'] and status['installed_alt_diffusion']:\n",
        "                pipe_used = \"AltDiffusion Image-to-Image\"\n",
        "                with torch.autocast(\"cuda\"):\n",
        "                  images = pipe_alt_diffusion_img2img(prompt=pr, negative_prompt=arg['negative_prompt'], image=init_img, strength= 1 - arg['init_image_strength'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback=callback_fn, callback_steps=1).images\n",
        "              elif prefs['use_depth2img'] and status['installed_depth2img']:\n",
        "                pipe_used = \"Depth-to-Image\"\n",
        "                images = pipe_depth(prompt=pr, negative_prompt=arg['negative_prompt'], image=init_img, strength=arg['init_image_strength'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback=callback_fn, callback_steps=1).images\n",
        "              elif prefs['use_inpaint_model'] and status['installed_img2img']:\n",
        "                pipe_used = \"Diffusers Inpaint Image-to-Image\"\n",
        "                white_mask = PILImage.new(\"RGB\", (arg['width'], arg['height']), (255, 255, 255))\n",
        "                images = pipe_img2img(prompt=pr, negative_prompt=arg['negative_prompt'], image=init_img, mask_image=white_mask, strength= 1 - arg['init_image_strength'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback=callback_fn, callback_steps=1).images\n",
        "              elif prefs['use_imagic'] and status['installed_imagic']:\n",
        "                pipe_used = \"iMagic Image-to-Image\"\n",
        "                #only one element tensors can be converted to Python scalars\n",
        "                total_steps = None\n",
        "                res = pipe_imagic.train(pr, init_img, num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback=callback_fn, callback_steps=1).images\n",
        "                images = []\n",
        "                # TODO: alpha= arguments to customize which to make\n",
        "                total_steps = 0\n",
        "                res = pipe_imagic(alpha=1, callback=callback_fn, callback_steps=1)\n",
        "                images.append(res.images[0])\n",
        "                res = pipe_imagic(alpha=1.5, callback=callback_fn, callback_steps=1)\n",
        "                images.append(res.images[0])\n",
        "                res = pipe_imagic(alpha=2, callback=callback_fn, callback_steps=1)\n",
        "                images.append(res.images[0])\n",
        "              else:\n",
        "                pipe_used = \"Long Prompt Weight Image-to-Image\"\n",
        "                images = pipe.img2img(prompt=pr, negative_prompt=arg['negative_prompt'], image=init_img, strength= 1 - arg['init_image_strength'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback=callback_fn, callback_steps=1).images\n",
        "              clear_last()\n",
        "              page.auto_scrolling(True)\n",
        "            elif bool(arg['prompt2']):\n",
        "              if pipe is None:\n",
        "                pipe = get_txt2img_pipe()\n",
        "              #with precision_scope(\"cuda\"):\n",
        "              #    with torch.no_grad():\n",
        "              pipe_used = \"LPW Tween Lerp\"\n",
        "              images_tween = pipe.lerp_between_prompts(pr, arg[\"prompt2\"], length = arg['tweens'], save=False, height=arg['height'], width=arg['width'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator)\n",
        "              #print(str(images_tween))\n",
        "              images = images_tween['images']\n",
        "              #images = pipe(pr, height=arg['height'], width=arg['width'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator)[\"sample\"]\n",
        "            else:\n",
        "              if prefs['use_composable'] and status['installed_composable']:\n",
        "                clear_pipes(\"composable\")\n",
        "                if pipe_composable is None:\n",
        "                  prt(Installing(\"Initializing Composable Text2Image Pipeline...\"))\n",
        "                  pipe_composable = get_composable_pipe()\n",
        "                  clear_last()\n",
        "              elif prefs['use_alt_diffusion'] and status['installed_alt_diffusion']:\n",
        "                clear_pipes(\"alt_diffusion\")\n",
        "                if pipe_alt_diffusion is None:\n",
        "                  prt(Installing(\"Initializing AltDiffusion Text2Image Pipeline...\"))\n",
        "                  pipe_alt_diffusion = get_alt_diffusion_pipe()\n",
        "                  clear_last()\n",
        "              elif prefs['use_SAG'] and status['installed_SAG']:\n",
        "                clear_pipes(\"SAG\")\n",
        "                if pipe_SAG is None:\n",
        "                  prt(Installing(\"Initializing Self-Attention Guidance Text2Image Pipeline...\"))\n",
        "                  pipe_SAG = get_SAG_pipe()\n",
        "                  clear_last()\n",
        "              elif prefs['use_attend_and_excite'] and status['installed_attend_and_excite']:\n",
        "                clear_pipes(\"attend_and_excite\")\n",
        "                if pipe_attend_and_excite is None:\n",
        "                  prt(Installing(\"Initializing Attend and Excite Text2Image Pipeline...\"))\n",
        "                  pipe_attend_and_excite = get_attend_and_excite_pipe()\n",
        "                  clear_last()\n",
        "              elif prefs['use_versatile'] and status['installed_versatile']:\n",
        "                clear_pipes(\"versatile_text2img\")\n",
        "                if pipe_versatile_text2img is None:\n",
        "                  prt(Installing(\"Initializing Versatile Text2Image Pipeline...\"))\n",
        "                  pipe_versatile_text2img = get_versatile_text2img_pipe()\n",
        "                  clear_last()\n",
        "              elif prefs['use_safe'] and status['installed_safe']:\n",
        "                clear_pipes(\"safe\")\n",
        "                if pipe_safe is None:\n",
        "                  prt(Installing(\"Initializing Safe Stable Diffusion Pipeline...\"))\n",
        "                  pipe_safe = get_safe_pipe()\n",
        "                  clear_last()\n",
        "              elif prefs['use_panorama'] and status['installed_panorama']:\n",
        "                clear_pipes(\"panorama\")\n",
        "                if pipe_panorama is None:\n",
        "                  prt(Installing(\"Initializing Panorama MultiDiffusion Pipeline...\"))\n",
        "                  pipe_panorama = get_panorama_pipe()\n",
        "                  clear_last()\n",
        "              elif pipe is None:\n",
        "                clear_pipes(\"txt2img\")\n",
        "                prt(Installing(\"Initializing Long Prompt Weighting Text2Image Pipeline...\"))\n",
        "                pipe = get_txt2img_pipe()\n",
        "                clear_last()\n",
        "              '''with io.StringIO() as buf, redirect_stdout(buf):\n",
        "                get_text2image(page)\n",
        "                output = buf.getvalue()\n",
        "                page.Images.content.controls.append(Text(output.strip())\n",
        "                page.Images.content.update()\n",
        "                page.Images.update()\n",
        "                page.update()'''\n",
        "              total_steps = arg['steps']\n",
        "              prt(pb)\n",
        "              page.auto_scrolling(False)\n",
        "              if prefs['use_composable'] and status['installed_composable']:\n",
        "                weights = arg['negative_prompt'] #\" 1 | 1\"  # Equal weight to each prompt. Can be negative\n",
        "                if not bool(weights):\n",
        "                  segments = len(pr.split('|'))\n",
        "                  weights = '|'.join(['1' * segments])\n",
        "                pipe_used = \"Composable Text-to-Image\"\n",
        "                images = pipe_composable(pr, height=arg['height'], width=arg['width'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], weights=weights, generator=generator, callback=callback_fn, callback_steps=1).images\n",
        "              elif prefs['use_alt_diffusion'] and status['installed_alt_diffusion']:\n",
        "                pipe_used = \"AltDiffusion Text-to-Image\"\n",
        "                with torch.autocast(\"cuda\"):\n",
        "                  images = pipe_alt_diffusion(prompt=pr, negative_prompt=arg['negative_prompt'], height=arg['height'], width=arg['width'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback=callback_fn, callback_steps=1).images\n",
        "              elif prefs['use_SAG'] and status['installed_SAG']:\n",
        "                pipe_used = \"Self-Attention Guidance Text-to-Image\"\n",
        "                #size = pipe_SAG.unet.config.sample_size * pipe_SAG.vae_scale_factor\n",
        "                #arg['width'] = size\n",
        "                #arg['height'] = size\n",
        "                with torch.autocast(\"cuda\"): #, height=arg['height'], width=arg['width']\n",
        "                  images = pipe_SAG(prompt=pr, negative_prompt=arg['negative_prompt'], sag_scale=prefs['sag_scale'], height=arg['height'], width=arg['width'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback=callback_fn, callback_steps=1).images\n",
        "              elif prefs['use_attend_and_excite'] and status['installed_attend_and_excite']:\n",
        "                pipe_used = \"Attend and Excite Text-to-Image\"\n",
        "                size = pipe_attend_and_excite.unet.config.sample_size * pipe_attend_and_excite.vae_scale_factor\n",
        "                arg['width'] = size\n",
        "                arg['height'] = size\n",
        "                token_indices = []\n",
        "                words = []\n",
        "                ptext = pr[0] if type(pr) == list else pr\n",
        "                ntext = arg['negative_prompt'][0] if type(arg['negative_prompt']) == list else arg['negative_prompt']\n",
        "                for ti, w in enumerate(ptext.split()):\n",
        "                  if w[:1] == '+':\n",
        "                    token_indices.append(ti + 1)\n",
        "                    words.append(w[1:])\n",
        "                    #print(f'indices: {ti + 1} - {w[1:]}')\n",
        "                  else:\n",
        "                    words.append(w)\n",
        "                ptext = ' '.join(words)\n",
        "                pr = [ptext * arg['batch_size']] if type(pr) == list else ptext\n",
        "                print(f\"token_indices: {token_indices} | ptext: {ptext}\")\n",
        "                if len(token_indices) < 1:\n",
        "                  token_indices = pipe_attend_and_excite.get_indices(ptext)\n",
        "                print(f\"indices: {token_indices}\")\n",
        "                #, negative_prompt=arg['negative_prompt'], num_images_per_prompt=int(arg['batch_size'])\n",
        "                #images = pipe_attend_and_excite(prompt=ptext, token_indices=token_indices, max_iter_to_alter=int(prefs['max_iter_to_alter']), height=arg['height'], width=arg['width'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback=callback_fn, callback_steps=1).images\n",
        "                images = pipe_attend_and_excite(prompt=ptext, negative_prompt=ntext, token_indices=token_indices, max_iter_to_alter=int(prefs['max_iter_to_alter']), height=arg['height'], width=arg['width'], num_images_per_prompt=int(arg['batch_size']), num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback=callback_fn, callback_steps=1).images\n",
        "              elif prefs['use_versatile'] and status['installed_versatile']:\n",
        "                pipe_used = \"Versatile Text-to-Image\"\n",
        "                images = pipe_versatile_text2img(prompt=pr, negative_prompt=arg['negative_prompt'], height=arg['height'], width=arg['width'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback=callback_fn, callback_steps=1).images\n",
        "              elif prefs['use_panorama'] and status['installed_panorama']:\n",
        "                pipe_used = \"MultiDiffusion Panorama Text-to-Image\"\n",
        "                arg['width'] = prefs['panorama_width']\n",
        "                arg['height'] = 512\n",
        "                images = pipe_panorama(prompt=pr, negative_prompt=arg['negative_prompt'], height=arg['height'], width=arg['width'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback=callback_fn, callback_steps=1).images\n",
        "              elif prefs['use_safe'] and status['installed_safe']:\n",
        "                from diffusers.pipelines.stable_diffusion_safe import SafetyConfig\n",
        "                s = prefs['safety_config']\n",
        "                safety = SafetyConfig.WEAK if s == 'Weak' else SafetyConfig.MEDIUM if s == 'Medium' else SafetyConfig.STRONG if s == 'Strong' else SafetyConfig.MAX if s == 'Max' else SafetyConfig.STRONG \n",
        "                pipe_used = f\"Safe Diffusion {safety}\"\n",
        "                images = pipe_safe(prompt=pr, negative_prompt=arg['negative_prompt'], height=arg['height'], width=arg['width'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback=callback_fn, callback_steps=1, **safety).images\n",
        "              else:\n",
        "                pipe_used = \"Long Prompt Weight Text-to-Image\"\n",
        "                images = pipe.text2img(prompt=pr, negative_prompt=arg['negative_prompt'], height=arg['height'], width=arg['width'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback=callback_fn, callback_steps=1).images\n",
        "              '''if prefs['precision'] == \"autocast\":\n",
        "                with autocast(\"cuda\"):\n",
        "                  images = pipe(pr, height=arg['height'], width=arg['width'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], seed = arg['seed'], generator=generator, callback=callback_fn, callback_steps=1)[\"sample\"]\n",
        "              else:\n",
        "                with precision_scope(\"cuda\"):\n",
        "                  with torch.no_grad():\n",
        "                    images = pipe(pr, height=arg['height'], width=arg['width'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], seed = arg['seed'], generator=generator, callback=callback_fn, callback_steps=1)[\"sample\"]'''\n",
        "              clear_last()\n",
        "              page.auto_scrolling(True)\n",
        "        except RuntimeError as e:\n",
        "          clear_last()\n",
        "          if 'out of memory' in str(e):\n",
        "            alert_msg(page, f\"CRITICAL ERROR: GPU ran out of memory! Flushing memory to save session... Try reducing image size.\", content=Text(str(e).strip()))\n",
        "            pass\n",
        "          else:\n",
        "            alert_msg(page, f\"RUNTIME ERROR: Unknown error processing image. Check parameters and try again. Restart app if persists.\", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip(), selectable=True)]))\n",
        "            pass\n",
        "        except Exception as e:\n",
        "          alert_msg(page, f\"EXCEPTION ERROR: Unknown error processing image. Check parameters and try again. Restart app if persists.\", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip(), selectable=True)]))\n",
        "          pass\n",
        "        finally:\n",
        "          gc.collect()\n",
        "          torch.cuda.empty_cache()\n",
        "\n",
        "      txt2img_output = stable_dir #f'{stable_dir}/stable-diffusion/outputs/txt2img-samples'\n",
        "      batch_output = prefs['image_output']\n",
        "      if bool(prefs['batch_folder_name']):\n",
        "        txt2img_output = os.path.join(stable_dir, prefs['batch_folder_name'])\n",
        "        batch_output = os.path.join(prefs['image_output'], prefs['batch_folder_name'])\n",
        "      if not os.path.exists(txt2img_output):\n",
        "        os.makedirs(txt2img_output)\n",
        "      if save_to_GDrive or storage_type == \"Colab Google Drive\":\n",
        "        if not os.path.exists(batch_output):\n",
        "          os.makedirs(batch_output)\n",
        "      elif storage_type == \"PyDrive Google Drive\": # TODO: I'm not getting the parent folder id right, their docs got confusing\n",
        "        newFolder = gdrive.CreateFile({'title': prefs['batch_folder_name'], \"parents\": [{\"kind\": \"drive#fileLink\", \"id\": prefs['image_output']}],\"mimeType\": \"application/vnd.google-apps.folder\"})\n",
        "        newFolder.Upload()\n",
        "        batch_output = newFolder\n",
        "      else:\n",
        "        if not os.path.exists(batch_output):\n",
        "          os.makedirs(batch_output)\n",
        "\n",
        "      filename = format_filename(pr[0] if type(pr) == list else pr)\n",
        "      if images is None:\n",
        "        prt(f\"ERROR: Problem generating images, check your settings and run above blocks again, or report the error to Skquark if it really seems broken.\")\n",
        "        images = []\n",
        "\n",
        "      idx = num = 0\n",
        "      for image in images:\n",
        "        cur_seed = arg['seed']\n",
        "        if idx > 0:\n",
        "          cur_seed += idx\n",
        "          i_count = f'  ({idx + 1} of {len(images)})  '\n",
        "          prt(Row([Text(i_count), Text(pr[0] if type(pr) == list else pr, expand=True, weight=FontWeight.BOLD), Text(f'seed: {cur_seed}     ')]))\n",
        "          #prt(f'{pr[0] if type(pr) == list else pr} - seed:{cur_seed}')\n",
        "        seed_suffix = \"\" if not prefs['file_suffix_seed'] else f\"-{cur_seed}\"\n",
        "        if prefs['use_imagic'] and status['installed_imagic'] and bool(arg['init_image'] and not bool(arg['mask_image'])):\n",
        "          if idx == 0: seed_suffix += '-alpha_1'\n",
        "          if idx == 1: seed_suffix += '-alpha_1_5'\n",
        "          if idx == 2: seed_suffix += '-alpha_2'\n",
        "        fname = f'{prefs[\"file_prefix\"]}{filename}{seed_suffix}'\n",
        "        image_path = available_file(txt2img_output, fname, idx)\n",
        "        num = int(image_path.rpartition('-')[2].partition('.')[0])\n",
        "        #image_path = os.path.join(txt2img_output, f'{fname}-{idx}.png')\n",
        "        image.save(image_path)\n",
        "        #print(f'size:{os.path.getsize(f\"{fname}-{idx}.png\")}')\n",
        "        if os.path.getsize(image_path) < 2000 or not usable_image: #False: #not sum(image.convert(\"L\").getextrema()) in (0, 2): #image.getbbox():#\n",
        "          os.remove(os.path.join(txt2img_output, f'{fname}-{num}.png'))\n",
        "          if strikes >= retry_attempts_if_NSFW:\n",
        "            if retry_attempts_if_NSFW != 0: prt(\"Giving up on finding safe image...\")\n",
        "            strikes = 0\n",
        "            continue\n",
        "          else: strikes += 1\n",
        "          new_dream = None\n",
        "          if isinstance(p, Dream):\n",
        "            new_dream = p\n",
        "            new_dream.prompt = pr[0] if type(pr) == list else pr\n",
        "            new_dream.arg['seed'] = random.randint(0,4294967295)\n",
        "          else:\n",
        "            new_dream = Dream(p, arg=dict(seed=random.randint(0,4294967295)))\n",
        "          updated_prompts.insert(p_idx+1, new_dream)\n",
        "          prt(f\"Filtered NSFW image, retrying prompt with new seed. Attempt {strikes} of {retry_attempts_if_NSFW}...\")\n",
        "          continue\n",
        "        else: strikes = 0\n",
        "        #if not prefs['display_upscaled_image'] or not prefs['apply_ESRGAN_upscale']:\n",
        "          #print(f\"Image path:{image_path}\")\n",
        "          #time.sleep(0.4)\n",
        "          #prt(Row([Img(src=image_path, width=arg['width'], height=arg['height'], fit=ImageFit.FILL, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "          #display(image)\n",
        "        #if bool(batch_folder_name):\n",
        "        #  fpath = os.path.join(txt2img_output, batch_folder_name, f'{fname}-{idx}.png')\n",
        "        #fpath = os.path.join(txt2img_output, f'{fname}-{idx}.png')\n",
        "        #fpath = available_file(txt2img_output, fname, idx)\n",
        "        fpath = image_path\n",
        "        if txt2img_output != batch_output:\n",
        "          new_file = available_file(batch_output, fname, num)\n",
        "        else:\n",
        "          new_file = image_path\n",
        "        #print(f'fpath: {fpath} - idx: {idx}')\n",
        "        if prefs['centipede_prompts_as_init_images']:\n",
        "          shutil.copy(fpath, os.path.join(root_dir, 'init_images'))\n",
        "          last_image = os.path.join(root_dir, 'init_images', f'{fname}-{num}.png')\n",
        "        page.auto_scrolling(True)\n",
        "        if not prefs['display_upscaled_image'] or not prefs['apply_ESRGAN_upscale']:\n",
        "          #print(f\"Image path:{image_path}\")\n",
        "          upscaled_path = new_file #os.path.join(batch_output if save_to_GDrive else txt2img_output, new_file)\n",
        "          #time.sleep(0.2)\n",
        "          #prt(Row([GestureDetector(content=Img(src_base64=get_base64(fpath), width=arg['width'], height=arg['height'], fit=ImageFit.FILL, gapless_playback=True), data=new_file, on_long_press_end=download_image, on_secondary_tap=download_image)], alignment=MainAxisAlignment.CENTER))\n",
        "          #prt(Row([GestureDetector(content=Img(src=fpath, width=arg['width'], height=arg['height'], fit=ImageFit.FILL, gapless_playback=True), data=new_file, on_long_press_end=download_image, on_secondary_tap=download_image)], alignment=MainAxisAlignment.CENTER))\n",
        "          prt(Row([ImageButton(src=fpath, data=new_file, width=arg['width'], height=arg['height'], subtitle=pr[0] if type(pr) == list else pr, center=True, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "          #prt(ImageButton(src=fpath, width=arg['width'], height=arg['height'], data=new_file, subtitle=pr[0] if type(pr) == list else pr, center=True, page=page))\n",
        "          #time.sleep(0.3)\n",
        "          #display(image)\n",
        "        if prefs['use_upscale'] and status['installed_upscale']:\n",
        "          clear_pipes(['upscale'])\n",
        "          if pipe_upscale == None:\n",
        "            prt(Installing(\"Initializing Stable Diffusion 2 Upscale Pipeline...\"))\n",
        "            pipe_upscale = get_upscale_pipe()\n",
        "            clear_last()\n",
        "          prt(Row([Text(\"Upscaling 4X\"), pb]))\n",
        "          try:\n",
        "            output = pipe_upscale(prompt=pr, image=image, guidance_scale=arg['guidance_scale'], generator=generator, noise_level=prefs['upscale_noise_level'], callback=callback_fn, callback_steps=1)\n",
        "            output.images[0].save(fpath)\n",
        "            #clear_upscale()\n",
        "          except Exception as e:\n",
        "            alert_msg(page, \"Error Upscaling Image.  Most likely out of Memory... Reduce image size to less than 512px.\", content=Text(e))\n",
        "            pass\n",
        "          clear_last()\n",
        "          #clear_upscale_pipe()\n",
        "        if prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:\n",
        "          w = int(arg['width'] * prefs[\"enlarge_scale\"])\n",
        "          h = int(arg['height'] * prefs[\"enlarge_scale\"])\n",
        "          prt(Row([Text(f'Enlarging {prefs[\"enlarge_scale\"]}X to {w}x{h}')], alignment=MainAxisAlignment.CENTER))\n",
        "          os.chdir(os.path.join(dist_dir, 'Real-ESRGAN'))\n",
        "          upload_folder = 'upload'\n",
        "          result_folder = 'results'     \n",
        "          if os.path.isdir(upload_folder):\n",
        "              shutil.rmtree(upload_folder)\n",
        "          if os.path.isdir(result_folder):\n",
        "              shutil.rmtree(result_folder)\n",
        "          os.mkdir(upload_folder)\n",
        "          os.mkdir(result_folder)\n",
        "          short_name = f'{fname[:80]}-{num}.png'\n",
        "          dst_path = os.path.join(dist_dir, 'Real-ESRGAN', upload_folder, short_name)\n",
        "          #print(f'Moving {fpath} to {dst_path}')\n",
        "          #shutil.move(fpath, dst_path)\n",
        "          shutil.copy(fpath, dst_path)\n",
        "          faceenhance = ' --face_enhance' if prefs[\"face_enhance\"] else ''\n",
        "          #python inference_realesrgan.py -n RealESRGAN_x4plus -i upload --outscale {enlarge_scale}{faceenhance}\n",
        "          run_sp(f'python inference_realesrgan.py -n realesr-general-x4v3 -i upload --outscale {prefs[\"enlarge_scale\"]}{faceenhance}', cwd=os.path.join(dist_dir, 'Real-ESRGAN'), realtime=False)\n",
        "          out_file = short_name.rpartition('.')[0] + '_out.png'\n",
        "          #print(f'move {root_dir}Real-ESRGAN/{result_folder}/{out_file} to {fpath}')\n",
        "          #shutil.move(f'{root_dir}Real-ESRGAN/{result_folder}/{out_file}', fpath)\n",
        "          shutil.move(os.path.join(dist_dir, 'Real-ESRGAN', result_folder, out_file), fpath)\n",
        "          # !python inference_realesrgan.py --model_path experiments/pretrained_models/RealESRGAN_x4plus.pth --input upload --netscale 4 --outscale 3.5 --half --face_enhance\n",
        "          os.chdir(stable_dir)\n",
        "          clear_last(update=False)\n",
        "        \n",
        "        config_json = arg.copy()\n",
        "        del config_json['batch_size']\n",
        "        del config_json['n_iterations']\n",
        "        del config_json['precision']\n",
        "        config_json['prompt'] = pr[0] if type(pr) == list else pr\n",
        "        config_json['sampler'] = prefs['generation_sampler'] if prefs['use_Stability_api'] else prefs['scheduler_mode']\n",
        "        if bool(prefs['meta_ArtistName']): config_json['artist'] = prefs['meta_ArtistName']\n",
        "        if bool(prefs['meta_Copyright']): config_json['copyright'] = prefs['meta_Copyright']\n",
        "        if prefs['use_Stability_api']: del config_json['eta']\n",
        "        del config_json['use_Stability']\n",
        "        if not bool(config_json['negative_prompt']): del config_json['negative_prompt']\n",
        "        if not bool(config_json['prompt2']):\n",
        "          del config_json['prompt2']\n",
        "          del config_json['tweens']\n",
        "        if not bool(config_json['init_image']):\n",
        "          del config_json['init_image']\n",
        "          del config_json['init_image_strength']\n",
        "          del config_json['alpha_mask']\n",
        "        if not bool(config_json['mask_image']):\n",
        "          del config_json['mask_image']\n",
        "          del config_json['invert_mask']\n",
        "        if not bool(config_json['use_clip_guided_model']):\n",
        "          del config_json[\"use_clip_guided_model\"]\n",
        "          del config_json[\"clip_prompt\"]\n",
        "          del config_json[\"clip_guidance_scale\"]\n",
        "          del config_json[\"num_cutouts\"]\n",
        "          del config_json[\"use_cutouts\"]\n",
        "          del config_json[\"unfreeze_unet\"]\n",
        "          del config_json[\"unfreeze_vae\"]\n",
        "        else:\n",
        "          config_json[\"clip_model_id\"] = prefs['clip_model_id']\n",
        "        if prefs['apply_ESRGAN_upscale']:\n",
        "          config_json['upscale'] = f\"Upscaled {prefs['enlarge_scale']}x with ESRGAN\" + (\" with GFPGAN Face-Enhance\" if prefs['face_enhance'] else \"\")\n",
        "        sampler_str = prefs['generation_sampler'] if prefs['use_Stability_api'] else prefs['scheduler_mode']\n",
        "        config_json['pipeline'] = pipe_used\n",
        "        config_json['scheduler_mode'] = sampler_str\n",
        "        config_json['model_path'] = model_path\n",
        "        if prefs['save_image_metadata']:\n",
        "          img = PILImage.open(fpath)\n",
        "          metadata = PngInfo()\n",
        "          metadata.add_text(\"artist\", prefs['meta_ArtistName'])\n",
        "          metadata.add_text(\"copyright\", prefs['meta_Copyright'])\n",
        "          metadata.add_text(\"software\", \"Stable Diffusion Deluxe\" + f\", upscaled {prefs['enlarge_scale']}x with ESRGAN\" if prefs['apply_ESRGAN_upscale'] else \"\")\n",
        "          metadata.add_text(\"title\", pr[0] if type(pr) == list else pr)\n",
        "          if prefs['save_config_in_metadata']:\n",
        "            config = f\"prompt: {pr[0] if type(pr) == list else pr}, seed: {cur_seed}, steps: {arg['steps']}, CGS: {arg['guidance_scale']}, iterations: {arg['n_iterations']}\" + f\", eta: {arg['eta']}\" if not prefs['use_Stability_api'] else \"\"\n",
        "            config += f\", sampler: {sampler_str}\"\n",
        "            if bool(arg['init_image']): config += f\", init_image: {arg['init_image']}, init_image_strength: {arg['init_image_strength']}\"\n",
        "            metadata.add_text(\"config\", config)\n",
        "            #metadata.add_text(\"prompt\", p)\n",
        "            metadata.add_text(\"config_json\", json.dumps(config_json, ensure_ascii=True, indent=4))\n",
        "          img.save(fpath, pnginfo=metadata)\n",
        "\n",
        "        #new_file = available_file(batch_output if save_to_GDrive else txt2img_output, fname, idx)\n",
        "        #new_file = fname #.rpartition('.')[0] #f'{file_prefix}{filename}'\n",
        "        #if os.path.isfile(os.path.join(batch_output if save_to_GDrive else txt2img_output, f'{new_file}-{idx}.png')):\n",
        "        #  new_file += '-' + random.choice(string.ascii_letters) + random.choice(string.ascii_letters)\n",
        "        #new_file += f'-{idx}.png'\n",
        "        if save_to_GDrive:\n",
        "          shutil.copy(fpath, new_file)#os.path.join(batch_output, new_file))\n",
        "          #shutil.move(fpath, os.path.join(batch_output, new_file))\n",
        "        elif storage_type == \"PyDrive Google Drive\":\n",
        "          #batch_output\n",
        "          out_file = gdrive.CreateFile({'title': new_file})\n",
        "          out_file.SetContentFile(fpath)\n",
        "          out_file.Upload()\n",
        "        elif bool(prefs['image_output']):\n",
        "          shutil.copy(fpath, new_file)#os.path.join(batch_output, new_file))\n",
        "        if prefs['save_config_json']:\n",
        "          json_file = new_file.rpartition('.')[0] + '.json'\n",
        "          with open(os.path.join(stable_dir, json_file), \"w\") as f:\n",
        "            json.dump(config_json, f, ensure_ascii=False, indent=4)\n",
        "          if save_to_GDrive:\n",
        "            shutil.copy(os.path.join(stable_dir, json_file), os.path.join(batch_output, json_file))\n",
        "          elif storage_type == \"PyDrive Google Drive\":\n",
        "            #batch_output\n",
        "            out_file = gdrive.CreateFile({'title': json_file})\n",
        "            out_file.SetContentFile(os.path.join(stable_dir, json_file))\n",
        "            out_file.Upload()\n",
        "        output_files.append(os.path.join(batch_output if save_to_GDrive else txt2img_output, new_file))\n",
        "        if prefs['display_upscaled_image'] and prefs['apply_ESRGAN_upscale']:\n",
        "          upscaled_path = os.path.join(batch_output if save_to_GDrive else txt2img_output, new_file)\n",
        "          time.sleep(0.4)\n",
        "          #prt(Row([GestureDetector(content=Img(src_base64=get_base64(upscaled_path), width=arg['width'] * float(prefs[\"enlarge_scale\"]), height=arg['height'] * float(prefs[\"enlarge_scale\"]), fit=ImageFit.CONTAIN, gapless_playback=True), data=upscaled_path, on_long_press_end=download_image, on_secondary_tap=download_image)], alignment=MainAxisAlignment.CENTER))\n",
        "          #prt(Row([GestureDetector(content=Img(src=upscaled_path, width=arg['width'] * float(prefs[\"enlarge_scale\"]), height=arg['height'] * float(prefs[\"enlarge_scale\"]), fit=ImageFit.CONTAIN, gapless_playback=True), data=upscaled_path, on_long_press_end=download_image, on_secondary_tap=download_image)], alignment=MainAxisAlignment.CENTER))\n",
        "          prt(Row([ImageButton(src=upscaled_path, width=arg['width'] * float(prefs[\"enlarge_scale\"]), height=arg['height'] * float(prefs[\"enlarge_scale\"]), data=upscaled_path, subtitle=pr[0] if type(pr) == list else pr, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "          #prt(Row([Img(src=upscaled_path, width=arg['width'] * float(prefs[\"enlarge_scale\"]), height=arg['height'] * float(prefs[\"enlarge_scale\"]), fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "          #prt(Img(src=upscaled_path))\n",
        "          #upscaled = PILImage.open(os.path.join(batch_output, new_file))\n",
        "          #display(upscaled)\n",
        "        #else:\n",
        "          #time.sleep(0.4)\n",
        "          #prt(Row([Img(src=new_file, width=arg['width'], height=arg['height'], fit=ImageFit.FILL, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "        prt(Row([Text(fpath.rpartition(slash)[2])], alignment=MainAxisAlignment.CENTER))\n",
        "        idx += 1\n",
        "        if abort_run:\n",
        "          prt(Text(\"üõë   Aborting Current Diffusion Run...\"))\n",
        "          abort_run = False\n",
        "          return\n",
        "      p_idx += 1\n",
        "      if abort_run:\n",
        "        prt(Text(\"üõë   Aborting Current Diffusion Run...\"))\n",
        "        abort_run = False\n",
        "        return\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "  else:\n",
        "    clear_pipes(\"interpolation\")\n",
        "    if pipe_interpolation is None:\n",
        "      pipe_interpolation = get_interpolation_pipe()\n",
        "    txt2img_output = os.path.join(stable_dir, prefs['batch_folder_name'] if bool(prefs['batch_folder_name']) else 'dreams')\n",
        "    batch_output = prefs['image_output']\n",
        "    if not os.path.exists(txt2img_output):\n",
        "      os.makedirs(txt2img_output)\n",
        "    #dream_name = prefs['batch_folder_name'] if bool(prefs['batch_folder_name']) else None\n",
        "    #first = prompts[0]\n",
        "    arg = args.copy()\n",
        "    arg['width'] = int(arg['width'])\n",
        "    arg['height'] = int(arg['height'])\n",
        "    arg['seed'] = int(arg['seed'])\n",
        "    arg['guidance_scale'] = float(arg['guidance_scale'])\n",
        "    arg['steps'] = int(arg['steps'])\n",
        "    arg['eta'] = float(arg['eta'])\n",
        "    walk_prompts = []\n",
        "    walk_seeds = []\n",
        "    for p in prompts:\n",
        "      walk_prompts.append(p.prompt)\n",
        "      if int(p.arg['seed']) < 1 or arg['seed'] is None:\n",
        "        walk_seeds.append(random.randint(0,4294967295))\n",
        "      else:\n",
        "        walk_seeds.append(int(p.arg['seed']))\n",
        "    img_idx = 0\n",
        "    from watchdog.observers import Observer\n",
        "    from watchdog.events import LoggingEventHandler, FileSystemEventHandler\n",
        "    class Handler(FileSystemEventHandler):\n",
        "      def __init__(self):\n",
        "        super().__init__()\n",
        "      def on_created(self,event):\n",
        "        nonlocal img_idx\n",
        "        if event.is_directory:\n",
        "          return None\n",
        "        elif event.event_type == 'created':\n",
        "          page.auto_scrolling(True)\n",
        "          clear_last()\n",
        "          #p_count = f'[{img_idx + 1} of {(len(walk_prompts) -1) * int(prefs['num_interpolation_steps'])}]  '\n",
        "          #prt(Divider(height=6, thickness=2))\n",
        "          #prt(Row([Text(p_count), Text(walk_prompts[img_idx], expand=True, weight=FontWeight.BOLD), Text(f'seed: {walk_seeds[img_idx]}')]))\n",
        "          prt(Row([Img(src=event.src_path, width=arg['width'], height=arg['height'], fit=ImageFit.FILL, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "          prt(Row([ImageButton(src=event.src_path, data=event.src_path, width=arg['width'], height=arg['height'], subtitle=f\"Frame {img_idx} - {event.src_path}\", center=True, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "          prt(Row([Text(f'{event.src_path}')], alignment=MainAxisAlignment.CENTER))\n",
        "          page.update()\n",
        "          time.sleep(0.2)\n",
        "          prt(pb)\n",
        "          page.auto_scrolling(False)\n",
        "          img_idx += 1\n",
        "    # TODO: Rename files with to-from prompt text between each\n",
        "    image_handler = Handler()\n",
        "    observer = Observer()\n",
        "    observer.schedule(image_handler, txt2img_output, recursive = True)\n",
        "    observer.start()\n",
        "    prt(f\"Interpolating latent space between {len(walk_prompts)} prompts with {int(prefs['num_interpolation_steps'])} steps between each.\")\n",
        "    prt(Divider(height=6, thickness=2))\n",
        "    prt(pb)\n",
        "    page.auto_scrolling(False)\n",
        "    #prt(Row([Text(p_count), Text(pr[0] if type(pr) == list else pr, expand=True, weight=FontWeight.BOLD), Text(f'seed: {arg[\"seed\"]}')]))\n",
        "    images = pipe_interpolation.walk(prompts=walk_prompts, seeds=walk_seeds, num_interpolation_steps=int(prefs['num_interpolation_steps']), batch_size=int(prefs['batch_size']), output_dir=txt2img_output, width=arg['width'], height=arg['height'], guidance_scale=arg['guidance_scale'], num_inference_steps=int(arg['steps']), eta=arg['eta'], callback=callback_fn, callback_steps=1)\n",
        "    observer.stop()\n",
        "    clear_last()\n",
        "    fpath = images[0].rpartition(slash)[0]\n",
        "    bfolder = fpath.rpartition(slash)[2]\n",
        "    if prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:\n",
        "      prt('Applying Real-ESRGAN Upscaling to images...')\n",
        "      os.chdir(os.path.join(dist_dir, 'Real-ESRGAN'))\n",
        "      upload_folder = 'upload'\n",
        "      result_folder = 'results'     \n",
        "      if os.path.isdir(upload_folder):\n",
        "          shutil.rmtree(upload_folder)\n",
        "      if os.path.isdir(result_folder):\n",
        "          shutil.rmtree(result_folder)\n",
        "      os.mkdir(upload_folder)\n",
        "      os.mkdir(result_folder)\n",
        "      for i in images:\n",
        "        fname = i.rpartition(slash)[2]\n",
        "        dst_path = os.path.join(dist_dir, 'Real-ESRGAN', upload_folder, fname)\n",
        "        shutil.move(i, dst_path)\n",
        "      faceenhance = ' --face_enhance' if prefs[\"face_enhance\"] else ''\n",
        "      run_sp(f'python inference_realesrgan.py -n realesr-general-x4v3 -i upload --outscale {prefs[\"enlarge_scale\"]}{faceenhance}', cwd=os.path.join(dist_dir, 'Real-ESRGAN'), realtime=False)\n",
        "      filenames = os.listdir(os.path.join(dist_dir, 'Real-ESRGAN', 'results'))\n",
        "      for oname in filenames:\n",
        "        fparts = oname.rpartition('_out')\n",
        "        fname_clean = fparts[0] + fparts[2]\n",
        "        opath = os.path.join(fpath, fname_clean)\n",
        "        shutil.move(os.path.join(dist_dir, 'Real-ESRGAN', result_folder, oname), opath)\n",
        "      os.chdir(stable_dir)\n",
        "    os.makedirs(os.path.join(batch_output, bfolder), exist_ok=True)\n",
        "    imgs = os.listdir(fpath)\n",
        "    for i in imgs:\n",
        "      #prt(f'Created {i}')\n",
        "      #fname = i.rpartition(slash)[2]\n",
        "      if save_to_GDrive:\n",
        "        shutil.copy(os.path.join(fpath, i), os.path.join(batch_output, bfolder, i))\n",
        "      elif storage_type == \"PyDrive Google Drive\":\n",
        "        #batch_output\n",
        "        out_file = gdrive.CreateFile({'title': i})\n",
        "        out_file.SetContentFile(fpath)\n",
        "        out_file.Upload()\n",
        "      elif bool(prefs['image_output']):\n",
        "        shutil.copy(os.path.join(fpath, i), os.path.join(batch_output, bfolder, i))\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "\n",
        "\n",
        "def wget(url, output):\n",
        "    import subprocess\n",
        "    res = subprocess.run(['wget', '-q', url, '-O', output], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "    print(res)\n",
        "\n",
        "nspterminology = None\n",
        "\n",
        "def nsp_parse(prompt):\n",
        "    import random, os, json\n",
        "    global nspterminology\n",
        "    new_prompt = ''\n",
        "    new_prompts = []\n",
        "    new_dict = {}\n",
        "    ptype = type(prompt)\n",
        "    #if not os.path.exists('./nsp_pantry.json'):\n",
        "    #    wget('https://raw.githubusercontent.com/WASasquatch/noodle-soup-prompts/main/nsp_pantry.json', f'.{slash}nsp_pantry.json')\n",
        "    if nspterminology is None:\n",
        "        response = requests.get(\"https://raw.githubusercontent.com/WASasquatch/noodle-soup-prompts/main/nsp_pantry.json\")\n",
        "        nspterminology = json.loads(response.content)\n",
        "    if ptype == dict:\n",
        "        for pstep, pvalue in prompt.items():\n",
        "            if type(pvalue) == list:\n",
        "                for prompt in pvalue:\n",
        "                    new_prompt = prompt\n",
        "                    for term in nspterminology:\n",
        "                        tkey = f'_{term}_'\n",
        "                        tcount = prompt.count(tkey)\n",
        "                        for i in range(tcount):\n",
        "                            new_prompt = new_prompt.replace(tkey, random.choice(nspterminology[term]), 1)\n",
        "                    new_prompts.append(new_prompt)\n",
        "                new_dict[pstep] = new_prompts\n",
        "                new_prompts = []\n",
        "        return new_dict\n",
        "    elif ptype == list:\n",
        "        for pstr in prompt:\n",
        "            new_prompt = pstr\n",
        "            for term in nspterminology:\n",
        "                tkey = f'_{term}_'\n",
        "                tcount = new_prompt.count(tkey)\n",
        "                for i in range(tcount):\n",
        "                    new_prompt = new_prompt.replace(tkey, random.choice(nspterminology[term]), 1)\n",
        "            new_prompts.append(new_prompt)\n",
        "            new_prompt = None\n",
        "        return new_prompts\n",
        "    elif ptype == str:\n",
        "        new_prompt = prompt\n",
        "        for term in nspterminology:\n",
        "            tkey = f'_{term}_'\n",
        "            tcount = new_prompt.count(tkey)\n",
        "            for i in range(tcount):\n",
        "                new_prompt = new_prompt.replace(tkey, random.choice(nspterminology[term]), 1)\n",
        "        return new_prompt\n",
        "    else:\n",
        "        return\n",
        "\n",
        "\n",
        "artists = ( \"Ivan Aivazovsky\", \"Beeple\", \"Zdzislaw Beksinski\", \"Albert Bierstadt\", \"Noah Bradley\", \"Jim Burns\", \"John Harris\", \"John Howe\", \"Thomas Kinkade\", \"Gediminas Pranckevicius\", \"Andreas Rocha\", \"Marc Simonetti\", \"Simon Stalenhag\", \"Yuumei\", \"Asher Brown Durand\", \"Tyler Edlin\", \"Jesper Ejsing\", \"Peter Mohrbacher\", \"RHADS\", \"Greg Rutkowski\", \"H.P. Lovecraft\", \"George Lucas\", \"Benoit B. Mandelbrot\", \"Edwin Austin Abbey\", \"Ansel Adams\", \"Arthur Adams\", \"Charles Addams\", \"Alena Aenami\", \"Pieter Aertsen\", \"Hilma af Klint\", \"Affandi\", \"Leonid Afremov\", \"Eileen Agar\", \"Ivan Aivazovsky\", \"Anni Albers\", \"Josef Albers\", \"Ivan Albright\", \"Yoshitaka Amano\", \"Cuno Amiet\", \"Sophie Anderson\", \"Wes Anderson\", \"Esao Andrews\", \"Charles Angrand\", \"Sofonisba Anguissola\", \"Hirohiko Araki\", \"Nobuyoshi Araki\", \"Shinji Aramaki\", \"Diane Arbus\", \"Giuseppe Arcimboldo\", \"Steve Argyle\", \"Jean Arp\", \"Artgerm\", \"John James Audubon\", \"Frank Auerbach\", \"Milton Avery\", \"Tex Avery\", \"Harriet Backer\", \"Francis Bacon\", \"Peter Bagge\", \"Tom Bagshaw\", \"Karol Bak\", \"Christopher Balaskas\", \"Hans Baldung\", \"Ronald Balfour\", \"Giacomo Balla\", \"Banksy\", \"Cicely Mary Barker\", \"Carl Barks\", \"Wayne Barlowe\", \"Jean-Michel Basquiat\", \"Jules Bastien-Lepage\", \"David Bates\", \"John Bauer\", \"Aubrey Beardsley\", \"Jasmine Becket-Griffith\", \"Max Beckmann\", \"Beeple\", \"Zdzislaw Beksinski\", \"Zdzis≈Çaw Beksi≈Ñski\", \"Julie Bell\", \"Hans Bellmer\", \"John Berkey\", \"√âmile Bernard\", \"Elsa Beskow\", \"Albert Bierstadt\", \"Enki Bilal\", \"Ivan Bilibin\", \"Simon Bisley\", \"Charles Blackman\", \"Thomas Blackshear\", \"Mary Blair\", \"Quentin Blake\", \"William Blake\", \"Antoine Blanchard\", \"John Blanche\", \"Pascal Blanch√©\", \"Karl Blossfeldt\", \"Don Bluth\", \"Umberto Boccioni\", \"Arnold B√∂cklin\", \"Chesley Bonestell\", \"Franklin Booth\", \"Guido Borelli da Caluso\", \"Marius Borgeaud\", \"Hieronymous Bosch\", \"Hieronymus Bosch\", \"Sam Bosma\", \"Johfra Bosschart\", \"Sandro Botticelli\", \"William-Adolphe Bouguereau\", \"Louise Bourgeois\", \"Eleanor Vere Boyle\", \"Noah Bradley\", \"Victor Brauner\", \"Austin Briggs\", \"Raymond Briggs\", \"Mark Briscoe\", \"Romero Britto\", \"Gerald Brom\", \"Mark Brooks\", \"Patrick Brown\", \"Pieter Bruegel the Elder\", \"Bernard Buffet\", \"Laurel Burch\", \"Charles E. Burchfield\", \"David Burdeny\", \"Richard Burlet\", \"David Burliuk\", \"Edward Burne-Jones\", \"Jim Burns\", \"William S. Burroughs\", \"Gaston Bussi√®re\", \"Kaethe Butcher\", \"Jack Butler Yeats\", \"Bob Byerley\", \"Alexandre Cabanel\", \"Ray Caesar\", \"Claude Cahun\", \"Zhichao Cai\", \"Randolph Caldecott\", \"Alexander Milne Calder\", \"Clyde Caldwell\", \"Eddie Campbell\", \"Pascale Campion\", \"Canaletto\", \"Caravaggio\", \"Annibale Carracci\", \"Carl Gustav Carus\", \"Santiago Caruso\", \"Mary Cassatt\", \"Paul C√©zanne\", \"Marc Chagall\", \"Marcel Chagall\", \"Yanjun Cheng\", \"Sandra Chevrier\", \"Judy Chicago\", \"James C. Christensen\", \"Frederic Church\", \"Mikalojus Konstantinas Ciurlionis\", \"Pieter Claesz\", \"Amanda Clark\", \"Harry Clarke\", \"Thomas Cole\", \"Mat Collishaw\", \"John Constable\", \"Cassius Marcellus Coolidge\", \"Richard Corben\", \"Lovis Corinth\", \"Joseph Cornell\", \"Camille Corot\", \"cosmic nebulae\", \"Gustave Courbet\", \"Lucas Cranach the Elder\", \"Walter Crane\", \"Craola\", \"Gregory Crewdson\", \"Henri-Edmond Cross\", \"Robert Crumb\", \"Tivadar Csontv√°ry Kosztka\", \"Krenz Cushart\", \"Leonardo da Vinci\", \"Richard Dadd\", \"Louise Dahl-Wolfe\", \"Salvador Dal√≠\", \"Farel Dalrymple\", \"Geof Darrow\", \"Honor√© Daumier\", \"Jack Davis\", \"Marc Davis\", \"Stuart Davis\", \"Craig Davison\", \"Walter Percy Day\", \"Pierre Puvis de Chavannes\", \"Giorgio de Chirico\", \"Pieter de Hooch\", \"Elaine de Kooning\", \"Willem de Kooning\", \"Evelyn De Morgan\", \"Henri de Toulouse-Lautrec\", \"Richard Deacon\", \"Roger Dean\", \"Michael Deforge\", \"Edgar Degas\", \"Lise Deharme\", \"Eugene Delacroix\", \"Beauford Delaney\", \"Sonia Delaunay\", \"Nicolas Delort\", \"Paul Delvaux\", \"Jean Delville\", \"Martin Deschambault\", \"Brian Despain\", \"Vincent Di Fate\", \"Steve Dillon\", \"Walt Disney\", \"Tony DiTerlizzi\", \"Steve Ditko\", \"Anna Dittmann\", \"Otto Dix\", \"√ìscar Dom√≠nguez\", \"Russell Dongjun Lu\", \"Stanley Donwood\", \"Gustave Dor√©\", \"Dave Dorman\", \"Arthur Dove\", \"Richard Doyle\", \"Tim Doyle\", \"Philippe Druillet\", \"Joseph Ducreux\", \"Edmund Dulac\", \"Asher Brown Durand\", \"Albrecht D√ºrer\", \"Thomas Eakins\", \"Eyvind Earle\", \"Jeff Easley\", \"Tyler Edlin\", \"Jason Edmiston\", \"Les Edwards\", \"Bob Eggleton\", \"Jesper Ejsing\", \"El Greco\", \"Olafur Eliasson\", \"Harold Elliott\", \"Dean Ellis\", \"Larry Elmore\", \"Peter Elson\", \"Ed Emshwiller\", \"Kilian Eng\", \"James Ensor\", \"Max Ernst\", \"Elliott Erwitt\", \"M.C. Escher\", \"Richard Eurich\", \"Glen Fabry\", \"Anton Fadeev\", \"Shepard Fairey\", \"John Philip Falter\", \"Lyonel Feininger\", \"Joe Fenton\", \"Agust√≠n Fern√°ndez\", \"Roberto Ferri\", \"Hugh Ferriss\", \"David Finch\", \"Virgil Finlay\", \"Howard Finster\", \"Anton Otto Fischer\", \"Paul Gustav Fischer\", \"Paul Gustave Fischer\", \"Art Fitzpatrick\", \"Dan Flavin\", \"Kaja Foglio\", \"Phil Foglio\", \"Chris Foss\", \"Hal Foster\", \"Jean-Honor√© Fragonard\", \"Victoria Franc√©s\", \"Lisa Frank\", \"Frank Frazetta\", \"Kelly Freas\", \"Lucian Freud\", \"Caspar David Friedrich\", \"Brian Froud\", \"Wendy Froud\", \"Ernst Fuchs\", \"Goro Fujita\", \"Henry Fuseli\", \"Thomas Gainsborough\", \"Emile Galle\", \"Stephen Gammell\", \"Hope Gangloff\", \"Antoni Gaudi\", \"Antoni Gaud√≠\", \"Jack Gaughan\", \"Paul Gauguin\", \"Giovanni Battista Gaulli\", \"Nikolai Ge\", \"Emma Geary\", \"Anne Geddes\", \"Jeremy Geddes\", \"Artemisia Gentileschi\", \"Justin Gerard\", \"Jean-Leon Gerome\", \"Jean-L√©on G√©r√¥me\", \"Atey Ghailan\", \"Alberto Giacometti\", \"Donato Giancola\", \"Dave Gibbons\", \"H. R. Giger\", \"James Gilleard\", \"Jean Giraud\", \"Milton Glaser\", \"Warwick Goble\", \"Andy Goldsworthy\", \"Hendrick Goltzius\", \"Natalia Goncharova\", \"Rob Gonsalves\", \"Josan Gonzalez\", \"Edward Gorey\", \"Arshile Gorky\", \"Francisco Goya\", \"J. J. Grandville\", \"Jane Graverol\", \"Mab Graves\", \"Laurie Greasley\", \"Kate Greenaway\", \"Alex Grey\", \"Peter Gric\", \"Carne Griffiths\", \"John Atkinson Grimshaw\", \"Henriette Grindat\", \"Matt Groening\", \"William Gropper\", \"George Grosz\", \"Matthias Gr√ºnewald\", \"Rebecca Guay\", \"James Gurney\", \"Philip Guston\", \"Sir James Guthrie\", \"Zaha Hadid\", \"Ernst Haeckel\", \"Sydney Prior Hall\", \"Asaf Hanuka\", \"Tomer Hanuka\", \"David A. Hardy\", \"Keith Haring\", \"John Harris\", \"Lawren Harris\", \"Marsden Hartley\", \"Ryohei Hase\", \"Jacob Hashimoto\", \"Martin Johnson Heade\", \"Erich Heckel\", \"Michael Heizer\", \"Steve Henderson\", \"Patrick Heron\", \"Ryan Hewett\", \"Jamie Hewlett\", \"Brothers Hildebrandt\", \"Greg Hildebrandt\", \"Tim Hildebrandt\", \"Miho Hirano\", \"Adolf Hitler\", \"Hannah Hoch\", \"David Hockney\", \"Filip Hodas\", \"Howard Hodgkin\", \"Ferdinand Hodler\", \"William Hogarth\", \"Katsushika Hokusai\", \"Carl Holsoe\", \"Winslow Homer\", \"Edward Hopper\", \"Aaron Horkey\", \"Kati Horna\", \"Ralph Horsley\", \"John Howe\", \"John Hoyland\", \"Arthur Hughes\", \"Edward Robert Hughes\", \"Friedensreich Regentag Dunkelbunt Hundertwasser\", \"Hundertwasser\", \"William Henry Hunt\", \"Louis Icart\", \"Ismail Inceoglu\", \"Bjarke Ingels\", \"George Inness\", \"Shotaro Ishinomori\", \"Junji Ito\", \"Johannes Itten\", \"Ub Iwerks\", \"Alexander Jansson\", \"Jaros≈Çaw Ja≈õnikowski\", \"James Jean\", \"Ruan Jia\", \"Martine Johanna\", \"Richard S. Johnson\", \"Jeffrey Catherine Jones\", \"Peter Andrew Jones\", \"Kim Jung Gi\", \"Joe Jusko\", \"Frida Kahlo\", \"M.W. Kaluta\", \"Wassily Kandinsky\", \"Terada Katsuya\", \"Audrey Kawasaki\", \"Hasui Kawase\", \"Zhang Kechun\", \"Felix Kelly\", \"John Frederick Kensett\", \"Rockwell Kent\", \"Hendrik Kerstens\", \"Brian Kesinger\", \"Jeremiah Ketner\", \"Adonna Khare\", \"Kitty Lange Kielland\", \"Thomas Kinkade\", \"Jack Kirby\", \"Ernst Ludwig Kirchner\", \"Tatsuro Kiuchi\", \"Mati Klarwein\", \"Jon Klassen\", \"Paul Klee\", \"Yves Klein\", \"Heinrich Kley\", \"Gustav Klimt\", \"Daniel Ridgway Knight\", \"Nick Knight\", \"Daniel Ridgway Knights\", \"Ayami Kojima\", \"Oskar Kokoschka\", \"K√§the Kollwitz\", \"Satoshi Kon\", \"Jeff Koons\", \"Konstantin Korovin\", \"Leon Kossoff\", \"Hugh Kretschmer\", \"Barbara Kruger\", \"Alfred Kubin\", \"Arkhyp Kuindzhi\", \"Kengo Kuma\", \"Yasuo Kuniyoshi\", \"Yayoi Kusama\", \"Ilya Kuvshinov\", \"Chris LaBrooy\", \"Raphael Lacoste\", \"Wilfredo Lam\", \"Mikhail Larionov\", \"Abigail Larson\", \"Jeffrey T. Larson\", \"Carl Larsson\", \"Dorothy Lathrop\", \"John Lavery\", \"Edward Lear\", \"Andr√© Leblanc\", \"Bastien Lecouffe-Deharme\", \"Alan Lee\", \"Jim Lee\", \"Heinrich Lefler\", \"Paul Lehr\", \"Edmund Leighton\", \"Frederick Lord Leighton\", \"Jeff Lemire\", \"Isaac Levitan\", \"J.C. Leyendecker\", \"Roy Lichtenstein\", \"Rob Liefeld\", \"Malcolm Liepke\", \"Jeremy Lipking\", \"Filippino Lippi\", \"Laurie Lipton\", \"Michal Lisowski\", \"Scott Listfield\", \"Cory Loftis\", \"Travis Louie\", \"George Luks\", \"Dora Maar\", \"August Macke\", \"Margaret Macdonald Mackintosh\", \"Clive Madgwick\", \"Lee Madgwick\", \"Rene Magritte\", \"Don Maitz\", \"Kazimir Malevich\", \"√âdouard Manet\", \"Jeremy Mann\", \"Sally Mann\", \"Franz Marc\", \"Chris Mars\", \"Otto Marseus van Schrieck\", \"John Martin\", \"Masaaki Masamoto\", \"Andr√© Masson\", \"Henri Matisse\", \"Leiji Matsumoto\", \"Taiy≈ç Matsumoto\", \"Roberto Matta\", \"Rodney Matthews\", \"David B. Mattingly\", \"Peter Max\", \"Marco Mazzoni\", \"Robert McCall\", \"Todd McFarlane\", \"Ryan McGinley\", \"Dave McKean\", \"Kelly McKernan\", \"Angus McKie\", \"Ralph McQuarrie\", \"Ian McQue\", \"Syd Mead\", \"J√≥zef Mehoffer\", \"Eddie Mendoza\", \"Adolph Menzel\", \"Maria Sibylla Merian\", \"Daniel Merriam\", \"Jean Metzinger\", \"Michelangelo\", \"Mike Mignola\", \"Frank Miller\", \"Ian Miller\", \"Russ Mills\", \"Victor Adame Minguez\", \"Joan Miro\", \"Kentaro Miura\", \"Paula Modersohn-Becker\", \"Amedeo Modigliani\", \"Moebius\", \"Peter Mohrbacher\", \"Piet Mondrian\", \"Claude Monet\", \"Jean-Baptiste Monge\", \"Kent Monkman\", \"Alyssa Monks\", \"Sailor Moon\", \"Chris Moore\", \"Gustave Moreau\", \"William Morris\", \"Igor Morski\", \"John Kenn Mortensen\", \"Victor Moscoso\", \"Grandma Moses\", \"Robert Motherwell\", \"Alphonse Mucha\", \"Craig Mullins\", \"Augustus Edwin Mulready\", \"Dan Mumford\", \"Edvard Munch\", \"Gabriele M√ºnter\", \"Gerhard Munthe\", \"Takashi Murakami\", \"Patrice Murciano\", \"Go Nagai\", \"Hiroshi Nagai\", \"Tibor Nagy\", \"Ted Nasmith\", \"Alice Neel\", \"Odd Nerdrum\", \"Mikhail Nesterov\", \"C. R. W. Nevinson\", \"Helmut Newton\", \"Victo Ngai\", \n",
        "           \"Dustin Nguyen\", \"Kay Nielsen\", \"Tsutomu Nihei\", \"Yasushi Nirasawa\", \"Sidney Nolan\", \"Emil Nolde\", \"Sven Nordqvist\", \"Earl Norem\", \"Marianne North\", \"Georgia O'Keeffe\", \"Terry Oakes\", \"Takeshi Obata\", \"Eiichiro Oda\", \"Koson Ohara\", \"Noriyoshi Ohrai\", \"Marek Okon\", \"M√©ret Oppenheim\", \"Katsuhiro Otomo\", \"Shohei Otomo\", \"Siya Oum\", \"Ida Rentoul Outhwaite\", \"James Paick\", \"David Palumbo\", \"Michael Parkes\", \"Keith Parkinson\", \"Maxfield Parrish\", \"Alfred Parsons\", \"Max Pechstein\", \"Agnes Lawrence Pelton\", \"Bruce Pennington\", \"John Perceval\", \"Gaetano Pesce\", \"Coles Phillips\", \"Francis Picabia\", \"Pablo Picasso\", \"Mauro Picenardi\", \"Anton Pieck\", \"Bonnard Pierre\", \"Yuri Ivanovich Pimenov\", \"Robert Antoine Pinchon\", \"Giovanni Battista Piranesi\", \"Camille Pissarro\", \"Patricia Polacco\", \"Jackson Pollock\", \"Lyubov Popova\", \"Candido Portinari\", \"Beatrix Potter\", \"Beatrix Potter\", \"Gediminas Pranckevicius\", \"Dod Procter\", \"Howard Pyle\", \"Arthur Rackham\", \"Alice Rahon\", \"Paul Ranson\", \"Raphael\", \"Robert Rauschenberg\", \"Man Ray\", \"Odilon Redon\", \"Pierre-Auguste Renoir\", \"Ilya Repin\", \"RHADS\", \"Gerhard Richter\", \"Diego Rivera\", \"Hubert Robert\", \"Andrew Robinson\", \"Charles Robinson\", \"W. Heath Robinson\", \"Andreas Rocha\", \"Norman Rockwell\", \"Nicholas Roerich\", \"Conrad Roset\", \"Bob Ross\", \"Jessica Rossier\", \"Ed Roth\", \"Mark Rothko\", \"Georges Rouault\", \"Henri Rousseau\", \"Luis Royo\", \"Jakub Rozalski\", \"Joao Ruas\", \"Peter Paul Rubens\", \"Mark Ryden\", \"Jan Pietersz Saenredam\", \"Pieter Jansz Saenredam\", \"Kay Sage\", \"Apollonia Saintclair\", \"John Singer Sargent\", \"Martiros Saryan\", \"Masaaki Sasamoto\", \"Thomas W Schaller\", \"Miriam Schapiro\", \"Yohann Schepacz\", \"Egon Schiele\", \"Karl Schmidt-Rottluff\", \"Charles Schulz\", \"Charles Schulz\", \"Carlos Schwabe\", \"Sean Scully\", \"Franz Sedlacek\", \"Maurice Sendak\", \"Zinaida Serebriakova\", \"Georges Seurat\", \"Ben Shahn\", \"Barclay Shaw\", \"E. H. Shepard\", \"Cindy Sherman\", \"Makoto Shinkai\", \"Yoji Shinkawa\", \"Chiharu Shiota\", \"Masamune Shirow\", \"Ivan Shishkin\", \"Bill Sienkiewicz\", \"Greg Simkins\", \"Marc Simonetti\", \"Kevin Sloan\", \"Adrian Smith\", \"Douglas Smith\", \"Jeffrey Smith\", \"Pamela Coleman Smith\", \"Zack Snyder\", \"Simeon Solomon\", \"Joaqu√≠n Sorolla\", \"Ettore Sottsass\", \"Cha√Øm Soutine\", \"Austin Osman Spare\", \"Sparth \", \"Art Spiegelman\", \"Simon Stalenhag\", \"Ralph Steadman\", \"William Steig\", \"Joseph Stella\", \"Irma Stern\", \"Anne Stokes\", \"James Stokoe\", \"William Stout\", \"George Stubbs\", \"Tatiana Suarez\", \"Ken Sugimori\", \"Hiroshi Sugimoto\", \"Brian Sum\", \"Matti Suuronen\", \"Raymond Swanland\", \"Naoko Takeuchi\", \"Rufino Tamayo\", \"Shaun Tan\", \"Yves Tanguay\", \"Henry Ossawa Tanner\", \"Dorothea Tanning\", \"Ben Templesmith\", \"theCHAMBA\", \"Tom Thomson\", \"Storm Thorgerson\", \"Bridget Bate Tichenor\", \"Louis Comfort Tiffany\", \"Tintoretto\", \"James Tissot\", \"Titian\", \"Akira Toriyama\", \"Ross Tran\", \"Clovis Trouille\", \"J.M.W. Turner\", \"James Turrell\", \"Daniela Uhlig\", \"Boris Vallejo\", \"Gustave Van de Woestijne\", \"Frits Van den Berghe\", \"Anthony van Dyck\", \"Jan van Eyck\", \"Vincent Van Gogh\", \"Willem van Haecht\", \"Rembrandt van Rijn\", \"Jacob van Ruisdael\", \"Salomon van Ruysdael\", \"Theo van Rysselberghe\", \"Remedios Varo\", \"Viktor Vasnetsov\", \"Kuno Veeber\", \"Diego Vel√°zquez\", \"Giovanni Battista Venanzi\", \"Johannes Vermeer\", \"Alexej von Jawlensky\", \"Marianne von Werefkin\", \"Hendrick Cornelisz Vroom\", \"Mikhail Vrubel\", \"Louis Wain\", \"Ron Walotsky\", \"Andy Warhol\", \"John William Waterhouse\", \"Jean-Antoine Watteau\", \"George Frederic Watts\", \"Max Weber\", \"Gerda Wegener\", \"Edward Weston\", \"Michael Whelan\", \"James Abbott McNeill Whistler\", \"Tim White\", \"Coby Whitmore\", \"John Wilhelm\", \"Robert Williams\", \"Al Williamson\", \"Carel Willink\", \"Mike Winkelmann\", \"Franz Xaver Winterhalter\", \"Klaus Wittmann\", \"Liam Wong\", \"Paul Wonner\", \"Ashley Wood\", \"Grant Wood\", \"Patrick Woodroffe\", \"Frank Lloyd Wright\", \"Bernie Wrightson\", \"Andrew Wyeth\", \"Qian Xuan\", \"Takato Yamamoto\", \"Liu Ye\", \"Jacek Yerka\", \"Akihiko Yoshida\", \"Hiroshi Yoshida\", \"Skottie Young\", \"Konstantin Yuon\", \"Yuumei\", \"Amir Zand\", \"Fenghua Zhong\", \"Nele Zirnite\", \"Anders Zorn\") \n",
        "styles = ( \"1970s era\", \"2001: A Space Odyssey\", \"60s kitsch and psychedelia\", \"Aaahh!!! Real Monsters\", \"abstract illusionism\", \"afrofuturism\", \"alabaster\", \"alhambresque\", \"ambrotype\", \"american romanticism\", \"amethyst\", \"amigurumi\", \"anaglyph effect\", \"anaglyph filter\", \"Ancient Egyptian\", \"ancient Greek architecture\", \"anime\", \"art nouveau\", \"astrophotography\", \"at dawn\", \"at dusk\", \"at high noon\", \"at night\", \"atompunk\", \"aureolin\", \"avant-garde\", \"Avatar The Last Airbender\", \"Babylonian\", \"Baker-Miller pink\", \"Baroque\", \"Bauhaus\", \"biopunk\", \"bismuth\", \"Blade Runner 2049\", \"blueprint\", \"bokeh\", \"bonsai\", \"bright\", \"bronze\", \"brutalism\", \"burgundy\", \"Byzantine\", \"calotype\", \"Cambrian\", \"camcorder effect\", \"carmine\", \"cassette futurism\", \"cassettepunk\", \"catholicpunk\", \"cerulean\", \"chalk art\", \"chartreuse\", \"chiaroscuro\", \"chillwave\", \"chromatic aberration\", \"chrome\", \"Cirque du Soleil\", \"claymation\", \"clockpunk\", \"cloudpunk\", \"cobalt\", \"colored pencil art\", \"Concept Art World\", \"copper patina\", \"copper verdigris\", \"Coraline\", \"cosmic horror\", \"cottagecore\", \"crayon art\", \"crimson\", \"CryEngine\", \"crystalline lattice\", \"cubic zirconia\", \"cubism\", \"cyanotype\", \"cyber noir\", \"cyberpunk\", \"cyclopean masonry\", \"daguerreotype\", \"Danny Phantom\", \"dark academia\", \"dark pastel\", \"dark rainbow\", \"DayGlo\", \"decopunk\", \"Dexter's Lab\", \"diamond\", \"dieselpunk\", \"Digimon\", \"digital art\", \"doge\", \"dollpunk\", \"Doom engine\", \"Dreamworks\", \"dutch golden age\", \"Egyptian\", \"eldritch\", \"emerald\", \"empyrean\", \"Eraserhead\", \"ethereal\", \"expressionism\", \"Fantastic Planet\", \"Fendi\", \"figurativism\", \"fire\", \"fisheye lens\", \"fluorescent\", \"forestpunk\", \"fractal manifold\", \"fractalism\", \"fresco\", \"fuchsia\", \"futuresynth\", \"Game of Thrones\", \"german romanticism\", \"glitch art\", \"glittering\", \"golden\", \"golden hour\", \"gothic\", \"gothic art\", \"graffiti\", \"graphite\", \"grim dark\", \"Harry Potter\", \"holography\", \"Howl‚Äôs Moving Castle\", \"hygge\", \"hyperrealism\", \"icy\", \"ikebana\", \"impressionism\", \"in Ancient Egypt\", \"in Egypt\", \"in Italy\", \"in Japan\", \"in the Central African Republic\", \"in the desert\", \"in the jungle\", \"in the swamp\", \"in the tundra\", \"incandescent\", \"indigo\", \"infrared\", \"Interstellar\", \"inverted colors\", \"iridescent\", \"iron\", \"islandpunk\", \"isotype\", \"Kai Fine Art\", \"khaki\", \"kokedama\", \"Korean folk art\", \"lapis lazuli\", \"Lawrence of Arabia\", \"leather\", \"leopard print\", \"lilac\", \"liminal space\", \"long exposure\", \"Lord of the Rings\", \"Louis Vuitton\", \"Lovecraftian\", \"low poly\", \"mac and cheese\", \"macro lens\", \"magenta\", \"magic realism\", \"manga\", \"mariachi\", \"marimekko\", \"maroon\", \"Medieval\", \"Mediterranean\", \"modernism\", \"Monster Rancher\", \"moonstone\", \"Moulin Rouge!\", \"multiple exposure\", \"Myst\", \"nacreous\", \"narrative realism\", \"naturalism\", \"neon\", \"Nosferatu\", \"obsidian\", \"oil and canvas\", \"opalescent\", \"optical illusion\", \"optical art\", \"organometallics\", \"ossuary\", \"outrun\", \"Paleolithic\", \"Pan's Labyrinth\", \"pastel\", \"patina\", \"pearlescent\", \"pewter\", \"Pixar\", \"Play-Doh\", \"pointillism\", \"Pokemon\", \"polaroid\", \"porcelain\", \"positivism\", \"postcyberpunk\", \"Pride & Prejudice\", \"prismatic\", \"pyroclastic flow\", \"Quake engine\", \"quartz\", \"rainbow\", \"reflective\", \"Renaissance\", \"retrowave\", \"Rococo\", \"rococopunk\", \"ruby\", \"rusty\", \"Salad Fingers\", \"sapphire\", \"scarlet\", \"shimmering\", \"silk\", \"sketched\", \"Slenderman\", \"smoke\", \"snakeskin\", \"Spaceghost Coast to Coast\", \"stained glass\", \"Star Wars\", \"steampunk\", \"steel\", \"steelpunk\", \"still life\", \"stonepunk\", \"Stranger Things\", \"street art\", \"stuckism\", \"Studio Ghibli\", \"Sumerian\", \"surrealism\", \"symbolism\", \"synthwave\", \"telephoto lens\", \"thalassophobia\", \"thangka\", \"the matrix\", \"tiger print\", \"tilt-shift\", \"tintype\", \"tonalism\", \"Toonami\", \"turquoise\", \"Ukiyo-e\", \"ultramarine\", \"ultraviolet\", \"umber\", \"underwater photography\", \"Unreal Engine\", \"vantablack\", \"vaporwave\", \"verdigris\", \"Versacci\", \"viridian\", \"wabi-sabi\", \"watercolor painting\", \"wooden\", \"x-ray photography\", \"minimalist\", \"dadaist\", \"neo-expressionist\", \"post-impressionist\", \"hyper real\", \"Art brut\", \"3D rendering\", \"uncanny valley\", \"fractal landscape\", \"fractal flames\", \"Mandelbulb\", \"inception dream\", \"waking life\", \"occult inscriptions\", \"barr relief\", \"marble sculpture\", \"wood carving\", \"church stained glass\", \"Japanese jade\", \"Zoetrope\", \"beautiful\", \"wide-angle\", \"Digital Painting\", \"glossy reflections\", \"cinematic\", \"spooky\", \"Digital paint concept art\", \"dramatic\", \"global illumination\", \"immaculate\", \"woods\", ) \n",
        "\n",
        "#Code a function in Python programming language named list_variations, which takes a list and returns a set of lists with possible permutations of the list. Example list_variations([1,2,3]) returns [[1,2,3],[1,2],[1,3],[2,3],[1],[2],[3]] */\n",
        "def list_variations(lst):\n",
        "    result = []\n",
        "    for i in range(len(lst)):\n",
        "        for j in range(i+1, len(lst)+1):\n",
        "            result.append(lst[i:j])\n",
        "    return result\n",
        "#print(str(list_variations([1,2,3])))\n",
        "def and_list(lst):\n",
        "  return \" and \".join([\", \".join(lst[:-1]),lst[-1]] if len(lst) > 2 else lst)\n",
        "\n",
        "generator_request_modes = [\"visually detailed\",\n",
        "  \"with long detailed colorful interesting artistic scenic visual descriptions\",\n",
        "  \"that is highly detailed, artistically interesting, describes a scene, colorful poetic language, with intricate visual descriptions\",\n",
        "  \"that are strange, descriptive, graphically visual, full of interesting subjects described in great detail, painted by an artist\",\n",
        "  \"that is technical, wordy, extra detailed, confusingly tangental, colorfully worded, dramatically narrative\",\n",
        "  \"that is creative, imaginative, funny, interesting, scenic, dark, witty, visual, unexpected, wild\",\n",
        "  \"that includes many subjects with descriptions, color details, artistic expression, point of view\",\n",
        "  \"complete sentence using many words to describe a landscape in an epic fantasy genre that includes a lot adjectives\",]\n",
        "\n",
        "def run_prompt_generator(page):\n",
        "  import random as rnd\n",
        "  global artists, styles, status\n",
        "  try:\n",
        "    import openai\n",
        "  except:\n",
        "    run_sp(\"pip install --upgrade openai\")\n",
        "    import openai\n",
        "    pass\n",
        "  try:\n",
        "    openai.api_key = prefs['OpenAI_api_key']\n",
        "  except:\n",
        "    alert_msg(page, \"Invalid OpenAI API Key. Change in Settings...\")\n",
        "    return\n",
        "  status['installed_OpenAI'] = True\n",
        "  prompts_gen = []\n",
        "  prompt_results = []\n",
        "  subject = \"\"\n",
        "  if bool(prefs['prompt_generator']['subject_detail']):\n",
        "      subject = \", and \" + prefs['prompt_generator']['subject_detail']\n",
        "\n",
        "  def prompt_gen():\n",
        "    prompt = f'''Write a list of {prefs['prompt_generator']['amount'] if prefs['prompt_generator']['phrase_as_subject'] else (prefs['prompt_generator']['amount'] + 4)} image generation prompts about \"{prefs['prompt_generator']['phrase']}\"{subject}, {generator_request_modes[int(prefs['prompt_generator']['request_mode'])]}, and unique without repetition:\n",
        "\n",
        "'''\n",
        "    #print(prompt)\n",
        "    if prefs['prompt_generator']['phrase_as_subject']:\n",
        "      prompt += \"\\n*\"\n",
        "    else:\n",
        "      prompt += f\"\"\"* A beautiful painting of a serene landscape with a river running through it, lush trees, golden sun illuminating\n",
        "* Fireflies illuminating autumnal woods, an Autumn in the Brightwood glade, with warm yellow lantern lights\n",
        "* The Fabric of spacetime continuum over a large cosmological vista, pieces of dark matter, space dust and nebula doted with small dots that seem to form fractal patterns and glowing bright lanterns in distances, also with an stardust effect towards the plane of the galaxy\n",
        "* Midnight landscape painting of a city under a starry sky, owl in the shaman forest knowing the ways of magic, warm glow over the buildings\n",
        "* {prefs['prompt_generator']['phrase']}\"\"\"\n",
        "    if prefs['prompt_generator']['AI_engine'] == \"OpenAI GPT-3\":\n",
        "      response = openai.Completion.create(engine=\"text-davinci-003\", prompt=prompt, max_tokens=2400, temperature=prefs['prompt_generator']['AI_temperature'], presence_penalty=1)\n",
        "      #print(response)\n",
        "      result = response[\"choices\"][0][\"text\"].strip()\n",
        "    elif prefs['prompt_generator']['AI_engine'] == \"ChatGPT-3.5 Turbo\":\n",
        "      response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\", \n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "      )\n",
        "      #print(str(response))\n",
        "      result = response[\"choices\"][0][\"message\"][\"content\"].strip()\n",
        "    #if result[-1] == '.': result = result[:-1]\n",
        "    #print(str(result))\n",
        "    for p in result.split('\\n'):\n",
        "      pr = p.strip()\n",
        "      if not bool(pr): continue\n",
        "      if pr[-1] == '.': pr = pr[:-1]\n",
        "      if pr[0] == '*': pr = pr[1:].strip()\n",
        "      elif '.' in pr: # Sometimes got 1. 2.\n",
        "        pr = pr.partition('.')[2].strip()\n",
        "      if '\"' in pr: pr = pr.replace('\"', '')\n",
        "      prompt_results.append(pr)\n",
        "  #print(f\"Request mode influence: {request_modes[prefs['prompt_generator']['request_mode']]}\\n\")\n",
        "  page.prompt_generator_list.controls.append(Installing(\"Requesting Prompts from the AI...\"))\n",
        "  page.prompt_generator_list.update()\n",
        "  prompt_gen()\n",
        "  del page.prompt_generator_list.controls[-1]\n",
        "  page.prompt_generator_list.update()\n",
        "  if len(prompt_results) < prefs['prompt_generator']['amount']:\n",
        "    additional = prefs['prompt_generator']['amount'] - len(prompt_results)\n",
        "    print(f\"Didn't make enough prompts.. Needed {additional} more.\")\n",
        "  n=1\n",
        "  for p in prompt_results:\n",
        "    random_artist=[]\n",
        "    for a in range(prefs['prompt_generator']['random_artists']):\n",
        "      random_artist.append(rnd.choice(artists))\n",
        "    #print(list_variations(random_artist))\n",
        "    artist = \" and \".join([\", \".join(random_artist[:-1]),random_artist[-1]] if len(random_artist) > 2 else random_artist)\n",
        "    random_style = []\n",
        "    for s in range(prefs['prompt_generator']['random_styles']):\n",
        "      random_style.append(rnd.choice(styles))\n",
        "    style = \", \".join(random_style)\n",
        "    if not prefs['prompt_generator']['phrase_as_subject'] and n == 1:\n",
        "      p = prefs['prompt_generator']['phrase'] + \" \" + p\n",
        "    text_prompt = p\n",
        "    if prefs['prompt_generator']['random_artists'] > 0: text_prompt += f\", by {artist}\"\n",
        "    if prefs['prompt_generator']['random_styles'] > 0: text_prompt += f\", style of {style}\"\n",
        "    if prefs['prompt_generator']['random_styles'] != 0 and prefs['prompt_generator']['permutate_artists']:\n",
        "      prompts_gen.append(text_prompt)\n",
        "    if prefs['prompt_generator']['permutate_artists']:\n",
        "      \n",
        "      for a in list_variations(random_artist):\n",
        "        prompt_variation = p + f\", by {and_list(a)}\"\n",
        "        prompts_gen.append(prompt_variation)\n",
        "      if prefs['prompt_generator']['random_styles'] > 0:\n",
        "        prompts_gen.append(p + f\", style of {style}\")\n",
        "    else: prompts_gen.append(text_prompt)\n",
        "    n += 1\n",
        "  for item in prompts_gen:\n",
        "    page.add_to_prompt_generator(item)\n",
        "    #print(f'   \"{item}\",')\n",
        "\n",
        "remixer_request_modes = [\n",
        "      \"visually detailed wording, flowing sentences, extra long descriptions\",\n",
        "      \"that is similar but with more details, themes, imagination, interest, subjects, artistic style, poetry, tone, settings, adjectives, visualizations\",\n",
        "      \"that is completely rewritten, inspired by, paints a complete picture of an artistic scene\",\n",
        "      \"with detailed colorful interesting artistic scenic visual descriptions, described to a blind person\",\n",
        "      \"that is highly detailed, artistically interesting, describes a scene, colorful poetic language, with intricate visual descriptions\",\n",
        "      \"that replaces every noun, adjective, verb, pronoun, with related words\",\n",
        "      \"that is strange, descriptive, graphically visual, full of interesting subjects described in great detail, painted by an artist\",\n",
        "      \"that is highly technical, extremely wordy, extra detailed, confusingly tangental, colorfully worded, dramatically narrative\",\n",
        "      \"that is creative, imaginative, funny, interesting, scenic, dark, witty, visual, unexpected, wild\",\n",
        "      \"that includes more subjects with descriptions, textured color details, expressive\",]\n",
        "      #\"complete sentence using many words to describe a landscape in an epic fantasy genre that includes a lot adjectives\",\n",
        "\n",
        "def run_prompt_remixer(page):\n",
        "  import random as rnd\n",
        "  global artists, styles, status\n",
        "  try:\n",
        "    import openai\n",
        "  except:\n",
        "    run_sp(\"pip install --upgrade openai\")\n",
        "    import openai\n",
        "    pass\n",
        "  try:\n",
        "    openai.api_key = prefs['OpenAI_api_key']\n",
        "  except:\n",
        "    alert_msg(page, \"Invalid OpenAI API Key. Change in Settings...\")\n",
        "    return\n",
        "  status['installed_OpenAI'] = True\n",
        "  prompts_remix = []\n",
        "  prompt_results = []\n",
        "  \n",
        "  if '_' in prefs['prompt_remixer']['seed_prompt']:\n",
        "    seed_prompt = nsp_parse(prefs['prompt_remixer']['seed_prompt'])\n",
        "  else:\n",
        "    seed_prompt = prefs['prompt_remixer']['seed_prompt']\n",
        "  if '_' in prefs['prompt_remixer']['optional_about_influencer']:\n",
        "    optional_about_influencer = nsp_parse(prefs['prompt_remixer']['optional_about_influencer'])\n",
        "  else:\n",
        "    optional_about_influencer = prefs['prompt_remixer']['optional_about_influencer']\n",
        "  about =  f\" about {optional_about_influencer}\" if bool(optional_about_influencer) else \"\"\n",
        "  prompt = f'Write a list of {prefs[\"prompt_remixer\"][\"amount\"]} remixed variations from the following image generation prompt{about}, \"{prefs[\"prompt_remixer\"][\"seed_prompt\"]}\", {remixer_request_modes[int(prefs[\"prompt_remixer\"][\"request_mode\"])]}, and unique without repetition:\\n\\n*'\n",
        "  prompt_results = []\n",
        "  \n",
        "  def prompt_remix():\n",
        "    if prefs['prompt_remixer']['AI_engine'] == \"OpenAI GPT-3\":\n",
        "      response = openai.Completion.create(engine=\"text-davinci-003\", prompt=prompt, max_tokens=2400, temperature=prefs[\"prompt_remixer\"]['AI_temperature'], presence_penalty=1)\n",
        "      #print(response)\n",
        "      result = response[\"choices\"][0][\"text\"].strip()\n",
        "    elif prefs['prompt_remixer']['AI_engine'] == \"ChatGPT-3.5 Turbo\":\n",
        "      response = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=[{\"role\": \"user\", \"content\": prompt}])\n",
        "      #print(str(response))\n",
        "      result = response[\"choices\"][0][\"message\"][\"content\"].strip()\n",
        "    \n",
        "    #if result[-1] == '.': result = result[:-1]\n",
        "    #print(str(result))\n",
        "    for p in result.split('\\n'):\n",
        "      pr = p.strip()\n",
        "      if not bool(pr): continue\n",
        "      if pr[-1] == '.': pr = pr[:-1]\n",
        "      if pr[0] == '*': pr = pr[1:].strip()\n",
        "      elif '.' in pr: # Sometimes got 1. 2.\n",
        "        pr = pr.partition('.')[2].strip()\n",
        "      prompt_results.append(pr)\n",
        "  page.prompt_remixer_list.controls.append(Text(f\"Remixing {seed_prompt}\" + (f\", about {optional_about_influencer}\" if bool(optional_about_influencer) else \"\") + f\"\\nRequest mode influence: {remixer_request_modes[int(prefs['prompt_remixer']['request_mode'])]}\\n\"))\n",
        "  page.prompt_remixer_list.update()\n",
        "  #page.add_to_prompt_remixer(f\"Remixing {seed_prompt}\" + (f\", about {optional_about_influencer}\" if bool(optional_about_influencer) else \"\") + f\"\\nRequest mode influence: {remixer_request_modes[int(prefs['prompt_remixer']['request_mode'])]}\\n\")\n",
        "  #print(f\"Remixing {seed_prompt}\" + (f\", about {optional_about_influencer}\" if bool(optional_about_influencer) else \"\"))\n",
        "  #print(f\"Request mode influence: {remixer_request_modes[int(prefs['prompt_remixer']['request_mode'])]}\\n\")\n",
        "  page.prompt_remixer_list.controls.append(Installing(\"Requesting Prompt Remixes...\"))\n",
        "  page.prompt_remixer_list.update()\n",
        "  prompt_remix()\n",
        "  del page.prompt_remixer_list.controls[-1]\n",
        "  del page.prompt_remixer_list.controls[-1]\n",
        "  page.prompt_remixer_list.update()\n",
        "\n",
        "  for p in prompt_results:\n",
        "    random_artist=[]\n",
        "    for a in range(prefs['prompt_remixer']['random_artists']):\n",
        "      random_artist.append(rnd.choice(artists))\n",
        "    #print(list_variations(random_artist))\n",
        "    artist = \" and \".join([\", \".join(random_artist[:-1]),random_artist[-1]] if len(random_artist) > 2 else random_artist)\n",
        "    random_style = []\n",
        "    for s in range(prefs['prompt_remixer']['random_styles']):\n",
        "      random_style.append(rnd.choice(styles))\n",
        "    style = \", \".join(random_style)\n",
        "    text_prompt = p\n",
        "    if prefs['prompt_remixer']['random_artists'] > 0: text_prompt += f\", by {artist}\"\n",
        "    if prefs['prompt_remixer']['random_styles'] > 0: text_prompt += f\", style of {style}\"\n",
        "    if prefs['prompt_remixer']['random_styles'] == 0 and prefs['prompt_remixer']['permutate_artists']:\n",
        "      prompts_remix.append(text_prompt)\n",
        "    if prefs['prompt_remixer']['permutate_artists']:\n",
        "      for a in list_variations(random_artist):\n",
        "        prompt_variation = p + f\", by {and_list(a)}\"\n",
        "        prompts_remix.append(prompt_variation)\n",
        "      if prefs['prompt_remixer']['random_styles'] > 0:\n",
        "        prompts_remix.append(p + f\", style of {style}\")\n",
        "    else: prompts_remix.append(text_prompt)\n",
        "  for item in prompts_remix:\n",
        "    page.add_to_prompt_remixer(item)\n",
        "\n",
        "def get_stable_lm(ai_model=\"StableLM 3b\"):\n",
        "    global pipe_stable_lm, tokenizer_stable_lm\n",
        "    clear_pipes('stable_lm')\n",
        "    if pipe_stable_lm != None:\n",
        "      return pipe_stable_lm\n",
        "    try:\n",
        "      import accelerate\n",
        "    except ModuleNotFoundError:\n",
        "      run_sp(\"pip install accelerate\", realtime=False)\n",
        "      import accelerate\n",
        "      pass\n",
        "    try:\n",
        "      os.environ['LD_LIBRARY_PATH'] += \"/usr/lib/wsl/lib:$LD_LIBRARY_PATH\"\n",
        "      import bitsandbytes\n",
        "    except ModuleNotFoundError:\n",
        "      if sys.platform.startswith(\"win\"):\n",
        "          run_sp(\"pip install bitsandbytes-windows\", realtime=False)\n",
        "      else:\n",
        "          run_sp(\"pip install bitsandbytes\", realtime=False)\n",
        "      import bitsandbytes\n",
        "      pass\n",
        "    try:\n",
        "      import transformers\n",
        "    except ModuleNotFoundError:\n",
        "      run_sp(\"pip install -q transformers==4.21.3 --upgrade --force-reinstall\", realtime=False)\n",
        "      import transformers\n",
        "      pass\n",
        "    import torch\n",
        "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "    if ai_model == \"StableLM 3b\":\n",
        "        model_name = \"stabilityai/stablelm-tuned-alpha-3b\"\n",
        "    else:\n",
        "        model_name = \"stabilityai/stablelm-base-alpha-7b\"\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        pipe_stable_lm = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            torch_dtype=\"float16\",\n",
        "            load_in_8bit=True,\n",
        "            device_map=\"auto\",\n",
        "            offload_folder=\"./offload\",\n",
        "            cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None\n",
        "        )\n",
        "    except Exception as e:\n",
        "      print(str(e))\n",
        "      return None\n",
        "    return pipe_stable_lm\n",
        "\n",
        "def stable_lm_request(input_sentence, temperature=0.5, max_tokens=2048, top_k=0, top_p=0.9, do_sample=True):\n",
        "    global pipe_stable_lm, tokenizer_stable_lm\n",
        "    inputs = tokenizer_stable_lm(input_sentence, return_tensors=\"pt\")\n",
        "    inputs.to(pipe_stable_lm.device)\n",
        "    tokens = pipe_stable_lm.generate(\n",
        "      **inputs,\n",
        "      max_new_tokens=max_tokens,\n",
        "      temperature=temperature,\n",
        "      top_k=top_k,\n",
        "      top_p=top_p,\n",
        "      do_sample=do_sample,\n",
        "      pad_token_id=tokenizer_stable_lm.eos_token_id,\n",
        "    )\n",
        "    completion_tokens = tokens[0][inputs['input_ids'].size(1):]\n",
        "    completion = tokenizer_stable_lm.decode(completion_tokens, skip_special_tokens=True)\n",
        "    return completion\n",
        "      \n",
        "brainstorm_request_modes = {\n",
        "    \"Brainstorm\":\"Brainstorm visual ideas for an image prompt about \",\n",
        "    \"Write\":\"Write an interesting visual scene about \",\n",
        "    \"Rewrite\":\"Rewrite new variations of \",\n",
        "    \"Edit\":\"Edit this text to improve details and structure: \",\n",
        "    \"Story\":\"Write an interesting story with visual details and poetic subjects about \",\n",
        "    \"Description\":\"Describe in graphic detail \",\n",
        "    \"Picture\":\"Paint a picture with words about \",\n",
        "    \"Raw Request\":\"\",\n",
        "}\n",
        "\n",
        "def run_prompt_brainstormer(page):\n",
        "    import random as rnd\n",
        "    global artists, styles, brainstorm_request_modes\n",
        "    global pipe_stable_lm, tokenizer_stable_lm\n",
        "    textsynth_engine = \"gptj_6B\" #param [\"gptj_6B\", \"boris_6B\", \"fairseq_gpt_13B\", \"gptneox_20B\", \"m2m100_1_2B\"]\n",
        "    #markdown HuggingFace Bloom AI Settings\n",
        "    max_tokens_length = 128 #param {type:'slider', min:1, max:64, step:1}\n",
        "    seed = int(2222 * prefs['prompt_brainstormer']['AI_temperature']) #param {type:'integer'}\n",
        "    API_URL = \"https://api-inference.huggingface.co/models/bigscience/bloom\"\n",
        "\n",
        "    good_key = True\n",
        "    if prefs['prompt_brainstormer']['AI_engine'] == \"TextSynth GPT-J\":\n",
        "      try: \n",
        "        if not bool(prefs['TextSynth_api_key']): good_key = False\n",
        "      except NameError: good_key = False\n",
        "      if not good_key:\n",
        "        print(f\"\\33[91mMissing TextSynth_api_key...\\33[0m Define your key up above.\")\n",
        "      else:\n",
        "        try:\n",
        "          from textsynthpy import TextSynth, Complete\n",
        "        except ImportError:\n",
        "          run_sp(\"pip install textsynthpy\")\n",
        "          clear_output()\n",
        "        finally:\n",
        "          from textsynthpy import TextSynth, Complete\n",
        "        textsynth = TextSynth(prefs['TextSynth_api_key'], engine=textsynth_engine) # Insert your API key in the previous cell\n",
        "    if prefs['prompt_brainstormer']['AI_engine'] == \"OpenAI GPT-3\" or  prefs['prompt_brainstormer']['AI_engine'] == \"ChatGPT-3.5 Turbo\":\n",
        "      try:\n",
        "        if not bool(prefs['OpenAI_api_key']): good_key = False\n",
        "      except NameError: good_key = False\n",
        "      if not good_key:\n",
        "        alert_msg(page, f\"Missing OpenAI_api_key... Define your key in Settings.\")\n",
        "        return\n",
        "      else:\n",
        "        try:\n",
        "          import openai\n",
        "        except ModuleNotFoundError:\n",
        "          run_sp(\"pip install --upgrade openai -qq\")\n",
        "          #clear_output()\n",
        "        finally:\n",
        "          import openai\n",
        "        try:\n",
        "          openai.api_key = prefs['OpenAI_api_key']\n",
        "        except:\n",
        "          alert_msg(page, \"Invalid OpenAI API Key. Change in Settings...\")\n",
        "          return\n",
        "    if prefs['prompt_brainstormer']['AI_engine'] == \"HuggingFace Bloom 176B\" or prefs['prompt_brainstormer']['AI_engine'] == \"HuggingFace Flan-T5 XXL\":\n",
        "      try:\n",
        "        if not bool(prefs['HuggingFace_api_key']): good_key = False\n",
        "      except NameError: good_key = False\n",
        "      if not good_key:\n",
        "        alert_msg(page, f\"Missing HuggingFace_api_key... Define your key in Settings.\")\n",
        "        return\n",
        "    #ask_OpenAI_instead = False #@param {type:'boolean'}\n",
        "\n",
        "    prompt_request_modes = [\n",
        "        \"visually detailed wording, flowing sentences, extra long descriptions\",\n",
        "        \"that is similar but with more details, themes, imagination, interest, subjects, artistic style, poetry, tone, settings, adjectives, visualizations\",\n",
        "        \"that is completely rewritten, inspired by, paints a complete picture of an artistic seen\",\n",
        "        \"with detailed colorful interesting artistic scenic visual descriptions, described to a blind person\",\n",
        "        \"that is highly detailed, artistically interesting, describes a scene, colorful poetic language, with intricate visual descriptions\",\n",
        "        \"that replaces every noun, adjective, verb, pronoun, with related words\",\n",
        "        \"that is strange, descriptive, graphically visual, full of interesting subjects described in great detail, painted by an artist\",\n",
        "        \"that is highly technical, extremely wordy, extra detailed, confusingly tangental, colorfully worded, dramatically narrative\",\n",
        "        \"that is creative, imaginative, funny, interesting, scenic, dark, witty, visual, unexpected, wild\",\n",
        "        \"that includes more subjects with descriptions, textured color details, expressive\",]\n",
        "        #\"complete sentence using many words to describe a landscape in an epic fantasy genre that includes a lot adjectives\",\n",
        "    \n",
        "    request = f'{brainstorm_request_modes[prefs[\"prompt_brainstormer\"][\"request_mode\"]]}\"{prefs[\"prompt_brainstormer\"][\"about_prompt\"]}\":' if prefs['prompt_brainstormer']['request_mode'] != \"Raw Request\" else prefs['prompt_brainstormer']['about_prompt']\n",
        "\n",
        "    def query(payload):\n",
        "        #print(payload)\n",
        "        response = requests.request(\"POST\", API_URL, json=payload, headers={\"Authorization\": f\"Bearer {prefs['HuggingFace_api_key']}\"})\n",
        "        #print(response.text)\n",
        "        return json.loads(response.content.decode(\"utf-8\"))\n",
        "\n",
        "    def bloom_request(input_sentence):\n",
        "        parameters = {\n",
        "            \"max_new_tokens\": max_tokens_length,\n",
        "            \"do_sample\": False,\n",
        "            \"seed\": seed,\n",
        "            \"early_stopping\": False,\n",
        "            \"length_penalty\": 0.0,\n",
        "            \"eos_token_id\": None,}\n",
        "        payload = {\"inputs\": input_sentence, \"parameters\": parameters,\"options\" : {\"use_cache\": False} }\n",
        "        data = query(payload)\n",
        "        if \"error\" in data:\n",
        "            return f\"\\33[31mERROR: {data['error']}\\33[0m\"\n",
        "\n",
        "        generation = data[0][\"generated_text\"].split(input_sentence, 1)[1]\n",
        "        #return data[0][\"generated_text\"]\n",
        "        return generation\n",
        "    \n",
        "    def flan_query(payload):\n",
        "        #print(payload)\n",
        "        response = requests.request(\"POST\", \"https://api-inference.huggingface.co/models/google/flan-t5-xxl\", json=payload, headers={\"Authorization\": f\"Bearer {prefs['HuggingFace_api_key']}\"})\n",
        "        #print(response.text)\n",
        "        return json.loads(response.content.decode(\"utf-8\"))\n",
        "\n",
        "    def flan_request(input_sentence):\n",
        "        parameters = {\n",
        "            \"max_new_tokens\": max_tokens_length,\n",
        "            \"do_sample\": False,\n",
        "            \"seed\": seed,\n",
        "            \"early_stopping\": False,\n",
        "            \"length_penalty\": 0.0,\n",
        "            \"eos_token_id\": None,}\n",
        "        payload = {\"inputs\": input_sentence, \"parameters\": parameters,\"options\" : {\"use_cache\": False} }\n",
        "        data = flan_query(payload)\n",
        "        if \"error\" in data:\n",
        "            return f\"\\33[31mERROR: {data['error']}\\33[0m\"\n",
        "\n",
        "        generation = data[0][\"generated_text\"].split(input_sentence, 1)[1]\n",
        "        #return data[0][\"generated_text\"]\n",
        "        return generation\n",
        "    \n",
        "    def stable_lm_request(input_sentence):\n",
        "        inputs = tokenizer_stable_lm(input_sentence, return_tensors=\"pt\")\n",
        "        inputs.to(pipe_stable_lm.device)\n",
        "        tokens = pipe_stable_lm.generate(\n",
        "          **inputs,\n",
        "          max_new_tokens=2048,\n",
        "          temperature=prefs['prompt_brainstormer']['AI_temperature'],\n",
        "          top_k=0,\n",
        "          top_p=0.9,\n",
        "          do_sample=True,\n",
        "          pad_token_id=tokenizer_stable_lm.eos_token_id,\n",
        "        )\n",
        "        completion_tokens = tokens[0][inputs['input_ids'].size(1):]\n",
        "        completion = tokenizer_stable_lm.decode(completion_tokens, skip_special_tokens=True)\n",
        "        return completion\n",
        "      \n",
        "    def prompt_brainstormer():\n",
        "      #(prompt=prompt, temperature=AI_temperature, presence_penalty=1, stop= \"\\n\")\n",
        "      page.prompt_brainstormer_list.controls.append(Installing(\"Storming the AI's Brain...\"))\n",
        "      page.prompt_brainstormer_list.update()\n",
        "\n",
        "      if prefs['prompt_brainstormer']['AI_engine'] == \"TextSynth GPT-J\":\n",
        "        response = textsynth.text_complete(prompt=request, max_tokens=200, temperature=prefs['prompt_brainstormer']['AI_temperature'], presence_penalty=1)\n",
        "        #print(str(response))\n",
        "        result = response.text.strip()\n",
        "      elif prefs['prompt_brainstormer']['AI_engine'] == \"OpenAI GPT-3\":\n",
        "        response = openai.Completion.create(engine=\"text-davinci-003\", prompt=request, max_tokens=2400, temperature=prefs['prompt_brainstormer']['AI_temperature'], presence_penalty=1)\n",
        "        result = response[\"choices\"][0][\"text\"].strip()\n",
        "      elif prefs['prompt_brainstormer']['AI_engine'] == \"ChatGPT-3.5 Turbo\":\n",
        "        response = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=[{\"role\": \"user\", \"content\": request}])\n",
        "        result = response[\"choices\"][0][\"message\"][\"content\"].strip()\n",
        "      elif prefs['prompt_brainstormer']['AI_engine'] == \"HuggingFace Bloom 176B\":\n",
        "        result = bloom_request(request)\n",
        "      elif prefs['prompt_brainstormer']['AI_engine'] == \"HuggingFace Flan-T5\":\n",
        "        result = flan_request(request) \n",
        "      elif prefs['prompt_brainstormer']['AI_engine'].startswith(\"Stable\"):\n",
        "        if pipe_stable_lm != None and tokenizer_stable_lm != None:\n",
        "          page.add_to_prompt_brainstormer(Installing(\"Installing StableLM-Alpha Pipeline...\"))\n",
        "          pipe_stable_lm = get_stable_lm(prefs['prompt_brainstormer']['AI_engine'])\n",
        "          del page.prompt_brainstormer_list.controls[-1]\n",
        "          page.prompt_brainstormer_list.update()\n",
        "        result = stable_lm_request(request, temperature=prefs['prompt_brainstormer']['AI_temperature']) \n",
        "      page.add_to_prompt_brainstormer(str(result) + '\\n')\n",
        "    #print(f\"Remixing {seed_prompt}\" + (f\", about {optional_about_influencer}\" if bool(optional_about_influencer) else \"\"))\n",
        "    if good_key:\n",
        "      #print(request)\n",
        "      prompt_brainstormer()\n",
        "\n",
        "def run_prompt_writer(page):\n",
        "    '''try:\n",
        "        import nsp_pantry\n",
        "        from nsp_pantry import nsp_parse\n",
        "    except ModuleNotFoundError:\n",
        "        run_sp(\"wget -qq --show-progress --no-cache --backups=1 https://raw.githubusercontent.com/WASasquatch/noodle-soup-prompts/main/nsp_pantry.py\")\n",
        "        #print(subprocess.run(['wget', '-q', '--show-progress', '--no-cache', '--backups=1', 'https://raw.githubusercontent.com/WASasquatch/noodle-soup-prompts/main/nsp_pantry.py'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "    finally:\n",
        "        import nsp_pantry\n",
        "        from nsp_pantry import nsp_parse'''\n",
        "    import random as rnd\n",
        "    global artists, styles\n",
        "    def generate_prompt():\n",
        "      text_prompts = []\n",
        "      global art_Subjects, by_Artists, art_Styles\n",
        "      nsSubjects = nsp_parse(prefs['prompt_writer']['art_Subjects'])\n",
        "      nsArtists = nsp_parse(prefs['prompt_writer']['by_Artists'])\n",
        "      nsStyles = nsp_parse(prefs['prompt_writer']['art_Styles'])\n",
        "      prompt = nsSubjects\n",
        "      random_artist=[]\n",
        "      if nsArtists: random_artist.append(nsArtists)\n",
        "      for a in range(prefs['prompt_writer']['random_artists']):\n",
        "        random_artist.append(rnd.choice(artists))\n",
        "      artist = and_list(random_artist)\n",
        "      #artist = random.choice(artists) + \" and \" + random.choice(artists)\n",
        "      random_style = []\n",
        "      if prefs['prompt_writer']['art_Styles']: random_style.append(nsStyles)\n",
        "      for s in range(prefs['prompt_writer']['random_styles']):\n",
        "        random_style.append(rnd.choice(styles))\n",
        "      style = \", \".join(random_style)\n",
        "      subject_prompt = prompt\n",
        "      if len(artist) > 0: prompt += f\", by {artist}\"\n",
        "      if len(style) > 0: prompt += f\", style of {style}\"\n",
        "      if not prefs['prompt_writer']['permutate_artists']:\n",
        "        return prompt\n",
        "      if prefs['prompt_writer']['random_styles'] > 0 and prefs['prompt_writer']['permutate_artists']:\n",
        "        text_prompts.append(prompt)\n",
        "      if prefs['prompt_writer']['permutate_artists']:\n",
        "        for a in list_variations(random_artist):\n",
        "          prompt_variation = subject_prompt + f\", by {and_list(a)}\"\n",
        "          text_prompts.append(prompt_variation)\n",
        "        if prefs['prompt_writer']['random_styles'] > 0:\n",
        "          text_prompts.append(subject_prompt + f\", style of {style}\")\n",
        "        return text_prompts\n",
        "      #if mod_Custom and mod_Custom.strip(): prompt += mod_Custom)\n",
        "      #return prompt\n",
        "    prompts_writer = []\n",
        "    for p in range(prefs['prompt_writer']['amount']):\n",
        "      prompts_writer.append(generate_prompt())\n",
        "    for item in prompts_writer:\n",
        "      if type(item) is str:\n",
        "        page.add_to_prompt_writer(item)\n",
        "      if type(item) is list:\n",
        "        for i in item:\n",
        "          page.add_to_prompt_writer(i)\n",
        "\n",
        "def run_magic_prompt(page):\n",
        "    #import random as rnd\n",
        "    global artists, styles, magic_prompt_prefs, pipe_gpt2\n",
        "    def prt(line):\n",
        "      if type(line) == str:\n",
        "        line = Text(line, size=17)\n",
        "      page.magic_prompt_output.controls.append(line)\n",
        "      page.magic_prompt_output.update()\n",
        "    def clear_last():\n",
        "      del page.magic_prompt_output.controls[-1]\n",
        "      page.magic_prompt_output.update()\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    prt(Installing(\"Installing Magic Prompt GPT-2 Pipeline...\"))\n",
        "    try:\n",
        "        import jinja2\n",
        "    except:\n",
        "        run_sp(\"pip install -q jinja2==3.0.3\")\n",
        "        pass\n",
        "    try:\n",
        "        from transformers import pipeline, set_seed\n",
        "    except:\n",
        "        run_sp(\"pip install -qq --upgrade git+https://github.com/huggingface/transformers\")\n",
        "        #run_sp(\"pip install torch\")\n",
        "        from transformers import pipeline, set_seed\n",
        "        pass\n",
        "    try:\n",
        "        import sentencepiece\n",
        "    except:\n",
        "        run_sp(\"pip install -q sentencepiece\")\n",
        "        pass\n",
        "    import re\n",
        "    ideas = os.path.join(root_dir, \"ideas.txt\")\n",
        "    if not os.path.exists(ideas):\n",
        "        download_file(\"https://huggingface.co/spaces/Gustavosta/MagicPrompt-Stable-Diffusion/raw/main/ideas.txt\")\n",
        "    prompts_magic = []\n",
        "    prompt_results = []\n",
        "    if '_' in magic_prompt_prefs['seed_prompt']:\n",
        "        seed_prompt = nsp_parse(magic_prompt_prefs['seed_prompt'])\n",
        "    else:\n",
        "        seed_prompt = magic_prompt_prefs['seed_prompt']\n",
        "    clear_pipes(\"gpt2\")\n",
        "    if pipe_gpt2 == None:\n",
        "        try:\n",
        "            pipe_gpt2 = pipeline('text-generation', model='Gustavosta/MagicPrompt-Stable-Diffusion', tokenizer='gpt2')\n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            alert_msg(page, \"Error Initializing GPT-2 Pipeline\", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))\n",
        "            return\n",
        "    with open(ideas, \"r\") as f:\n",
        "        line = f.readlines()\n",
        "    clear_last()\n",
        "    prt(\"Generating Magic Prompts from your Text Input...\")\n",
        "    prt(progress)\n",
        "\n",
        "    def generate(starting_text):\n",
        "        random_seed = int(magic_prompt_prefs['seed']) if int(magic_prompt_prefs['seed']) > 0 else rnd.randint(100, 1000000)\n",
        "        set_seed(random_seed)\n",
        "        if starting_text == \"\":\n",
        "            starting_text: str = line[rnd.randrange(0, len(line))].replace(\"\\n\", \"\").lower().capitalize()\n",
        "            starting_text: str = re.sub(r\"[,:\\-‚Äì.!;?_]\", '', starting_text)\n",
        "        response = pipe_gpt2(starting_text, max_length=(len(starting_text) + rnd.randint(60, 90)), num_return_sequences=int(magic_prompt_prefs['amount']))\n",
        "        response_list = []\n",
        "        for x in response:\n",
        "            resp = x['generated_text'].strip()\n",
        "            if resp != starting_text and len(resp) > (len(starting_text) + 4) and resp.endswith((\":\", \"-\", \"‚Äî\")) is False:\n",
        "                response_list.append(resp)\n",
        "        response_end = \"\\n\".join(response_list)\n",
        "        response_end = re.sub('[^ ]+\\.[^ ]+','', response_end)\n",
        "        response_end = response_end.replace(\"<\", \"\").replace(\">\", \"\")\n",
        "        if response_end != \"\":\n",
        "            return response_end.split(\"\\n\")\n",
        "        else:\n",
        "            prt(\"Error Generating Magic Prompt Responses...\")\n",
        "            return []\n",
        "\n",
        "    #txt = grad.Textbox(lines=1, label=\"Initial Text\", placeholder=\"English Text here\")\n",
        "    #out = grad.Textbox(lines=4, label=\"Generated Prompts\")\n",
        "    #examples = []\n",
        "    #for x in range(8):\n",
        "    #    examples.append(line[rnd.randrange(0, len(line))].replace(\"\\n\", \"\").lower().capitalize())\n",
        "    #title = \"Stable Diffusion Prompt Generator\"\n",
        "    #description = 'This is a demo of the model series: \"MagicPrompt\", in this case, aimed at: \"Stable Diffusion\". To use it, simply submit your text or click on one of the examples. To learn more about the model, [click here](https://huggingface.co/Gustavosta/MagicPrompt-Stable-Diffusion).<br>'\n",
        "    prompt_results = generate(seed_prompt)\n",
        "    clear_last()\n",
        "    clear_last()\n",
        "    for p in prompt_results:\n",
        "        random_artist=[]\n",
        "        for a in range(magic_prompt_prefs['random_artists']):\n",
        "            random_artist.append(rnd.choice(artists))\n",
        "        #print(list_variations(random_artist))\n",
        "        artist = \" and \".join([\", \".join(random_artist[:-1]),random_artist[-1]] if len(random_artist) > 2 else random_artist)\n",
        "        random_style = []\n",
        "        for s in range(magic_prompt_prefs['random_styles']):\n",
        "            random_style.append(rnd.choice(styles))\n",
        "        style = \", \".join(random_style)\n",
        "        text_prompt = p\n",
        "        prompts_magic.append(text_prompt)\n",
        "        if magic_prompt_prefs['random_artists'] > 0: text_prompt += f\", by {artist}\"\n",
        "        if magic_prompt_prefs['random_styles'] > 0: text_prompt += f\", style of {style}\"\n",
        "        #if magic_prompt_prefs['random_styles'] == 0 and magic_prompt_prefs['permutate_artists']:\n",
        "        #    prompts_magic.append(text_prompt)\n",
        "        if magic_prompt_prefs['permutate_artists']:\n",
        "            for a in list_variations(random_artist):\n",
        "                prompt_variation = p + f\", by {and_list(a)}\"\n",
        "                prompts_magic.append(prompt_variation)\n",
        "            if magic_prompt_prefs['random_styles'] > 0:\n",
        "                prompts_magic.append(p + f\", style of {style}\")\n",
        "        else: prompts_magic.append(text_prompt)\n",
        "    for item in prompts_magic:\n",
        "        page.add_to_magic_prompt(item)\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "def run_distil_gpt2(page):\n",
        "    #import random as rnd\n",
        "    global artists, styles, distil_gpt2_prefs, pipe_distil_gpt2\n",
        "    def prt(line):\n",
        "      if type(line) == str:\n",
        "        line = Text(line, size=17)\n",
        "      page.distil_gpt2_output.controls.append(line)\n",
        "      page.distil_gpt2_output.update()\n",
        "    def clear_last():\n",
        "      del page.distil_gpt2_output.controls[-1]\n",
        "      page.distil_gpt2_output.update()\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    prt(Installing(\"Installing Distil GPT-2 Pipeline...\"))\n",
        "\n",
        "    try:\n",
        "        from transformers import GPT2Tokenizer, GPT2LMHeadModel, set_seed\n",
        "    except:\n",
        "        run_sp(\"pip install -qq --upgrade git+https://github.com/huggingface/transformers\")\n",
        "        #run_sp(\"pip install torch\")\n",
        "        from transformers import GPT2Tokenizer, GPT2LMHeadModel, set_seed\n",
        "        pass\n",
        "\n",
        "    prompts_distil = []\n",
        "    prompt_results = []\n",
        "    if '_' in distil_gpt2_prefs['seed_prompt']:\n",
        "        seed_prompt = nsp_parse(distil_gpt2_prefs['seed_prompt'])\n",
        "    else:\n",
        "        seed_prompt = distil_gpt2_prefs['seed_prompt']\n",
        "    clear_pipes(\"distil_gpt2\")\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\n",
        "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "    if pipe_distil_gpt2 == None:\n",
        "        try:\n",
        "            pipe_distil_gpt2 = GPT2LMHeadModel.from_pretrained('FredZhang7/distilgpt2-stable-diffusion-v2')\n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            alert_msg(page, \"Error Initializing Distil GPT-2 Pipeline\", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))\n",
        "            return\n",
        "    \n",
        "    clear_last()\n",
        "    prt(\"Generating Distil GPT-2 Results from your Text Input...\")\n",
        "    prt(progress)\n",
        "\n",
        "    def generate(starting_text):\n",
        "        random_seed = int(distil_gpt2_prefs['seed']) if int(distil_gpt2_prefs['seed']) > 0 else rnd.randint(100, 1000000)\n",
        "        set_seed(random_seed)\n",
        "        input_ids = tokenizer(starting_text, return_tensors='pt').input_ids\n",
        "        output = pipe_distil_gpt2.generate(input_ids, do_sample=True, temperature=distil_gpt2_prefs['AI_temperature'], top_k=distil_gpt2_prefs['top_k'], max_length=distil_gpt2_prefs['max_length'], num_return_sequences=distil_gpt2_prefs['amount'], repetition_penalty=distil_gpt2_prefs['repetition_penalty'], penalty_alpha=distil_gpt2_prefs['penalty_alpha'], no_repeat_ngram_size=distil_gpt2_prefs['no_repeat_ngram_size'], early_stopping=True)\n",
        "        results = []\n",
        "        for i in range(len(output)):\n",
        "            results.append(tokenizer.decode(output[i], skip_special_tokens=True))\n",
        "        return results\n",
        "    prompt_results = generate(seed_prompt)\n",
        "    clear_last()\n",
        "    clear_last()\n",
        "    for p in prompt_results:\n",
        "        random_artist=[]\n",
        "        for a in range(distil_gpt2_prefs['random_artists']):\n",
        "            random_artist.append(rnd.choice(artists))\n",
        "        #print(list_variations(random_artist))\n",
        "        artist = \" and \".join([\", \".join(random_artist[:-1]),random_artist[-1]] if len(random_artist) > 2 else random_artist)\n",
        "        random_style = []\n",
        "        for s in range(distil_gpt2_prefs['random_styles']):\n",
        "            random_style.append(rnd.choice(styles))\n",
        "        style = \", \".join(random_style)\n",
        "        text_prompt = p\n",
        "        prompts_distil.append(text_prompt)\n",
        "        if distil_gpt2_prefs['random_artists'] > 0: text_prompt += f\", by {artist}\"\n",
        "        if distil_gpt2_prefs['random_styles'] > 0: text_prompt += f\", style of {style}\"\n",
        "        #if distil_gpt2_prefs['random_styles'] == 0 and distil_gpt2_prefs['permutate_artists']:\n",
        "        #    prompts_distil.append(text_prompt)\n",
        "        if distil_gpt2_prefs['permutate_artists']:\n",
        "            for a in list_variations(random_artist):\n",
        "                prompt_variation = p + f\", by {and_list(a)}\"\n",
        "                prompts_distil.append(prompt_variation)\n",
        "            if distil_gpt2_prefs['random_styles'] > 0:\n",
        "                prompts_distil.append(p + f\", style of {style}\")\n",
        "        else: prompts_distil.append(text_prompt)\n",
        "    for item in prompts_distil:\n",
        "        page.add_to_distil_gpt2(item)\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "\n",
        "def run_upscaling(page):\n",
        "    #print(str(ESRGAN_prefs))\n",
        "    if not status['installed_ESRGAN']:\n",
        "      alert_msg(page, \"You must Install Real-ESRGAN first\")\n",
        "      return\n",
        "    import re\n",
        "    from collections import Counter\n",
        "    enlarge_scale = ESRGAN_prefs['enlarge_scale']\n",
        "    face_enhance = ESRGAN_prefs['face_enhance']\n",
        "    image_path = ESRGAN_prefs['image_path']\n",
        "    save_to_GDrive = ESRGAN_prefs['save_to_GDrive']\n",
        "    upload_file = ESRGAN_prefs['upload_file']\n",
        "    download_locally = ESRGAN_prefs['download_locally']\n",
        "    display_image = ESRGAN_prefs['display_image']\n",
        "    dst_image_path = ESRGAN_prefs['dst_image_path']\n",
        "    filename_suffix = ESRGAN_prefs['filename_suffix']\n",
        "    split_image_grid = ESRGAN_prefs['split_image_grid']\n",
        "    rows = ESRGAN_prefs['rows']\n",
        "    cols = ESRGAN_prefs['cols']\n",
        "    def split(im, rows, cols, img_path, should_cleanup=False):\n",
        "        im_width, im_height = im.size\n",
        "        row_width = int(im_width / rows)\n",
        "        row_height = int(im_height / cols)\n",
        "        n = 0\n",
        "        for i in range(0, cols):\n",
        "            for j in range(0, rows):\n",
        "                box = (j * row_width, i * row_height, j * row_width +\n",
        "                      row_width, i * row_height + row_height)\n",
        "                outp = im.crop(box)\n",
        "                name, ext = os.path.splitext(img_path)\n",
        "                outp_path = name + \"-\" + str(n) + ext\n",
        "                #print(\"Exporting image tile: \" + outp_path)\n",
        "                outp.save(outp_path)\n",
        "                n += 1\n",
        "        if should_cleanup:\n",
        "            #print(\"Cleaning up: \" + img_path)\n",
        "            os.remove(img_path)\n",
        "    \n",
        "    os.chdir(os.path.join(dist_dir, 'Real-ESRGAN'))\n",
        "    upload_folder = 'upload'\n",
        "    result_folder = 'results'\n",
        "    if os.path.isdir(upload_folder):\n",
        "        shutil.rmtree(upload_folder)\n",
        "    if os.path.isdir(result_folder):\n",
        "        shutil.rmtree(result_folder)\n",
        "    os.mkdir(upload_folder)\n",
        "    os.mkdir(result_folder)\n",
        "\n",
        "    uploaded = None\n",
        "    if not upload_file:\n",
        "      if not image_path:\n",
        "         alert_msg(page, 'Provide path to image, local or url')\n",
        "         return\n",
        "      if '.' in image_path:\n",
        "        if os.path.exists(image_path):\n",
        "          uploaded = {image_path: image_path.rpartition(slash)[2]}\n",
        "        else:\n",
        "          alert_msg(page, 'File does not exist')\n",
        "          return\n",
        "      else:\n",
        "        if os.path.isdir(image_path):\n",
        "          uploaded = {}\n",
        "          for f in os.listdir(image_path):\n",
        "            uploaded[ os.path.join(image_path, f)] = f\n",
        "        else:\n",
        "          alert_msg(page, 'Image Path directory does not exist')\n",
        "          return\n",
        "    else:\n",
        "      uploaded = files.upload()\n",
        "    page.clear_ESRGAN_output(uploaded)\n",
        "    page.add_to_ESRGAN_output(Text(f\"Upscaling {len(uploaded)} images..\"))\n",
        "    for filename in uploaded.keys():\n",
        "      if not os.path.isfile(filename):\n",
        "        #print(\"Skipping \" + filename)\n",
        "        continue\n",
        "      fname = filename.rpartition(slash)[2] if slash in filename else filename\n",
        "      dst_path = os.path.join(upload_folder, fname)\n",
        "      #print(f'Copy {filename} to {dst_path}')\n",
        "      shutil.copy(filename, dst_path)\n",
        "      if split_image_grid:\n",
        "        img = PILImage.open(dst_path)\n",
        "        split(img, rows, cols, dst_path, True)\n",
        "    os.chdir(os.path.join(dist_dir, 'Real-ESRGAN'))\n",
        "    faceenhance = ' --face_enhance' if face_enhance else ''\n",
        "    run_sp(f'python inference_realesrgan.py -n realesr-general-x4v3 -i {upload_folder} --outscale {enlarge_scale}{faceenhance}', cwd=os.path.join(dist_dir, 'Real-ESRGAN'), realtime=False)\n",
        "    os.chdir(root_dir)\n",
        "    if is_Colab:\n",
        "      from google.colab import files\n",
        "    if not bool(dst_image_path.strip()):\n",
        "      if os.path.isdir(image_path):\n",
        "          dst_image_path = image_path\n",
        "      else:\n",
        "          dst_image_path = prefs['image_output'] #image_path.rpartition(slash)[0]\n",
        "    filenames = os.listdir(os.path.join(dist_dir, 'Real-ESRGAN', 'results'))\n",
        "    for fname in filenames:\n",
        "      fparts = fname.rpartition('_out')\n",
        "      fname_clean = fparts[0] + filename_suffix + fparts[2]\n",
        "      #print(f'Copying {fname_clean}')\n",
        "      if save_to_GDrive:\n",
        "        if not os.path.isdir(dst_image_path):\n",
        "          os.makedirs(dst_image_path)\n",
        "        shutil.copy(os.path.join(dist_dir, 'Real-ESRGAN', 'results', fname), os.path.join(dst_image_path, fname_clean))\n",
        "      else: # TODO PyDrive\n",
        "        shutil.copy(os.path.join(dist_dir, 'Real-ESRGAN', 'results', fname), os.path.join(dst_image_path, fname_clean))\n",
        "      if download_locally:\n",
        "        files.download(os.path.join(dist_dir, 'Real-ESRGAN', 'results', fname))\n",
        "      if display_image:\n",
        "        page.add_to_ESRGAN_output(Image(src=os.path.join(dist_dir, 'Real-ESRGAN', 'results', fname)))\n",
        "      page.add_to_ESRGAN_output(Row([Text(os.path.join(dst_image_path, fname_clean))], alignment=MainAxisAlignment.CENTER))\n",
        "\n",
        "def run_retrieve(page):\n",
        "    upload_file = retrieve_prefs['upload_file']\n",
        "    image_path = retrieve_prefs['image_path']\n",
        "    display_full_metadata = retrieve_prefs['display_full_metadata']\n",
        "    display_image = retrieve_prefs['display_image']\n",
        "    add_to_prompts = retrieve_prefs['add_to_prompts']\n",
        "\n",
        "    import json\n",
        "    if is_Colab:\n",
        "      from google.colab import files\n",
        "    def meta_dream(meta):\n",
        "      if meta is not None and len(meta) > 1:\n",
        "          #d = Dream(meta[\"prompt\"])\n",
        "          print(str(meta))\n",
        "          arg = {}\n",
        "          p = ''\n",
        "          dream = '    Dream('\n",
        "          if meta.get('title'):\n",
        "            dream += f'\"{meta[\"title\"]}\"'\n",
        "            p = meta[\"title\"]\n",
        "          if meta.get('prompt'):\n",
        "            dream += f'\"{meta[\"prompt\"]}\"'\n",
        "            p = meta[\"prompt\"]\n",
        "          if meta.get('config'):\n",
        "            meta = meta['config']\n",
        "          if meta.get('prompt'):\n",
        "            #dream += f'\"{meta[\"prompt\"]}\"'\n",
        "            p = meta[\"prompt\"]\n",
        "          if meta.get('prompt2'):\n",
        "            dream += f', prompt2=\"{meta[\"prompt2\"]}\"'\n",
        "            arg[\"prompt2\"] = meta[\"prompt2\"]\n",
        "          if meta.get('negative_prompt'):\n",
        "            dream += f', negative=\"{meta[\"negative_prompt\"]}\"'\n",
        "            arg[\"negative_prompt\"] = meta[\"negative_prompt\"]\n",
        "          if meta.get('tweens'):\n",
        "            dream += f', tweens={meta[\"tweens\"]}'\n",
        "            arg[\"tweens\"] = meta[\"tweens\"]\n",
        "          if meta.get('width'):\n",
        "            dream += f', width={meta[\"width\"]}'\n",
        "            arg[\"width\"] = meta[\"width\"]\n",
        "          if meta.get('height'):\n",
        "            dream += f', height={meta[\"height\"]}'\n",
        "            arg[\"height\"] = meta[\"height\"]\n",
        "          if meta.get('guidance_scale'):\n",
        "            dream += f', guidance_scale={meta[\"guidance_scale\"]}'\n",
        "            arg[\"guidance_scale\"] = meta[\"guidance_scale\"]\n",
        "          elif meta.get('CGS'):\n",
        "            dream += f', guidance_scale={meta[\"CGS\"]}'\n",
        "            arg[\"guidance_scale\"] = meta[\"CGS\"]\n",
        "          if meta.get('steps'):\n",
        "            dream += f', steps={meta[\"steps\"]}'\n",
        "            arg[\"steps\"] = meta[\"steps\"]\n",
        "          if meta.get('eta'):\n",
        "            dream += f', eta={meta[\"eta\"]}'\n",
        "            arg[\"eta\"] = meta[\"eta\"]\n",
        "          if meta.get('seed'):\n",
        "            dream += f', seed={meta[\"seed\"]}'\n",
        "            arg[\"seed\"] = meta[\"seed\"]\n",
        "          if meta.get('init_image'):\n",
        "            dream += f', init_image=\"{meta[\"init_image\"]}\"'\n",
        "            arg[\"init_image\"] = meta[\"init_image\"]\n",
        "          if meta.get('mask_image'):\n",
        "            dream += f', mask_image=\"{meta[\"mask_image\"]}\"'\n",
        "            arg[\"mask_image\"] = meta[\"mask_image\"]\n",
        "          if meta.get('init_image_strength'):\n",
        "            dream += f', init_image_strength={meta[\"init_image_strength\"]}'\n",
        "            arg[\"init_image_strength\"] = meta[\"init_image_strength\"]\n",
        "          dream += '),'\n",
        "          page.add_to_retrieve_output(Text(dream, selectable=True))\n",
        "          if display_full_metadata:\n",
        "            page.add_to_retrieve_output(Text(str(metadata)))\n",
        "          if add_to_prompts:\n",
        "            page.add_to_prompts(p, arg)\n",
        "      else:\n",
        "          alert_msg(page, 'Problem reading your config json image meta data.')\n",
        "          return\n",
        "    uploaded = {}\n",
        "    if not upload_file:\n",
        "      if not bool(image_path):\n",
        "        alert_msg(page, 'Provide path to image, local or url')\n",
        "        return\n",
        "      if '.' in image_path:\n",
        "        if os.path.exists(image_path):\n",
        "          uploaded = {image_path: image_path.rpartition(slash)[2]}\n",
        "        else:\n",
        "          alert_msg(page, 'File does not exist')\n",
        "          return\n",
        "      else:\n",
        "        if os.path.isdir(image_path):\n",
        "          uploaded = {}\n",
        "          for f in os.listdir(image_path):\n",
        "            uploaded[ os.path.join(image_path, f)] = f\n",
        "        else:\n",
        "          alert_msg(page, 'The image_path directory does not exist')\n",
        "          return\n",
        "    else:\n",
        "      if not is_Colab:\n",
        "        uploaded = files.upload()\n",
        "        alert_msg(page, \"Can't upload an image easily from non-Colab systems\")\n",
        "        return\n",
        "    if len(uploaded) > 1:\n",
        "      page.add_to_retrieve_output(Text(f\"Revealing Dream of {len(uploaded)} images..\\n\"))\n",
        "    for filename in uploaded.keys():\n",
        "      if not os.path.isfile(filename):\n",
        "        #print(\"Skipping subfolder \" + filename)\n",
        "        continue\n",
        "      #print(filename)\n",
        "      if filename.rpartition('.')[2] == 'json':\n",
        "        meta = json.load(filename)\n",
        "        meta_dream(meta)\n",
        "      elif filename.rpartition('.')[2] == 'png':\n",
        "        img = PILImage.open(filename)\n",
        "        metadata = img.info\n",
        "        if display_image:\n",
        "          page.add_to_retrieve_output(Img(src=filename, gapless_playback=True))\n",
        "          #display(img)\n",
        "        if metadata is None or len(metadata) < 1:\n",
        "          alert_msg(page, 'Sorry, image has no exif data.')\n",
        "          return\n",
        "          #print(metadata)\n",
        "        else:\n",
        "          if metadata.get('config_json'):\n",
        "            json_txt = metadata['config_json']\n",
        "            #print(json_txt)\n",
        "            meta = json.loads(json_txt)\n",
        "            meta_dream(meta)\n",
        "          elif metadata.get('config'):\n",
        "            config = metadata['config']\n",
        "            meta = {}\n",
        "            key = \"\"\n",
        "            val = \"\"\n",
        "            if metadata.get('title'):\n",
        "              meta['prompt'] = metadata['title']\n",
        "            for col in config.split(':'):\n",
        "              #print(col.strip())\n",
        "              if ',' not in col:\n",
        "                key = col\n",
        "              else:\n",
        "                parts = col.rpartition(',')\n",
        "                val = parts[0].strip()\n",
        "                if bool(key) and bool(val):\n",
        "                  meta[key] = val\n",
        "                  val = ''\n",
        "                key = parts[2].strip()\n",
        "            #print(meta)\n",
        "            meta_dream(meta)\n",
        "            #print(dream)\n",
        "          else:\n",
        "            alert_msg(page, \"No Enhanced Stable Diffusion config metadata found inside image.\")\n",
        "\n",
        "def run_initfolder(page):\n",
        "    prompt_string = initfolder_prefs['prompt_string']\n",
        "    init_folder = initfolder_prefs['init_folder']\n",
        "    include_strength = initfolder_prefs['include_strength']\n",
        "    image_strength = initfolder_prefs['image_strength']\n",
        "    #init_image='/content/ pic.png', init_image_strength=0.4\n",
        "    if bool(prompt_string):\n",
        "      p_str = f'\"{prompt_string.strip()}\"'\n",
        "      skip_str = f', init_image_strength={image_strength}' if bool(include_strength) else ''\n",
        "      if os.path.isdir(init_folder):\n",
        "        arg = {}\n",
        "        #print(\"prompts = [\")\n",
        "        for f in os.listdir(init_folder):\n",
        "          init_path = os.path.join(init_folder, f)\n",
        "          if os.path.isdir(init_path): continue\n",
        "          if f.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "            page.add_to_initfolder_output(Text(f'    Dream({p_str}, init_image=\"{init_path}\"{skip_str}),'))\n",
        "            if bool(initfolder_prefs['negative_prompt']):\n",
        "              arg['negative_prompt'] = initfolder_prefs['negative_prompt']\n",
        "            arg['init_image'] = init_path\n",
        "            if bool(include_strength):\n",
        "              arg['init_image_strength'] = image_strength\n",
        "            page.add_to_prompts(prompt_string, arg)\n",
        "        if not bool(status['installed_img2img']):\n",
        "          alert_msg(page, 'Make sure you Install the Image2Image module before running Stable Diffusion on prompts...')\n",
        "       # print(\"]\")\n",
        "      else:\n",
        "        alert_msg(page, 'The init_folder directory does not exist.')\n",
        "    else: alert_msg(page, 'Your prompt_string is empty. What do you want to apply to images?')\n",
        "\n",
        "def run_init_video(page):\n",
        "    prompt = init_video_prefs['prompt']\n",
        "    video_file = init_video_prefs['video_file']\n",
        "    file_prefix = init_video_prefs['file_prefix']\n",
        "    try:\n",
        "        start_time = float(init_video_prefs['start_time'])\n",
        "        end_time = float(init_video_prefs['end_time'])\n",
        "        fps = int(init_video_prefs['fps'])\n",
        "    except Exception:\n",
        "        alert_msg(page, \"Make sure your Numbers are actual numbers...\")\n",
        "        return\n",
        "    max_size = init_video_prefs['max_size']\n",
        "    show_images = init_video_prefs['show_images']\n",
        "    output_dir = os.path.join(stable_dir, init_video_prefs['batch_folder_name'])\n",
        "    if not bool(prompt):\n",
        "        alert_msg(page, \"Provide a good prompt to apply to All Frames in List.\")\n",
        "        return\n",
        "    def clear_last():\n",
        "      del page.init_video_output.controls[-1]\n",
        "      page.init_video_output.update()\n",
        "    page.init_video_output.controls.clear()\n",
        "    page.init_video_output.update()\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "    \n",
        "    if video_file.startswith(\"http\"):\n",
        "        page.add_to_init_video_output(Text(\"Downloading the Video File...\"))\n",
        "        local = download_file(video_file)\n",
        "        vname = local.rpartition(slash)\n",
        "        vout = os.path.join(root_dir, vname)\n",
        "        shutil.move(local, vout)\n",
        "        video_file = vout\n",
        "        clear_last()\n",
        "    if not bool(video_file):\n",
        "        alert_msg(page, \"Provide a valid Video File to Extract...\")\n",
        "        return\n",
        "    else:\n",
        "        if not os.path.exists(video_file):\n",
        "            alert_msg(page, \"The provided Video File doesn't Exist...\")\n",
        "            return\n",
        "    \n",
        "    '''try:\n",
        "        import ffmpeg\n",
        "    except Exception:\n",
        "        run_process(\"pip install -q ffmpeg\", page=page)\n",
        "        import ffmpeg\n",
        "        pass\n",
        "    def convert_video_to_images(video_file, fps, start_time, end_time, output_dir):\n",
        "        \"\"\"\n",
        "        Convert a video file to a sequence of images.\n",
        "        \"\"\"\n",
        "        # Check if the output directory exists, otherwise create it.\n",
        "        command = ['ffmpeg',\n",
        "                '-i', video_file,\n",
        "                #'-r', str(fps),\n",
        "                #'-ss', str(start_time),\n",
        "                #'-t', str(end_time),\n",
        "                '-vf', f'fps={fps}'\n",
        "                #'-vf', \"select='between(t,2,6)+between(t,15,24)'\"\n",
        "                #'-qscale:v', '2',\n",
        "                '-vsync', \"0\"\n",
        "                '-f', 'image2',\n",
        "                os.path.join(output_dir, '%08d.png')]\n",
        "\n",
        "        run_process(command, page, print=True)\n",
        "    try:\n",
        "        import imageio\n",
        "    except Exception:\n",
        "        run_process(\"pip install -q imageio\", page=page)\n",
        "        import imageio\n",
        "        pass\n",
        "    def convert_video_to_images(video_file, fps, start_time, end_time, resolution):\n",
        "        # create video reader object \n",
        "        reader = imageio.get_reader(video_file)\n",
        "        # set the reader parameters\n",
        "        reader.set_fps(fps)\n",
        "        reader.set_start_time(start_time)\n",
        "        reader.set_end_time(end_time)\n",
        "        # get list of frames\n",
        "        frames = reader.get_meta_data()['nframes']\n",
        "        # loop through each frame and save as png with frame number in filename\n",
        "        for frame in range(frames):\n",
        "            frame_img = reader.get_data(frame)\n",
        "            frame_img = frame_img.resize(resolution)\n",
        "            imageio.imwrite(\"frame_{}.png\".format(frame), frame_img)  \n",
        "        reader.close()'''\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    page.add_to_init_video_output(Installing(\"Processing Video File...\"))\n",
        "    page.add_to_init_video_output(progress)\n",
        "    try:\n",
        "        import cv2\n",
        "    except ModuleNotFoundError:\n",
        "        run_process(\"pip install -q opencv-python\", page=page)\n",
        "        import cv2\n",
        "        pass\n",
        "    try:\n",
        "        cap = cv2.VideoCapture(video_file)\n",
        "    except Exception as e:\n",
        "        alert_msg(page, \"ERROR Reading Video File. May be Incompatible Format...\")\n",
        "        clear_last()\n",
        "        return\n",
        "    count = 0\n",
        "    files = []\n",
        "    frames = []\n",
        "    w = h = 0\n",
        "    cap.set(cv2.CAP_PROP_FPS, fps)\n",
        "    video_length = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
        "    start_frame = int(start_time * fps)\n",
        "    if end_time == 0 or end_time == 0.0:\n",
        "        end_frame = int(video_length)\n",
        "    else:\n",
        "        end_frame = int(end_time * fps)\n",
        "    total = end_frame - start_frame\n",
        "\n",
        "    #print(f\"Length: {video_length}, start_frame: {start_frame}, end_frame: {end_frame}\")\n",
        "    for i in range(start_frame, end_frame):\n",
        "        # Read frame\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
        "        success, image = cap.read()\n",
        "        \n",
        "        # Save frame as png\n",
        "        if success:\n",
        "            filename = os.path.join(output_dir, f'{file_prefix}{count}.png')\n",
        "            if w == 0:\n",
        "                shape = image.shape\n",
        "                w, h = scale_dimensions(shape[1], shape[0], max=max_size, multiple=16)\n",
        "                clear_last()\n",
        "                page.add_to_init_video_output(Text(f'Extracting {len(frames)} frames at {w}x{h}, {video_length} seconds long...'))\n",
        "            image = cv2.resize(image, (w, h), interpolation = cv2.INTER_AREA)\n",
        "            percent = count / total\n",
        "            progress.value = percent\n",
        "            progress.update()\n",
        "            #print(f\"{count / total}% - Saving {filename} - {shape}\")\n",
        "            cv2.imwrite(os.path.join(output_dir, filename), image)\n",
        "            files.append(filename)\n",
        "            if show_images:\n",
        "                page.add_to_init_video_output(Row([Img(src=filename, width=w, height=h, fit=ImageFit.FILL, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "                #page.add_to_init_video_output(Row([Text(filename)], alignment=MainAxisAlignment.CENTER))\n",
        "            count += 1\n",
        "    cap.release()\n",
        "    if show_images:\n",
        "        progress.value = 0.0\n",
        "        progress.update()\n",
        "    else:\n",
        "        clear_last()\n",
        "        clear_last()\n",
        "    '''\n",
        "    cap.set(cv2.CAP_PROP_POS_MSEC, start_time * 1000)\n",
        "    end_time = min(end_time, video_length / fps)\n",
        "    cap.set(cv2.CAP_PROP_FPS, fps)\n",
        "    print(f\"Length: {video_length}, end_time: {end_time}\")\n",
        "    \n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        current_time = cap.get(cv2.CAP_PROP_POS_MSEC) / 1000.0\n",
        "        # Break if past end time\n",
        "        if current_time > end_time and (end_time != 0.0 or end_time == 0):\n",
        "            break\n",
        "        frames.append(frame)\n",
        "        print(f\"{count} - ret:{ret}, time: {current_time}\")\n",
        "        count += 1\n",
        "    for i, frame in enumerate(frames):\n",
        "        filename = os.path.join(output_dir, f'{file_prefix}{i}.png')\n",
        "        if w == 0:\n",
        "            shape = frame.shape\n",
        "            w, h = scale_dimensions(shape[1], shape[0], max=max_size, multiple=16)\n",
        "            clear_last()\n",
        "            page.add_to_init_video_output(Text(f'Extracting {len(frames)} frames at {w}x{h}, {video_length} seconds long...'))\n",
        "        frame = cv2.resize(frame, (w, h), interpolation = cv2.INTER_AREA)\n",
        "        cv2.imwrite(filename, frame)\n",
        "        files.append(filename)\n",
        "        if show_images:\n",
        "            page.add_to_init_video_output(Row([Img(src=filename, width=w, height=h, fit=ImageFit.FILL, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "            page.add_to_init_video_output(Row([Text(filename)], alignment=MainAxisAlignment.CENTER))\n",
        "    cap.release()'''\n",
        "    if not bool(files):\n",
        "        alert_msg(page, \"ERROR Creating Images from Video...\")\n",
        "        return\n",
        "    \n",
        "    for f in files:\n",
        "        arg = {}\n",
        "        if bool(init_video_prefs['negative_prompt']):\n",
        "            arg['negative_prompt'] = init_video_prefs['negative_prompt']\n",
        "        arg['init_image'] = f\n",
        "        arg['width'] = w\n",
        "        arg['height'] = h\n",
        "        if bool(init_video_prefs['include_strength']):\n",
        "            arg['init_image_strength'] = init_video_prefs['image_strength']\n",
        "        page.add_to_prompts(prompt, arg)\n",
        "    page.add_to_init_video_output(Text(f'Added {len(files)} Files as Init-Image in Prompts List...', weight=FontWeight.BOLD))\n",
        "    page.add_to_init_video_output(Text(f'Saved to {output_dir}'))\n",
        "    if prefs['enable_sounds']: page.snd_drop.play()\n",
        "\n",
        "def multiple_of_64(x):\n",
        "    return int(round(x/64)*64)\n",
        "def multiple_of_8(x):\n",
        "    return int(round(x/8)*8)\n",
        "def multiple_of(x, num):\n",
        "    return int(round(x/num)*num)\n",
        "def scale_dimensions(width, height, max=1024, multiple=16):\n",
        "  max = int(max)\n",
        "  r_width = width\n",
        "  r_height = height\n",
        "  if width < max and height < max:\n",
        "    if width >= height:\n",
        "      ratio = max / width\n",
        "      r_width = max\n",
        "      r_height = int(height * ratio)\n",
        "    else:\n",
        "      ratio = max / height\n",
        "      r_height = max\n",
        "      r_width = int(width * ratio)\n",
        "    width = r_width\n",
        "    height = r_height\n",
        "  if width >= height:\n",
        "    if width > max:\n",
        "      r_width = max\n",
        "      r_height = int(height * (max/width))\n",
        "    else:\n",
        "      r_width = width\n",
        "      r_height = height\n",
        "  else:\n",
        "    if height > max:\n",
        "      r_height = max\n",
        "      r_width = int(width * (max/height))\n",
        "    else:\n",
        "      r_width = width\n",
        "      r_height = height\n",
        "  return multiple_of(r_width, multiple), multiple_of(r_height, multiple)\n",
        "\n",
        "def run_repainter(page):\n",
        "    global repaint_prefs, prefs, status, pipe_repaint\n",
        "    if not status['installed_diffusers']:\n",
        "      alert_msg(page, \"You need to Install HuggingFace Diffusers before using...\")\n",
        "      return\n",
        "    if not bool(repaint_prefs['original_image']) or not bool(repaint_prefs['mask_image']):\n",
        "      alert_msg(page, \"You must provide the Original Image and the Mask Image to process...\")\n",
        "      return\n",
        "    def prt(line):\n",
        "      if type(line) == str:\n",
        "        line = Text(line, size=17)\n",
        "      page.RePainter.controls.append(line)\n",
        "      page.RePainter.update()\n",
        "    def clear_last():\n",
        "      del page.RePainter.controls[-1]\n",
        "      page.RePainter.update()\n",
        "    def autoscroll(scroll=True):\n",
        "      page.RePainter.auto_scroll = scroll\n",
        "      page.RePainter.update()\n",
        "    def clear_list():\n",
        "      page.RePainter.controls = page.RePainter.controls[:1]\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:\n",
        "      callback_fnc.has_been_called = True\n",
        "      nonlocal progress\n",
        "      total_steps = len(latents)\n",
        "      percent = (step +1)/ total_steps\n",
        "      progress.value = percent\n",
        "      progress.tooltip = f\"{step +1} / {total_steps}  Timestep: {timestep}\"\n",
        "      progress.update()\n",
        "      #print(f'{type(latents)} {len(latents)}- {str(latents)}')\n",
        "    autoscroll(True)\n",
        "    clear_list()\n",
        "    prt(Installing(\"Installing RePaint Pipeline...\"))\n",
        "    import requests, random\n",
        "    from io import BytesIO\n",
        "    from PIL import ImageOps\n",
        "    if repaint_prefs['original_image'].startswith('http'):\n",
        "      #response = requests.get(repaint_prefs['original_image'])\n",
        "      #original_img = PILImage.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "      original_img = PILImage.open(requests.get(repaint_prefs['original_image'], stream=True).raw)\n",
        "    else:\n",
        "      if os.path.isfile(repaint_prefs['original_image']):\n",
        "        original_img = PILImage.open(repaint_prefs['original_image'])\n",
        "      else:\n",
        "        alert_msg(page, f\"ERROR: Couldn't find your original_image {repaint_prefs['original_image']}\")\n",
        "        return\n",
        "    width, height = original_img.size\n",
        "    width, height = scale_dimensions(width, height, repaint_prefs['max_size'])\n",
        "    original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS)\n",
        "    original_img = ImageOps.exif_transpose(original_img).convert(\"RGB\")\n",
        "    mask_img = None\n",
        "    if repaint_prefs['mask_image'].startswith('http'):\n",
        "      #response = requests.get(repaint_prefs['mask_image'])\n",
        "      #mask_img = PILImage.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "      mask_img = PILImage.open(requests.get(repaint_prefs['mask_image'], stream=True).raw)\n",
        "    else:\n",
        "      if os.path.isfile(repaint_prefs['mask_image']):\n",
        "        mask_img = PILImage.open(repaint_prefs['mask_image'])\n",
        "      else:\n",
        "        alert_msg(page, f\"ERROR: Couldn't find your mask_image {repaint_prefs['mask_image']}\")\n",
        "        return\n",
        "    #mask_img = mask_img.convert(\"L\")\n",
        "    #mask_img = mask_img.convert(\"1\")\n",
        "    if repaint_prefs['invert_mask']:\n",
        "       mask_img = ImageOps.invert(mask_img.convert('RGB'))\n",
        "    mask_img = mask_img.resize((width, height), resample=PILImage.NEAREST)\n",
        "    mask_img = ImageOps.exif_transpose(mask_img).convert(\"RGB\")\n",
        "    #print(f'Resize to {width}x{height}')\n",
        "    clear_pipes('repaint')\n",
        "    if not status['installed_repaint']:\n",
        "      get_repaint(page)\n",
        "      status['installed_repaint'] = True\n",
        "    if pipe_repaint is None:\n",
        "      pipe_repaint = get_repaint_pipe()\n",
        "    clear_last()\n",
        "    prt(\"Generating Repaint of your Image...\")\n",
        "    prt(progress)\n",
        "    autoscroll(False)\n",
        "    random_seed = int(repaint_prefs['seed']) if int(repaint_prefs['seed']) > 0 else random.randint(0,4294967295)\n",
        "    generator = torch.Generator(device=torch_device).manual_seed(random_seed)\n",
        "#Sizes of tensors must match except in dimension 1. Expected size 58 but got size 59 for tensor number 1 in the list.\n",
        "    try:\n",
        "      #from IPython.utils.capture import capture_output\n",
        "      #with capture_output() as captured:\n",
        "      image = pipe_repaint(image=original_img, mask_image=mask_img, num_inference_steps=repaint_prefs['num_inference_steps'], eta=repaint_prefs['eta'], jump_length=repaint_prefs['jump_length'], jump_n_sample=repaint_prefs['jump_length'], generator=generator, callback=callback_fnc, callback_steps=1).images[0]\n",
        "      #print(str(captured.stdout))\n",
        "    except Exception as e:\n",
        "      clear_last()\n",
        "      alert_msg(page, f\"ERROR: Couldn't Repaint your image for some reason.  Possibly out of memory or something wrong with my code...\", content=Text(str(e)))\n",
        "      return\n",
        "    fname = repaint_prefs['original_image'].rpartition('.')[0]\n",
        "    fname = fname.rpartition(slash)[2]\n",
        "    if prefs['file_suffix_seed']: fname += f\"-{random_seed}\"\n",
        "    image_path = available_file(stable_dir, fname, 1)\n",
        "    image.save(image_path)\n",
        "    out_path = image_path\n",
        "    clear_last()\n",
        "    clear_last()\n",
        "    autoscroll(True)\n",
        "    prt(Row([ImageButton(src=image_path, width=width, height=height, data=image_path, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "    #prt(Row([Img(src=image_path, width=width, height=height, fit=ImageFit.FILL, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "    #TODO: ESRGAN, Metadata & PyDrive\n",
        "    if storage_type == \"Colab Google Drive\":\n",
        "      new_file = available_file(prefs['image_output'], fname, 1)\n",
        "      out_path = new_file\n",
        "      shutil.copy(image_path, new_file)\n",
        "    elif bool(prefs['image_output']):\n",
        "      new_file = available_file(prefs['image_output'], fname, 1)\n",
        "      out_path = new_file\n",
        "      shutil.copy(image_path, new_file)\n",
        "    prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))\n",
        "    autoscroll(False)\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "def run_image_variation(page):\n",
        "    global image_variation_prefs, pipe_image_variation\n",
        "    if not status['installed_diffusers']:\n",
        "      alert_msg(page, \"You must Install the HuggingFace Diffusers Library first... \")\n",
        "      return\n",
        "    def prt(line):\n",
        "      if type(line) == str:\n",
        "        line = Text(line)\n",
        "      page.ImageVariation.controls.append(line)\n",
        "      page.ImageVariation.update()\n",
        "    def clear_last():\n",
        "      del page.ImageVariation.controls[-1]\n",
        "      page.ImageVariation.update()\n",
        "    def autoscroll(scroll=True):\n",
        "      page.ImageVariation.auto_scroll = scroll\n",
        "      page.ImageVariation.update()\n",
        "    def clear_list():\n",
        "      page.ImageVariation.controls = page.ImageVariation.controls[:1]\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:\n",
        "      callback_fnc.has_been_called = True\n",
        "      nonlocal progress\n",
        "      total_steps = image_variation_prefs['num_inference_steps']#len(latents)\n",
        "      percent = (step +1)/ total_steps\n",
        "      progress.value = percent\n",
        "      progress.tooltip = f\"{step +1} / {total_steps}  Timestep: {timestep}\"\n",
        "      progress.update()\n",
        "    autoscroll(True)\n",
        "    clear_list()\n",
        "    from io import BytesIO\n",
        "    from PIL import ImageOps\n",
        "    if image_variation_prefs['init_image'].startswith('http'):\n",
        "      init_img = PILImage.open(requests.get(image_variation_prefs['init_image'], stream=True).raw)\n",
        "    else:\n",
        "      if os.path.isfile(image_variation_prefs['init_image']):\n",
        "        init_img = PILImage.open(image_variation_prefs['init_image'])\n",
        "      else:\n",
        "        alert_msg(page, f\"ERROR: Couldn't find your init_image {image_variation_prefs['init_image']}\")\n",
        "        return\n",
        "    width, height = init_img.size\n",
        "    width, height = scale_dimensions(width, height, image_variation_prefs['max_size'])\n",
        "    tform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Resize(\n",
        "            (width, height),\n",
        "            interpolation=transforms.InterpolationMode.BICUBIC,\n",
        "            antialias=False,\n",
        "            ),\n",
        "        transforms.Normalize(\n",
        "          [0.48145466, 0.4578275, 0.40821073],\n",
        "          [0.26862954, 0.26130258, 0.27577711]),\n",
        "    ])\n",
        "    init_img = tform(init_img).to(torch_device)\n",
        "    #init_img = init_img.resize((width, height), resample=PILImage.Resampling.LANCZOS)\n",
        "    #init_img = ImageOps.exif_transpose(init_img).convert(\"RGB\")\n",
        "    clear_pipes('image_variation')\n",
        "    if pipe_image_variation == None:\n",
        "        from diffusers import StableDiffusionImageVariationPipeline\n",
        "        prt(Installing(\"Downloading Image Variation Pipeline\"))\n",
        "        model_id = \"fusing/sd-image-variations-diffusers\"\n",
        "        pipe_image_variation = StableDiffusionImageVariationPipeline.from_pretrained(model_id, safety_checker=None, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)\n",
        "        #pipe_image_variation.to(torch_device)\n",
        "        pipe_image_variation = pipeline_scheduler(pipe_image_variation)\n",
        "        pipe_image_variation = optimize_pipe(pipe_image_variation)\n",
        "        #pipe_image_variation.set_progress_bar_config(disable=True)\n",
        "        clear_last()\n",
        "    s = \"s\" if image_variation_prefs['num_images'] > 1 else \"\"\n",
        "    prt(f\"Generating Variation{s} of your Image...\")\n",
        "    prt(progress)\n",
        "    autoscroll(False)\n",
        "    random_seed = int(image_variation_prefs['seed']) if int(image_variation_prefs['seed']) > 0 else rnd.randint(0,4294967295)\n",
        "    generator = torch.Generator(device=torch_device).manual_seed(random_seed)\n",
        "\n",
        "    try:\n",
        "        images = pipe_image_variation(image=init_img, height=height, width=width, num_inference_steps=image_variation_prefs['num_inference_steps'], guidance_scale=image_variation_prefs['guidance_scale'], eta=image_variation_prefs['eta'], num_images_per_prompt=image_variation_prefs['num_images'], generator=generator, callback=callback_fnc, callback_steps=1).images\n",
        "    except Exception as e:\n",
        "        clear_last()\n",
        "        clear_last()\n",
        "        alert_msg(page, \"Error running pipeline\", content=Text(str(e)))\n",
        "        return\n",
        "    clear_last()\n",
        "    clear_last()\n",
        "    autoscroll(True)\n",
        "    fname = image_variation_prefs['init_image'].rpartition('.')[0]\n",
        "    fname = fname.rpartition(slash)[2]\n",
        "    if prefs['file_suffix_seed']: fname += f\"-{random_seed}\"\n",
        "    for image in images:\n",
        "        image_path = available_file(stable_dir, fname, 1)\n",
        "        image.save(image_path)\n",
        "        out_path = image_path\n",
        "        #prt(Row([Img(src=image_path, width=width, height=height, fit=ImageFit.FILL, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "        prt(Row([ImageButton(src=image_path, width=width, height=height, data=image_path, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "        #TODO: ESRGAN, Metadata & PyDrive\n",
        "        if storage_type == \"Colab Google Drive\":\n",
        "            new_file = available_file(prefs['image_output'], fname, 1)\n",
        "            out_path = new_file\n",
        "            shutil.copy(image_path, new_file)\n",
        "        elif bool(prefs['image_output']):\n",
        "            new_file = available_file(prefs['image_output'], fname, 1)\n",
        "            out_path = new_file\n",
        "            shutil.copy(image_path, new_file)\n",
        "        prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))\n",
        "    page.ImageVariation.auto_scroll = False\n",
        "    page.ImageVariation.update()\n",
        "    autoscroll(True)\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "def run_EDICT(page):\n",
        "    global EDICT_prefs, prefs, status, pipe_EDICT, text_encoder_EDICT\n",
        "    if not status['installed_diffusers']:\n",
        "      alert_msg(page, \"You need to Install HuggingFace Diffusers before using...\")\n",
        "      return\n",
        "    if not bool(EDICT_prefs['init_image']):\n",
        "      alert_msg(page, \"You must provide the Original Image and the Mask Image to process...\")\n",
        "      return\n",
        "    if not bool(EDICT_prefs['base_prompt']) or not bool(EDICT_prefs['target_prompt']):\n",
        "      alert_msg(page, \"You must provide a base prompt describing image and target prompt to process...\")\n",
        "      return\n",
        "    def prt(line):\n",
        "      if type(line) == str:\n",
        "        line = Text(line, size=17)\n",
        "      page.EDICT.controls.append(line)\n",
        "      page.EDICT.update()\n",
        "    def clear_last():\n",
        "      del page.EDICT.controls[-1]\n",
        "      page.EDICT.update()\n",
        "    def autoscroll(scroll=True):\n",
        "      page.EDICT.auto_scroll = scroll\n",
        "      page.EDICT.update()\n",
        "    def clear_list():\n",
        "      page.EDICT.controls = page.EDICT.controls[:1]\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    total_steps = EDICT_prefs['num_inference_steps']\n",
        "    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:\n",
        "      callback_fnc.has_been_called = True\n",
        "      nonlocal progress, total_steps\n",
        "      #total_steps = len(latents)\n",
        "      percent = (step +1)/ total_steps\n",
        "      progress.value = percent\n",
        "      progress.tooltip = f\"{step +1} / {total_steps}  Timestep: {timestep}\"\n",
        "      progress.update()\n",
        "      #print(f'{type(latents)} {len(latents)}- {str(latents)}')\n",
        "    def center_crop_resize(im):\n",
        "        width, height = im.size\n",
        "        d = min(width, height)\n",
        "        left = (width - d) / 2\n",
        "        upper = (height - d) / 2\n",
        "        right = (width + d) / 2\n",
        "        lower = (height + d) / 2\n",
        "        return im.crop((left, upper, right, lower)).resize((EDICT_prefs['max_size'], EDICT_prefs['max_size']))\n",
        "    autoscroll(True)\n",
        "    clear_list()\n",
        "    prt(Installing(\"Installing EDICT Editor Pipeline...\"))\n",
        "    import requests, random\n",
        "    from io import BytesIO\n",
        "    from PIL import ImageOps\n",
        "    from PIL.PngImagePlugin import PngInfo\n",
        "    if EDICT_prefs['init_image'].startswith('http'):\n",
        "      #response = requests.get(EDICT_prefs['init_image'])\n",
        "      #original_img = PILImage.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "      original_img = PILImage.open(requests.get(EDICT_prefs['init_image'], stream=True).raw)\n",
        "    else:\n",
        "      if os.path.isfile(EDICT_prefs['init_image']):\n",
        "        original_img = PILImage.open(EDICT_prefs['init_image'])\n",
        "      else:\n",
        "        alert_msg(page, f\"ERROR: Couldn't find your init_image {EDICT_prefs['init_image']}\")\n",
        "        return\n",
        "    #width, height = original_img.size\n",
        "    #width, height = scale_dimensions(width, height, EDICT_prefs['max_size'])\n",
        "    original_img = center_crop_resize(original_img)\n",
        "    clear_pipes('EDICT')\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.reset_max_memory_allocated()\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "    torch_dtype = torch.float16\n",
        "    model_id = get_model(prefs['model_ckpt'])['path']\n",
        "    if pipe_EDICT is None:\n",
        "        from diffusers import DiffusionPipeline, DDIMScheduler\n",
        "        from transformers import CLIPTextModel\n",
        "        try:\n",
        "            scheduler = DDIMScheduler(\n",
        "                num_train_timesteps=1000,\n",
        "                beta_start=0.00085,\n",
        "                beta_end=0.012,\n",
        "                beta_schedule=\"scaled_linear\",\n",
        "                set_alpha_to_one=False,\n",
        "                clip_sample=False,\n",
        "            )\n",
        "            text_encoder_EDICT = CLIPTextModel.from_pretrained(pretrained_model_name_or_path=\"openai/clip-vit-large-patch14\", torch_dtype=torch_dtype)\n",
        "            pipe_EDICT = DiffusionPipeline.from_pretrained(\n",
        "                pretrained_model_name_or_path=model_id,\n",
        "                custom_pipeline=\"edict_pipeline\",\n",
        "                revision=\"fp16\",\n",
        "                scheduler=scheduler,\n",
        "                text_encoder=text_encoder_EDICT,\n",
        "                leapfrog_steps=True,\n",
        "                torch_dtype=torch_dtype,\n",
        "                cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,\n",
        "            ).to(torch_device)\n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            alert_msg(page, f\"ERROR: Couldn't Initialize EDICT Pipeline for some reason.  Possibly out of memory or something wrong with my code...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "            return\n",
        "    clear_last()\n",
        "    prt(\"Generating EDICT Edit of your Image...\")\n",
        "    prt(progress)\n",
        "    autoscroll(False)\n",
        "    batch_output = os.path.join(stable_dir, EDICT_prefs['batch_folder_name'])\n",
        "    if not os.path.isdir(batch_output):\n",
        "      os.makedirs(batch_output)\n",
        "    batch_output = os.path.join(prefs['image_output'], EDICT_prefs['batch_folder_name'])\n",
        "    if not os.path.isdir(batch_output):\n",
        "      os.makedirs(batch_output)\n",
        "    random_seed = int(EDICT_prefs['seed']) if int(EDICT_prefs['seed']) > 0 else random.randint(0,4294967295)\n",
        "    for i in range(EDICT_prefs['num_images']):\n",
        "        generator = torch.Generator(device=\"cpu\").manual_seed(random_seed)\n",
        "        #generator = torch.manual_seed(random_seed)\n",
        "        try:\n",
        "            images = pipe_EDICT(base_prompt=EDICT_prefs['base_prompt'], target_prompt=EDICT_prefs['target_prompt'], image=original_img, num_inference_steps=EDICT_prefs['num_inference_steps'], strength=EDICT_prefs['strength'], guidance_scale=EDICT_prefs['guidance_scale'], generator=generator)#.images\n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            alert_msg(page, f\"ERROR: Couldn't EDICT Edit your image for some reason.  Possibly out of memory or something wrong with my code...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "            return\n",
        "        clear_last()\n",
        "        clear_last()\n",
        "        filename = format_filename(EDICT_prefs['target_prompt'])\n",
        "        #if prefs['file_suffix_seed']: fname += f\"-{random_seed}\"\n",
        "        num = 0\n",
        "        for image in images:\n",
        "            random_seed += num\n",
        "            fname = filename + (f\"-{random_seed}\" if prefs['file_suffix_seed'] else \"\")\n",
        "            image_path = available_file(os.path.join(stable_dir, EDICT_prefs['batch_folder_name']), fname, i)\n",
        "            unscaled_path = image_path\n",
        "            output_file = image_path.rpartition(slash)[2]\n",
        "            image.save(image_path)\n",
        "            width, height = image.size\n",
        "            out_path = image_path.rpartition(slash)[0]\n",
        "            upscaled_path = os.path.join(out_path, output_file)\n",
        "            if not EDICT_prefs['display_upscaled_image'] or not EDICT_prefs['apply_ESRGAN_upscale']:\n",
        "                prt(Row([ImageButton(src=unscaled_path, data=upscaled_path, width=width, height=height, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "                #prt(Row([Img(src=unscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "            if EDICT_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:\n",
        "                os.chdir(os.path.join(dist_dir, 'Real-ESRGAN'))\n",
        "                upload_folder = 'upload'\n",
        "                result_folder = 'results'     \n",
        "                if os.path.isdir(upload_folder):\n",
        "                    shutil.rmtree(upload_folder)\n",
        "                if os.path.isdir(result_folder):\n",
        "                    shutil.rmtree(result_folder)\n",
        "                os.mkdir(upload_folder)\n",
        "                os.mkdir(result_folder)\n",
        "                short_name = f'{fname[:80]}-{num}.png'\n",
        "                dst_path = os.path.join(dist_dir, 'Real-ESRGAN', upload_folder, short_name)\n",
        "                #print(f'Moving {fpath} to {dst_path}')\n",
        "                #shutil.move(fpath, dst_path)\n",
        "                shutil.copy(image_path, dst_path)\n",
        "                #faceenhance = ' --face_enhance' if EDICT_prefs[\"face_enhance\"] else ''\n",
        "                faceenhance = ''\n",
        "                run_sp(f'python inference_realesrgan.py -n realesr-general-x4v3 -i upload --outscale {EDICT_prefs[\"enlarge_scale\"]}{faceenhance}', cwd=os.path.join(dist_dir, 'Real-ESRGAN'), realtime=False)\n",
        "                out_file = short_name.rpartition('.')[0] + '_out.png'\n",
        "                shutil.move(os.path.join(dist_dir, 'Real-ESRGAN', result_folder, out_file), upscaled_path)\n",
        "                image_path = upscaled_path\n",
        "                os.chdir(stable_dir)\n",
        "                if EDICT_prefs['display_upscaled_image']:\n",
        "                    time.sleep(0.6)\n",
        "                    prt(Row([ImageButton(src=upscaled_path, data=upscaled_path, width=width * float(EDICT_prefs[\"enlarge_scale\"]), height=height * float(EDICT_prefs[\"enlarge_scale\"]), page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "                    #prt(Row([Img(src=upscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "            if prefs['save_image_metadata']:\n",
        "                img = PILImage.open(image_path)\n",
        "                metadata = PngInfo()\n",
        "                metadata.add_text(\"artist\", prefs['meta_ArtistName'])\n",
        "                metadata.add_text(\"copyright\", prefs['meta_Copyright'])\n",
        "                metadata.add_text(\"software\", \"Stable Diffusion Deluxe\" + f\", upscaled {EDICT_prefs['enlarge_scale']}x with ESRGAN\" if EDICT_prefs['apply_ESRGAN_upscale'] else \"\")\n",
        "                metadata.add_text(\"pipeline\", \"EDICT Editor\")\n",
        "                if prefs['save_config_in_metadata']:\n",
        "                    config_json = EDICT_prefs.copy()\n",
        "                    config_json['model_path'] = model_id\n",
        "                    config_json['seed'] = random_seed\n",
        "                    del config_json['num_images']\n",
        "                    del config_json['max_size']\n",
        "                    del config_json['display_upscaled_image']\n",
        "                    del config_json['batch_folder_name']\n",
        "                    if not config_json['apply_ESRGAN_upscale']:\n",
        "                        del config_json['enlarge_scale']\n",
        "                        del config_json['apply_ESRGAN_upscale']\n",
        "                metadata.add_text(\"config_json\", json.dumps(config_json, ensure_ascii=True, indent=4))\n",
        "                img.save(image_path, pnginfo=metadata)\n",
        "            #TODO: PyDrive\n",
        "            if storage_type == \"Colab Google Drive\":\n",
        "                new_file = available_file(os.path.join(prefs['image_output'], EDICT_prefs['batch_folder_name']), fname, num)\n",
        "                out_path = new_file\n",
        "                shutil.copy(image_path, new_file)\n",
        "            elif bool(prefs['image_output']):\n",
        "                new_file = available_file(os.path.join(prefs['image_output'], EDICT_prefs['batch_folder_name']), fname, num)\n",
        "                out_path = new_file\n",
        "                shutil.copy(image_path, new_file)\n",
        "            time.sleep(0.2)\n",
        "            prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))\n",
        "            num += 1\n",
        "        random_seed += 1\n",
        "    autoscroll(False)\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "def run_DiffEdit(page):\n",
        "    global DiffEdit_prefs, prefs, status, pipe_DiffEdit, text_encoder_DiffEdit\n",
        "    if not status['installed_diffusers']:\n",
        "      alert_msg(page, \"You need to Install HuggingFace Diffusers before using...\")\n",
        "      return\n",
        "    if not bool(DiffEdit_prefs['init_image']):\n",
        "      alert_msg(page, \"You must provide the Original Image and the Mask Image to process...\")\n",
        "      return\n",
        "    if not bool(DiffEdit_prefs['target_prompt']):\n",
        "      alert_msg(page, \"You must provide a target prompt to process...\")\n",
        "      return\n",
        "    def prt(line):\n",
        "      if type(line) == str:\n",
        "        line = Text(line, size=17)\n",
        "      page.DiffEdit.controls.append(line)\n",
        "      page.DiffEdit.update()\n",
        "    def clear_last():\n",
        "      del page.DiffEdit.controls[-1]\n",
        "      page.DiffEdit.update()\n",
        "    def autoscroll(scroll=True):\n",
        "      page.DiffEdit.auto_scroll = scroll\n",
        "      page.DiffEdit.update()\n",
        "    def clear_list():\n",
        "      page.DiffEdit.controls = page.DiffEdit.controls[:1]\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    total_steps = DiffEdit_prefs['num_inference_steps']\n",
        "    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:\n",
        "      callback_fnc.has_been_called = True\n",
        "      nonlocal progress, total_steps\n",
        "      #total_steps = len(latents)\n",
        "      percent = (step +1)/ total_steps\n",
        "      progress.value = percent\n",
        "      progress.tooltip = f\"{step +1} / {total_steps}  Timestep: {timestep}\"\n",
        "      progress.update()\n",
        "      #print(f'{type(latents)} {len(latents)}- {str(latents)}')\n",
        "    def center_crop_resize(im):\n",
        "        width, height = im.size\n",
        "        d = min(width, height)\n",
        "        left = (width - d) / 2\n",
        "        upper = (height - d) / 2\n",
        "        right = (width + d) / 2\n",
        "        lower = (height + d) / 2\n",
        "        return im.crop((left, upper, right, lower)).resize((DiffEdit_prefs['max_size'], DiffEdit_prefs['max_size']))\n",
        "    autoscroll(True)\n",
        "    clear_list()\n",
        "    source_prompt = DiffEdit_prefs['source_prompt']\n",
        "    target_prompt = DiffEdit_prefs['target_prompt']\n",
        "    prt(Installing(f'Installing DiffEdit Pipeline{\" and Caption Generator\" if not bool(source_prompt) else \"\"}...'))\n",
        "    clear_pipes('DiffEdit')\n",
        "    import requests, random\n",
        "    from io import BytesIO\n",
        "    from PIL import ImageOps\n",
        "    from PIL.PngImagePlugin import PngInfo\n",
        "    if DiffEdit_prefs['init_image'].startswith('http'):\n",
        "      #response = requests.get(DiffEdit_prefs['init_image'])\n",
        "      #original_img = PILImage.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "      original_img = PILImage.open(requests.get(DiffEdit_prefs['init_image'], stream=True).raw)\n",
        "    else:\n",
        "      if os.path.isfile(DiffEdit_prefs['init_image']):\n",
        "        original_img = PILImage.open(DiffEdit_prefs['init_image'])\n",
        "      else:\n",
        "        alert_msg(page, f\"ERROR: Couldn't find your init_image {DiffEdit_prefs['init_image']}\")\n",
        "        return\n",
        "    #width, height = original_img.size\n",
        "    #width, height = scale_dimensions(width, height, DiffEdit_prefs['max_size'])\n",
        "    original_img = center_crop_resize(original_img)\n",
        "    \n",
        "    @torch.no_grad()\n",
        "    def generate_caption(images, caption_generator, caption_processor):\n",
        "        text = \"a photograph of\"\n",
        "        inputs = caption_processor(images, text, return_tensors=\"pt\").to(device=\"cuda\", dtype=caption_generator.dtype)\n",
        "        caption_generator.to(\"cuda\")\n",
        "        outputs = caption_generator.generate(**inputs, max_new_tokens=128)\n",
        "        # offload caption generator\n",
        "        caption_generator.to(\"cpu\")\n",
        "        caption = caption_processor.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "        print(f\"Caption: {caption}\")\n",
        "        del inputs\n",
        "        del caption_generator\n",
        "        return caption\n",
        "    if not bool(source_prompt):\n",
        "        prt(\"Generating Caption from Image with Blip...\")\n",
        "        from transformers import BlipForConditionalGeneration, BlipProcessor\n",
        "        captioner_id = \"Salesforce/blip-image-captioning-base\"\n",
        "        processor = BlipProcessor.from_pretrained(captioner_id)\n",
        "        blip_model = BlipForConditionalGeneration.from_pretrained(captioner_id, torch_dtype=torch.float16, low_cpu_mem_usage=True)\n",
        "        source_prompt = generate_caption(original_img, blip_model, processor)\n",
        "        del processor\n",
        "        del blip_model\n",
        "        clear_last()\n",
        "        \n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.reset_max_memory_allocated()\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "    torch_dtype = torch.float16\n",
        "    model_id = get_model(prefs['model_ckpt'])['path']\n",
        "    if pipe_DiffEdit is None:\n",
        "        from diffusers import DDIMScheduler, DDIMInverseScheduler, StableDiffusionDiffEditPipeline\n",
        "        try:\n",
        "            pipe_DiffEdit = StableDiffusionDiffEditPipeline.from_pretrained(\n",
        "                model_id,\n",
        "                torch_dtype=torch.float16,\n",
        "                cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,\n",
        "                safety_checker=None,\n",
        "            )\n",
        "            pipe_DiffEdit.scheduler = DDIMScheduler.from_config(pipe_DiffEdit.scheduler.config)\n",
        "            #pipe_DiffEdit.scheduler = pipeline_scheduler(pipe_DiffEdit)\n",
        "            pipe_DiffEdit.inverse_scheduler = DDIMInverseScheduler.from_config(pipe_DiffEdit.scheduler.config)\n",
        "            pipe_DiffEdit.enable_model_cpu_offload()\n",
        "            pipe_DiffEdit.enable_vae_slicing()\n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            alert_msg(page, f\"ERROR: Couldn't Initialize DiffEdit Pipeline for some reason.  Possibly out of memory or something wrong with my code...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "            return\n",
        "    clear_last()\n",
        "    prt(\"Generating DiffEdit of your Image...\")\n",
        "    prt(progress)\n",
        "    autoscroll(False)\n",
        "    batch_output = os.path.join(stable_dir, DiffEdit_prefs['batch_folder_name'])\n",
        "    if not os.path.isdir(batch_output):\n",
        "      os.makedirs(batch_output)\n",
        "    batch_output = os.path.join(prefs['image_output'], DiffEdit_prefs['batch_folder_name'])\n",
        "    if not os.path.isdir(batch_output):\n",
        "      os.makedirs(batch_output)\n",
        "    random_seed = int(DiffEdit_prefs['seed']) if int(DiffEdit_prefs['seed']) > 0 else random.randint(0,4294967295)\n",
        "\n",
        "    for i in range(DiffEdit_prefs['num_images']):\n",
        "        generator = torch.manual_seed(random_seed)\n",
        "        #generator = torch.manual_seed(random_seed)\n",
        "        try:\n",
        "            mask_image = pipe_DiffEdit.generate_mask(\n",
        "                image=original_img,\n",
        "                source_prompt=source_prompt,\n",
        "                target_prompt=target_prompt,\n",
        "                generator=generator,\n",
        "            )\n",
        "            inv_latents = pipe_DiffEdit.invert(prompt=source_prompt, image=original_img, generator=generator).latents\n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            alert_msg(page, f\"ERROR: Couldn't generate mask for DiffEdit your image for some reason.  Possibly out of memory or something wrong with my code...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "            return\n",
        "        try:\n",
        "            images = pipe_DiffEdit(prompt=target_prompt, mask_image=mask_image, image_latents=inv_latents, negative_prompt=source_prompt, num_inference_steps=DiffEdit_prefs['num_inference_steps'], inpaint_strength=DiffEdit_prefs['strength'], guidance_scale=DiffEdit_prefs['guidance_scale'], generator=generator, callback=callback_fnc, callback_steps=1).images\n",
        "            #images = pipe_DiffEdit(source_prompt=source_prompt, target_prompt=target_prompt, mask_image=mask_image, image_latents=inv_latents, num_inference_steps=DiffEdit_prefs['num_inference_steps'], inpaint_strength=DiffEdit_prefs['strength'], guidance_scale=DiffEdit_prefs['guidance_scale'], generator=generator, callback=callback_fnc, callback_steps=1).images\n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            alert_msg(page, f\"ERROR: Couldn't generate DiffEdit your image for some reason.  Possibly out of memory or something wrong with my code...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "            return\n",
        "        clear_last()\n",
        "        clear_last()\n",
        "        filename = format_filename(DiffEdit_prefs['target_prompt'])\n",
        "        #if prefs['file_suffix_seed']: fname += f\"-{random_seed}\"\n",
        "        num = 0\n",
        "        for image in images:\n",
        "            random_seed += num\n",
        "            fname = filename + (f\"-{random_seed}\" if prefs['file_suffix_seed'] else \"\")\n",
        "            image_path = available_file(os.path.join(stable_dir, DiffEdit_prefs['batch_folder_name']), fname, i)\n",
        "            unscaled_path = image_path\n",
        "            output_file = image_path.rpartition(slash)[2]\n",
        "            image.save(image_path)\n",
        "            width, height = image.size\n",
        "            out_path = image_path.rpartition(slash)[0]\n",
        "            upscaled_path = os.path.join(out_path, output_file)\n",
        "            if not DiffEdit_prefs['display_upscaled_image'] or not DiffEdit_prefs['apply_ESRGAN_upscale']:\n",
        "                prt(Row([ImageButton(src=unscaled_path, data=upscaled_path, width=width, height=height, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "                #prt(Row([Img(src=unscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "            if DiffEdit_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:\n",
        "                os.chdir(os.path.join(dist_dir, 'Real-ESRGAN'))\n",
        "                upload_folder = 'upload'\n",
        "                result_folder = 'results'     \n",
        "                if os.path.isdir(upload_folder):\n",
        "                    shutil.rmtree(upload_folder)\n",
        "                if os.path.isdir(result_folder):\n",
        "                    shutil.rmtree(result_folder)\n",
        "                os.mkdir(upload_folder)\n",
        "                os.mkdir(result_folder)\n",
        "                short_name = f'{fname[:80]}-{num}.png'\n",
        "                dst_path = os.path.join(dist_dir, 'Real-ESRGAN', upload_folder, short_name)\n",
        "                #print(f'Moving {fpath} to {dst_path}')\n",
        "                #shutil.move(fpath, dst_path)\n",
        "                shutil.copy(image_path, dst_path)\n",
        "                #faceenhance = ' --face_enhance' if DiffEdit_prefs[\"face_enhance\"] else ''\n",
        "                faceenhance = ''\n",
        "                run_sp(f'python inference_realesrgan.py -n realesr-general-x4v3 -i upload --outscale {DiffEdit_prefs[\"enlarge_scale\"]}{faceenhance}', cwd=os.path.join(dist_dir, 'Real-ESRGAN'), realtime=False)\n",
        "                out_file = short_name.rpartition('.')[0] + '_out.png'\n",
        "                shutil.move(os.path.join(dist_dir, 'Real-ESRGAN', result_folder, out_file), upscaled_path)\n",
        "                image_path = upscaled_path\n",
        "                os.chdir(stable_dir)\n",
        "                if DiffEdit_prefs['display_upscaled_image']:\n",
        "                    time.sleep(0.6)\n",
        "                    prt(Row([ImageButton(src=upscaled_path, data=upscaled_path, width=width * float(DiffEdit_prefs[\"enlarge_scale\"]), height=height * float(DiffEdit_prefs[\"enlarge_scale\"]), page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "                    #prt(Row([Img(src=upscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "            if prefs['save_image_metadata']:\n",
        "                img = PILImage.open(image_path)\n",
        "                metadata = PngInfo()\n",
        "                metadata.add_text(\"artist\", prefs['meta_ArtistName'])\n",
        "                metadata.add_text(\"copyright\", prefs['meta_Copyright'])\n",
        "                metadata.add_text(\"software\", \"Stable Diffusion Deluxe\" + f\", upscaled {DiffEdit_prefs['enlarge_scale']}x with ESRGAN\" if DiffEdit_prefs['apply_ESRGAN_upscale'] else \"\")\n",
        "                metadata.add_text(\"pipeline\", \"DiffEdit\")\n",
        "                if prefs['save_config_in_metadata']:\n",
        "                    config_json = DiffEdit_prefs.copy()\n",
        "                    config_json['model_path'] = model_id\n",
        "                    config_json['seed'] = random_seed\n",
        "                    del config_json['num_images']\n",
        "                    del config_json['max_size']\n",
        "                    del config_json['display_upscaled_image']\n",
        "                    del config_json['batch_folder_name']\n",
        "                    if not config_json['apply_ESRGAN_upscale']:\n",
        "                        del config_json['enlarge_scale']\n",
        "                        del config_json['apply_ESRGAN_upscale']\n",
        "                metadata.add_text(\"config_json\", json.dumps(config_json, ensure_ascii=True, indent=4))\n",
        "                img.save(image_path, pnginfo=metadata)\n",
        "            #TODO: PyDrive\n",
        "            if storage_type == \"Colab Google Drive\":\n",
        "                new_file = available_file(os.path.join(prefs['image_output'], DiffEdit_prefs['batch_folder_name']), fname, num)\n",
        "                out_path = new_file\n",
        "                shutil.copy(image_path, new_file)\n",
        "            elif bool(prefs['image_output']):\n",
        "                new_file = available_file(os.path.join(prefs['image_output'], DiffEdit_prefs['batch_folder_name']), fname, num)\n",
        "                out_path = new_file\n",
        "                shutil.copy(image_path, new_file)\n",
        "            time.sleep(0.2)\n",
        "            prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))\n",
        "            num += 1\n",
        "        random_seed += 1\n",
        "    autoscroll(False)\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "def run_CLIPstyler(page):\n",
        "    def prt(line):\n",
        "      if type(line) == str:\n",
        "        line = Text(line)\n",
        "      page.CLIPstyler.controls.append(line)\n",
        "      page.CLIPstyler.update()\n",
        "    def clear_last():\n",
        "      del page.CLIPstyler.controls[-1]\n",
        "      page.CLIPstyler.update()\n",
        "    def autoscroll(scroll=True):\n",
        "      page.CLIPstyler.auto_scroll = scroll\n",
        "      page.CLIPstyler.update()\n",
        "    def clear_list():\n",
        "      page.CLIPstyler.controls = page.CLIPstyler.controls[:1]\n",
        "    clipstyler_dir = os.path.join(root_dir, \"CLIPstyler\")\n",
        "    if not os.path.exists(clipstyler_dir):\n",
        "          os.mkdir(clipstyler_dir)\n",
        "    if CLIPstyler_prefs['original_image'].startswith('http'):\n",
        "        import requests\n",
        "        from io import BytesIO\n",
        "        response = requests.get(CLIPstyler_prefs['original_image'])\n",
        "        fpath = os.path.join(clipstyler_dir, CLIPstyler_prefs['original_image'].rpartition(slash)[2])\n",
        "        original_img = PILImage.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "        #width, height = original_img.size\n",
        "        #width, height = scale_dimensions(width, height)\n",
        "        original_img = original_img.resize((CLIPstyler_prefs['width'], CLIPstyler_prefs['height']), resample=PILImage.Resampling.LANCZOS).convert(\"RGB\")\n",
        "        original_img.save(fpath)\n",
        "        CLIPstyler_prefs['image_dir'] = fpath\n",
        "    elif os.path.isfile(CLIPstyler_prefs['original_image']):\n",
        "        fpath = os.path.join(clipstyler_dir, CLIPstyler_prefs['original_image'].rpartition(slash)[2])\n",
        "        original_img = PILImage.open(CLIPstyler_prefs['original_image'])\n",
        "        #width, height = original_img.size\n",
        "        #width, height = scale_dimensions(width, height)\n",
        "        original_img = original_img.resize((CLIPstyler_prefs['width'], CLIPstyler_prefs['height']), resample=PILImage.Resampling.LANCZOS).convert(\"RGB\")\n",
        "        original_img.save(fpath)\n",
        "        CLIPstyler_prefs['image_dir'] = fpath\n",
        "    else:\n",
        "        alert_msg(page, \"Couldn't find a valid File, Path or URL...\")\n",
        "        return\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    autoscroll(True)\n",
        "    clear_list()\n",
        "    prt(Installing(\"Downloading CLIP-Styler Packages...\"))\n",
        "    run_process(\"pip install ftfy regex tqdm\", realtime=False, page=page)\n",
        "    run_sp(\"pip install git+https://github.com/openai/CLIP.git\", realtime=False)\n",
        "    #os.chdir(clipstyler_dir)\n",
        "    os.chdir(root_dir)\n",
        "    run_sp(\"pip install git+https://github.com/cyclomon/CLIPstyler.git\", realtime=True)\n",
        "    #!git clone https://github.com/cyclomon/CLIPstyler/\n",
        "    run_sp(f\"git clone https://github.com/cyclomon/CLIPstyler/ {clipstyler_dir}\", realtime=True)\n",
        "    os.chdir(root_dir)\n",
        "    #run_process(f\"git clone https://github.com/paper11667/CLIPstyler/ {clipstyler_dir}\", realtime=False, page=page)\n",
        "    sys.path.append(clipstyler_dir)\n",
        "\n",
        "    import torch.nn\n",
        "    import torch.optim as optim\n",
        "    from torchvision import transforms, models\n",
        "    import StyleNet\n",
        "    import utils\n",
        "    import clip\n",
        "    import torch.nn.functional as F\n",
        "    from template import imagenet_templates\n",
        "    from torchvision import utils as vutils\n",
        "    import argparse\n",
        "    from torchvision.transforms.functional import adjust_contrast\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    VGG = models.vgg19(pretrained=True).features\n",
        "    VGG.to(device)\n",
        "    save_dir = stable_dir\n",
        "    if bool(CLIPstyler_prefs['batch_folder_name']):\n",
        "        save_dir = os.path.join(stable_dir, CLIPstyler_prefs['batch_folder_name'])\n",
        "    new_file = format_filename(CLIPstyler_prefs[\"prompt_text\"])\n",
        "    images = []\n",
        "    for parameter in VGG.parameters():\n",
        "        parameter.requires_grad_(False)\n",
        "        \n",
        "    def img_denormalize(image):\n",
        "        mean=torch.tensor([0.485, 0.456, 0.406]).to(device)\n",
        "        std=torch.tensor([0.229, 0.224, 0.225]).to(device)\n",
        "        mean = mean.view(1,-1,1,1)\n",
        "        std = std.view(1,-1,1,1)\n",
        "        image = image*std +mean\n",
        "        return image\n",
        "\n",
        "    def img_normalize(image):\n",
        "        mean=torch.tensor([0.485, 0.456, 0.406]).to(device)\n",
        "        std=torch.tensor([0.229, 0.224, 0.225]).to(device)\n",
        "        mean = mean.view(1,-1,1,1)\n",
        "        std = std.view(1,-1,1,1)\n",
        "        image = (image-mean)/std\n",
        "        return image\n",
        "\n",
        "    def clip_normalize(image,device):\n",
        "        image = F.interpolate(image,size=224,mode='bicubic')\n",
        "        mean=torch.tensor([0.48145466, 0.4578275, 0.40821073]).to(device)\n",
        "        std=torch.tensor([0.26862954, 0.26130258, 0.27577711]).to(device)\n",
        "        mean = mean.view(1,-1,1,1)\n",
        "        std = std.view(1,-1,1,1)\n",
        "        image = (image-mean)/std\n",
        "        return image\n",
        "        \n",
        "    def get_image_prior_losses(inputs_jit):\n",
        "        diff1 = inputs_jit[:, :, :, :-1] - inputs_jit[:, :, :, 1:]\n",
        "        diff2 = inputs_jit[:, :, :-1, :] - inputs_jit[:, :, 1:, :]\n",
        "        diff3 = inputs_jit[:, :, 1:, :-1] - inputs_jit[:, :, :-1, 1:]\n",
        "        diff4 = inputs_jit[:, :, :-1, :-1] - inputs_jit[:, :, 1:, 1:]\n",
        "        loss_var_l2 = torch.norm(diff1) + torch.norm(diff2) + torch.norm(diff3) + torch.norm(diff4)\n",
        "        return loss_var_l2\n",
        "\n",
        "    from argparse import Namespace\n",
        "    source = CLIPstyler_prefs['source']\n",
        "\n",
        "    training_args = {\n",
        "        \"lambda_tv\": 2e-3,\n",
        "        \"lambda_patch\": 9000,\n",
        "        \"lambda_dir\": 500,\n",
        "        \"lambda_c\": 150,\n",
        "        \"crop_size\": CLIPstyler_prefs['crop_size'],\n",
        "        \"num_crops\":CLIPstyler_prefs['num_crops'],\n",
        "        \"img_height\":CLIPstyler_prefs['height'],\n",
        "        \"img_width\":CLIPstyler_prefs['width'],\n",
        "        \"max_step\":CLIPstyler_prefs['training_iterations'],\n",
        "        \"lr\":5e-4,\n",
        "        \"thresh\":0.7,\n",
        "        \"content_path\":CLIPstyler_prefs['image_dir'],\n",
        "        \"text\":CLIPstyler_prefs['prompt_text']\n",
        "    }\n",
        "\n",
        "    style_args = Namespace(**training_args)\n",
        "\n",
        "    def compose_text_with_templates(text: str, templates=imagenet_templates) -> list:\n",
        "        return [template.format(text) for template in templates]\n",
        "\n",
        "    content_path = style_args.content_path\n",
        "    content_image = utils.load_image2(content_path, img_height=style_args.img_height,img_width =style_args.img_width)\n",
        "    content_image = content_image.to(device)\n",
        "    content_features = utils.get_features(img_normalize(content_image), VGG)\n",
        "    target = content_image.clone().requires_grad_(True).to(device)\n",
        "    style_net = StyleNet.UNet()\n",
        "    style_net.to(device)\n",
        "\n",
        "    style_weights = {'conv1_1': 0.1,\n",
        "                    'conv2_1': 0.2,\n",
        "                    'conv3_1': 0.4,\n",
        "                    'conv4_1': 0.8,\n",
        "                    'conv5_1': 1.6}\n",
        "    clear_last()\n",
        "    prt(\"Generating Stylized Image from your source... Check console output for progress.\")\n",
        "    prt(progress)\n",
        "    autoscroll(False)\n",
        "\n",
        "    content_weight = style_args.lambda_c\n",
        "    show_every = 20\n",
        "    optimizer = optim.Adam(style_net.parameters(), lr=style_args.lr)\n",
        "    s_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.5)\n",
        "    steps = style_args.max_step\n",
        "    content_loss_epoch = []\n",
        "    style_loss_epoch = []\n",
        "    total_loss_epoch = []\n",
        "    output_image = content_image\n",
        "    m_cont = torch.mean(content_image,dim=(2,3),keepdim=False).squeeze(0)\n",
        "    m_cont = [m_cont[0].item(),m_cont[1].item(),m_cont[2].item()]\n",
        "    cropper = transforms.Compose([transforms.RandomCrop(style_args.crop_size)])\n",
        "    augment = transforms.Compose([\n",
        "        transforms.RandomPerspective(fill=0, p=1,distortion_scale=0.5),\n",
        "        transforms.Resize(224)\n",
        "    ])\n",
        "\n",
        "    clip_model, preprocess = clip.load('ViT-B/32', device, jit=False)\n",
        "    prompt = style_args.text\n",
        "\n",
        "    with torch.no_grad():\n",
        "        template_text = compose_text_with_templates(prompt, imagenet_templates)\n",
        "        tokens = clip.tokenize(template_text).to(device)\n",
        "        text_features = clip_model.encode_text(tokens).dcrop_sizech()\n",
        "        text_features = text_features.mean(axis=0, keepdim=True)\n",
        "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "        template_source = compose_text_with_templates(source, imagenet_templates)\n",
        "        tokens_source = clip.tokenize(template_source).to(device)\n",
        "        text_source = clip_model.encode_text(tokens_source).dcrop_sizech()\n",
        "        text_source = text_source.mean(axis=0, keepdim=True)\n",
        "        text_source /= text_source.norm(dim=-1, keepdim=True)\n",
        "        source_features = clip_model.encode_image(clip_normalize(content_image,device))\n",
        "        source_features /= (source_features.clone().norm(dim=-1, keepdim=True))\n",
        "\n",
        "        \n",
        "    num_crops = style_args.num_crops\n",
        "    for epoch in range(0, steps+1):\n",
        "        s_scheduler.step()\n",
        "        target = style_net(content_image,use_sigmoid=True).to(device)\n",
        "        target.requires_grad_(True)\n",
        "        target_features = utils.get_features(img_normalize(target), VGG)\n",
        "        content_loss = 0\n",
        "        content_loss += torch.mean((target_features['conv4_2'] - content_features['conv4_2']) ** 2)\n",
        "        content_loss += torch.mean((target_features['conv5_2'] - content_features['conv5_2']) ** 2)\n",
        "        loss_patch=0 \n",
        "        img_proc =[]\n",
        "        for n in range(num_crops):\n",
        "            target_crop = cropper(target)\n",
        "            target_crop = augment(target_crop)\n",
        "            img_proc.append(target_crop)\n",
        "        img_proc = torch.cat(img_proc,dim=0)\n",
        "        img_aug = img_proc\n",
        "        image_features = clip_model.encode_image(clip_normalize(img_aug,device))\n",
        "        image_features /= (image_features.clone().norm(dim=-1, keepdim=True))\n",
        "        img_direction = (image_features-source_features)\n",
        "        img_direction /= img_direction.clone().norm(dim=-1, keepdim=True)\n",
        "        text_direction = (text_features-text_source).repeat(image_features.size(0),1)\n",
        "        text_direction /= text_direction.norm(dim=-1, keepdim=True)\n",
        "        loss_temp = (1- torch.cosine_similarity(img_direction, text_direction, dim=1))\n",
        "        loss_temp[loss_temp<style_args.thresh] =0\n",
        "        loss_patch+=loss_temp.mean()\n",
        "        glob_features = clip_model.encode_image(clip_normalize(target,device))\n",
        "        glob_features /= (glob_features.clone().norm(dim=-1, keepdim=True))\n",
        "        glob_direction = (glob_features-source_features)\n",
        "        glob_direction /= glob_direction.clone().norm(dim=-1, keepdim=True)\n",
        "        loss_glob = (1- torch.cosine_similarity(glob_direction, text_direction, dim=1)).mean()\n",
        "        reg_tv = style_args.lambda_tv*get_image_prior_losses(target)\n",
        "        total_loss = style_args.lambda_patch*loss_patch + content_weight * content_loss+ reg_tv+ style_args.lambda_dir*loss_glob\n",
        "        total_loss_epoch.append(total_loss)\n",
        "        optimizer.zero_grad()\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "        autoscroll(True)\n",
        "        if epoch % show_every == 0:\n",
        "            prt(\"After %d iters:\" % epoch)\n",
        "            prt('  Total loss: ', total_loss.item())\n",
        "            prt('  Content loss: ', content_loss.item())\n",
        "            prt('  patch loss: ', loss_patch.item())\n",
        "            prt('  dir loss: ', loss_glob.item())\n",
        "            prt('  TV loss: ', reg_tv.item())\n",
        "        \n",
        "        if epoch % show_every == 0:\n",
        "            output_image = target.clone()\n",
        "            output_image = torch.clamp(output_image,0,1)\n",
        "            output_image = adjust_contrast(output_image,1.5)\n",
        "            img = utils.im_convert2(output_image)\n",
        "            save_file = available_file(save_dir, new_file, 1)\n",
        "            img.save(save_file)\n",
        "            prt(Row([ImageButton(src=save_file, width=CLIPstyler_prefs['width'], height=CLIPstyler_prefs['height'], subtitle=save_file, show_subtitle=True, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "            #prt(Row([Img(src=save_file, width=CLIPstyler_prefs['width'], height=CLIPstyler_prefs['height'], fit=ImageFit.FILL, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "            #prt(Row([Text(save_file)], alignment=MainAxisAlignment.CENTER))\n",
        "            images.append(save_file)\n",
        "            #plt.imshow(utils.im_convert2(output_image))\n",
        "            #plt.show()\n",
        "        progress.value = (epoch) / steps\n",
        "        progress.tooltip = f'[{(epoch)} / {steps}]'\n",
        "        progress.update()\n",
        "    #clear_last()\n",
        "    # TODO: ESRGAN and copy to GDrive and Metadata\n",
        "    autoscroll(False)\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "def run_semantic(page):\n",
        "    global semantic_prefs, prefs, status, pipe_semantic, model_path\n",
        "    if not status['installed_diffusers']:\n",
        "      alert_msg(page, \"You need to Install HuggingFace Diffusers before using...\")\n",
        "      return\n",
        "    def prt(line):\n",
        "      if type(line) == str:\n",
        "        line = Text(line, size=17)\n",
        "      page.SemanticGuidance.controls.append(line)\n",
        "      page.SemanticGuidance.update()\n",
        "    def clear_last():\n",
        "      del page.SemanticGuidance.controls[-1]\n",
        "      page.SemanticGuidance.update()\n",
        "    def autoscroll(scroll=True):\n",
        "      page.SemanticGuidance.auto_scroll = scroll\n",
        "      page.SemanticGuidance.update()\n",
        "    def clear_list():\n",
        "      page.SemanticGuidance.controls = page.SemanticGuidance.controls[:1]\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    total_steps = semantic_prefs['num_inference_steps']\n",
        "    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:\n",
        "      callback_fnc.has_been_called = True\n",
        "      nonlocal progress, total_steps\n",
        "      #total_steps = len(latents)\n",
        "      percent = (step +1)/ total_steps\n",
        "      progress.value = percent\n",
        "      progress.tooltip = f\"{step +1} / {total_steps}  Timestep: {timestep}\"\n",
        "      progress.update()\n",
        "      #print(f'{type(latents)} {len(latents)}- {str(latents)}')\n",
        "    clear_list()\n",
        "    autoscroll(True)\n",
        "    prt(Installing(\"Installing Semantic Guidance Pipeline...\"))\n",
        "    \n",
        "    clear_pipes('semantic')\n",
        "    if pipe_semantic is None:\n",
        "        from diffusers import SemanticStableDiffusionPipeline\n",
        "        pipe_semantic = SemanticStableDiffusionPipeline.from_pretrained(model_path, torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, safety_checker=None if prefs['disable_nsfw_filter'] else StableDiffusionSafetyChecker.from_pretrained(\"CompVis/stable-diffusion-safety-checker\"))\n",
        "        pipe_semantic = pipeline_scheduler(pipe_semantic)\n",
        "        pipe_semantic = optimize_pipe(pipe_semantic, vae=False)\n",
        "        pipe_semantic.set_progress_bar_config(disable=True)\n",
        "    else:\n",
        "        pipe_semantic = pipeline_scheduler(pipe_semantic)\n",
        "    clear_last()\n",
        "    prt(\"Generating Semantic Guidance of your Image...\")\n",
        "    prt(progress)\n",
        "    autoscroll(False)\n",
        "    batch_output = os.path.join(stable_dir, semantic_prefs['batch_folder_name'])\n",
        "    if not os.path.isdir(batch_output):\n",
        "      os.makedirs(batch_output)\n",
        "    batch_output = os.path.join(prefs['image_output'], semantic_prefs['batch_folder_name'])\n",
        "    if not os.path.isdir(batch_output):\n",
        "      os.makedirs(batch_output)\n",
        "    random_seed = int(semantic_prefs['seed']) if int(semantic_prefs['seed']) > 0 else rnd.randint(0,4294967295)\n",
        "    generator = torch.Generator(device=torch_device).manual_seed(random_seed)\n",
        "    #generator = torch.manual_seed(random_seed)\n",
        "    width = semantic_prefs['width']\n",
        "    height = semantic_prefs['height']\n",
        "    editing_prompt = []\n",
        "    edit_warmup_steps = []\n",
        "    edit_guidance_scale = []\n",
        "    edit_threshold = []\n",
        "    edit_weights = []\n",
        "    reverse_editing_direction = []\n",
        "    for ep in semantic_prefs['editing_prompts']:\n",
        "        editing_prompt.append(ep['editing_prompt'])\n",
        "        edit_warmup_steps.append(int(ep['edit_warmup_steps']))\n",
        "        edit_guidance_scale.append(ep['edit_guidance_scale'])\n",
        "        edit_threshold.append(ep['edit_threshold'])\n",
        "        edit_weights.append(ep['edit_weights'])\n",
        "        reverse_editing_direction.append(ep['reverse_editing_direction'])\n",
        "    try:\n",
        "      #print(f\"prompt={semantic_prefs['prompt']}, negative_prompt={semantic_prefs['negative_prompt']}, editing_prompt={editing_prompt}, edit_warmup_steps={edit_warmup_steps}, edit_guidance_scale={edit_guidance_scale}, edit_threshold={edit_threshold}, edit_weights={edit_weights}, reverse_editing_direction={reverse_editing_direction}, edit_momentum_scale={semantic_prefs['edit_momentum_scale']}, edit_mom_beta={semantic_prefs['edit_mom_beta']}, num_inference_steps={semantic_prefs['num_inference_steps']}, eta={semantic_prefs['eta']}, guidance_scale={semantic_prefs['guidance_scale']}\")\n",
        "      images = pipe_semantic(prompt=semantic_prefs['prompt'], negative_prompt=semantic_prefs['negative_prompt'], editing_prompt=editing_prompt, edit_warmup_steps=edit_warmup_steps, edit_guidance_scale=edit_guidance_scale, edit_threshold=edit_threshold, edit_weights=edit_weights, reverse_editing_direction=reverse_editing_direction, edit_momentum_scale=semantic_prefs['edit_momentum_scale'], edit_mom_beta=semantic_prefs['edit_mom_beta'], num_inference_steps=semantic_prefs['num_inference_steps'], eta=semantic_prefs['eta'], guidance_scale=semantic_prefs['guidance_scale'], width=width, height=height, num_images_per_prompt=semantic_prefs['num_images'], generator=generator, callback=callback_fnc, callback_steps=1).images\n",
        "    except Exception as e:\n",
        "      clear_last()\n",
        "      alert_msg(page, f\"ERROR: Couldn't Semantic Guidance your image for some reason.  Possibly out of memory or something wrong with my code...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "      return\n",
        "    clear_last()\n",
        "    clear_last()\n",
        "    filename = f\"{prefs['file_prefix']}{format_filename(semantic_prefs['prompt'])}\"\n",
        "    filename = filename[:int(prefs['file_max_length'])]\n",
        "    #if prefs['file_suffix_seed']: filename += f\"-{random_seed}\"\n",
        "    autoscroll(True)\n",
        "    num = 0\n",
        "    for image in images:\n",
        "        random_seed += num\n",
        "        fname = filename + (f\"-{random_seed}\" if prefs['file_suffix_seed'] else \"\")\n",
        "        image_path = available_file(os.path.join(stable_dir, semantic_prefs['batch_folder_name']), fname, num)\n",
        "        unscaled_path = image_path\n",
        "        output_file = image_path.rpartition(slash)[2]\n",
        "        image.save(image_path)\n",
        "        out_path = image_path.rpartition(slash)[0]\n",
        "        upscaled_path = os.path.join(out_path, output_file)\n",
        "        if not semantic_prefs['display_upscaled_image'] or not semantic_prefs['apply_ESRGAN_upscale']:\n",
        "            prt(Row([ImageButton(src=unscaled_path, data=upscaled_path, width=width, height=height, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "        if semantic_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:\n",
        "            os.chdir(os.path.join(dist_dir, 'Real-ESRGAN'))\n",
        "            upload_folder = 'upload'\n",
        "            result_folder = 'results'     \n",
        "            if os.path.isdir(upload_folder):\n",
        "                shutil.rmtree(upload_folder)\n",
        "            if os.path.isdir(result_folder):\n",
        "                shutil.rmtree(result_folder)\n",
        "            os.mkdir(upload_folder)\n",
        "            os.mkdir(result_folder)\n",
        "            short_name = f'{fname[:80]}-{num}.png'\n",
        "            dst_path = os.path.join(dist_dir, 'Real-ESRGAN', upload_folder, short_name)\n",
        "            #print(f'Moving {fpath} to {dst_path}')\n",
        "            #shutil.move(fpath, dst_path)\n",
        "            shutil.copy(image_path, dst_path)\n",
        "            #faceenhance = ' --face_enhance' if semantic_prefs[\"face_enhance\"] else ''\n",
        "            faceenhance = ''\n",
        "            run_sp(f'python inference_realesrgan.py -n realesr-general-x4v3 -i upload --outscale {semantic_prefs[\"enlarge_scale\"]}{faceenhance}', cwd=os.path.join(dist_dir, 'Real-ESRGAN'), realtime=False)\n",
        "            out_file = short_name.rpartition('.')[0] + '_out.png'\n",
        "            shutil.move(os.path.join(dist_dir, 'Real-ESRGAN', result_folder, out_file), upscaled_path)\n",
        "            image_path = upscaled_path\n",
        "            os.chdir(stable_dir)\n",
        "            if semantic_prefs['display_upscaled_image']:\n",
        "                time.sleep(0.6)\n",
        "                prt(Row([ImageButton(src=upscaled_path, data=upscaled_path, width=width * float(semantic_prefs[\"enlarge_scale\"]), height=height * float(semantic_prefs[\"enlarge_scale\"]), page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "                #prt(Row([Img(src=upscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "        if prefs['save_image_metadata']:\n",
        "            img = PILImage.open(image_path)\n",
        "            metadata = PngInfo()\n",
        "            metadata.add_text(\"artist\", prefs['meta_ArtistName'])\n",
        "            metadata.add_text(\"copyright\", prefs['meta_Copyright'])\n",
        "            metadata.add_text(\"software\", \"Stable Diffusion Deluxe\" + f\", upscaled {semantic_prefs['enlarge_scale']}x with ESRGAN\" if semantic_prefs['apply_ESRGAN_upscale'] else \"\")\n",
        "            metadata.add_text(\"pipeline\", \"Semantic Guidance\")\n",
        "            if prefs['save_config_in_metadata']:\n",
        "              config_json = semantic_prefs.copy()\n",
        "              config_json['model_path'] = model_path\n",
        "              config_json['scheduler_mode'] = prefs['scheduler_mode']\n",
        "              config_json['seed'] = random_seed\n",
        "              del config_json['num_images']\n",
        "              del config_json['width']\n",
        "              del config_json['height']\n",
        "              del config_json['display_upscaled_image']\n",
        "              del config_json['batch_folder_name']\n",
        "              if not config_json['apply_ESRGAN_upscale']:\n",
        "                del config_json['enlarge_scale']\n",
        "                del config_json['apply_ESRGAN_upscale']\n",
        "              metadata.add_text(\"config_json\", json.dumps(config_json, ensure_ascii=True, indent=4))\n",
        "            img.save(image_path, pnginfo=metadata)\n",
        "        #TODO: PyDrive\n",
        "        if storage_type == \"Colab Google Drive\":\n",
        "            new_file = available_file(os.path.join(prefs['image_output'], semantic_prefs['batch_folder_name']), fname, num)\n",
        "            out_path = new_file\n",
        "            shutil.copy(image_path, new_file)\n",
        "        elif bool(prefs['image_output']):\n",
        "            new_file = available_file(os.path.join(prefs['image_output'], semantic_prefs['batch_folder_name']), fname, num)\n",
        "            out_path = new_file\n",
        "            shutil.copy(image_path, new_file)\n",
        "        prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))\n",
        "        num += 1\n",
        "    autoscroll(False)\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "def run_image2text(page):\n",
        "    def prt(line):\n",
        "      if type(line) == str:\n",
        "        line = Text(line)\n",
        "      page.image2text_output.controls.append(line)\n",
        "      page.image2text_output.update()\n",
        "    def clear_last():\n",
        "      del page.image2text_output.controls[-1]\n",
        "      page.image2text_output.update()\n",
        "    if image2text_prefs['use_AIHorde'] and not bool(prefs['AIHorde_api_key']):\n",
        "      alert_msg(page, \"To use AIHorde API service, you must provide an API key in settings\")\n",
        "      return\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    #if not status['installed_diffusers']:\n",
        "    #  alert_msg(page, \"You must Install the HuggingFace Diffusers Library first... \")\n",
        "    #  return\n",
        "    if len(image2text_prefs['images']) < 1:\n",
        "      alert_msg(page, \"You must add one or more files to interrogate first... \")\n",
        "      return\n",
        "    if not image2text_prefs['use_AIHorde']:\n",
        "        prt(Installing(\"Downloading Image2Text CLIP-Interrogator Blips...\"))\n",
        "        try:\n",
        "            if transformers.__version__ != \"4.21.3\": # Diffusers conflict\n",
        "              run_process(\"pip uninstall -y transformers\", realtime=False)\n",
        "        except Exception:\n",
        "            pass\n",
        "        run_process(\"pip install ftfy regex tqdm timm fairscale requests\", realtime=False)\n",
        "        #run_sp(\"pip install --upgrade transformers==4.21.2\", realtime=False)\n",
        "        run_process(\"pip install -q transformers==4.21.3 --upgrade --force-reinstall\", realtime=False)\n",
        "        run_process(\"pip install -e git+https://github.com/openai/CLIP.git@main#egg=clip\", realtime=False)\n",
        "        run_process(\"pip install -e git+https://github.com/pharmapsychotic/BLIP.git@lib#egg=blip\", realtime=False)\n",
        "        run_process(\"pip clone https://github.com/pharmapsychotic/clip-interrogator.git\", realtime=False)\n",
        "            #['pip', 'install', 'ftfy', 'gradio', 'regex', 'tqdm', 'transformers==4.21.2', 'timm', 'fairscale', 'requests'],\n",
        "        #    pass\n",
        "        # Have to force downgrade of transformers because error with cache_dir, but should upgrade after run\n",
        "        run_process(\"pip install clip-interrogator\", realtime=False)\n",
        "        \n",
        "        '''def setup():\n",
        "            install_cmds = [\n",
        "                ['pip', 'install', 'ftfy', 'gradio', 'regex', 'tqdm', 'transformers==4.21.2', 'timm', 'fairscale', 'requests'],\n",
        "                ['pip', 'install', 'git+https://github.com/openai/CLIP.git@main#egg=clip'],\n",
        "                ['pip', 'install', 'git+https://github.com/pharmapsychotic/BLIP.git@lib#egg=blip'],\n",
        "                ['git', 'clone', 'https://github.com/pharmapsychotic/clip-interrogator.git'],\n",
        "                ['pip', 'install', 'clip-interrogator'],\n",
        "            ]\n",
        "            for cmd in install_cmds:\n",
        "                print(subprocess.run(cmd, stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "        setup()'''\n",
        "        #run_sp(\"pip install git+https://github.com/openai/CLIP.git\", realtime=False)\n",
        "        import argparse, sys, time\n",
        "        sys.path.append('src/blip')\n",
        "        sys.path.append('src/clip')\n",
        "        sys.path.append('clip-interrogator')\n",
        "        import clip\n",
        "        import torch\n",
        "        from clip_interrogator import Interrogator, Config\n",
        "        clear_last()\n",
        "        prt(\"Interrogating Images to Describe Prompt... Check console output for progress.\")\n",
        "        prt(progress)\n",
        "        try:\n",
        "            ci = Interrogator(Config())\n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            alert_msg(page, \"ERROR: Problem running Interrogator. Try running before installing Diffusers...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip(), selectable=True)]))\n",
        "            return\n",
        "        def inference(image, mode):\n",
        "            nonlocal ci\n",
        "            image = image.convert('RGB')\n",
        "            if mode == 'best':\n",
        "                return ci.interrogate(image)\n",
        "            elif mode == 'classic':\n",
        "                return ci.interrogate_classic(image)\n",
        "            else:\n",
        "                return ci.interrogate_fast(image)\n",
        "        folder_path = image2text_prefs['folder_path']\n",
        "        mode = image2text_prefs['mode'] #'best' #param [\"best\",\"classic\", \"fast\"]\n",
        "        #files = [f for f in os.listdir(folder_path) if f.endswith('.jpg') or  f.endswith('.png')] if os.path.exists(folder_path) else []\n",
        "        files = image2text_prefs['images']\n",
        "        clear_last()\n",
        "        i2t_prompts = []\n",
        "        for file in files:\n",
        "            image = PILImage.open(os.path.join(folder_path, file)).convert('RGB')\n",
        "            prompt = inference(image, mode)\n",
        "            i2t_prompts.append(prompt)\n",
        "            page.add_to_image2text(prompt)\n",
        "            #thumb = image.copy()\n",
        "            #thumb.thumbnail([256, 256])\n",
        "            #display(thumb)\n",
        "            #print(prompt)\n",
        "        if image2text_prefs['save_csv']:\n",
        "            if len(i2t_prompts):\n",
        "                import csv\n",
        "                csv_path = os.path.join(folder_path, 'img2txt_prompts.csv')\n",
        "                with open(csv_path, 'w', encoding='utf-8', newline='') as f:\n",
        "                    w = csv.writer(f, quoting=csv.QUOTE_MINIMAL)\n",
        "                    w.writerow(['image', 'prompt'])\n",
        "                    for file, prompt in zip(files, i2t_prompts):\n",
        "                        w.writerow([file, prompt])\n",
        "\n",
        "                prt(f\"\\n\\n\\nGenerated {len(i2t_prompts)} and saved to {csv_path}, enjoy!\")\n",
        "            else:\n",
        "                prt(f\"Sorry, we couldn't find any images in {folder_path}\")\n",
        "        run_process(\"pip uninstall -y git+https://github.com/pharmapsychotic/BLIP.git@lib#egg=blip\", realtime=False)\n",
        "        run_process(\"pip uninstall -y clip-interrogator\", realtime=False)\n",
        "        run_process(\"pip uninstall -y transformers\", realtime=False)\n",
        "        run_process(\"pip install --upgrade transformers\", realtime=False)\n",
        "        clear_last()\n",
        "    else:\n",
        "        if not status['installed_AIHorde']:\n",
        "          prt(Installing(\"Installing AIHorde API Library...\"))\n",
        "          get_AIHorde(page)\n",
        "          clear_last()\n",
        "        import requests\n",
        "        from io import BytesIO\n",
        "        \n",
        "        api_host = 'https://stablehorde.net/api'\n",
        "        api_check_url = f\"{api_host}/v2/generate/check/\"\n",
        "        api_get_result_url = f\"{api_host}/v2/interrogate/status/\"\n",
        "        interrogate_url = f\"{api_host}/v2/interrogate/async\"\n",
        "        mode = image2text_prefs['mode'] #'best' #param [\"best\",\"classic\", \"fast\"]\n",
        "        headers = {\n",
        "            #'Content-Type': 'application/json',\n",
        "            #'Accept': 'application/json',\n",
        "            'apikey': prefs['AIHorde_api_key'],\n",
        "        }\n",
        "        payload = {\n",
        "            \"forms\": [{\"name\": \"caption\"}],\n",
        "            \"slow_workers\": False if mode == \"fast\" else True,\n",
        "        }\n",
        "        def get_captions(filename):\n",
        "          with open(filename, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "          captions = []\n",
        "          for line in lines:\n",
        "            if 'caption result:' in line:\n",
        "              captions.append(line.split('caption result: ')[1])\n",
        "          return captions\n",
        "        def interrogate_line(j, cat):\n",
        "          l = []\n",
        "          for t in j[cat]:\n",
        "            l.append(f\"{t['text']}:{round(t['confidence'], 1)}\")\n",
        "          return f\"**{cat.capitalize()}:** {', '.join(l)}\\n  \\n  \"\n",
        "        def get_interrogation(j):\n",
        "          attr = \"\"\n",
        "          for t in j.keys():\n",
        "            attr += interrogate_line(j, t)\n",
        "          return attr\n",
        "        def get_interrogation_cat(j, cat):\n",
        "          l = []\n",
        "          for t in j[cat]:\n",
        "            if t['confidence'] > 3:\n",
        "              l.append(t['text'])\n",
        "          if cat == \"artist\" or cat == \"artists\":\n",
        "            return and_list(l)\n",
        "          else:\n",
        "            return ', '.join(l)\n",
        "        def get_interrogation_prompt(j):\n",
        "          attrs = {}\n",
        "          for t in j.keys():\n",
        "            attrs[t] = get_interrogation_cat(j, t)\n",
        "          full = f\", by {attrs['artist' if 'artist' in attrs else 'artists']}, as {attrs['medium' if 'medium' in attrs else 'mediums']}, style of {attrs['flavors' if 'flavors' in attrs else 'flavor']}, {attrs['tags' if 'tags' in attrs else 'tag']}, {attrs['movement' if 'movement' in attrs else 'movements']}, technique of {attrs['techniques' if 'techniques' in attrs else 'technique']}, trending on {attrs['trending' if 'trending' in attrs else 'sites']}\"\n",
        "          return full\n",
        "        import yaml\n",
        "        AI_Horde = os.path.join(dist_dir, \"AI-Horde-CLI\")\n",
        "        cli_response = os.path.join(AI_Horde, \"cliRequests.log\")\n",
        "        alchemy_yml = os.path.join(AI_Horde, \"cliRequestsData_Alchemy.yml\")\n",
        "        interrogation_txt = os.path.join(AI_Horde, \"cliRequestsData_Alchemy.yml_interrogation.txt\")\n",
        "        def make_yml(request):\n",
        "            alchemy = f'''\n",
        "filename: \"horde_alchemy\"\n",
        "api_key: \"{prefs[\"AIHorde_api_key\"]}\"\n",
        "submit_dict:\n",
        "    trusted_workers: {image2text_prefs[\"trusted_workers\"]}\n",
        "    slow_workers: {image2text_prefs[\"slow_workers\"]}\n",
        "    forms: \n",
        "        - name: \"{request}\"'''\n",
        "\n",
        "            al = yaml.safe_load(alchemy)\n",
        "\n",
        "            with open(alchemy_yml, 'w') as file:\n",
        "                yaml.dump(al, file)\n",
        "        if image2text_prefs[\"request_mode\"] == \"Caption\" or image2text_prefs[\"request_mode\"] == \"Interrogation\":\n",
        "            make_yml(image2text_prefs[\"request_mode\"].lower())\n",
        "        elif image2text_prefs[\"request_mode\"] == \"Full Prompt\":\n",
        "            make_yml(\"caption\")\n",
        "        #print(open('names.yaml').read())\n",
        "        folder_path = image2text_prefs['folder_path']\n",
        "        #files = [f for f in image2text_prefs['images'] if f.endswith('.jpg') or  f.endswith('.png')] else []\n",
        "        #files = [f for f in os.listdir(folder_path) if f.endswith('.jpg') or  f.endswith('.png')] if os.path.exists(folder_path) else []\n",
        "        #print(f\"Files: {len(files)}\")\n",
        "        i2t_prompts = []\n",
        "        for file in image2text_prefs['images']:\n",
        "            prt(f\"Getting {image2text_prefs['request_mode']} with the Horde to Describe Image...\")\n",
        "            prt(progress)\n",
        "            stats = Text(\"Stable Horde API Interrogation \")\n",
        "            #prt(stats)\n",
        "            img_file = os.path.join(folder_path, file)\n",
        "            run_process(f'python cli_request_alchemy.py --api_key {prefs[\"AIHorde_api_key\"]} --file \"{alchemy_yml}\" --source_image \"{img_file}\"', cwd=AI_Horde, realtime=True)\n",
        "            clear_last()\n",
        "            clear_last()\n",
        "            if image2text_prefs[\"request_mode\"] == \"Caption\":\n",
        "                captions = get_captions(cli_response)\n",
        "                for r in captions:\n",
        "                  prompt = to_title(r.strip(), sentence=True)\n",
        "                  i2t_prompts.append(prompt)\n",
        "                  page.add_to_image2text(prompt)\n",
        "                new_log = available_file(AI_Horde, \"cliRequests\", 0, ext=\"log\")\n",
        "                shutil.move(cli_response, new_log)\n",
        "            elif image2text_prefs[\"request_mode\"] == \"Interrogation\":\n",
        "                with open(interrogation_txt) as json_file:\n",
        "                  interrogation = json.load(json_file)\n",
        "                prt(Markdown(get_interrogation(interrogation), selectable=True))\n",
        "                new_interrogation = available_file(AI_Horde, \"cliRequestsData_Alchemy.yml_interrogation\", 0, ext=\"txt\")\n",
        "                shutil.move(interrogation_txt, new_interrogation)\n",
        "                os.remove(cli_response)\n",
        "            elif image2text_prefs[\"request_mode\"] == \"Full Prompt\":\n",
        "                captions = get_captions(cli_response)\n",
        "                make_yml(\"interrogation\")\n",
        "                prt(f\"Getting Interrogation with the Horde to Describe Image...\")\n",
        "                prt(progress)\n",
        "                run_process(f'python cli_request_alchemy.py --api_key {prefs[\"AIHorde_api_key\"]} --file \"{alchemy_yml}\" --source_image \"{img_file}\"', cwd=AI_Horde, realtime=True)\n",
        "                clear_last()\n",
        "                clear_last()\n",
        "                with open(interrogation_txt) as json_file:\n",
        "                  interrogation = json.load(json_file)\n",
        "                prt(Markdown(get_interrogation(interrogation), selectable=True))\n",
        "                new_interrogation = available_file(AI_Horde, \"cliRequestsData_Alchemy.yml_interrogation\", 0, ext=\"txt\")\n",
        "                shutil.move(interrogation_txt, new_interrogation)\n",
        "                styles = get_interrogation_prompt(interrogation)\n",
        "                for r in captions:\n",
        "                  prompt = to_title(r.strip(), sentence=True)\n",
        "                  i2t_prompts.append(f\"{prompt}{styles}\")\n",
        "                  page.add_to_image2text(f\"{prompt}{styles}\")\n",
        "                new_log = available_file(AI_Horde, \"cliRequests\", 0, ext=\"log\")\n",
        "                shutil.move(cli_response, new_log)\n",
        "            ''' API Method, want to go back to if I can upload source_image\n",
        "            image = PILImage.open(os.path.join(folder_path, file)).convert('RGB')\n",
        "            buff = io.BytesIO()\n",
        "            image.save(buff, format=\"PNG\")\n",
        "            buff.seek(0)\n",
        "            img_str = io.BufferedReader(buff).read()\n",
        "            payload['source_image'] = img_str.decode()\n",
        "            \n",
        "            response = requests.post(interrogate_url, headers=headers, json=json.dumps(payload))\n",
        "            if response != None:\n",
        "              if response.status_code != 202:\n",
        "                if response.status_code == 400:\n",
        "                  alert_msg(page, \"Stable Horde-API ERROR: Validation Error...\", content=Text(str(response.text)))\n",
        "                  return\n",
        "                else:\n",
        "                  prt(Text(f\"Stable Horde-API ERROR {response.status_code}: \" + str(response.text), selectable=True))\n",
        "                  print(payload)\n",
        "                  continue\n",
        "            artifacts = json.loads(response.content)\n",
        "            q_id = artifacts['id']\n",
        "            print(f\"ID: {q_id}\")\n",
        "            elapsed_seconds = 0\n",
        "            try:\n",
        "              while True:\n",
        "                check_response = requests.get(api_check_url + q_id)\n",
        "                check = json.loads(check_response.content)\n",
        "                div = check['wait_time'] + elapsed_seconds\n",
        "                if div == 0: continue\n",
        "                try:\n",
        "                  percentage = (1 - check['wait_time'] / div)\n",
        "                except Exception:\n",
        "                  continue\n",
        "                pb.value = percentage\n",
        "                pb.update()\n",
        "                status_txt = f\"Stable Horde API Interrogation - Queued Position: {check['queue_position']} - Waiting: {check['waiting']} - Wait Time: {check['wait_time']}\"\n",
        "                stats.value = status_txt #str(check)\n",
        "                stats.update()\n",
        "                if bool(check['done']):\n",
        "                  break\n",
        "                time.sleep(1)\n",
        "                elapsed_seconds += 1\n",
        "            except Exception as e:\n",
        "              alert_msg(page, f\"EXCEPTION ERROR: Unknown error processing image. Check parameters and try again. Restart app if persists.\", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip(), selectable=True)]))\n",
        "              return\n",
        "            get_response = requests.get(api_get_result_url + q_id)\n",
        "            final_results = json.loads(get_response.content)\n",
        "            clear_last()\n",
        "            clear_last()\n",
        "            prt(str(final_results))\n",
        "            for r in final_results['forms']:\n",
        "              prompt = r['result']['*']\n",
        "              i2t_prompts.append(prompt)\n",
        "              page.add_to_image2text(prompt)'''\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "def run_BLIP2_image2text(page):\n",
        "    def prt(line):\n",
        "      if type(line) == str:\n",
        "        line = Text(line)\n",
        "      page.BLIP2_image2text_output.controls.append(line)\n",
        "      page.BLIP2_image2text_output.update()\n",
        "    def clear_last():\n",
        "      if len(page.BLIP2_image2text_output.controls) < 1: return\n",
        "      del page.BLIP2_image2text_output.controls[-1]\n",
        "      page.BLIP2_image2text_output.update()\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    #if not status['installed_diffusers']:\n",
        "    #  alert_msg(page, \"You must Install the HuggingFace Diffusers Library first... \")\n",
        "    #  return\n",
        "    prt(Installing(\"Installing BLIP2 Image2Text from Salesforce LAVIS...\"))\n",
        "\n",
        "    try:\n",
        "        import lavis\n",
        "        #from lavis.models import load_model_and_preprocess\n",
        "    except ModuleNotFoundError:\n",
        "        try:\n",
        "            #run_process(\"pip install clip-salesforce-lavis\", page=page, show=True)\n",
        "            run_sp(\"pip install -q salesforce-lavis\", realtime=True)\n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            alert_msg(page, \"ERROR Installing salesforce-lavis. Try running before installing Diffusers...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip(), selectable=True)]))\n",
        "            return\n",
        "        pass\n",
        "    finally:\n",
        "        from lavis.models import load_model_and_preprocess\n",
        "    import requests\n",
        "    device = torch.device(torch_device)\n",
        "    clear_last()\n",
        "    prt(Installing(\"Downloading BLIP2 Image2Text from Salesforce LAVIS... It's huge, see console for progress.\"))\n",
        "    model_name = \"blip2_t5\"\n",
        "    if '.' in BLIP2_image2text_prefs['model_type']:\n",
        "      model_name = \"blip2_opt\"\n",
        "    if BLIP2_image2text_prefs['model_type'] == \"base\":\n",
        "      model_name = \"img2prompt_vqa\"\n",
        "    try:\n",
        "        model, vis_processors, _ = load_model_and_preprocess(\n",
        "            name=model_name, model_type=BLIP2_image2text_prefs['model_type'], is_eval=True, device=device)\n",
        "    except Exception as e:\n",
        "        clear_last()\n",
        "        alert_msg(page, \"ERROR: Problem running Interrogator. Try running before installing Diffusers...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip(), selectable=True)]))\n",
        "        return\n",
        "    vis_keys = vis_processors.keys()\n",
        "    print(str(vis_keys))\n",
        "    prt(\"  Examining Images to Describe Prompt... Check console output for progress, first run is slow.\")\n",
        "    prt(progress)\n",
        "    folder_path = BLIP2_image2text_prefs['folder_path']\n",
        "    files = [f for f in os.listdir(folder_path) if f.endswith('.jpg') or  f.endswith('.png')] if os.path.exists(folder_path) else []\n",
        "    clear_last()\n",
        "    BLIP2_i2t_prompts = []\n",
        "    try:\n",
        "        for file in files:\n",
        "            img = PILImage.open(os.path.join(folder_path, file)).convert('RGB')\n",
        "            image = vis_processors[\"eval\"](img).unsqueeze(0).to(device)\n",
        "            if BLIP2_image2text_prefs['num_captions'] == 1:\n",
        "                prompt = model.generate({\"image\": image})\n",
        "            else:\n",
        "                prompt = model.generate({\"image\": image}, use_nucleus_sampling=True, num_captions=BLIP2_image2text_prefs['num_captions'])\n",
        "            print(prompt)\n",
        "            if isinstance(prompt, str):\n",
        "                BLIP2_i2t_prompts.append(prompt)\n",
        "                page.add_to_BLIP2_image2text(prompt)\n",
        "            else:\n",
        "                for p in prompt:\n",
        "                    BLIP2_i2t_prompts.append(p)\n",
        "                    page.add_to_BLIP2_image2text(p)\n",
        "            if bool(BLIP2_image2text_prefs['question_prompt']):\n",
        "                question = f\"Question: {BLIP2_image2text_prefs['question_prompt']} Answer:\"\n",
        "                answer = model.generate({\"image\": image, \"prompt\": question})\n",
        "                a = answer.rpartition(':')[2].strip()\n",
        "                prt(f\"Answer: {a}\")\n",
        "    except Exception as e:\n",
        "        clear_last()\n",
        "        alert_msg(page, \"ERROR: Problem Generating from Model. Probably out of memory...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip(), selectable=True)]))\n",
        "        return\n",
        "    clear_last()\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "def run_dance_diffusion(page):\n",
        "    if not status['installed_diffusers']:\n",
        "      alert_msg(page, \"You must Install the HuggingFace Diffusers Library first... \")\n",
        "      return\n",
        "    global dance_pipe, dance_prefs\n",
        "    if dance_prefs['dance_model'] == 'Community' or dance_prefs['dance_model'] == 'Custom':\n",
        "      alert_msg(page, \"Custom Community Checkpoints are not functional yet, working on it so check back later... \")\n",
        "      return\n",
        "    from diffusers import DanceDiffusionPipeline\n",
        "    import scipy.io.wavfile, random\n",
        "    try:\n",
        "      import gdown\n",
        "    except ImportError:\n",
        "      run_sp(\"pip -q install gdown\")\n",
        "    finally:\n",
        "      import gdown\n",
        "    #import sys\n",
        "    #sys.path.append('drive/gdrive/MyDrive/NotebookDatasets/CMVRLG')\n",
        "    #print(dir(os))\n",
        "    #print(dir(os.path))\n",
        "    def prt(line):\n",
        "      if type(line) == str:\n",
        "        line = Text(line)\n",
        "      page.dance_output.controls.append(line)\n",
        "      page.dance_output.update()\n",
        "    def clear_last():\n",
        "      del page.dance_output.controls[-1]\n",
        "      page.dance_output.update()\n",
        "    def play_audio(e):\n",
        "      e.control.data.play()\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    prt(Installing(\"Downloading Dance Diffusion Models\"))\n",
        "    diffusers_dir = os.path.join(root_dir, \"diffusers\")\n",
        "    if not os.path.exists(diffusers_dir):\n",
        "      run_process(\"git clone https://github.com/Skquark/diffusers.git\", realtime=False, cwd=root_dir)\n",
        "    dance_model_file = f\"harmonai/{dance_prefs['dance_model']}\"\n",
        "    if dance_prefs['dance_model'] == 'Community':\n",
        "      models_path = os.path.join(root_dir, 'dancediffusion_models')\n",
        "      os.makedirs(models_path, exist_ok=True)\n",
        "      for c in community_dance_diffusion_models:\n",
        "        if c['name'] == dance_prefs['community_model']:\n",
        "          community = c\n",
        "      model_out = os.path.join(models_path, format_filename(community['name'], use_dash=True))\n",
        "      if not os.path.exists(model_out):\n",
        "        prt(\"Converting Checkpoint to Diffusers...\")\n",
        "        os.makedirs(model_out, exist_ok=True)\n",
        "        if bool(community['download']):\n",
        "          dance_model_file = os.path.join(models_path, community['ckpt'])\n",
        "          if community['download'].startswith('https://drive'):\n",
        "            gdown.download(community['download'], dance_model_file, quiet=True)\n",
        "          elif community['download'].startswith('http'):\n",
        "            local = download_file(community['download'])\n",
        "            print(f\"Download {community['download']} local:{local}\")\n",
        "            shutil.move(local, os.path.join(models_path, community['ckpt']))\n",
        "          else:\n",
        "            dance_model_file = community['download']\n",
        "          sample_generator = os.path.join(root_dir, 'sample-generator')\n",
        "          if not os.path.exists(sample_generator):\n",
        "            run_process(\"git clone https://github.com/harmonai-org/sample-generator\", page=page, cwd=root_dir)\n",
        "          import sys\n",
        "          sys.path.insert(1, os.path.join(sample_generator, 'audio_diffusion'))\n",
        "          run_sp(f\"pip install {sample_generator}\", cwd=sample_generator, realtime=False)\n",
        "          v_diffusion_pytorch = os.path.join(root_dir, 'v-diffusion-pytorch')\n",
        "          if not os.path.exists(v_diffusion_pytorch):\n",
        "            run_sp(\"git clone --recursive https://github.com/crowsonkb/v-diffusion-pytorch\", cwd=root_dir, realtime=False)\n",
        "          run_sp(f\"pip install {v_diffusion_pytorch}\", cwd=v_diffusion_pytorch, realtime=False)\n",
        "          run_sp(f\"python3 {os.path.join(diffusers_dir, 'scripts', 'convert_dance_diffusion_to_diffusers.py')} --model_path {dance_model_file} --checkpoint_path {model_out}\", cwd=os.path.join(sample_generator, 'audio_diffusion'))#os.path.join(diffusers_dir, 'scripts'))\n",
        "          clear_last()\n",
        "          dance_model_file = model_out\n",
        "          #run_sp(f'gdown {community['download']} {dance_model_file}')\n",
        "          #run_sp(f\"wget {community['download']} -O {models_path}\")\n",
        "      else:\n",
        "        dance_model_file = model_out\n",
        "    if dance_prefs['dance_model'] == 'Custom':\n",
        "      models_path = os.path.join(root_dir, 'dancediffusion_models')\n",
        "      os.makedirs(models_path, exist_ok=True)\n",
        "      if bool(dance_prefs['custom_model']):\n",
        "        if dance_prefs['custom_model'].startswith('https://drive'):\n",
        "          dance_model_file = os.path.join(models_path, \"custom_dance.ckpt\")\n",
        "          gdown.download(dance_prefs['custom_model'], dance_model_file, quiet=True)\n",
        "        elif dance_prefs['custom_model'].startswith('http'):\n",
        "          fname = dance_prefs['custom_model'].rpartition('/')[2]\n",
        "          local = download_file(dance_prefs['custom_model'])\n",
        "          dance_model_file = os.path.join(models_path, fname)\n",
        "          print(f\"Download {dance_prefs['custom_model']} local:{local}\")\n",
        "          shutil.move(local, dance_model_file)\n",
        "        else:\n",
        "          dance_model_file = dance_prefs['custom_model']\n",
        "    if dance_prefs['train_custom']:\n",
        "      dance_audio = os.path.join(root_dir, 'dance-audio')\n",
        "      sample_generator = os.path.join(root_dir, 'sample-generator')\n",
        "      if not os.path.exists(sample_generator):\n",
        "        run_process(\"git clone https://github.com/harmonai-org/sample-generator\", page=page, cwd=root_dir)\n",
        "      run_process(f\"pip install {sample_generator}\", page=page, cwd=root_dir, show=True)\n",
        "      v_diffusion_pytorch = os.path.join(root_dir, 'v-diffusion-pytorch')\n",
        "      if not os.path.exists(v_diffusion_pytorch):\n",
        "        run_sp(\"git clone --recursive https://github.com/crowsonkb/v-diffusion-pytorch\", cwd=root_dir, realtime=False)\n",
        "      run_sp(f\"pip install {v_diffusion_pytorch}\", cwd=v_diffusion_pytorch, realtime=False)\n",
        "      run_cmd = \"python3 \" + os.path.join(sample_generator, 'train_uncond.py')\n",
        "      custom_name = format_filename(dance_prefs['custom_name'], use_dash=True)\n",
        "      output_dir = os.path.join(dance_audio, custom_name)\n",
        "      output_dir = output_dir.replace(f\" \", f\"\\ \")\n",
        "      random_crop_str = f\"--random-crop True\" if dance_prefs['random_crop'] else \"\"\n",
        "      run_cmd += f''' --ckpt-path {dance_model_file}\\\n",
        "          --name {custom_name}\\\n",
        "          --training-dir {dance_audio}\\\n",
        "          --sample-size {dance_prefs['sammple_size']}\\\n",
        "          --accum-batches {dance_prefs['accumulate_batches']}\\\n",
        "          --sample-rate {dance_prefs['sample_rate']}\\\n",
        "          --batch-size {dance_prefs['finetune_batch_size']}\\\n",
        "          --demo-every {dance_prefs['demo_every']}\\\n",
        "          --checkpoint-every {dance_prefs['checkpoint_every']}\\\n",
        "          --num-workers 2\\\n",
        "          --num-gpus 1\\\n",
        "          {random_crop_str}\\\n",
        "          --save-path {output_dir}'''\n",
        "      prt(\" Training with \" + run_cmd)\n",
        "      prt(progress)\n",
        "      try:\n",
        "        run_process(run_cmd, page=page, cwd=sample_generator, show=True)\n",
        "      except Exception as e:\n",
        "        clear_last()\n",
        "        alert_msg(page, f\"ERROR: CUDA Out of Memory, or something else. Try changing parameters and try again...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip())]))\n",
        "        with torch.no_grad():\n",
        "          torch.cuda.empty_cache()\n",
        "        return\n",
        "      if dance_prefs['save_model']:\n",
        "        print(\"Upload to HuggingFace (todo)\")\n",
        "      clear_last()\n",
        "      clear_last()\n",
        "      #dance_model_file = os.path.join(output_dir, custom_name + '.ckpt')\n",
        "    try:\n",
        "      dance_pipe = DanceDiffusionPipeline.from_pretrained(dance_model_file, torch_dtype=torch.float16, device_map=\"auto\")\n",
        "      dance_pipe = dance_pipe.to(torch_device)\n",
        "    except Exception as e:\n",
        "      clear_last()\n",
        "      alert_msg(page, f\"ERROR: Problem getting DanceDiffusion Pipeline. Try changing parameters and try again...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip())]))\n",
        "      return\n",
        "    dance_pipe.set_progress_bar_config(disable=True)\n",
        "    random_seed = int(dance_prefs['seed']) if int(dance_prefs['seed']) > 0 else random.randint(0,4294967295)\n",
        "    dance_generator = torch.Generator(device=torch_device).manual_seed(random_seed)\n",
        "    clear_last()\n",
        "    pb.width=(page.window_width or page.width) - 50\n",
        "    prt(pb)\n",
        "    if prefs['higher_vram_mode']:\n",
        "      output = dance_pipe(generator=dance_generator, batch_size=int(dance_prefs['batch_size']), num_inference_steps=int(dance_prefs['inference_steps']), audio_length_in_s=float(dance_prefs['audio_length_in_s']))\n",
        "    else:\n",
        "      output = dance_pipe(generator=dance_generator, batch_size=int(dance_prefs['batch_size']), num_inference_steps=int(dance_prefs['inference_steps']), audio_length_in_s=float(dance_prefs['audio_length_in_s'])) #, torch_dtype=torch.float16\n",
        "    #, callback=callback_fn, callback_steps=1)\n",
        "    audio = output.audios\n",
        "    audio_slice = audio[0, -3:, -3:]\n",
        "    clear_last()\n",
        "    #prt(f'audio: {type(audio[0])}, audio_slice: {type(audio_slice)}, len:{len(audio)}')\n",
        "    #audio_slice.tofile(\"/content/dance-test.wav\")\n",
        "    audio_name = f\"dance-{dance_prefs['dance_model']}\" + (f\"-{random_seed}\" if prefs['file_suffix_seed'] else '')\n",
        "    audio_local = os.path.join(root_dir, \"audio_out\")\n",
        "    audio_out = audio_local\n",
        "    os.makedirs(audio_local, exist_ok=True)\n",
        "    if storage_type == \"Colab Google Drive\":\n",
        "      audio_out = prefs['image_output'].rpartition(slash)[0] + slash + 'audio_out'\n",
        "      os.makedirs(audio_out, exist_ok=True)\n",
        "    i = 0\n",
        "    for a in audio:\n",
        "      fname = available_file(audio_local, audio_name, i, ext=\"wav\")\n",
        "      scipy.io.wavfile.write(fname, dance_pipe.unet.sample_rate, a.transpose())\n",
        "      os.path.abspath(fname)\n",
        "      a_out = Audio(src=fname, autoplay=False)\n",
        "      page.overlay.append(a_out)\n",
        "      page.update()\n",
        "      display_name = fname\n",
        "      #a.tofile(f\"/content/dance-{i}.wav\")\n",
        "      if storage_type == \"Colab Google Drive\":\n",
        "        audio_save = available_file(audio_out, audio_name, i, ext='wav')\n",
        "        shutil.copy(fname, audio_save)\n",
        "        display_name = audio_save\n",
        "      prt(Row([IconButton(icon=icons.PLAY_CIRCLE_FILLED, icon_size=48, on_click=play_audio, data=a_out), Text(display_name)]))\n",
        "      i += 1\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "def run_audio_diffusion(page):\n",
        "    global audio_diffusion_prefs, pipe_audio_diffusion, prefs\n",
        "    def prt(line):\n",
        "      if type(line) == str:\n",
        "        line = Text(line)\n",
        "      page.audio_diffusion_output.controls.append(line)\n",
        "      page.audio_diffusion_output.update()\n",
        "    def clear_last():\n",
        "      if len(page.audio_diffusion_output.controls) < 1: return\n",
        "      del page.audio_diffusion_output.controls[-1]\n",
        "      page.audio_diffusion_output.update()\n",
        "    def play_audio(e):\n",
        "      e.control.data.play()\n",
        "    #if not bool(audio_diffusion_prefs['text']):\n",
        "    #  alert_msg(page, \"Provide Text for the AI to create the sound of...\")\n",
        "    #  return\n",
        "    if not status['installed_diffusers']:\n",
        "      alert_msg(page, \"You must Install the HuggingFace Diffusers Library first... \")\n",
        "      return\n",
        "    total_steps = audio_diffusion_prefs['steps']\n",
        "    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:\n",
        "      callback_fnc.has_been_called = True\n",
        "      nonlocal progress, total_steps\n",
        "      #total_steps = len(latents)\n",
        "      percent = (step +1)/ total_steps\n",
        "      progress.value = percent\n",
        "      progress.tooltip = f\"{step +1} / {total_steps}  Timestep: {timestep}\"\n",
        "      progress.update()\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    state_text = Text(\" Downloading Audio Diffusion Pipeline...\", weight=FontWeight.BOLD)\n",
        "    prt(Row([ProgressRing(), state_text]))\n",
        "    audio_diffusion_dir = os.path.join(root_dir, \"audio_diffusion\")\n",
        "\n",
        "    try:\n",
        "        import mel\n",
        "    except ModuleNotFoundError:\n",
        "        try:\n",
        "            run_process(\"pip install -q mel\", page=page, show=True, print=True)\n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            alert_msg(page, \"Error Installing AudioDiffusion requirements\", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip())]))\n",
        "            return\n",
        "        pass\n",
        "    finally:\n",
        "        import mel\n",
        "    import scipy.io.wavfile\n",
        "    from diffusers import DiffusionPipeline, DDIMScheduler\n",
        "    model_id = audio_diffusion_prefs['audio_model']\n",
        "    clear_pipes('audio_diffusion')\n",
        "    # This will download all the models used by Audio Diffusion from the HuggingFace hub.\n",
        "    if pipe_audio_diffusion == None or audio_diffusion_prefs['loaded_model'] != model_id:\n",
        "      try:\n",
        "          # TODO: Switch DDPM\n",
        "        a_scheduler = DDIMScheduler()\n",
        "        pipe_audio_diffusion = DiffusionPipeline.from_pretrained(model_id, scheduler=a_scheduler, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)\n",
        "        pipe_audio_diffusion = pipe_audio_diffusion.to(torch_device)\n",
        "        pipe_audio_diffusion.set_progress_bar_config(disable=True)\n",
        "        audio_diffusion_prefs['loaded_model'] = model_id\n",
        "      except Exception as e:\n",
        "        clear_last()\n",
        "        alert_msg(page, \"Error setting up Audio Diffusion Pipeline\", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))\n",
        "        return\n",
        "    init = audio_diffusion_prefs['audio_file']\n",
        "    if init.startswith('http'):\n",
        "        init_audio = download_file(init)\n",
        "    else:\n",
        "        if os.path.isfile(init):\n",
        "            init_audio = init\n",
        "        else:\n",
        "            init_audio = None\n",
        "            #alert_msg(page, f\"ERROR: Couldn't find your init audio {init}\")\n",
        "            #return\n",
        "    clear_last()\n",
        "    prt(Text(\"  Generating Audio Diffusion Sounds...\", weight=FontWeight.BOLD))\n",
        "    prt(progress)\n",
        "    random_seed = int(audio_diffusion_prefs['seed']) if int(audio_diffusion_prefs['seed']) > 0 else rnd.randint(0,4294967295)\n",
        "    generator = torch.Generator(device=torch_device).manual_seed(random_seed)\n",
        "    try:\n",
        "        output = pipe_audio_diffusion(audio_file=init_audio, slice=audio_diffusion_prefs['slice'], steps=audio_diffusion_prefs['steps'], start_step=audio_diffusion_prefs['start_step'], mask_start_secs=audio_diffusion_prefs['mask_start_secs'], mask_end_secs=audio_diffusion_prefs['mask_end_secs'], eta=audio_diffusion_prefs['eta'], batch_size=int(audio_diffusion_prefs['batch_size']), generator=generator)#, callback=callback_fnc)\n",
        "    except Exception as e:\n",
        "        clear_last()\n",
        "        alert_msg(page, \"Error Generating Audio Diffusion Output\", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))\n",
        "        return\n",
        "    images = output.images\n",
        "    audios = output.audios\n",
        "    sample_rate = pipe_audio_diffusion.mel.get_sample_rate()\n",
        "    save_dir = os.path.join(root_dir, 'audio_out', audio_diffusion_prefs['batch_folder_name'])\n",
        "    if not os.path.exists(save_dir):\n",
        "      os.makedirs(save_dir, exist_ok=True)\n",
        "    audio_out = os.path.join(prefs['image_output'].rpartition(slash)[0], 'audio_out')\n",
        "    if bool(audio_diffusion_prefs['batch_folder_name']):\n",
        "      audio_out = os.path.join(audio_out, audio_diffusion_prefs['batch_folder_name'])\n",
        "    os.makedirs(audio_out, exist_ok=True)\n",
        "    #voice_dirs = os.listdir(os.path.join(root_dir, \"audio_diffusion-tts\", 'audio_diffusion', 'voices'))\n",
        "    #print(str(voice_dirs))\n",
        "    clear_last()\n",
        "    clear_last()\n",
        "    a_name = audio_diffusion_prefs['audio_name']\n",
        "    if not bool(a_name):\n",
        "        if bool(init_audio):\n",
        "            a_name = init_audio.rpartition(slash)[2].rpartition('.')[0]\n",
        "        else:\n",
        "            a_name = f\"audio_diffusion-{model_id.rpartition('/')[2]}\"\n",
        "    fname = format_filename(a_name, force_underscore=True)\n",
        "    if fname[-1] == '.': fname = fname[:-1]\n",
        "    file_prefix = audio_diffusion_prefs['file_prefix']\n",
        "    audio_name = f'{file_prefix}{fname}'\n",
        "    audio_name = audio_name[:int(prefs['file_max_length'])]\n",
        "    for image in images:\n",
        "        iname = available_file(save_dir, audio_name, 0)\n",
        "        image.save(iname)\n",
        "        out_path = iname\n",
        "        prt(Row([Img(src=iname, fit=ImageFit.FILL, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "        if storage_type == \"Colab Google Drive\":\n",
        "            new_file = available_file(prefs['image_output'], fname, 0)\n",
        "            out_path = new_file\n",
        "            shutil.copy(iname, new_file)\n",
        "        elif bool(prefs['image_output']):\n",
        "            new_file = available_file(prefs['image_output'], fname, 0)\n",
        "            out_path = new_file\n",
        "            shutil.copy(iname, new_file)\n",
        "        #prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))\n",
        "    for a in audios:\n",
        "        aname = available_file(save_dir, audio_name, 0, ext=\"wav\")\n",
        "        #for i in range(waveform.shape[0]):\n",
        "        #    sf.write(aname, waveform[i, 0], samplerate=sample_rate)\n",
        "        #torchaudio.save(fname, gen.squeeze(0).cpu(), 24000)\n",
        "        #IPython.display.Audio('generated.wav')\n",
        "        scipy.io.wavfile.write(aname, sample_rate, a.transpose())\n",
        "        a_out = Audio(src=aname, autoplay=False)\n",
        "        page.overlay.append(a_out)\n",
        "        page.update()\n",
        "        display_name = aname\n",
        "        #a.tofile(f\"/content/dance-{i}.wav\")\n",
        "        if storage_type == \"Colab Google Drive\":\n",
        "            audio_save = available_file(audio_out, audio_name, 0, ext='wav')\n",
        "            shutil.copy(aname, audio_save)\n",
        "        elif bool(prefs['image_output']):\n",
        "            audio_save = available_file(audio_out, audio_name, 0, ext='wav')\n",
        "            shutil.copy(aname, audio_save)\n",
        "        else: audio_save = aname\n",
        "        display_name = audio_save\n",
        "        prt(Row([IconButton(icon=icons.PLAY_CIRCLE_FILLED, icon_size=48, on_click=play_audio, data=a_out), Text(display_name)]))\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "    \n",
        "\n",
        "# https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_dreambooth_training.ipynb\n",
        "def run_dreambooth(page):\n",
        "    global dreambooth_prefs, prefs\n",
        "    def prt(line):\n",
        "      if type(line) == str:\n",
        "        line = Text(line)\n",
        "      page.dreambooth_output.controls.append(line)\n",
        "      page.dreambooth_output.update()\n",
        "    def clear_last():\n",
        "      del page.dreambooth_output.controls[-1]\n",
        "      page.dreambooth_output.update()\n",
        "    if not status['installed_diffusers']:\n",
        "      alert_msg(page, \"You must Install the HuggingFace Diffusers Library first... \")\n",
        "      return\n",
        "    save_path = os.path.join(root_dir, \"my_concept\")\n",
        "    error = False\n",
        "    if not os.path.exists(save_path):\n",
        "      error = True\n",
        "    elif len(os.listdir(save_path)) == 0:\n",
        "      error = True\n",
        "    if len(page.db_file_list.controls) == 0:\n",
        "      error = True\n",
        "    if error:\n",
        "      alert_msg(page, \"Couldn't find a list of images to train concept. Add image files to the list...\")\n",
        "      return\n",
        "    prt(Installing(\"Downloading DreamBooth Conceptualizers\"))\n",
        "    #os.chdir(root_dir)\n",
        "    #run_process(\"pip clone https://github.com/Skquark/diffusers.git\", realtime=False)\n",
        "    #run_process(\"pip install -e .\", realtime=False)\n",
        "    #os.chdir(os.path.join(root_dir, \"diffusers\", \"examples\", \"dreambooth\"))\n",
        "    #run_process(\"pip install -r requirements.txt\", realtime=False)\n",
        "    os.chdir(root_dir)\n",
        "    try:\n",
        "        os.environ['LD_LIBRARY_PATH'] += \"/usr/lib/wsl/lib:$LD_LIBRARY_PATH\"\n",
        "        #run_sp(\"export LD_LIBRARY_PATH=/usr/lib/wsl/lib:$LD_LIBRARY_PATH\", realtime=False)\n",
        "        import bitsandbytes\n",
        "    except Exception:\n",
        "        if sys.platform.startswith(\"win\"):\n",
        "            run_sp(\"pip install bitsandbytes-windows\", realtime=False)\n",
        "        else:\n",
        "            run_sp(\"pip install --upgrade bitsandbytes\", realtime=False)\n",
        "        import bitsandbytes\n",
        "        pass\n",
        "    import argparse\n",
        "    import itertools\n",
        "    import math\n",
        "    from contextlib import nullcontext\n",
        "    import random\n",
        "    import torch.nn.functional as F\n",
        "    import torch.utils.checkpoint\n",
        "    from torch.utils.data import Dataset\n",
        "    from accelerate import Accelerator\n",
        "    from accelerate.logging import get_logger\n",
        "    from accelerate.utils import set_seed\n",
        "    from diffusers import AutoencoderKL, DDPMScheduler, PNDMScheduler, StableDiffusionPipeline, UNet2DConditionModel\n",
        "    #from diffusers.hub_utils import init_git_repo, push_to_hub\n",
        "    from diffusers.optimization import get_scheduler\n",
        "    from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker\n",
        "    \n",
        "    from torchvision import transforms\n",
        "    from tqdm.auto import tqdm\n",
        "    from transformers import CLIPFeatureExtractor, CLIPTextModel, CLIPTokenizer\n",
        "    \n",
        "    import bitsandbytes as bnb\n",
        "    import gc\n",
        "    import glob\n",
        "    from io import BytesIO\n",
        "\n",
        "    def download_image(url):\n",
        "      try:\n",
        "        response = requests.get(url)\n",
        "      except:\n",
        "        return None\n",
        "      return PILImage.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "    #images = list(filter(None,[download_image(url) for url in dreambooth_prefs['urls']]))\n",
        "    #save_path = \"./my_concept\"\n",
        "    #if not os.path.exists(save_path):\n",
        "    #  os.mkdir(save_path)\n",
        "    #[image.save(f\"{save_path}/{i}.jpeg\") for i, image in enumerate(images)]\n",
        "    #image_grid(images, 1, len(images))\n",
        "\n",
        "    prior_preservation_class_folder = dreambooth_prefs['prior_preservation_class_folder']\n",
        "    class_data_root=prior_preservation_class_folder\n",
        "    class_prompt=dreambooth_prefs['prior_preservation_class_prompt']\n",
        "    class_data_root=prior_preservation_class_folder\n",
        "    dreambooth_prefs['instance_prompt'] = dreambooth_prefs['instance_prompt'].strip()\n",
        "    clear_pipes()\n",
        "    num_new_images = None\n",
        "    if(dreambooth_prefs['prior_preservation']):\n",
        "        class_images_dir = Path(class_data_root)\n",
        "        if not class_images_dir.exists():\n",
        "            class_images_dir.mkdir(parents=True)\n",
        "        cur_class_images = len(list(class_images_dir.iterdir()))\n",
        "\n",
        "        if cur_class_images < dreambooth_prefs['num_class_images']:\n",
        "            if prefs['higher_vram_mode']:\n",
        "              pipeline = StableDiffusionPipeline.from_pretrained(model_path).to(\"cuda\")\n",
        "            else:\n",
        "              pipeline = StableDiffusionPipeline.from_pretrained(model_path, revision=\"fp16\", torch_dtype=torch.float16).to(\"cuda\")\n",
        "            if prefs['enable_attention_slicing']:\n",
        "              pipeline.enable_attention_slicing()\n",
        "            pipeline.set_progress_bar_config(disable=True)\n",
        "\n",
        "            num_new_images = dreambooth_prefs['num_class_images'] - cur_class_images\n",
        "            print(f\"Number of class images to sample: {num_new_images}.\")\n",
        "\n",
        "            sample_dataset = PromptDataset(class_prompt, num_new_images)\n",
        "            sample_dataloader = torch.utils.data.DataLoader(sample_dataset, batch_size=dreambooth_prefs['sample_batch_size'])\n",
        "\n",
        "            for example in tqdm(sample_dataloader, desc=\"Generating class images\"):\n",
        "                images = pipeline(example[\"prompt\"]).images\n",
        "\n",
        "                for i, image in enumerate(images):\n",
        "                    image.save(class_images_dir / f\"{example['index'][i] + cur_class_images}.jpg\")\n",
        "            pipeline = None\n",
        "            gc.collect()\n",
        "            del pipeline\n",
        "            with torch.no_grad():\n",
        "              torch.cuda.empty_cache()\n",
        "    text_encoder = CLIPTextModel.from_pretrained(model_path, subfolder=\"text_encoder\")\n",
        "    vae = AutoencoderKL.from_pretrained(model_path, subfolder=\"vae\")\n",
        "    unet = UNet2DConditionModel.from_pretrained(model_path, subfolder=\"unet\")\n",
        "    tokenizer = CLIPTokenizer.from_pretrained(model_path,subfolder=\"tokenizer\")\n",
        "    \n",
        "    from argparse import Namespace\n",
        "    dreambooth_args = Namespace(\n",
        "        pretrained_model_name_or_path=model_path,\n",
        "        resolution=dreambooth_prefs['max_size'],\n",
        "        center_crop=True,\n",
        "        instance_data_dir=save_path,\n",
        "        instance_prompt=dreambooth_prefs['instance_prompt'],\n",
        "        learning_rate=dreambooth_prefs['learning_rate'],#5e-06,\n",
        "        max_train_steps=dreambooth_prefs['max_train_steps'],#450,\n",
        "        train_batch_size=1,\n",
        "        gradient_accumulation_steps=2,\n",
        "        max_grad_norm=1.0,\n",
        "        mixed_precision=\"no\", # set to \"fp16\" for mixed-precision training.\n",
        "        gradient_checkpointing=True, # set this to True to lower the memory usage.\n",
        "        use_8bit_adam=not prefs['higher_vram_mode'], # use 8bit optimizer from bitsandbytes\n",
        "        seed=dreambooth_prefs['seed'],#3434554,\n",
        "        with_prior_preservation=dreambooth_prefs['prior_preservation'], \n",
        "        prior_loss_weight=dreambooth_prefs['prior_loss_weight'],\n",
        "        sample_batch_size=dreambooth_prefs['sample_batch_size'],\n",
        "        class_data_dir=dreambooth_prefs['prior_preservation_class_folder'], \n",
        "        class_prompt=class_prompt, \n",
        "        num_class_images=dreambooth_prefs['num_class_images'], \n",
        "        output_dir=\"dreambooth-concept\",\n",
        "    )\n",
        "\n",
        "    from accelerate.utils import set_seed\n",
        "    def training_function(text_encoder, vae, unet):\n",
        "        logger = get_logger(__name__)\n",
        "        accelerator = Accelerator(\n",
        "            gradient_accumulation_steps=dreambooth_args.gradient_accumulation_steps,\n",
        "            mixed_precision=dreambooth_args.mixed_precision,\n",
        "        )\n",
        "        set_seed(dreambooth_args.seed)\n",
        "        if dreambooth_args.gradient_checkpointing:\n",
        "            unet.enable_gradient_checkpointing()\n",
        "        # Use 8-bit Adam for lower memory usage or to fine-tune the model in 16GB GPUs\n",
        "        if dreambooth_args.use_8bit_adam:\n",
        "            optimizer_class = bnb.optim.AdamW8bit\n",
        "        else:\n",
        "            optimizer_class = torch.optim.AdamW\n",
        "        optimizer = optimizer_class(unet.parameters(),lr=dreambooth_args.learning_rate)\n",
        "        noise_scheduler = DDPMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)     \n",
        "        train_dataset = DreamBoothDataset(\n",
        "            instance_data_root=dreambooth_args.instance_data_dir,\n",
        "            instance_prompt=dreambooth_args.instance_prompt,\n",
        "            class_data_root=dreambooth_args.class_data_dir if dreambooth_args.with_prior_preservation else None,\n",
        "            class_prompt=dreambooth_args.class_prompt,\n",
        "            tokenizer=tokenizer,\n",
        "            size=dreambooth_args.resolution,\n",
        "            center_crop=dreambooth_args.center_crop,\n",
        "        )\n",
        "        def collate_fn(examples):\n",
        "            input_ids = [example[\"instance_prompt_ids\"] for example in examples]\n",
        "            pixel_values = [example[\"instance_images\"] for example in examples]\n",
        "\n",
        "            # concat class and instance examples for prior preservation\n",
        "            if dreambooth_args.with_prior_preservation:\n",
        "                input_ids += [example[\"class_prompt_ids\"] for example in examples]\n",
        "                pixel_values += [example[\"class_images\"] for example in examples]\n",
        "\n",
        "            pixel_values = torch.stack(pixel_values)\n",
        "            pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()\n",
        "\n",
        "            input_ids = tokenizer.pad({\"input_ids\": input_ids}, padding=True, return_tensors=\"pt\").input_ids\n",
        "\n",
        "            batch = {\n",
        "                \"input_ids\": input_ids,\n",
        "                \"pixel_values\": pixel_values,\n",
        "            }\n",
        "            return batch\n",
        "        train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=dreambooth_args.train_batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "        unet, optimizer, train_dataloader = accelerator.prepare(unet, optimizer, train_dataloader)\n",
        "        # Move text_encode and vae to gpu\n",
        "        text_encoder.to(accelerator.device)\n",
        "        vae.to(accelerator.device)\n",
        "        # We need to recalculate our total training steps as the size of the training dataloader may have changed.\n",
        "        num_update_steps_per_epoch = math.ceil(len(train_dataloader) / dreambooth_args.gradient_accumulation_steps)\n",
        "        num_train_epochs = math.ceil(dreambooth_args.max_train_steps / num_update_steps_per_epoch)\n",
        "        clear_last()\n",
        "        total_batch_size = dreambooth_args.train_batch_size * accelerator.num_processes * dreambooth_args.gradient_accumulation_steps\n",
        "\n",
        "        prt(\"***** Running training *****\")\n",
        "        prt(f\"  Number of examples = {len(train_dataset)}\")\n",
        "        if num_new_images != None: prt(f\"  Number of class images to sample: {num_new_images}.\")\n",
        "        prt(f\"  Instantaneous batch size per device = {dreambooth_args.train_batch_size}\")\n",
        "        prt(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n",
        "        prt(f\"  Gradient Accumulation steps = {dreambooth_args.gradient_accumulation_steps}\")\n",
        "        prt(f\"  Total optimization steps = {dreambooth_args.max_train_steps}\")\n",
        "        progress = ProgressBar(bar_height=8)\n",
        "        prt(progress)\n",
        "        progress_bar = tqdm(range(dreambooth_args.max_train_steps), disable=not accelerator.is_local_main_process)\n",
        "        progress_bar.set_description(\"Steps\")\n",
        "        global_step = 0\n",
        "\n",
        "        for epoch in range(num_train_epochs):\n",
        "            unet.train()\n",
        "            for step, batch in enumerate(train_dataloader):\n",
        "                with accelerator.accumulate(unet):\n",
        "                    # Convert images to latent space\n",
        "                    with torch.no_grad():\n",
        "                        latents = vae.encode(batch[\"pixel_values\"]).latent_dist.sample()\n",
        "                        latents = latents * 0.18215\n",
        "                    # Sample noise that we'll add to the latents\n",
        "                    noise = torch.randn(latents.shape).to(latents.device)\n",
        "                    bsz = latents.shape[0]\n",
        "                    # Sample a random timestep for each image\n",
        "                    timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz,), device=latents.device).long()\n",
        "\n",
        "                    # Add noise to the latents according to the noise magnitude at each timestep\n",
        "                    # (this is the forward diffusion process)\n",
        "                    noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
        "\n",
        "                    # Get the text embedding for conditioning\n",
        "                    with torch.no_grad():\n",
        "                        encoder_hidden_states = text_encoder(batch[\"input_ids\"])[0]\n",
        "\n",
        "                    # Predict the noise residual\n",
        "                    noise_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
        "\n",
        "                    if dreambooth_args.with_prior_preservation:\n",
        "                        # Chunk the noise and noise_pred into two parts and compute the loss on each part separately.\n",
        "                        noise_pred, noise_pred_prior = torch.chunk(noise_pred, 2, dim=0)\n",
        "                        noise, noise_prior = torch.chunk(noise, 2, dim=0)\n",
        "\n",
        "                        # Compute instance loss\n",
        "                        loss = F.mse_loss(noise_pred, noise, reduction=\"none\").mean([1, 2, 3]).mean()\n",
        "\n",
        "                        # Compute prior loss\n",
        "                        prior_loss = F.mse_loss(noise_pred_prior, noise_prior, reduction=\"none\").mean([1, 2, 3]).mean()\n",
        "\n",
        "                        # Add the prior loss to the instance loss.\n",
        "                        loss = loss + dreambooth_args.prior_loss_weight * prior_loss\n",
        "                    else:\n",
        "                        loss = F.mse_loss(noise_pred, noise, reduction=\"none\").mean([1, 2, 3]).mean()\n",
        "\n",
        "                    accelerator.backward(loss)\n",
        "                    if accelerator.sync_gradients:\n",
        "                        accelerator.clip_grad_norm_(unet.parameters(), dreambooth_args.max_grad_norm)\n",
        "                    optimizer.step()\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                # Checks if the accelerator has performed an optimization step behind the scenes\n",
        "                if accelerator.sync_gradients:\n",
        "                    progress_bar.update(1)\n",
        "                    global_step += 1\n",
        "\n",
        "                logs = {\"loss\": loss.detach().item()}\n",
        "                progress_bar.set_postfix(**logs)\n",
        "                progress.value = (global_step + 1) / dreambooth_args.max_train_steps\n",
        "                progress.tooltip = f'[{(global_step + 1)} / {dreambooth_args.max_train_steps}]'\n",
        "                progress.update()\n",
        "\n",
        "                if global_step >= dreambooth_args.max_train_steps:\n",
        "                    break\n",
        "\n",
        "            accelerator.wait_for_everyone()\n",
        "        \n",
        "        # Create the pipeline using the trained modules and save it.\n",
        "        if accelerator.is_main_process:\n",
        "            pipeline = StableDiffusionPipeline(\n",
        "                text_encoder=text_encoder,\n",
        "                vae=vae,\n",
        "                unet=accelerator.unwrap_model(unet),\n",
        "                tokenizer=tokenizer,\n",
        "                scheduler=PNDMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", skip_prk_steps=True),\n",
        "                safety_checker=None if prefs['disable_nsfw_filter'] else StableDiffusionSafetyChecker.from_pretrained(\"CompVis/stable-diffusion-safety-checker\"),\n",
        "                feature_extractor=CLIPFeatureExtractor.from_pretrained(\"openai/clip-vit-base-patch32\"),\n",
        "            )\n",
        "            pipeline.save_pretrained(dreambooth_args.output_dir)\n",
        "    \n",
        "    import accelerate\n",
        "    try:\n",
        "      accelerate.notebook_launcher(training_function, args=(text_encoder, vae, unet))\n",
        "    except Exception as e:\n",
        "      clear_last()\n",
        "      alert_msg(page, \"ERROR: CUDA Ran Out of Memory. Try reducing parameters and try again...\", content=Text(str(e)))\n",
        "      with torch.no_grad():\n",
        "        torch.cuda.empty_cache()\n",
        "      return\n",
        "    clear_last()\n",
        "    with torch.no_grad():\n",
        "        torch.cuda.empty_cache()\n",
        "    name_of_your_concept = dreambooth_prefs['name_of_your_concept']\n",
        "    if(dreambooth_prefs['save_concept']):\n",
        "      from huggingface_hub import HfApi, HfFolder, CommitOperationAdd\n",
        "      from huggingface_hub import create_repo\n",
        "      from IPython.display import display_markdown\n",
        "      api = HfApi()\n",
        "      your_username = api.whoami()[\"name\"]\n",
        "      dreambooth_pipe = StableDiffusionPipeline.from_pretrained(\n",
        "        dreambooth_args.output_dir,\n",
        "        torch_dtype=torch.float16,\n",
        "      ).to(\"cuda\")\n",
        "      os.makedirs(\"fp16_model\",exist_ok=True)\n",
        "      dreambooth_pipe.save_pretrained(\"fp16_model\")\n",
        "      del dreambooth_pipe\n",
        "      hf_token = prefs['HuggingFace_api_key']\n",
        "      if(dreambooth_prefs['where_to_save_concept'] == \"Public Library\"):\n",
        "        repo_id = f\"sd-dreambooth-library/{format_filename(name_of_your_concept, use_dash=True)}\"\n",
        "        #Join the Concepts Library organization if you aren't part of it already\n",
        "        run_sp(f\"curl -X POST -H 'Authorization: Bearer '{hf_token} -H 'Content-Type: application/json' https://huggingface.co/organizations/sd-dreambooth-library/share/SSeOwppVCscfTEzFGQaqpfcjukVeNrKNHX\", realtime=False)\n",
        "        #!curl -X POST -H 'Authorization: Bearer '$hf_token -H 'Content-Type: application/json' https://huggingface.co/organizations/sd-dreambooth-library/share/SSeOwppVCscfTEzFGQaqpfcjukVeNrKNHX\n",
        "      else:\n",
        "        repo_id = f\"{your_username}/{format_filename(name_of_your_concept, use_dash=True)}\"\n",
        "      output_dir = dreambooth_args.output_dir\n",
        "      if(not prefs['HuggingFace_api_key']):\n",
        "        with open(HfFolder.path_token, 'r') as fin: hf_token = fin.read();\n",
        "      else:\n",
        "        hf_token = prefs['HuggingFace_api_key'] \n",
        "      \n",
        "      images_upload = os.listdir(save_path)\n",
        "      image_string = \"\"\n",
        "      #repo_id = f\"sd-dreambooth-library/{slugify(name_of_your_concept)}\"\n",
        "      print(f\"repo_id={repo_id}\")\n",
        "      for i, image in enumerate(images_upload):\n",
        "          image_string = f'''{image_string}![image {i}](https://huggingface.co/{repo_id}/resolve/main/concept_images/{image})\n",
        "    '''\n",
        "      description = dreambooth_prefs['readme_description']\n",
        "      if bool(description.strip()):\n",
        "        description = dreambooth_prefs['readme_description'] + '\\n\\n'\n",
        "      readme_text = f'''---\n",
        "license: mit\n",
        "---\n",
        "### {name_of_your_concept} on Stable Diffusion via Dreambooth using [Stable Diffusion Deluxe](https://colab.research.google.com/github/Skquark/AI-Friends/blob/main/Stable_Diffusion_Deluxe.ipynb)\n",
        "#### model by {api.whoami()[\"name\"]}\n",
        "This your the Stable Diffusion model fine-tuned the {name_of_your_concept} concept taught to Stable Diffusion with Dreambooth.\n",
        "It can be used by modifying the `instance_prompt`: **{dreambooth_prefs['instance_prompt']}**\n",
        "\n",
        "{description}You can also train your own concepts and upload them to the library by using [this notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_dreambooth_training.ipynb).\n",
        "And you can run your new concept via `diffusers`: [Colab Notebook for Inference](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_dreambooth_inference.ipynb), [Spaces with the Public Concepts loaded](https://huggingface.co/spaces/sd-dreambooth-library/stable-diffusion-dreambooth-concepts)\n",
        "\n",
        "Here are the images used for training this concept:\n",
        "{image_string}\n",
        "'''\n",
        "      #Save the readme to a file\n",
        "      readme_file = open(\"README.md\", \"w\")\n",
        "      readme_file.write(readme_text)\n",
        "      readme_file.close()\n",
        "      #Save the token identifier to a file\n",
        "      text_file = open(\"token_identifier.txt\", \"w\")\n",
        "      text_file.write(dreambooth_prefs['instance_prompt'])\n",
        "      text_file.close()\n",
        "      operations = [\n",
        "        CommitOperationAdd(path_in_repo=\"token_identifier.txt\", path_or_fileobj=\"token_identifier.txt\"),\n",
        "        CommitOperationAdd(path_in_repo=\"README.md\", path_or_fileobj=\"README.md\"),\n",
        "      ]\n",
        "      try:\n",
        "        create_repo(repo_id, private=True, token=hf_token)\n",
        "      except Exception as e:\n",
        "        alert_msg(page, f\"ERROR Creating repo {repo_id}... Make sure your HF token has Write access.\", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip(), selectable=True)]))\n",
        "        return\n",
        "      \n",
        "      api.create_commit(repo_id=repo_id, operations=operations, commit_message=f\"Upload the concept {name_of_your_concept} embeds and token\",token=hf_token)\n",
        "      api.upload_folder(folder_path=\"fp16_model\", path_in_repo=\"\", repo_id=repo_id,token=hf_token)\n",
        "      api.upload_folder(folder_path=save_path, path_in_repo=\"concept_images\", repo_id=repo_id, token=hf_token)\n",
        "      prefs['custom_model'] = repo_id\n",
        "      prefs['custom_models'].append({'name': name_of_your_concept, 'path':repo_id})\n",
        "      page.custom_model.value = repo_id\n",
        "      try:\n",
        "        page.custom_model.update()\n",
        "      except Exception: pass\n",
        "      prt(Markdown(f\"## Your concept was saved successfully to _{repo_id}_.<br>[Click here to access it](https://huggingface.co/{repo_id}) and go to _Installers->Model Checkpoint->Custom Model Path_ to use. Include Token in prompts.\", on_tap_link=lambda e: e.page.launch_url(e.data)))\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "try:\n",
        "    from torchvision import transforms\n",
        "except Exception:\n",
        "    run_sp(\"pip install torchvision\", realtime=False)\n",
        "    from torchvision import transforms\n",
        "    pass\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class DreamBoothDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        instance_data_root,\n",
        "        instance_prompt,\n",
        "        tokenizer,\n",
        "        class_data_root=None,\n",
        "        class_prompt=None,\n",
        "        size=dreambooth_prefs['max_size'],\n",
        "        center_crop=False,\n",
        "    ):\n",
        "        self.size = size\n",
        "        self.center_crop = center_crop\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        self.instance_data_root = Path(instance_data_root)\n",
        "        if not self.instance_data_root.exists():\n",
        "            raise ValueError(\"Instance images root doesn't exists.\")\n",
        "\n",
        "        self.instance_images_path = list(Path(instance_data_root).iterdir())\n",
        "        self.num_instance_images = len(self.instance_images_path)\n",
        "        self.instance_prompt = instance_prompt\n",
        "        self._length = self.num_instance_images\n",
        "\n",
        "        if class_data_root is not None:\n",
        "            self.class_data_root = Path(class_data_root)\n",
        "            self.class_data_root.mkdir(parents=True, exist_ok=True)\n",
        "            self.class_images_path = list(Path(class_data_root).iterdir())\n",
        "            self.num_class_images = len(self.class_images_path)\n",
        "            self._length = max(self.num_class_images, self.num_instance_images)\n",
        "            self.class_prompt = class_prompt\n",
        "        else:\n",
        "            self.class_data_root = None\n",
        "\n",
        "        self.image_transforms = transforms.Compose([\n",
        "            transforms.Resize(size, interpolation=transforms.InterpolationMode.BILINEAR),\n",
        "            transforms.CenterCrop(size) if center_crop else transforms.RandomCrop(size),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.5], [0.5])]\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return self._length\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        example = {}\n",
        "        instance_image = PILImage.open(self.instance_images_path[index % self.num_instance_images])\n",
        "        if not instance_image.mode == \"RGB\":\n",
        "            instance_image = instance_image.convert(\"RGB\")\n",
        "        example[\"instance_images\"] = self.image_transforms(instance_image)\n",
        "        example[\"instance_prompt_ids\"] = self.tokenizer(\n",
        "            self.instance_prompt,\n",
        "            padding=\"do_not_pad\",\n",
        "            truncation=True,\n",
        "            max_length=self.tokenizer.model_max_length,\n",
        "        ).input_ids\n",
        "\n",
        "        if self.class_data_root:\n",
        "            class_image = PILImage.open(self.class_images_path[index % self.num_class_images])\n",
        "            if not class_image.mode == \"RGB\":\n",
        "                class_image = class_image.convert(\"RGB\")\n",
        "            example[\"class_images\"] = self.image_transforms(class_image)\n",
        "            example[\"class_prompt_ids\"] = self.tokenizer(\n",
        "                self.class_prompt,\n",
        "                padding=\"do_not_pad\",\n",
        "                truncation=True,\n",
        "                max_length=self.tokenizer.model_max_length,\n",
        "            ).input_ids\n",
        "        \n",
        "        return example\n",
        "\n",
        "class PromptDataset(Dataset):\n",
        "    def __init__(self, prompt, num_samples):\n",
        "        self.prompt = prompt\n",
        "        self.num_samples = num_samples\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        example = {}\n",
        "        example[\"prompt\"] = self.prompt\n",
        "        example[\"index\"] = index\n",
        "        return example\n",
        "\n",
        "\n",
        "def run_dreambooth2(page):\n",
        "    global dreambooth_prefs, prefs\n",
        "    def prt(line):\n",
        "      if type(line) == str:\n",
        "        line = Text(line)\n",
        "      page.dreambooth_output.controls.append(line)\n",
        "      page.dreambooth_output.update()\n",
        "    def clear_last():\n",
        "      del page.dreambooth_output.controls[-1]\n",
        "      page.dreambooth_output.update()\n",
        "    if not status['installed_diffusers']:\n",
        "      alert_msg(page, \"You must Install the HuggingFace Diffusers Library first... \")\n",
        "      return\n",
        "    save_path = os.path.join(root_dir, \"my_concept\")\n",
        "    error = False\n",
        "    if not os.path.exists(save_path):\n",
        "      error = True\n",
        "    elif len(os.listdir(save_path)) == 0:\n",
        "      error = True\n",
        "    if len(page.db_file_list.controls) == 0:\n",
        "      error = True\n",
        "    if error:\n",
        "      alert_msg(page, \"Couldn't find a list of images to train concept. Add image files to the list...\")\n",
        "      return\n",
        "    prt(Installing(\"Downloading DreamBooth Conceptualizers\"))\n",
        "    diffusers_dir = os.path.join(root_dir, \"diffusers\")\n",
        "    if not os.path.exists(diffusers_dir):\n",
        "      os.chdir(root_dir)\n",
        "      run_process(\"git clone https://github.com/Skquark/diffusers.git\", realtime=False, cwd=root_dir)\n",
        "    os.chdir(diffusers_dir)\n",
        "    #run_process('pip install -e \".[training]\"', cwd=diffusers_dir, realtime=False)\n",
        "    run_process('pip install \"git+https://github.com/Skquark/diffusers.git#egg=diffusers[training]\"', cwd=root_dir, realtime=False)\n",
        "    dreambooth_dir = os.path.join(diffusers_dir, \"examples\", \"dreambooth\")\n",
        "    os.chdir(dreambooth_dir)\n",
        "    run_process(\"pip install -r requirements.txt\", cwd=dreambooth_dir, realtime=False)\n",
        "    try:\n",
        "      os.environ['LD_LIBRARY_PATH'] += \"/usr/lib/wsl/lib:$LD_LIBRARY_PATH\"\n",
        "      import bitsandbytes\n",
        "    except ModuleNotFoundError:\n",
        "      if sys.platform.startswith(\"win\"):\n",
        "          run_sp(\"pip install bitsandbytes-windows\", realtime=False)\n",
        "      else:\n",
        "          run_sp(\"pip install bitsandbytes\", realtime=False)\n",
        "      import bitsandbytes\n",
        "      pass\n",
        "    #from accelerate.utils import write_basic_config\n",
        "    #write_basic_config()\n",
        "    import argparse\n",
        "    from io import BytesIO\n",
        "    #save_path = \"./my_concept\"\n",
        "    #if not os.path.exists(save_path):\n",
        "    #  os.mkdir(save_path)\n",
        "\n",
        "    clear_pipes()\n",
        "    clear_last()\n",
        "    num_new_images = None\n",
        "    \n",
        "    from argparse import Namespace\n",
        "    dreambooth_args = Namespace(\n",
        "        pretrained_model_name_or_path=model_path,\n",
        "        resolution=dreambooth_prefs['max_size'],\n",
        "        center_crop=True,\n",
        "        instance_data_dir=save_path,\n",
        "        instance_prompt=dreambooth_prefs['instance_prompt'].strip(),\n",
        "        learning_rate=dreambooth_prefs['learning_rate'],#5e-06,\n",
        "        max_train_steps=dreambooth_prefs['max_train_steps'],#450,\n",
        "        train_batch_size=dreambooth_prefs['train_batch_size'],\n",
        "        gradient_accumulation_steps=2,\n",
        "        max_grad_norm=1.0,\n",
        "        #mixed_precision=\"no\", # set to \"fp16\" for mixed-precision training.\n",
        "        gradient_checkpointing=True, # set this to True to lower the memory usage.\n",
        "        use_8bit_adam=not prefs['higher_vram_mode'], # use 8bit optimizer from bitsandbytes\n",
        "        enable_xformers_memory_efficient_attention = status['installed_xformers'],\n",
        "        seed=dreambooth_prefs['seed'],#3434554,\n",
        "        with_prior_preservation=dreambooth_prefs['prior_preservation'], \n",
        "        prior_loss_weight=dreambooth_prefs['prior_loss_weight'],\n",
        "        sample_batch_size=dreambooth_prefs['sample_batch_size'],\n",
        "        class_data_dir=dreambooth_prefs['prior_preservation_class_folder'], \n",
        "        class_prompt=dreambooth_prefs['prior_preservation_class_prompt'],\n",
        "        num_class_images=dreambooth_prefs['num_class_images'], \n",
        "        output_dir=os.path.join(root_dir, \"dreambooth-concept\"),\n",
        "    )\n",
        "    if not os.path.exists(dreambooth_args.output_dir): os.mkdir(dreambooth_args.output_dir)\n",
        "    arg_str = \"accelerate launch ./train_dreambooth.py\"\n",
        "    for k, v in vars(dreambooth_args).items():\n",
        "      if isinstance(v, str):\n",
        "        if ' ' in v:\n",
        "          v = f'\"{v}\"'\n",
        "      if isinstance(v, bool):\n",
        "        if bool(v):\n",
        "          arg_str += f\" --{k}\"\n",
        "      else:\n",
        "        arg_str += f\" --{k}={v}\"\n",
        "    prt(\"***** Running training *****\")\n",
        "    #if num_new_images != None: prt(f\"  Number of class images to sample: {num_new_images}.\")\n",
        "    #prt(f\"  Instantaneous batch size per device = {dreambooth_args.train_batch_size}\")\n",
        "    #prt(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n",
        "    #prt(f\"  Gradient Accumulation steps = {dreambooth_args.gradient_accumulation_steps}\")\n",
        "    #prt(f\"  Total optimization steps = {dreambooth_args.max_train_steps}\")\n",
        "    prt(arg_str)\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    prt(progress)\n",
        "    try:\n",
        "      run_process(arg_str, page=page, cwd=dreambooth_dir)\n",
        "      #run_sp(arg_str, cwd=dreambooth_dir)\n",
        "    except Exception as e:\n",
        "      clear_last()\n",
        "      alert_msg(page, f\"ERROR: CUDA Ran Out of Memory. Try reducing parameters and try again...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip())]))\n",
        "      with torch.no_grad():\n",
        "        torch.cuda.empty_cache()\n",
        "      return\n",
        "    clear_last()\n",
        "    name_of_your_concept = dreambooth_prefs['name_of_your_concept']\n",
        "    if(dreambooth_prefs['save_concept']):\n",
        "      from huggingface_hub import HfApi, HfFolder, CommitOperationAdd\n",
        "      from huggingface_hub import create_repo\n",
        "      from diffusers import StableDiffusionPipeline\n",
        "      api = HfApi()\n",
        "      your_username = api.whoami()[\"name\"]\n",
        "      dreambooth_pipe = StableDiffusionPipeline.from_pretrained(\n",
        "        dreambooth_args.output_dir,\n",
        "        torch_dtype=torch.float16,\n",
        "      ).to(\"cuda\")\n",
        "      os.makedirs(\"fp16_model\",exist_ok=True)\n",
        "      dreambooth_pipe.save_pretrained(\"fp16_model\")\n",
        "      hf_token = prefs['HuggingFace_api_key']\n",
        "      if(dreambooth_prefs['where_to_save_concept'] == \"Public Library\"):\n",
        "        repo_id = f\"sd-dreambooth-library/{format_filename(name_of_your_concept, use_dash=True)}\"\n",
        "        #Join the Concepts Library organization if you aren't part of it already\n",
        "        run_sp(f\"curl -X POST -H 'Authorization: Bearer '{hf_token} -H 'Content-Type: application/json' https://huggingface.co/organizations/sd-dreambooth-library/share/SSeOwppVCscfTEzFGQaqpfcjukVeNrKNHX\", realtime=False)\n",
        "        #!curl -X POST -H 'Authorization: Bearer '$hf_token -H 'Content-Type: application/json' https://huggingface.co/organizations/sd-dreambooth-library/share/SSeOwppVCscfTEzFGQaqpfcjukVeNrKNHX\n",
        "      else:\n",
        "        repo_id = f\"{your_username}/{format_filename(name_of_your_concept, use_dash=True)}\"\n",
        "      output_dir = dreambooth_args.output_dir\n",
        "      if(not prefs['HuggingFace_api_key']):\n",
        "        with open(HfFolder.path_token, 'r') as fin: hf_token = fin.read();\n",
        "      else:\n",
        "        hf_token = prefs['HuggingFace_api_key'] \n",
        "      \n",
        "      images_upload = os.listdir(save_path)\n",
        "      image_string = \"\"\n",
        "      #repo_id = f\"sd-dreambooth-library/{slugify(name_of_your_concept)}\"\n",
        "      for i, image in enumerate(images_upload):\n",
        "          image_string = f'''{image_string}![image {i}](https://huggingface.co/{repo_id}/resolve/main/concept_images/{image})\n",
        "'''\n",
        "      description = dreambooth_prefs['readme_description']\n",
        "      if bool(description.strip()):\n",
        "        description = dreambooth_prefs['readme_description'] + '\\n\\n'\n",
        "      readme_text = f'''---\n",
        "license: mit\n",
        "---\n",
        "### {name_of_your_concept} on Stable Diffusion via Dreambooth using [Stable Diffusion Deluxe](https://colab.research.google.com/github/Skquark/AI-Friends/blob/main/Stable_Diffusion_Deluxe.ipynb)\n",
        "#### model by {api.whoami()[\"name\"]}\n",
        "This your the Stable Diffusion model fine-tuned the {name_of_your_concept} concept taught to Stable Diffusion with Dreambooth.\n",
        "It can be used by modifying the `instance_prompt`: **{dreambooth_prefs['instance_prompt']}**\n",
        "\n",
        "{description}You can also train your own concepts and upload them to the library by using [this notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_dreambooth_training.ipynb).\n",
        "And you can run your new concept via `diffusers`: [Colab Notebook for Inference](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_dreambooth_inference.ipynb), [Spaces with the Public Concepts loaded](https://huggingface.co/spaces/sd-dreambooth-library/stable-diffusion-dreambooth-concepts)\n",
        "\n",
        "Here are the images used for training this concept:\n",
        "{image_string}\n",
        "'''\n",
        "      #Save the readme to a file\n",
        "      readme_file = open(\"README.md\", \"w\")\n",
        "      readme_file.write(readme_text)\n",
        "      readme_file.close()\n",
        "      #Save the token identifier to a file\n",
        "      text_file = open(\"token_identifier.txt\", \"w\")\n",
        "      text_file.write(dreambooth_prefs['instance_prompt'])\n",
        "      text_file.close()\n",
        "      operations = [\n",
        "        CommitOperationAdd(path_in_repo=\"token_identifier.txt\", path_or_fileobj=\"token_identifier.txt\"),\n",
        "        CommitOperationAdd(path_in_repo=\"README.md\", path_or_fileobj=\"README.md\"),\n",
        "      ]\n",
        "      print(repo_id)\n",
        "      print(readme_text)\n",
        "      try:\n",
        "        create_repo(repo_id, private=True, token=hf_token)\n",
        "      except Exception as e:\n",
        "        alert_msg(page, f\"ERROR Creating repo {repo_id}... Make sure your HF token has Write access.\", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip(), selectable=True)]))\n",
        "        return\n",
        "      api.create_commit(repo_id=repo_id, operations=operations, commit_message=f\"Upload the concept {name_of_your_concept} embeds and token\",token=hf_token)\n",
        "      api.upload_folder(folder_path=\"fp16_model\", path_in_repo=\"\", repo_id=repo_id,token=hf_token)\n",
        "      api.upload_folder(folder_path=save_path, path_in_repo=\"concept_images\", repo_id=repo_id, token=hf_token)\n",
        "      prefs['custom_model'] = repo_id\n",
        "      prefs['custom_models'].append({'name': name_of_your_concept, 'path':repo_id})\n",
        "      page.custom_model.value = repo_id\n",
        "      try:\n",
        "        page.custom_model.update()\n",
        "      except Exception: pass\n",
        "      prt(Markdown(f\"## Your concept was saved successfully to _{repo_id}_.<br>[Click here to access it](https://huggingface.co/{repo_id}) and go to _Installers->Model Checkpoint->Custom Model Path_ to use. Include Token in prompts.\", on_tap_link=lambda e: e.page.launch_url(e.data)))\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "def run_textualinversion(page):\n",
        "    global textualinversion_prefs, prefs\n",
        "    def prt(line):\n",
        "      if type(line) == str:\n",
        "        line = Text(line)\n",
        "      page.textualinversion_output.controls.append(line)\n",
        "      page.textualinversion_output.update()\n",
        "    def clear_last():\n",
        "      del page.textualinversion_output.controls[-1]\n",
        "      page.textualinversion_output.update()\n",
        "    if not status['installed_diffusers']:\n",
        "      alert_msg(page, \"You must Install the HuggingFace Diffusers Library first... \")\n",
        "      return\n",
        "    save_path = os.path.join(root_dir, \"my_concept\")\n",
        "    error = False\n",
        "    if not os.path.exists(save_path):\n",
        "      error = True\n",
        "    elif len(os.listdir(save_path)) == 0:\n",
        "      error = True\n",
        "    if len(page.ti_file_list.controls) == 0:\n",
        "      error = True\n",
        "    if error:\n",
        "      alert_msg(page, \"Couldn't find a list of images to train concept. Add image files to the list...\")\n",
        "      return\n",
        "    page.textualinversion_output.controls.clear()\n",
        "    page.textualinversion_output.update()\n",
        "    prt(Installing(\"Downloading Textual-Inversion Training Models\"))\n",
        "    \n",
        "    placeholder_token = textualinversion_prefs['placeholder_token'].strip()\n",
        "    if not placeholder_token.startswith('<'): placeholder_token = '<' + placeholder_token\n",
        "    if not placeholder_token.endswith('>'): placeholder_token = placeholder_token + '>'\n",
        "    initializer_token = textualinversion_prefs['initializer_token'].strip()\n",
        "    if bool(initializer_token):\n",
        "      if not initializer_token.startswith('<'): initializer_token = '<' + initializer_token\n",
        "      if not initializer_token.endswith('>'): initializer_token = initializer_token + '>'\n",
        "    diffusers_dir = os.path.join(root_dir, \"diffusers\")\n",
        "    if not os.path.exists(diffusers_dir):\n",
        "      os.chdir(root_dir)\n",
        "      run_process(\"git clone https://github.com/Skquark/diffusers.git\", realtime=False, cwd=root_dir)\n",
        "    run_process('pip install git+https://github.com/Skquark/diffusers.git#egg=diffusers[training]', cwd=root_dir, realtime=False)\n",
        "    os.chdir(diffusers_dir)\n",
        "    run_sp('pip install -e \".[training]\"', cwd=diffusers_dir, realtime=False)\n",
        "    textualinversion_dir = os.path.join(diffusers_dir, \"examples\", \"textual_inversion\")\n",
        "    #textualinversion_dir = os.path.join(diffusers_dir, \"examples\", \"text_to_image\")\n",
        "    os.chdir(textualinversion_dir)\n",
        "    run_sp(\"pip install -r requirements.txt\", cwd=textualinversion_dir, realtime=False)\n",
        "    run_process(\"pip install -qq bitsandbytes\", page=page)\n",
        "    run_sp(\"accelerate config default\", realtime=False)\n",
        "    #from accelerate.utils import write_basic_config\n",
        "    #write_basic_config()\n",
        "    import argparse\n",
        "    from io import BytesIO\n",
        "\n",
        "    clear_pipes()\n",
        "    clear_last()\n",
        "    #num_new_images = None\n",
        "    random_seed = int(textualinversion_prefs['seed']) if int(textualinversion_prefs['seed']) > 0 else rnd.randint(0,4294967295)\n",
        "    name_of_your_model = textualinversion_prefs['name_of_your_model']\n",
        "    from argparse import Namespace\n",
        "    textualinversion_args = Namespace(\n",
        "        pretrained_model_name_or_path=model_path,\n",
        "        resolution=textualinversion_prefs['resolution'],\n",
        "        center_crop=True,\n",
        "        #train_data_dir=save_path,\n",
        "        #caption_column=textualinversion_prefs['instance_prompt'].strip(),\n",
        "        train_data_dir=save_path,\n",
        "        validation_prompt=textualinversion_prefs['validation_prompt'].strip(),\n",
        "        placeholder_token=placeholder_token,\n",
        "        initializer_token=initializer_token,\n",
        "        learnable_property=textualinversion_prefs['what_to_teach'],\n",
        "        learning_rate=textualinversion_prefs['learning_rate'],#5e-06,'\n",
        "        lr_scheduler=textualinversion_prefs['lr_scheduler'],\n",
        "        lr_warmup_steps=textualinversion_prefs['lr_warmup_steps'],\n",
        "        scale_lr=textualinversion_prefs['scale_lr'],\n",
        "        max_train_steps=textualinversion_prefs['max_train_steps'],#450,\n",
        "        train_batch_size=textualinversion_prefs['train_batch_size'],\n",
        "        checkpointing_steps=textualinversion_prefs['checkpointing_steps'],\n",
        "        gradient_accumulation_steps=textualinversion_prefs['gradient_accumulation_steps'],\n",
        "        validation_steps=textualinversion_prefs['validation_steps'],\n",
        "        num_vectors=textualinversion_prefs['num_vectors'],\n",
        "        #mixed_precision=\"no\", # set to \"fp16\" for mixed-precision training.\n",
        "        gradient_checkpointing=True, # set this to True to lower the memory usage.\n",
        "        use_8bit_adam=not prefs['higher_vram_mode'], # use 8bit optimizer from bitsandbytes\n",
        "        enable_xformers_memory_efficient_attention = status['installed_xformers'],\n",
        "        seed=random_seed,\n",
        "        #with_prior_preservation=textualinversion_prefs['prior_preservation'], \n",
        "        #prior_loss_weight=textualinversion_prefs['prior_loss_weight'],\n",
        "        #sample_batch_size=textualinversion_prefs['sample_batch_size'],\n",
        "        #class_data_dir=textualinversion_prefs['class_data_dir'], \n",
        "        #class_prompt=textualinversion_prefs['class_prompt'],\n",
        "        num_class_images=textualinversion_prefs['num_class_images'],\n",
        "        output_dir=os.path.join(save_path, format_filename(textualinversion_prefs['name_of_your_concept'], use_dash=True)),\n",
        "    )\n",
        "    output_dir = textualinversion_args.output_dir\n",
        "    arg_str = \"accelerate launch textual_inversion.py\"\n",
        "    #arg_str = 'accelerate --mixed_precision=\"fp16\" launch train_text_to_image_lora.py'\n",
        "    for k, v in vars(textualinversion_args).items():\n",
        "      if isinstance(v, str):\n",
        "        if ' ' in v:\n",
        "          v = f'\"{v}\"'\n",
        "      if isinstance(v, bool):\n",
        "        if bool(v):\n",
        "          arg_str += f\" --{k}\"\n",
        "      else:\n",
        "        arg_str += f\" --{k}={v}\"\n",
        "    prt(Text(\"*** Running Training *** See Console for Progress\", weight=FontWeight.BOLD))\n",
        "    #if num_new_images != None: prt(f\"  Number of class images to sample: {num_new_images}.\")\n",
        "    #prt(f\"  Instantaneous batch size per device = {textualinversion_args.train_batch_size}\")\n",
        "    #prt(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n",
        "    #prt(f\"  Gradient Accumulation steps = {textualinversion_args.gradient_accumulation_steps}\")\n",
        "    #prt(f\"  Total optimization steps = {textualinversion_args.max_train_steps}\")\n",
        "    prt(arg_str)\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    prt(progress)\n",
        "    if(textualinversion_prefs['save_model']):\n",
        "      from huggingface_hub import HfApi, HfFolder, CommitOperationAdd\n",
        "      from huggingface_hub import Repository, create_repo, whoami\n",
        "      #from diffusers import StableDiffusionPipeline\n",
        "      api = HfApi()\n",
        "      your_username = api.whoami()[\"name\"]\n",
        "      #textualinversion_pipe = StableDiffusionPipeline.from_pretrained(\n",
        "      #  textualinversion_args.output_dir,\n",
        "      #  torch_dtype=torch.float16,\n",
        "      #).to(\"cuda\")\n",
        "      #os.makedirs(\"fp16_model\",exist_ok=True)\n",
        "      #textualinversion_pipe.save_pretrained(\"fp16_model\")\n",
        "      hf_token = prefs['HuggingFace_api_key']\n",
        "      private = False if textualinversion_prefs['where_to_save_model'] == \"Public HuggingFace\" else True\n",
        "      repo_id = f\"{your_username}/{format_filename(name_of_your_model, use_dash=True)}\"\n",
        "      output_dir = textualinversion_args.output_dir\n",
        "      if(not prefs['HuggingFace_api_key']):\n",
        "        with open(HfFolder.path_token, 'r') as fin: hf_token = fin.read();\n",
        "      else:\n",
        "        hf_token = prefs['HuggingFace_api_key']\n",
        "      try:\n",
        "        create_repo(repo_id, private=private, exist_ok=True, token=hf_token)\n",
        "        repo = Repository(output_dir, clone_from=repo_id, token=hf_token)\n",
        "      except Exception as e:\n",
        "        alert_msg(page, f\"ERROR Creating repo {repo_id}... Make sure your HF token has Write access.\", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip(), selectable=True)]))\n",
        "        return\n",
        "    else:\n",
        "      if not os.path.exists(output_dir): os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    try:\n",
        "      os.chdir(textualinversion_dir)\n",
        "      #!accelerate $arg_str # type: ignore\n",
        "      os.system(arg_str)\n",
        "      #run_sp(arg_str, cwd=textualinversion_dir, realtime=True)\n",
        "      #run_sp(arg_str, cwd=textualinversion_dir)\n",
        "    except Exception as e:\n",
        "      clear_last()\n",
        "      alert_msg(page, f\"ERROR: Out of Memory (or something else). Try reducing parameters and try again...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip())]))\n",
        "      with torch.no_grad():\n",
        "        torch.cuda.empty_cache()\n",
        "      return\n",
        "    clear_last()\n",
        "\n",
        "    #title Save your newly created concept to the [library of concepts](https://huggingface.co/sd-concepts-library)?\n",
        "    save_concept_to_public_library = textualinversion_prefs['save_concept']\n",
        "    name_of_your_concept = textualinversion_prefs['name_of_your_concept']\n",
        "    # `hf_token_write`: leave blank if you logged in with a token with `write access` in the [Initial Setup](#scrollTo=KbzZ9xe6dWwf). If not, [go to your tokens settings and create a write access token](https://huggingface.co/settings/tokens)\n",
        "    hf_token_write = prefs['HuggingFace_api_key']\n",
        "\n",
        "    if(save_concept_to_public_library):\n",
        "        from huggingface_hub import HfApi, HfFolder, CommitOperationAdd\n",
        "        from huggingface_hub import create_repo\n",
        "        api = HfApi()\n",
        "        your_username = api.whoami()[\"name\"]\n",
        "        repo_id = f\"sd-concepts-library/{format_filename(name_of_your_concept, use_dash=True)}\"\n",
        "        #output_dir = textualinversion_prefs[\"output_dir\"]\n",
        "        if(not hf_token_write):\n",
        "            with open(HfFolder.path_token, 'r') as fin: hf_token = fin.read();\n",
        "        else:\n",
        "            hf_token = hf_token_write\n",
        "        if(textualinversion_prefs['where_to_save_concept'] == \"Public Library\"):\n",
        "            #Join the Concepts Library organization if you aren't part of it already\n",
        "            run_sp(f\"curl -X POST -H 'Authorization: Bearer '{hf_token} -H 'Content-Type: application/json' https://huggingface.co/organizations/sd-concepts-library/share/VcLXJtzwwxnHYCkNMLpSJCdnNFZHQwWywv\", realtime=False)\n",
        "            # curl -X POST -H 'Authorization: Bearer '$hf_token -H 'Content-Type: application/json' https://huggingface.co/organizations/sd-concepts-library/share/VcLXJtzwwxnHYCkNMLpSJCdnNFZHQwWywv\n",
        "        else:\n",
        "            repo_id = f\"{your_username}/{format_filename(name_of_your_concept, use_dash=True)}\"\n",
        "        images_upload = os.listdir(save_path)\n",
        "        image_string = \"\"\n",
        "        repo_id = f\"sd-concepts-library/{format_filename(name_of_your_concept, use_dash=True)}\"\n",
        "        for i, image in enumerate(images_upload):\n",
        "            image_string = f'''{image_string}![{placeholder_token} {i}](https://huggingface.co/{repo_id}/resolve/main/concept_images/{image})\n",
        "        '''\n",
        "        if(textualinversion_prefs['what_to_teach'] == \"style\"):\n",
        "            what_to_teach_article = f\"a `{textualinversion_prefs['what_to_teach']}`\"\n",
        "        else:\n",
        "            what_to_teach_article = f\"an `{textualinversion_prefs['what_to_teach']}`\"\n",
        "        description = textualinversion_prefs['readme_description']\n",
        "        if bool(description.strip()):\n",
        "            description = textualinversion_prefs['readme_description'] + '\\n\\n'\n",
        "        readme_text = f'''---\n",
        "license: mit\n",
        "base_model: {model_path}\n",
        "tags:\n",
        "- stable-diffusion\n",
        "- stable-diffusion-diffusers\n",
        "- text-to-image\n",
        "- diffusers\n",
        "- textual_inversion\n",
        "inference: true\n",
        "---\n",
        "# Textual inversion text2image fine-tuning - {repo_id}\n",
        "### {name_of_your_concept} by {your_username} using [Stable Diffusion Deluxe](https://colab.research.google.com/github/Skquark/AI-Friends/blob/main/Stable_Diffusion_Deluxe.ipynb)\n",
        "This is the `{placeholder_token}` concept taught to Stable Diffusion via Textual Inversion. You can load this concept into the [Stable Diffusion Deluxe](https://colab.research.google.com/github/Skquark/AI-Friends/blob/main/Stable_Diffusion_Deluxe.ipynb) notebook. You can also train your own concepts and load them into the concept libraries there too, or using [this notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_textual_inversion_training.ipynb).\n",
        "\n",
        "{description}Here is the new concept you will be able to use as {what_to_teach_article}:\n",
        "{image_string}\n",
        "'''\n",
        "        #Save the readme to a file\n",
        "        readme_file = open(\"README.md\", \"w\")\n",
        "        readme_file.write(readme_text)\n",
        "        readme_file.close()\n",
        "        #Save the token identifier to a file\n",
        "        text_file = open(\"token_identifier.txt\", \"w\")\n",
        "        text_file.write(placeholder_token)\n",
        "        text_file.close()\n",
        "        #Save the type of teached thing to a file\n",
        "        type_file = open(\"type_of_concept.txt\",\"w\")\n",
        "        type_file.write(textualinversion_prefs['what_to_teach'])\n",
        "        type_file.close()\n",
        "        operations = [\n",
        "            CommitOperationAdd(path_in_repo=\"learned_embeds.bin\", path_or_fileobj=f\"{output_dir}/learned_embeds.bin\"),\n",
        "            CommitOperationAdd(path_in_repo=\"token_identifier.txt\", path_or_fileobj=\"token_identifier.txt\"),\n",
        "            CommitOperationAdd(path_in_repo=\"type_of_concept.txt\", path_or_fileobj=\"type_of_concept.txt\"),\n",
        "            CommitOperationAdd(path_in_repo=\"README.md\", path_or_fileobj=\"README.md\"),\n",
        "        ]\n",
        "        try:\n",
        "          create_repo(repo_id, private=True, token=hf_token)\n",
        "        except Exception as e:\n",
        "          alert_msg(page, f\"ERROR Creating repo {repo_id}... Make sure your HF token has Write access.\", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip(), selectable=True)]))\n",
        "          return\n",
        "        api = HfApi()\n",
        "        api.create_commit(\n",
        "            repo_id=repo_id,\n",
        "            operations=operations,\n",
        "            commit_message=f\"Upload the concept {name_of_your_concept} embeds and token\",\n",
        "            token=hf_token\n",
        "        )\n",
        "        api.upload_folder(\n",
        "            folder_path=save_path,\n",
        "            path_in_repo=\"concept_images\",\n",
        "            repo_id=repo_id,\n",
        "            token=hf_token\n",
        "        )\n",
        "        prefs['custom_model'] = repo_id\n",
        "        prefs['custom_models'].append({'name': name_of_your_concept, 'path':repo_id})\n",
        "        page.custom_model.value = repo_id\n",
        "        try:\n",
        "          page.custom_model.update()\n",
        "        except Exception: pass\n",
        "        prt(Markdown(f\"## Your concept was saved successfully to _{repo_id}_.<br>[Click here to access it](https://huggingface.co/{repo_id}) and go to _Installers->Model Checkpoint->Custom Model Path_ to use. Include Token to your Prompt text.\", on_tap_link=lambda e: e.page.launch_url(e.data)))\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "def run_textualinversion2(page):\n",
        "    global textualinversion_prefs, prefs\n",
        "    def prt(line):\n",
        "      if type(line) == str:\n",
        "        line = Text(line)\n",
        "      page.textualinversion_output.controls.append(line)\n",
        "      page.textualinversion_output.update()\n",
        "    def clear_last():\n",
        "      del page.textualinversion_output.controls[-1]\n",
        "      page.textualinversion_output.update()\n",
        "    if not status['installed_diffusers']:\n",
        "      alert_msg(page, \"You must Install the HuggingFace Diffusers Library first... \")\n",
        "      return\n",
        "    save_path = os.path.join(root_dir, \"my_concept\")\n",
        "    error = False\n",
        "    if not os.path.exists(save_path):\n",
        "      error = True\n",
        "    elif len(os.listdir(save_path)) == 0:\n",
        "      error = True\n",
        "    if len(page.ti_file_list.controls) == 0:\n",
        "      error = True\n",
        "    if error:\n",
        "      alert_msg(page, \"Couldn't find a list of images to train concept. Add image files to the list...\")\n",
        "      return\n",
        "    prt(Installing(\"Downloading Textual-Inversion Training Models\"))\n",
        "    #run_process(\"pip install -qq bitsandbytes\")\n",
        "    import argparse\n",
        "    import itertools\n",
        "    import math\n",
        "    import random\n",
        "\n",
        "    import torch.nn.functional as F\n",
        "    import torch.utils.checkpoint\n",
        "    from torch.utils.data import Dataset\n",
        "    from accelerate import Accelerator\n",
        "    from accelerate.logging import get_logger\n",
        "    from accelerate.utils import set_seed\n",
        "    from diffusers import AutoencoderKL, DDPMScheduler, PNDMScheduler, StableDiffusionPipeline, UNet2DConditionModel\n",
        "    #from diffusers.hub_utils import init_git_repo, push_to_hub\n",
        "    from diffusers.optimization import get_scheduler\n",
        "    from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker\n",
        "    from torchvision import transforms\n",
        "    from tqdm.auto import tqdm\n",
        "    from transformers import CLIPFeatureExtractor, CLIPTextModel, CLIPTokenizer\n",
        "\n",
        "    imagenet_templates_small = [\n",
        "        \"a photo of a {}\",\n",
        "        \"a rendering of a {}\",\n",
        "        \"a cropped photo of the {}\",\n",
        "        \"the photo of a {}\",\n",
        "        \"a photo of a clean {}\",\n",
        "        \"a photo of a dirty {}\",\n",
        "        \"a dark photo of the {}\",\n",
        "        \"a photo of my {}\",\n",
        "        \"a photo of the cool {}\",\n",
        "        \"a close-up photo of a {}\",\n",
        "        \"a bright photo of the {}\",\n",
        "        \"a cropped photo of a {}\",\n",
        "        \"a photo of the {}\",\n",
        "        \"a good photo of the {}\",\n",
        "        \"a photo of one {}\",\n",
        "        \"a close-up photo of the {}\",\n",
        "        \"a rendition of the {}\",\n",
        "        \"a photo of the clean {}\",\n",
        "        \"a rendition of a {}\",\n",
        "        \"a photo of a nice {}\",\n",
        "        \"a good photo of a {}\",\n",
        "        \"a photo of the nice {}\",\n",
        "        \"a photo of the small {}\",\n",
        "        \"a photo of the weird {}\",\n",
        "        \"a photo of the large {}\",\n",
        "        \"a photo of a cool {}\",\n",
        "        \"a photo of a small {}\",\n",
        "    ]\n",
        "\n",
        "    imagenet_style_templates_small = [\n",
        "        \"a painting in the style of {}\",\n",
        "        \"a rendering in the style of {}\",\n",
        "        \"a cropped painting in the style of {}\",\n",
        "        \"the painting in the style of {}\",\n",
        "        \"a clean painting in the style of {}\",\n",
        "        \"a dirty painting in the style of {}\",\n",
        "        \"a dark painting in the style of {}\",\n",
        "        \"a picture in the style of {}\",\n",
        "        \"a cool painting in the style of {}\",\n",
        "        \"a close-up painting in the style of {}\",\n",
        "        \"a bright painting in the style of {}\",\n",
        "        \"a cropped painting in the style of {}\",\n",
        "        \"a good painting in the style of {}\",\n",
        "        \"a close-up painting in the style of {}\",\n",
        "        \"a rendition in the style of {}\",\n",
        "        \"a nice painting in the style of {}\",\n",
        "        \"a small painting in the style of {}\",\n",
        "        \"a weird painting in the style of {}\",\n",
        "        \"a large painting in the style of {}\",\n",
        "    ]\n",
        "    tokenizer = CLIPTokenizer.from_pretrained(\n",
        "        model_path,\n",
        "        subfolder=\"tokenizer\",\n",
        "    )\n",
        "    placeholder_token = textualinversion_prefs['placeholder_token'].strip()\n",
        "    if not placeholder_token.startswith('<'): placeholder_token = '<' + placeholder_token\n",
        "    if not placeholder_token.endswith('>'): placeholder_token = placeholder_token + '>'\n",
        "    # Add the placeholder token in tokenizer\n",
        "    num_added_tokens = tokenizer.add_tokens(placeholder_token)\n",
        "    if num_added_tokens == 0:\n",
        "        raise ValueError(\n",
        "            f\"The tokenizer already contains the token {placeholder_token}. Please pass a different\"\n",
        "            \" `placeholder_token` that is not already in the tokenizer.\"\n",
        "        )\n",
        "\n",
        "    token_ids = tokenizer.encode(textualinversion_prefs['initializer_token'], add_special_tokens=False)\n",
        "    # Check if initializer_token is a single token or a sequence of tokens\n",
        "    if len(token_ids) > 1:\n",
        "        raise ValueError(\"The initializer token must be a single token.\")\n",
        "\n",
        "    initializer_token_id = token_ids[0]\n",
        "    placeholder_token_id = tokenizer.convert_tokens_to_ids(placeholder_token)\n",
        "\n",
        "    # Load the Stable Diffusion model\n",
        "    # Load models and create wrapper for stable diffusion\n",
        "    text_encoder = CLIPTextModel.from_pretrained(\n",
        "        model_path, subfolder=\"text_encoder\"\n",
        "    )\n",
        "    vae = AutoencoderKL.from_pretrained(\n",
        "        model_path, subfolder=\"vae\"\n",
        "    )\n",
        "    unet = UNet2DConditionModel.from_pretrained(\n",
        "        model_path, subfolder=\"unet\"\n",
        "    )\n",
        "\n",
        "    text_encoder.resize_token_embeddings(len(tokenizer))\n",
        "    token_embeds = text_encoder.get_input_embeddings().weight.data\n",
        "    token_embeds[placeholder_token_id] = token_embeds[initializer_token_id]\n",
        "\n",
        "    def freeze_params(params):\n",
        "        for param in params:\n",
        "            param.requires_grad = False\n",
        "\n",
        "    # Freeze vae and unet\n",
        "    freeze_params(vae.parameters())\n",
        "    freeze_params(unet.parameters())\n",
        "    # Freeze all parameters except for the token embeddings in text encoder\n",
        "    params_to_freeze = itertools.chain(\n",
        "        text_encoder.text_model.encoder.parameters(),\n",
        "        text_encoder.text_model.final_layer_norm.parameters(),\n",
        "        text_encoder.text_model.embeddings.position_embedding.parameters(),\n",
        "    )\n",
        "    freeze_params(params_to_freeze)\n",
        "    train_dataset = TextualInversionDataset(\n",
        "        data_root=save_path,\n",
        "        tokenizer=tokenizer,\n",
        "        size=512,\n",
        "        placeholder_token=placeholder_token,\n",
        "        repeats=100,\n",
        "        learnable_property=textualinversion_prefs['what_to_teach'], #Option selected above between object and style\n",
        "        center_crop=False,\n",
        "        set=\"train\",\n",
        "    )\n",
        "    def create_dataloader(train_batch_size=1):\n",
        "        return torch.utils.data.DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n",
        "    noise_scheduler = DDPMScheduler.from_pretrained(model_path, subfolder=\"scheduler\")\n",
        "    #noise_scheduler = DDPMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000, tensor_format=\"pt\")\n",
        "    def training_function(text_encoder, vae, unet):\n",
        "        logger = get_logger(__name__)\n",
        "\n",
        "        train_batch_size = textualinversion_prefs[\"train_batch_size\"]\n",
        "        gradient_accumulation_steps = textualinversion_prefs[\"gradient_accumulation_steps\"]\n",
        "        learning_rate = textualinversion_prefs[\"learning_rate\"]\n",
        "        max_train_steps = textualinversion_prefs[\"max_train_steps\"]\n",
        "        output_dir = textualinversion_prefs[\"output_dir\"]\n",
        "        accelerator = Accelerator(gradient_accumulation_steps=gradient_accumulation_steps)\n",
        "        train_dataloader = create_dataloader(train_batch_size)\n",
        "        if textualinversion_prefs[\"scale_lr\"]:\n",
        "            learning_rate = (learning_rate * gradient_accumulation_steps * train_batch_size * accelerator.num_processes)\n",
        "        # Initialize the optimizer\n",
        "        optimizer = torch.optim.AdamW(\n",
        "            text_encoder.get_input_embeddings().parameters(),  # only optimize the embeddings\n",
        "            lr=learning_rate,\n",
        "        )\n",
        "        text_encoder, optimizer, train_dataloader = accelerator.prepare(text_encoder, optimizer, train_dataloader)\n",
        "        vae.to(accelerator.device)\n",
        "        unet.to(accelerator.device)\n",
        "        vae.eval()\n",
        "        unet.eval()\n",
        "        # We need to recalculate our total training steps as the size of the training dataloader may have changed.\n",
        "        num_update_steps_per_epoch = math.ceil(len(train_dataloader) / gradient_accumulation_steps)\n",
        "        num_train_epochs = math.ceil(max_train_steps / num_update_steps_per_epoch)\n",
        "        clear_last()\n",
        "        # Train!\n",
        "        total_batch_size = train_batch_size * accelerator.num_processes * gradient_accumulation_steps\n",
        "\n",
        "        prt(\"***** Running training *****\")\n",
        "        prt(f\"  Num examples = {len(train_dataset)}\")\n",
        "        prt(f\"  Instantaneous batch size per device = {train_batch_size}\")\n",
        "        prt(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n",
        "        prt(f\"  Gradient Accumulation steps = {gradient_accumulation_steps}\")\n",
        "        prt(f\"  Total optimization steps = {max_train_steps}\")\n",
        "        progress = ProgressBar(bar_height=8)\n",
        "        prt(progress)\n",
        "        progress_bar = tqdm(range(max_train_steps), disable=not accelerator.is_local_main_process)\n",
        "        progress_bar.set_description(\"Steps\")\n",
        "        global_step = 0\n",
        "\n",
        "        for epoch in range(num_train_epochs):\n",
        "            text_encoder.train()\n",
        "            for step, batch in enumerate(train_dataloader):\n",
        "                with accelerator.accumulate(text_encoder):\n",
        "                    # Convert images to latent space\n",
        "                    latents = vae.encode(batch[\"pixel_values\"]).latent_dist.sample().detach()\n",
        "                    latents = latents * 0.18215\n",
        "\n",
        "                    # Sample noise that we'll add to the latents\n",
        "                    noise = torch.randn(latents.shape).to(latents.device)\n",
        "                    bsz = latents.shape[0]\n",
        "                    # Sample a random timestep for each image\n",
        "                    timesteps = torch.randint(0, noise_scheduler.num_train_timesteps, (bsz,), device=latents.device).long()\n",
        "\n",
        "                    # Add noise to the latents according to the noise magnitude at each timestep\n",
        "                    # (this is the forward diffusion process)\n",
        "                    noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
        "\n",
        "                    # Get the text embedding for conditioning\n",
        "                    encoder_hidden_states = text_encoder(batch[\"input_ids\"])[0]\n",
        "\n",
        "                    # Predict the noise residual\n",
        "                    noise_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
        "\n",
        "                    loss = F.mse_loss(noise_pred, noise, reduction=\"none\").mean([1, 2, 3]).mean()\n",
        "                    accelerator.backward(loss)\n",
        "\n",
        "                    # Zero out the gradients for all token embeddings except the newly added\n",
        "                    # embeddings for the concept, as we only want to optimize the concept embeddings\n",
        "                    if accelerator.num_processes > 1:\n",
        "                        grads = text_encoder.module.get_input_embeddings().weight.grad\n",
        "                    else:\n",
        "                        grads = text_encoder.get_input_embeddings().weight.grad\n",
        "                    # Get the index for tokens that we want to zero the grads for\n",
        "                    index_grads_to_zero = torch.arange(len(tokenizer)) != placeholder_token_id\n",
        "                    grads.data[index_grads_to_zero, :] = grads.data[index_grads_to_zero, :].fill_(0)\n",
        "\n",
        "                    optimizer.step()\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                # Checks if the accelerator has performed an optimization step behind the scenes\n",
        "                if accelerator.sync_gradients:\n",
        "                    progress_bar.update(1)\n",
        "                    global_step += 1\n",
        "                    progress.value = (global_step + 1) / max_train_steps\n",
        "                    progress.tooltip = f'[{(global_step + 1)} / {max_train_steps}]'\n",
        "                    progress.update()\n",
        "\n",
        "                logs = {\"loss\": loss.detach().item()}\n",
        "                progress_bar.set_postfix(**logs)\n",
        "\n",
        "                if global_step >= max_train_steps:\n",
        "                    break\n",
        "\n",
        "            accelerator.wait_for_everyone()\n",
        "\n",
        "\n",
        "        # Create the pipeline using using the trained modules and save it.\n",
        "        if accelerator.is_main_process:\n",
        "            pipeline = StableDiffusionPipeline(\n",
        "                text_encoder=accelerator.unwrap_model(text_encoder),\n",
        "                vae=vae,\n",
        "                unet=unet,\n",
        "                tokenizer=tokenizer,\n",
        "                scheduler=PNDMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", skip_prk_steps=True),\n",
        "                safety_checker=None if prefs['disable_nsfw_filter'] else StableDiffusionSafetyChecker.from_pretrained(\"CompVis/stable-diffusion-safety-checker\"),\n",
        "                feature_extractor=CLIPFeatureExtractor.from_pretrained(\"openai/clip-vit-base-patch32\"),\n",
        "            )\n",
        "            pipeline.save_pretrained(output_dir)\n",
        "            # Also save the newly trained embeddings\n",
        "            learned_embeds = accelerator.unwrap_model(text_encoder).get_input_embeddings().weight[placeholder_token_id]\n",
        "            learned_embeds_dict = {placeholder_token: learned_embeds.detach().cpu()}\n",
        "            torch.save(learned_embeds_dict, os.path.join(output_dir, \"learned_embeds.bin\"))\n",
        "    \n",
        "    import accelerate\n",
        "    try:\n",
        "        accelerate.notebook_launcher(training_function, args=(text_encoder, vae, unet))\n",
        "    except Exception as e:\n",
        "      clear_last()\n",
        "      alert_msg(page, \"ERROR: CUDA Ran Out of Memory. Try reducing parameters and try again...\", content=Text(str(e)))\n",
        "      with torch.no_grad():\n",
        "        torch.cuda.empty_cache()\n",
        "      return\n",
        "    clear_last()\n",
        "    #title Save your newly created concept to the [library of concepts](https://huggingface.co/sd-concepts-library)?\n",
        "    save_concept_to_public_library = textualinversion_prefs['save_concept']\n",
        "    name_of_your_concept = textualinversion_prefs['name_of_your_concept']\n",
        "    # `hf_token_write`: leave blank if you logged in with a token with `write access` in the [Initial Setup](#scrollTo=KbzZ9xe6dWwf). If not, [go to your tokens settings and create a write access token](https://huggingface.co/settings/tokens)\n",
        "    hf_token_write = prefs['HuggingFace_api_key']\n",
        "\n",
        "    if(save_concept_to_public_library):\n",
        "        from huggingface_hub import HfApi, HfFolder, CommitOperationAdd\n",
        "        from huggingface_hub import create_repo\n",
        "        api = HfApi()\n",
        "        your_username = api.whoami()[\"name\"]\n",
        "        repo_id = f\"sd-concepts-library/{format_filename(name_of_your_concept, use_dash=True)}\"\n",
        "        output_dir = textualinversion_prefs[\"output_dir\"]\n",
        "        if(not hf_token_write):\n",
        "            with open(HfFolder.path_token, 'r') as fin: hf_token = fin.read();\n",
        "        else:\n",
        "            hf_token = hf_token_write\n",
        "        if(textualinversion_prefs['where_to_save_concept'] == \"Public Library\"):\n",
        "            #Join the Concepts Library organization if you aren't part of it already\n",
        "            run_sp(f\"curl -X POST -H 'Authorization: Bearer '{hf_token} -H 'Content-Type: application/json' https://huggingface.co/organizations/sd-concepts-library/share/VcLXJtzwwxnHYCkNMLpSJCdnNFZHQwWywv\", realtime=False)\n",
        "            # curl -X POST -H 'Authorization: Bearer '$hf_token -H 'Content-Type: application/json' https://huggingface.co/organizations/sd-concepts-library/share/VcLXJtzwwxnHYCkNMLpSJCdnNFZHQwWywv\n",
        "        else:\n",
        "            repo_id = f\"{your_username}/{format_filename(name_of_your_concept, use_dash=True)}\"\n",
        "        images_upload = os.listdir(save_path)\n",
        "        image_string = \"\"\n",
        "        repo_id = f\"sd-concepts-library/{format_filename(name_of_your_concept, use_dash=True)}\"\n",
        "        for i, image in enumerate(images_upload):\n",
        "            image_string = f'''{image_string}![{placeholder_token} {i}](https://huggingface.co/{repo_id}/resolve/main/concept_images/{image})\n",
        "        '''\n",
        "        if(textualinversion_prefs['what_to_teach'] == \"style\"):\n",
        "            what_to_teach_article = f\"a `{textualinversion_prefs['what_to_teach']}`\"\n",
        "        else:\n",
        "            what_to_teach_article = f\"an `{textualinversion_prefs['what_to_teach']}`\"\n",
        "        description = textualinversion_prefs['readme_description']\n",
        "        if bool(description.strip()):\n",
        "            description = textualinversion_prefs['readme_description'] + '\\n\\n'\n",
        "        readme_text = f'''---\n",
        "license: mit\n",
        "---\n",
        "### {name_of_your_concept} by {your_username} using [Stable Diffusion Deluxe](https://colab.research.google.com/github/Skquark/AI-Friends/blob/main/Stable_Diffusion_Deluxe.ipynb)\n",
        "This is the `{placeholder_token}` concept taught to Stable Diffusion via Textual Inversion. You can load this concept into the [Stable Diffusion Deluxe](https://colab.research.google.com/github/Skquark/AI-Friends/blob/main/Stable_Diffusion_Deluxe.ipynb) notebook. You can also train your own concepts and load them into the concept libraries there too, or using [this notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_textual_inversion_training.ipynb).\n",
        "\n",
        "{description}Here is the new concept you will be able to use as {what_to_teach_article}:\n",
        "{image_string}\n",
        "'''\n",
        "        #Save the readme to a file\n",
        "        readme_file = open(\"README.md\", \"w\")\n",
        "        readme_file.write(readme_text)\n",
        "        readme_file.close()\n",
        "        #Save the token identifier to a file\n",
        "        text_file = open(\"token_identifier.txt\", \"w\")\n",
        "        text_file.write(placeholder_token)\n",
        "        text_file.close()\n",
        "        #Save the type of teached thing to a file\n",
        "        type_file = open(\"type_of_concept.txt\",\"w\")\n",
        "        type_file.write(textualinversion_prefs['what_to_teach'])\n",
        "        type_file.close()\n",
        "        operations = [\n",
        "            CommitOperationAdd(path_in_repo=\"learned_embeds.bin\", path_or_fileobj=f\"{output_dir}/learned_embeds.bin\"),\n",
        "            CommitOperationAdd(path_in_repo=\"token_identifier.txt\", path_or_fileobj=\"token_identifier.txt\"),\n",
        "            CommitOperationAdd(path_in_repo=\"type_of_concept.txt\", path_or_fileobj=\"type_of_concept.txt\"),\n",
        "            CommitOperationAdd(path_in_repo=\"README.md\", path_or_fileobj=\"README.md\"),\n",
        "        ]\n",
        "        try:\n",
        "          create_repo(repo_id, private=True, token=hf_token)\n",
        "        except Exception as e:\n",
        "          alert_msg(page, f\"ERROR Creating repo {repo_id}... Make sure your HF token has Write access.\", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip(), selectable=True)]))\n",
        "          return\n",
        "        api = HfApi()\n",
        "        api.create_commit(\n",
        "            repo_id=repo_id,\n",
        "            operations=operations,\n",
        "            commit_message=f\"Upload the concept {name_of_your_concept} embeds and token\",\n",
        "            token=hf_token\n",
        "        )\n",
        "        api.upload_folder(\n",
        "            folder_path=save_path,\n",
        "            path_in_repo=\"concept_images\",\n",
        "            repo_id=repo_id,\n",
        "            token=hf_token\n",
        "        )\n",
        "        prefs['custom_model'] = repo_id\n",
        "        prefs['custom_models'].append({'name': name_of_your_concept, 'path':repo_id})\n",
        "        page.custom_model.value = repo_id\n",
        "        try:\n",
        "          page.custom_model.update()\n",
        "        except Exception: pass\n",
        "        prt(Markdown(f\"## Your concept was saved successfully to _{repo_id}_.<br>[Click here to access it](https://huggingface.co/{repo_id}) and go to _Installers->Model Checkpoint->Custom Model Path_ to use. Include Token to your Prompt text.\", on_tap_link=lambda e: e.page.launch_url(e.data)))\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "imagenet_templates_small = [\n",
        "    \"a photo of a {}\",\n",
        "    \"a rendering of a {}\",\n",
        "    \"a cropped photo of the {}\",\n",
        "    \"the photo of a {}\",\n",
        "    \"a photo of a clean {}\",\n",
        "    \"a photo of a dirty {}\",\n",
        "    \"a dark photo of the {}\",\n",
        "    \"a photo of my {}\",\n",
        "    \"a photo of the cool {}\",\n",
        "    \"a close-up photo of a {}\",\n",
        "    \"a bright photo of the {}\",\n",
        "    \"a cropped photo of a {}\",\n",
        "    \"a photo of the {}\",\n",
        "    \"a good photo of the {}\",\n",
        "    \"a photo of one {}\",\n",
        "    \"a close-up photo of the {}\",\n",
        "    \"a rendition of the {}\",\n",
        "    \"a photo of the clean {}\",\n",
        "    \"a rendition of a {}\",\n",
        "    \"a photo of a nice {}\",\n",
        "    \"a good photo of a {}\",\n",
        "    \"a photo of the nice {}\",\n",
        "    \"a photo of the small {}\",\n",
        "    \"a photo of the weird {}\",\n",
        "    \"a photo of the large {}\",\n",
        "    \"a photo of a cool {}\",\n",
        "    \"a photo of a small {}\",\n",
        "]\n",
        "\n",
        "imagenet_style_templates_small = [\n",
        "    \"a painting in the style of {}\",\n",
        "    \"a rendering in the style of {}\",\n",
        "    \"a cropped painting in the style of {}\",\n",
        "    \"the painting in the style of {}\",\n",
        "    \"a clean painting in the style of {}\",\n",
        "    \"a dirty painting in the style of {}\",\n",
        "    \"a dark painting in the style of {}\",\n",
        "    \"a picture in the style of {}\",\n",
        "    \"a cool painting in the style of {}\",\n",
        "    \"a close-up painting in the style of {}\",\n",
        "    \"a bright painting in the style of {}\",\n",
        "    \"a cropped painting in the style of {}\",\n",
        "    \"a good painting in the style of {}\",\n",
        "    \"a close-up painting in the style of {}\",\n",
        "    \"a rendition in the style of {}\",\n",
        "    \"a nice painting in the style of {}\",\n",
        "    \"a small painting in the style of {}\",\n",
        "    \"a weird painting in the style of {}\",\n",
        "    \"a large painting in the style of {}\",\n",
        "]\n",
        "\n",
        "class TextualInversionDataset(Dataset):\n",
        "    import random as rnd\n",
        "    def __init__(\n",
        "        self,\n",
        "        data_root,\n",
        "        tokenizer,\n",
        "        learnable_property=\"object\",  # [object, style]\n",
        "        size=textualinversion_prefs['max_size'],\n",
        "        repeats=100,\n",
        "        interpolation=\"bicubic\",\n",
        "        flip_p=0.5,\n",
        "        set=\"train\",\n",
        "        placeholder_token=\"*\",\n",
        "        center_crop=False,\n",
        "    ):\n",
        "        self.data_root = data_root\n",
        "        self.tokenizer = tokenizer\n",
        "        self.learnable_property = learnable_property\n",
        "        self.size = size\n",
        "        self.placeholder_token = placeholder_token\n",
        "        self.center_crop = center_crop\n",
        "        self.flip_p = flip_p\n",
        "        self.image_paths = [os.path.join(self.data_root, file_path) for file_path in os.listdir(self.data_root)]\n",
        "        self.num_images = len(self.image_paths)\n",
        "        self._length = self.num_images\n",
        "        if set == \"train\":\n",
        "            self._length = self.num_images * repeats\n",
        "        self.interpolation = {\n",
        "            \"linear\": PIL.Image.LINEAR,\n",
        "            \"bilinear\": PIL.Image.BILINEAR,\n",
        "            \"bicubic\": PIL.Image.BICUBIC,\n",
        "            \"lanczos\": PIL.Image.LANCZOS,\n",
        "        }[interpolation]\n",
        "        self.templates = imagenet_style_templates_small if learnable_property == \"style\" else imagenet_templates_small\n",
        "        self.flip_transform = transforms.RandomHorizontalFlip(p=self.flip_p)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self._length\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        import random as rnd\n",
        "        example = {}\n",
        "        image = PILImage.open(self.image_paths[i % self.num_images])\n",
        "        if not image.mode == \"RGB\":\n",
        "            image = image.convert(\"RGB\")\n",
        "        placeholder_string = self.placeholder_token\n",
        "        text = rnd.choice(self.templates).format(placeholder_string)\n",
        "        example[\"input_ids\"] = self.tokenizer(\n",
        "            text,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.tokenizer.model_max_length,\n",
        "            return_tensors=\"pt\",\n",
        "        ).input_ids[0]\n",
        "        # default to score-sde preprocessing\n",
        "        img = np.array(image).astype(np.uint8)\n",
        "        if self.center_crop:\n",
        "            crop = min(img.shape[0], img.shape[1])\n",
        "            h, w, = (\n",
        "                img.shape[0],\n",
        "                img.shape[1],\n",
        "            )\n",
        "            img = img[(h - crop) // 2 : (h + crop) // 2, (w - crop) // 2 : (w + crop) // 2]\n",
        "        image = PILImage.fromarray(img)\n",
        "        image = image.resize((self.size, self.size), resample=self.interpolation)\n",
        "        image = self.flip_transform(image)\n",
        "        image = np.array(image).astype(np.uint8)\n",
        "        image = (image / 127.5 - 1.0).astype(np.float32)\n",
        "        example[\"pixel_values\"] = torch.from_numpy(image).permute(2, 0, 1)\n",
        "        return example\n",
        "\n",
        "\n",
        "def run_LoRA_dreambooth(page):\n",
        "    global LoRA_dreambooth_prefs, prefs\n",
        "    def prt(line):\n",
        "      if type(line) == str:\n",
        "        line = Text(line)\n",
        "      page.LoRA_dreambooth_output.controls.append(line)\n",
        "      page.LoRA_dreambooth_output.update()\n",
        "    def clear_last():\n",
        "      del page.LoRA_dreambooth_output.controls[-1]\n",
        "      page.LoRA_dreambooth_output.update()\n",
        "    if not status['installed_diffusers']:\n",
        "      alert_msg(page, \"You must Install the HuggingFace Diffusers Library first... \")\n",
        "      return\n",
        "    save_path = os.path.join(root_dir, \"my_model\")\n",
        "    error = False\n",
        "    if not os.path.exists(save_path):\n",
        "      error = True\n",
        "    elif len(os.listdir(save_path)) == 0:\n",
        "      error = True\n",
        "    if len(page.lora_dreambooth_file_list.controls) == 0:\n",
        "      error = True\n",
        "    if error:\n",
        "      alert_msg(page, \"Couldn't find a list of images to train model. Add image files to the list...\")\n",
        "      return\n",
        "    page.LoRA_dreambooth_output.controls.clear()\n",
        "    page.LoRA_dreambooth_output.update()\n",
        "    prt(Installing(\"Downloading LoRA DreamBooth Conceptualizers\"))\n",
        "    diffusers_dir = os.path.join(root_dir, \"diffusers\")\n",
        "    if not os.path.exists(diffusers_dir):\n",
        "      os.chdir(root_dir)\n",
        "      run_process(\"git clone https://github.com/Skquark/diffusers.git\", realtime=False, cwd=root_dir)\n",
        "    run_process('pip install git+https://github.com/Skquark/diffusers.git#egg=diffusers[training]', cwd=root_dir, realtime=False)\n",
        "    os.chdir(diffusers_dir)\n",
        "    run_sp('pip install -e \".[training]\"', cwd=diffusers_dir, realtime=False)\n",
        "    LoRA_dreambooth_dir = os.path.join(diffusers_dir, \"examples\", \"dreambooth\")\n",
        "    #LoRA_dreambooth_dir = os.path.join(diffusers_dir, \"examples\", \"text_to_image\")\n",
        "    os.chdir(LoRA_dreambooth_dir)\n",
        "    run_sp(\"pip install -r requirements.txt\", cwd=LoRA_dreambooth_dir, realtime=False)\n",
        "    run_process(\"pip install -qq bitsandbytes\", page=page)\n",
        "    run_sp(\"accelerate config default\", realtime=False)\n",
        "    #from accelerate.utils import write_basic_config\n",
        "    #write_basic_config()\n",
        "    import argparse\n",
        "    from io import BytesIO\n",
        "    #save_path = \"./my_model\"\n",
        "    #if not os.path.exists(save_path):\n",
        "    #  os.mkdir(save_path)\n",
        "\n",
        "    clear_pipes()\n",
        "    clear_last()\n",
        "    #num_new_images = None\n",
        "    random_seed = int(LoRA_dreambooth_prefs['seed']) if int(LoRA_dreambooth_prefs['seed']) > 0 else rnd.randint(0,4294967295)\n",
        "    name_of_your_model = LoRA_dreambooth_prefs['name_of_your_model']\n",
        "    from argparse import Namespace\n",
        "    LoRA_dreambooth_args = Namespace(\n",
        "        pretrained_model_name_or_path=model_path,\n",
        "        resolution=LoRA_dreambooth_prefs['resolution'],\n",
        "        center_crop=True,\n",
        "        #train_data_dir=save_path,\n",
        "        #caption_column=LoRA_dreambooth_prefs['instance_prompt'].strip(),\n",
        "        instance_data_dir=save_path,\n",
        "        instance_prompt=LoRA_dreambooth_prefs['instance_prompt'].strip(),\n",
        "        learning_rate=LoRA_dreambooth_prefs['learning_rate'],#5e-06,'\n",
        "        lr_scheduler=LoRA_dreambooth_prefs['lr_scheduler'],\n",
        "        lr_warmup_steps=LoRA_dreambooth_prefs['lr_warmup_steps'],\n",
        "        lr_num_cycles=LoRA_dreambooth_prefs['lr_num_cycles'],\n",
        "        lr_power=LoRA_dreambooth_prefs['lr_power'],\n",
        "        scale_lr=LoRA_dreambooth_prefs['scale_lr'],\n",
        "        max_train_steps=LoRA_dreambooth_prefs['max_train_steps'],#450,\n",
        "        train_batch_size=LoRA_dreambooth_prefs['train_batch_size'],\n",
        "        checkpointing_steps=LoRA_dreambooth_prefs['checkpointing_steps'],\n",
        "        gradient_accumulation_steps=LoRA_dreambooth_prefs['gradient_accumulation_steps'],\n",
        "        max_grad_norm=1.0,\n",
        "        #mixed_precision=\"no\", # set to \"fp16\" for mixed-precision training.\n",
        "        gradient_checkpointing=True, # set this to True to lower the memory usage.\n",
        "        use_8bit_adam=not prefs['higher_vram_mode'], # use 8bit optimizer from bitsandbytes\n",
        "        enable_xformers_memory_efficient_attention = status['installed_xformers'],\n",
        "        seed=random_seed,\n",
        "        with_prior_preservation=LoRA_dreambooth_prefs['prior_preservation'], \n",
        "        prior_loss_weight=LoRA_dreambooth_prefs['prior_loss_weight'],\n",
        "        sample_batch_size=LoRA_dreambooth_prefs['sample_batch_size'],\n",
        "        #class_data_dir=LoRA_dreambooth_prefs['class_data_dir'], \n",
        "        #class_prompt=LoRA_dreambooth_prefs['class_prompt'],\n",
        "        num_class_images=LoRA_dreambooth_prefs['num_class_images'],\n",
        "        output_dir=os.path.join(root_dir, \"LoRA-model\", format_filename(LoRA_dreambooth_prefs['name_of_your_model'], use_dash=True)),\n",
        "    )\n",
        "    output_dir = LoRA_dreambooth_args.output_dir\n",
        "    if not os.path.exists(os.path.join(root_dir, \"LoRA-model\")): os.makedirs(os.path.join(root_dir, \"LoRA-model\"))\n",
        "    arg_str = \"launch train_dreambooth_lora.py\"\n",
        "    #arg_str = 'accelerate --mixed_precision=\"fp16\" launch train_text_to_image_lora.py'\n",
        "    for k, v in vars(LoRA_dreambooth_args).items():\n",
        "      if isinstance(v, str):\n",
        "        if ' ' in v:\n",
        "          v = f'\"{v}\"'\n",
        "      if isinstance(v, bool):\n",
        "        if bool(v):\n",
        "          arg_str += f\" --{k}\"\n",
        "      else:\n",
        "        arg_str += f\" --{k}={v}\"\n",
        "    prt(Text(\"*** Running training ***\", weight=FontWeight.BOLD))\n",
        "    #if num_new_images != None: prt(f\"  Number of class images to sample: {num_new_images}.\")\n",
        "    #prt(f\"  Instantaneous batch size per device = {LoRA_dreambooth_args.train_batch_size}\")\n",
        "    #prt(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n",
        "    #prt(f\"  Gradient Accumulation steps = {LoRA_dreambooth_args.gradient_accumulation_steps}\")\n",
        "    #prt(f\"  Total optimization steps = {LoRA_dreambooth_args.max_train_steps}\")\n",
        "    prt(arg_str)\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    prt(progress)\n",
        "    if(LoRA_dreambooth_prefs['save_model']):\n",
        "      from huggingface_hub import HfApi, HfFolder, CommitOperationAdd\n",
        "      from huggingface_hub import Repository, create_repo, whoami\n",
        "      #from diffusers import StableDiffusionPipeline\n",
        "      api = HfApi()\n",
        "      your_username = api.whoami()[\"name\"]\n",
        "      #LoRA_dreambooth_pipe = StableDiffusionPipeline.from_pretrained(\n",
        "      #  LoRA_dreambooth_args.output_dir,\n",
        "      #  torch_dtype=torch.float16,\n",
        "      #).to(\"cuda\")\n",
        "      #os.makedirs(\"fp16_model\",exist_ok=True)\n",
        "      #LoRA_dreambooth_pipe.save_pretrained(\"fp16_model\")\n",
        "      hf_token = prefs['HuggingFace_api_key']\n",
        "      private = False if LoRA_dreambooth_prefs['where_to_save_model'] == \"Public HuggingFace\" else True\n",
        "      repo_id = f\"{your_username}/{format_filename(name_of_your_model, use_dash=True)}\"\n",
        "      output_dir = LoRA_dreambooth_args.output_dir\n",
        "      if(not prefs['HuggingFace_api_key']):\n",
        "        with open(HfFolder.path_token, 'r') as fin: hf_token = fin.read();\n",
        "      else:\n",
        "        hf_token = prefs['HuggingFace_api_key']\n",
        "      try:\n",
        "        create_repo(repo_id, private=private, exist_ok=True, token=hf_token)\n",
        "        repo = Repository(output_dir, clone_from=repo_id, token=hf_token)\n",
        "      except Exception as e:\n",
        "        alert_msg(page, f\"ERROR Creating repo {repo_id}... Make sure your HF token has Write access.\", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip(), selectable=True)]))\n",
        "        return\n",
        "    else:\n",
        "      if not os.path.exists(output_dir): os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    try:\n",
        "      #%cd $LoRA_dreambooth_dir # type: ignore\n",
        "      os.chdir(LoRA_dreambooth_dir)\n",
        "      #!accelerate $arg_str # type: ignore\n",
        "      os.system(\"accelerate\" + arg_str)\n",
        "      #run_sp(arg_str, cwd=LoRA_dreambooth_dir, realtime=True)\n",
        "      #run_sp(arg_str, cwd=LoRA_dreambooth_dir)\n",
        "    except Exception as e:\n",
        "      clear_last()\n",
        "      alert_msg(page, f\"ERROR: Out of Memory (or something else). Try reducing parameters and try again...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip())]))\n",
        "      with torch.no_grad():\n",
        "        torch.cuda.empty_cache()\n",
        "      return\n",
        "    clear_last()\n",
        "    if(LoRA_dreambooth_prefs['save_model']):\n",
        "      model_images = os.path.join(output_dir, 'model_images')\n",
        "      if not os.path.exists(model_images): os.makedirs(model_images, exist_ok=True)\n",
        "      images_upload = os.listdir(save_path)\n",
        "      image_string = \"\"\n",
        "      #repo_id = f\"sd-LoRA-library/{slugify(name_of_your_model)}\"\n",
        "      for i, image in enumerate(images_upload):\n",
        "          img_name = f\"image_{i}.png\"\n",
        "          shutil.copy(os.path.join(save_path, image), os.path.join(model_images, image))\n",
        "          #image.save(os.path.join(repo_folder, f\"image_{i}.png\"))\n",
        "          #img_str += f\"![img_{i}](./image_{i}.png)\\n\"\n",
        "          image_string = f'''{image_string}![img_{i}-{image}](https://huggingface.co/{repo_id}/resolve/main/model_images/{image})\n",
        "'''\n",
        "      description = LoRA_dreambooth_prefs['readme_description']\n",
        "      if bool(description.strip()):\n",
        "        description = LoRA_dreambooth_prefs['readme_description'] + '\\n\\n'\n",
        "      readme_text = f'''---\n",
        "license: mit\n",
        "---\n",
        "### {name_of_your_model} on Stable Diffusion via LoRA Dreambooth using [Stable Diffusion Deluxe](https://colab.research.google.com/github/Skquark/AI-Friends/blob/main/Stable_Diffusion_Deluxe.ipynb)\n",
        "#### model by {api.whoami()[\"name\"]}\n",
        "This is a model fine-tuned on the {model_path} model taught to Stable Diffusion with LoRA.\n",
        "It can be used by modifying the `instance_prompt`: **{LoRA_dreambooth_prefs['instance_prompt']}**\n",
        "\n",
        "{description}You can also train your own models and upload them to the library by using [this notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_dreambooth_training.ipynb).\n",
        "And you can run your new model via `diffusers`: [Colab Notebook for Inference](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_dreambooth_inference.ipynb), [Spaces with the Public Concepts loaded](https://huggingface.co/spaces/sd-dreambooth-library/stable-diffusion-dreambooth-models)\n",
        "\n",
        "Here are the images used for training this model:\n",
        "{image_string}\n",
        "'''\n",
        "      #Save the readme to a file\n",
        "      #readme_file = open(os.path.join(output_dir, \"README.md\"), \"w\")\n",
        "      #readme_file.write(readme_text)\n",
        "      #readme_file.close()\n",
        "      yaml = f\"\"\"\n",
        "---\n",
        "license: creativeml-openrail-m\n",
        "base_model: {model_path}\n",
        "tags:\n",
        "- stable-diffusion\n",
        "- stable-diffusion-diffusers\n",
        "- stable-diffusion-deluxe\n",
        "- text-to-image\n",
        "- diffusers\n",
        "inference: true\n",
        "---\n",
        "      \"\"\"\n",
        "      model_card = f\"\"\"\n",
        "# LoRA DreamBooth - {name_of_your_model}\n",
        "These are LoRA adaption weights for {name_of_your_model}. The weights were trained on {LoRA_dreambooth_args.instance_prompt} using [DreamBooth](https://dreambooth.github.io/).\\n\n",
        "### {repo_id} on Stable Diffusion via LoRA Dreambooth using [Stable Diffusion Deluxe](https://colab.research.google.com/github/Skquark/AI-Friends/blob/main/Stable_Diffusion_Deluxe.ipynb)\n",
        "#### Model by {api.whoami()[\"name\"]}\n",
        "\n",
        "{description}You can also train your own models and upload them to the library by using [Stable Diffusion Deluxe](https://colab.research.google.com/github/Skquark/AI-Friends/blob/main/Stable_Diffusion_Deluxe.ipynb) or [this notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_dreambooth_training.ipynb).\n",
        "\n",
        "Images used for training this model:\n",
        "{image_string}\n",
        "\"\"\"\n",
        "      readme_file = open(os.path.join(output_dir, \"README.md\"), \"w\")\n",
        "      readme_file.write(yaml + model_card)#(readme_text)\n",
        "      readme_file.close()\n",
        "      #with open(os.path.join(output_dir, \"README.md\"), \"w\") as f:\n",
        "      #    f.write(yaml + model_card)\n",
        "      #Save the token identifier to a file\n",
        "      #text_file = open(\"token_identifier.txt\", \"w\")\n",
        "      #text_file.write(LoRA_dreambooth_prefs['instance_prompt'])\n",
        "      #text_file.close()\n",
        "      #operations = [\n",
        "        #CommitOperationAdd(path_in_repo=\"token_identifier.txt\", path_or_fileobj=\"token_identifier.txt\"),\n",
        "        #CommitOperationAdd(path_in_repo=\"README.md\", path_or_fileobj=\"README.md\"),\n",
        "      #]\n",
        "      print(repo_id)\n",
        "      print(model_card)\n",
        "\n",
        "      \n",
        "      with open(os.path.join(output_dir, \".gitignore\"), \"w+\") as gitignore:\n",
        "        if \"step_*\" not in gitignore:\n",
        "            gitignore.write(\"step_*\\n\")\n",
        "        if \"epoch_*\" not in gitignore:\n",
        "            gitignore.write(\"epoch_*\\n\")\n",
        "      try:\n",
        "        #api.upload_folder(folder_path=output_dir, path_in_repo=\"\", repo_id=repo_id, token=hf_token)\n",
        "        #api.upload_folder(folder_path=save_path, path_in_repo=\"model_images\", repo_id=repo_id, token=hf_token)\n",
        "        #api.create_commit(repo_id=repo_id, operations=operations, commit_message=f\"Upload the model {name_of_your_model} embeds and token\",token=hf_token)\n",
        "        repo.push_to_hub(commit_message=f\"Upload the LoRA model {name_of_your_model} embeds and weights\", blocking=False, auto_lfs_prune=True)\n",
        "      except Exception as e:\n",
        "        alert_msg(page, f\"ERROR Pushing {name_of_your_model} Repository {repo_id}... Make sure your HF token has Write access.\", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip(), selectable=True)]))\n",
        "        return\n",
        "      #api.create_commit(repo_id=repo_id, operations=operations, commit_message=f\"Upload the model {name_of_your_model} embeds and token\",token=hf_token)\n",
        "      #api.upload_folder(folder_path=\"fp16_model\", path_in_repo=\"\", repo_id=repo_id,token=hf_token)\n",
        "      prefs['LoRA_dreambooth_model'] = name_of_your_model\n",
        "      prefs['custom_models'].append({'name': name_of_your_model, 'path':repo_id})\n",
        "      page.LoRA_dreambooth_model.options.insert(0, dropdown.Option(name_of_your_model))\n",
        "      page.LoRA_dreambooth_model.value = name_of_your_model\n",
        "      page.LoRA_dreambooth_model.update()\n",
        "      save_settings_file(page)\n",
        "      prt(Markdown(f\"## Your model was saved successfully to _{repo_id}_.\\n[Click here to access it](https://huggingface.co/{repo_id}). Use it in _Parameters->Use LaRA Model_ dropdown on top of any other Model loaded.\", on_tap_link=lambda e: e.page.launch_url(e.data)))\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "\n",
        "def run_LoRA(page):\n",
        "    global LoRA_prefs, prefs\n",
        "    def prt(line):\n",
        "      if type(line) == str:\n",
        "        line = Text(line)\n",
        "      page.LoRA_output.controls.append(line)\n",
        "      page.LoRA_output.update()\n",
        "    def clear_last():\n",
        "      del page.LoRA_output.controls[-1]\n",
        "      page.LoRA_output.update()\n",
        "    if not status['installed_diffusers']:\n",
        "      alert_msg(page, \"You must Install the HuggingFace Diffusers Library first... \")\n",
        "      return\n",
        "    save_path = os.path.join(root_dir, \"my_model\")\n",
        "    error = False\n",
        "    if not os.path.exists(save_path):\n",
        "      error = True\n",
        "    elif len(os.listdir(save_path)) == 0:\n",
        "      error = True\n",
        "    if len(page.lora_file_list.controls) == 0:\n",
        "      error = True\n",
        "    if error:\n",
        "      alert_msg(page, \"Couldn't find a list of images to train model. Add image files to the list...\")\n",
        "      return\n",
        "    page.LoRA_output.controls.clear()\n",
        "    page.LoRA_output.update()\n",
        "    prt(Installing(\"Downloading LoRA Conceptualizers\"))\n",
        "    diffusers_dir = os.path.join(root_dir, \"diffusers\")\n",
        "    if not os.path.exists(diffusers_dir):\n",
        "      os.chdir(root_dir)\n",
        "      run_process(\"git clone https://github.com/Skquark/diffusers.git\", realtime=False, cwd=root_dir)\n",
        "    run_process('pip install git+https://github.com/Skquark/diffusers.git#egg=diffusers[training]', cwd=root_dir, realtime=False)\n",
        "    os.chdir(diffusers_dir)\n",
        "    run_sp('pip install -e \".[training]\"', cwd=diffusers_dir, realtime=False)\n",
        "    #LoRA_dir = os.path.join(diffusers_dir, \"examples\", \"dreambooth\")\n",
        "    LoRA_dir = os.path.join(diffusers_dir, \"examples\", \"text_to_image\")\n",
        "    os.chdir(LoRA_dir)\n",
        "    run_sp(\"pip install -r requirements.txt\", cwd=LoRA_dir, realtime=False)\n",
        "    run_process(\"pip install -qq bitsandbytes\", page=page)\n",
        "    run_sp(\"accelerate config default\", realtime=False)\n",
        "    #from accelerate.utils import write_basic_config\n",
        "    #write_basic_config()\n",
        "    import argparse\n",
        "    from io import BytesIO\n",
        "    from huggingface_hub import HfApi, HfFolder, CommitOperationAdd\n",
        "    from huggingface_hub import Repository, create_repo, whoami\n",
        "    #from diffusers import StableDiffusionPipeline\n",
        "    api = HfApi()\n",
        "    your_username = api.whoami()[\"name\"]\n",
        "    hf_token = prefs['HuggingFace_api_key']\n",
        "    metadata_jsonl = []\n",
        "    for fl in page.lora_file_list.controls:\n",
        "        f = fl.title.value\n",
        "        fn = f.rpartition(slash)[2]\n",
        "        text = fl.subtitle.value\n",
        "        metadata_jsonl.append({'file_name':fn, 'text':text})\n",
        "    with open(os.path.join(save_path, \"metadata.jsonl\"), \"w\") as f:\n",
        "        for meta in metadata_jsonl:\n",
        "          print(json.dumps(meta), file=f)\n",
        "        #json.dump(metadata_jsonl, f, ensure_ascii=False, indent=4)\n",
        "    clear_pipes()\n",
        "    clear_last()\n",
        "    #num_new_images = None\n",
        "    random_seed = int(LoRA_prefs['seed']) if int(LoRA_prefs['seed']) > 0 else rnd.randint(0,4294967295)\n",
        "    name_of_your_model = LoRA_prefs['name_of_your_model']\n",
        "    repo_id = f\"{your_username}/{format_filename(name_of_your_model, use_dash=True)}\"\n",
        "    from argparse import Namespace\n",
        "    #--lr_num_cycles=1 --lr_power=1 --prior_loss_weight=1.0 --sample_batch_size=4 --num_class_images=100\n",
        "    LoRA_args = Namespace(\n",
        "        pretrained_model_name_or_path=model_path,\n",
        "        #dataset_name=repo_id,\n",
        "        train_data_dir=save_path,\n",
        "        resolution=LoRA_prefs['resolution'],\n",
        "        center_crop=True,\n",
        "        image_column=\"image\",\n",
        "        caption_column=\"text\",\n",
        "        #caption_column=LoRA_prefs['instance_prompt'].strip(),\n",
        "        #instance_data_dir=save_path,\n",
        "        validation_prompt=LoRA_prefs['validation_prompt'].strip(),\n",
        "        num_validation_images = LoRA_prefs['num_validation_images'],\n",
        "        validation_epochs=LoRA_prefs['validation_epochs'],\n",
        "        learning_rate=LoRA_prefs['learning_rate'],#5e-06,'\n",
        "        lr_scheduler=LoRA_prefs['lr_scheduler'],\n",
        "        lr_warmup_steps=LoRA_prefs['lr_warmup_steps'],\n",
        "        #lr_num_cycles=LoRA_prefs['lr_num_cycles'],\n",
        "        #lr_power=LoRA_prefs['lr_power'],\n",
        "        scale_lr=LoRA_prefs['scale_lr'],\n",
        "        max_train_steps=LoRA_prefs['max_train_steps'],#450,\n",
        "        train_batch_size=LoRA_prefs['train_batch_size'],\n",
        "        checkpointing_steps=LoRA_prefs['checkpointing_steps'],\n",
        "        gradient_accumulation_steps=LoRA_prefs['gradient_accumulation_steps'],\n",
        "        max_grad_norm=1.0,\n",
        "        mixed_precision=\"fp16\", # set to \"fp16\" for mixed-precision training.\n",
        "        gradient_checkpointing=LoRA_prefs['gradient_checkpointing'], # set this to True to lower the memory usage.\n",
        "        use_8bit_adam=not prefs['higher_vram_mode'], # use 8bit optimizer from bitsandbytes\n",
        "        enable_xformers_memory_efficient_attention = status['installed_xformers'],\n",
        "        seed=random_seed,\n",
        "        with_prior_preservation=LoRA_prefs['prior_preservation'], \n",
        "        #prior_loss_weight=LoRA_prefs['prior_loss_weight'],\n",
        "        #sample_batch_size=LoRA_prefs['sample_batch_size'],\n",
        "        #class_data_dir=LoRA_prefs['class_data_dir'], \n",
        "        #class_prompt=LoRA_prefs['class_prompt'],\n",
        "        #num_class_images=LoRA_prefs['num_class_images'],\n",
        "        cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,\n",
        "        hub_model_id=repo_id,\n",
        "        output_dir=os.path.join(root_dir, \"LoRA-model\", format_filename(LoRA_prefs['name_of_your_model'], use_dash=True)),\n",
        "    )\n",
        "    output_dir = LoRA_args.output_dir\n",
        "    if not os.path.exists(os.path.join(root_dir, \"LoRA-model\")): os.makedirs(os.path.join(root_dir, \"LoRA-model\"))\n",
        "    #arg_str = \"accelerate launch train_dreambooth_lora.py\"\n",
        "    arg_str = 'launch train_text_to_image_lora.py'\n",
        "    for k, v in vars(LoRA_args).items():\n",
        "      if isinstance(v, str):\n",
        "        if ' ' in v:\n",
        "          v = f'\"{v}\"'\n",
        "      if isinstance(v, bool) or v == None:\n",
        "        if bool(v):\n",
        "          arg_str += f\" --{k}\"\n",
        "      else:\n",
        "        arg_str += f\" --{k}={v}\"\n",
        "    prt(Text(\"*** Running training ***\", weight=FontWeight.BOLD))\n",
        "    #if num_new_images != None: prt(f\"  Number of class images to sample: {num_new_images}.\")\n",
        "    #prt(f\"  Instantaneous batch size per device = {LoRA_args.train_batch_size}\")\n",
        "    #prt(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n",
        "    #prt(f\"  Gradient Accumulation steps = {LoRA_args.gradient_accumulation_steps}\")\n",
        "    #prt(f\"  Total optimization steps = {LoRA_args.max_train_steps}\")\n",
        "    prt(arg_str)\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    prt(progress)\n",
        "    if(LoRA_prefs['save_model']):\n",
        "      \n",
        "      private = False if LoRA_prefs['where_to_save_model'] == \"Public HuggingFace\" else True\n",
        "      output_dir = LoRA_args.output_dir\n",
        "      if(not prefs['HuggingFace_api_key']):\n",
        "        with open(HfFolder.path_token, 'r') as fin: hf_token = fin.read();\n",
        "      else:\n",
        "        hf_token = prefs['HuggingFace_api_key']\n",
        "      try:\n",
        "        create_repo(repo_id, private=private, exist_ok=True, token=hf_token)\n",
        "        repo = Repository(output_dir, clone_from=repo_id, token=hf_token)\n",
        "      except Exception as e:\n",
        "        alert_msg(page, f\"ERROR Creating repo {repo_id}... Make sure your HF token has Write access.\", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip(), selectable=True)]))\n",
        "        return\n",
        "    else:\n",
        "      if not os.path.exists(output_dir): os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    try:\n",
        "      #run_sp(\"accelerate \" + arg_str, cwd=LoRA_dir, realtime=True)\n",
        "      #run_sp(arg_str, cwd=LoRA_dir)\n",
        "      #%cd $LoRA_dir # type: ignore\n",
        "      #!accelerate $arg_str # type: ignore\n",
        "      os.chdir(LoRA_dir)\n",
        "      os.system(\"accelerate\" + arg_str)\n",
        "    except Exception as e:\n",
        "      clear_last()\n",
        "      alert_msg(page, f\"ERROR: Out of Memory (or something else). Try reducing parameters and try again...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip())]))\n",
        "      with torch.no_grad():\n",
        "        torch.cuda.empty_cache()\n",
        "      return\n",
        "    clear_last()\n",
        "    if(LoRA_prefs['save_model']):\n",
        "      model_images = os.path.join(output_dir, 'model_images')\n",
        "      if not os.path.exists(model_images): os.makedirs(model_images, exist_ok=True)\n",
        "      images_upload = os.listdir(save_path)\n",
        "      image_string = \"\"\n",
        "      #repo_id = f\"sd-LoRA-library/{slugify(name_of_your_model)}\"\n",
        "      for i, image in enumerate(images_upload):\n",
        "          if image.endswith(\"jsonl\"): continue\n",
        "          img_name = f\"image_{i}.png\"\n",
        "          shutil.copy(os.path.join(save_path, image), os.path.join(model_images, image))\n",
        "          #image.save(os.path.join(repo_folder, f\"image_{i}.png\"))\n",
        "          #img_str += f\"![img_{i}](./image_{i}.png)\\n\"\n",
        "          image_string = f'''{image_string}![img_{i}-{image}](https://huggingface.co/{repo_id}/resolve/main/model_images/{image})\n",
        "'''\n",
        "      shutil.copy(os.path.join(save_path, \"metadata.jsonl\"), os.path.join(model_images, \"metadata.jsonl\"))\n",
        "      description = LoRA_prefs['readme_description']\n",
        "      if bool(description.strip()):\n",
        "        description = LoRA_prefs['readme_description'] + '\\n\\n'\n",
        "      readme_text = f'''---\n",
        "license: mit\n",
        "---\n",
        "### {name_of_your_model} on Stable Diffusion via LoRA Dreambooth using [Stable Diffusion Deluxe](https://colab.research.google.com/github/Skquark/AI-Friends/blob/main/Stable_Diffusion_Deluxe.ipynb)\n",
        "#### model by {api.whoami()[\"name\"]}\n",
        "This your the Stable Diffusion model fine-tuned the {name_of_your_model} model taught to Stable Diffusion with LoRA.\n",
        "It can be used by modifying the `validation_prompt`: **{LoRA_prefs['validation_prompt']}**\n",
        "\n",
        "{description}You can also train your own models and upload them to the library by using [this notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_dreambooth_training.ipynb).\n",
        "And you can run your new model via `diffusers`: [Colab Notebook for Inference](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_dreambooth_inference.ipynb), [Spaces with the Public Concepts loaded](https://huggingface.co/spaces/sd-dreambooth-library/stable-diffusion-dreambooth-models)\n",
        "\n",
        "Here are the images used for training this model:\n",
        "{image_string}\n",
        "'''\n",
        "      #Save the readme to a file\n",
        "      #readme_file = open(os.path.join(output_dir, \"README.md\"), \"w\")\n",
        "      #readme_file.write(readme_text)\n",
        "      #readme_file.close()\n",
        "      yaml = f\"\"\"\n",
        "---\n",
        "license: creativeml-openrail-m\n",
        "base_model: {model_path}\n",
        "tags:\n",
        "- stable-diffusion\n",
        "- stable-diffusion-diffusers\n",
        "- stable-diffusion-deluxe\n",
        "- text-to-image\n",
        "- diffusers\n",
        "- lora\n",
        "inference: true\n",
        "---\n",
        "      \"\"\"\n",
        "      model_card = f\"\"\"\n",
        "# LoRA Model - {name_of_your_model}\n",
        "These are LoRA adaption weights for {model_path}. The weights were validated with {LoRA_args.validation_prompt} using [DreamBooth](https://dreambooth.github.io/).\\n\n",
        "### {repo_id} on Stable Diffusion via LoRA using [Stable Diffusion Deluxe](https://colab.research.google.com/github/Skquark/AI-Friends/blob/main/Stable_Diffusion_Deluxe.ipynb)\n",
        "#### Model by {your_username}\n",
        "\n",
        "{description}You can also train your own models and upload them to the library by using [Stable Diffusion Deluxe](https://colab.research.google.com/github/Skquark/AI-Friends/blob/main/Stable_Diffusion_Deluxe.ipynb) or [this notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_dreambooth_training.ipynb).\n",
        "\n",
        "Images used for training this model:\n",
        "{image_string}\n",
        "\"\"\"\n",
        "      readme_file = open(os.path.join(output_dir, \"README.md\"), \"w\")\n",
        "      readme_file.write(yaml + model_card)#(readme_text)\n",
        "      readme_file.close()\n",
        "      #with open(os.path.join(output_dir, \"README.md\"), \"w\") as f:\n",
        "      #    f.write(yaml + model_card)\n",
        "      #Save the token identifier to a file\n",
        "      #text_file = open(\"token_identifier.txt\", \"w\")\n",
        "      #text_file.write(LoRA_prefs['instance_prompt'])\n",
        "      #text_file.close()\n",
        "      #operations = [\n",
        "        #CommitOperationAdd(path_in_repo=\"token_identifier.txt\", path_or_fileobj=\"token_identifier.txt\"),\n",
        "        #CommitOperationAdd(path_in_repo=\"README.md\", path_or_fileobj=\"README.md\"),\n",
        "      #]\n",
        "      print(repo_id)\n",
        "      print(model_card)\n",
        "\n",
        "      \n",
        "      with open(os.path.join(output_dir, \".gitignore\"), \"w+\") as gitignore:\n",
        "        if \"step_*\" not in gitignore:\n",
        "            gitignore.write(\"step_*\\n\")\n",
        "        if \"epoch_*\" not in gitignore:\n",
        "            gitignore.write(\"epoch_*\\n\")\n",
        "      try:\n",
        "        #api.upload_folder(folder_path=output_dir, path_in_repo=\"\", repo_id=repo_id, token=hf_token)\n",
        "        #api.upload_folder(folder_path=save_path, path_in_repo=\"model_images\", repo_id=repo_id, token=hf_token)\n",
        "        #api.create_commit(repo_id=repo_id, operations=operations, commit_message=f\"Upload the model {name_of_your_model} embeds and token\",token=hf_token)\n",
        "        repo.push_to_hub(commit_message=f\"Upload the LoRA model {name_of_your_model} embeds and weights\", blocking=False, auto_lfs_prune=True)\n",
        "      except Exception as e:\n",
        "        alert_msg(page, f\"ERROR Pushing {name_of_your_model} Repository {repo_id}... Make sure your HF token has Write access.\", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip(), selectable=True)]))\n",
        "        return\n",
        "      #api.create_commit(repo_id=repo_id, operations=operations, commit_message=f\"Upload the model {name_of_your_model} embeds and token\",token=hf_token)\n",
        "      #api.upload_folder(folder_path=\"fp16_model\", path_in_repo=\"\", repo_id=repo_id,token=hf_token)\n",
        "      prefs['LoRA_model'] = name_of_your_model\n",
        "      prefs['custom_LoRA_models'].append({'name': name_of_your_model, 'path':repo_id})\n",
        "      page.LoRA_model.options.insert(0, dropdown.Option(name_of_your_model))\n",
        "      page.LoRA_model.value = name_of_your_model\n",
        "      page.LoRA_model.update()\n",
        "      save_settings_file(page)\n",
        "      prt(Markdown(f\"## Your model was saved successfully to _{repo_id}_.\\n[Click here to access it](https://huggingface.co/{repo_id}). Use it in _Parameters->Use LaRA Model_ dropdown on top of any other Model loaded.\", on_tap_link=lambda e: e.page.launch_url(e.data)))\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "\n",
        "def run_converter(page):\n",
        "    global converter_prefs, prefs\n",
        "    #https://colab.research.google.com/github/camenduru/converter-colab/blob/main/converter_colab.ipynb\n",
        "    def prt(line):\n",
        "      if type(line) == str:\n",
        "        line = Text(line)\n",
        "      page.converter_output.controls.append(line)\n",
        "      page.converter_output.update()\n",
        "    def clear_last():\n",
        "      if len(page.converter_output.controls) == 0: return\n",
        "      del page.converter_output.controls[-1]\n",
        "      page.converter_output.update()\n",
        "    if not status['installed_diffusers']:\n",
        "      alert_msg(page, \"You must Install the HuggingFace Diffusers Library first... \")\n",
        "      return\n",
        "    model_name = converter_prefs['model_name']\n",
        "    model_path = converter_prefs['model_path']\n",
        "    if not bool(converter_prefs['model_name']):\n",
        "      alert(page, \"Provide a name to call the converted custom model\")\n",
        "      return\n",
        "    if not bool(model_path):\n",
        "      alert(page, \"Provide the path to the custom model to convert\")\n",
        "      return\n",
        "    model_file = format_filename(model_name, use_dash=True)\n",
        "    if converter_prefs['from_format'] == 'ckpt':\n",
        "      model_file += '.ckpt'\n",
        "    page.converter_output.controls.clear()\n",
        "    custom_models = os.path.join(root_dir, 'custom_models',)\n",
        "    custom_path = os.path.join(custom_models, format_filename(model_name, use_dash=True))\n",
        "    checkpoint_file = os.path.join(custom_models, model_file)\n",
        "    if not os.path.exists(custom_path):\n",
        "      os.makedirs(custom_path, exist_ok=True)\n",
        "    try:\n",
        "      import omegaconf\n",
        "    except Exception:\n",
        "      run_sp(\"pip install omegaconf\", realtime=False)\n",
        "      pass\n",
        "    diffusers_dir = os.path.join(root_dir, \"diffusers\")\n",
        "    if not os.path.exists(diffusers_dir):\n",
        "      run_process(\"git clone https://github.com/Skquark/diffusers.git\", realtime=False, cwd=root_dir)\n",
        "    run_process('pip install \"git+https://github.com/Skquark/diffusers.git#egg=diffusers[training]\"', cwd=root_dir, realtime=False)\n",
        "    scripts_dir = os.path.join(diffusers_dir, \"scripts\")\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    \n",
        "    if model_path.startswith('https://drive'):\n",
        "      gdown.download(model_path, checkpoint_file, quiet=True)\n",
        "    elif model_path.startswith('http'):\n",
        "      local = download_file(model_path)\n",
        "      print(f\"Download {model_path} local:{local}\")\n",
        "      shutil.move(local, checkpoint_file)\n",
        "    elif os.path.isfile(model_path):\n",
        "      shutil.copy(model_path, checkpoint_file)\n",
        "    elif os.path.isdir(model_path):\n",
        "      if os.path.exists(custom_path):\n",
        "        shutil.rmtree(custom_path)\n",
        "      shutil.copytree(model_path, custom_path, dirs_exist_ok=True)\n",
        "      checkpoint_file = custom_path\n",
        "    elif '/' in model_path and not model_path.startswith('/'):\n",
        "      checkpoint_file = model_path # hopefully Huggingface\n",
        "    else:\n",
        "      alert_msg(page, f\"Couldn't recognize source model file path {model_path}.\")\n",
        "      return\n",
        "    prt(Text(f'Converting {model_file} to {converter_prefs[\"to_format\"]}...', weight=FontWeight.BOLD))\n",
        "    prt(progress)\n",
        "    \n",
        "    if converter_prefs['from_format'] == \"lora_safetensors\":\n",
        "      run_cmd = f\"python3 {os.path.join(scripts_dir, 'convert_lora_safetensor_to_diffusers.py')}\"\n",
        "    else:\n",
        "      run_cmd = f\"python3 {os.path.join(scripts_dir, 'convert_original_stable_diffusion_to_diffusers.py')}\"\n",
        "    if converter_prefs['from_format'] == \"safetensors\":\n",
        "      run_cmd += f' --from_safetensors'\n",
        "    if converter_prefs['from_format'] == \"controlnet\":\n",
        "      run_cmd += f' --controlnet'\n",
        "    if converter_prefs['from_format'] == \"lora_safetensors\" and bool(converter_prefs['base_model']):\n",
        "      run_cmd += f' --base_model {converter_prefs[\"base_model\"]}'\n",
        "    if converter_prefs['from_format'] == \"ckpt\" or converter_prefs['from_format'] == \"safetensors\" or converter_prefs['from_format'] == \"lora_safetensors\":\n",
        "      run_cmd += f' --checkpoint_path {checkpoint_file}'\n",
        "    if converter_prefs['to_format'] == \"safetensors\":\n",
        "      run_cmd += f' --to_safetensors'\n",
        "    #if status['installed_xformers']:\n",
        "    if converter_prefs['model_type'] == \"SD v1.x text2image\" and converter_prefs['from_format'] != \"lora_safetensors\":\n",
        "      run_cmd += f' --image_size 512'\n",
        "    elif converter_prefs['model_type'] == \"SD v2.x text2image\" and converter_prefs['from_format'] != \"lora_safetensors\":\n",
        "      run_cmd += f' --image_size 768'\n",
        "      run_cmd += f' --upcast_attention'\n",
        "      run_cmd += f' --prediction_type v_prediction'\n",
        "    run_cmd += f' --scheduler_type {converter_prefs[\"scheduler_type\"]}'\n",
        "    if converter_prefs['half_percision']:\n",
        "      run_cmd += f' --half'\n",
        "    run_cmd += f' --dump_path {custom_path}'\n",
        "    prt(f\"Running {run_cmd}\")\n",
        "    try:\n",
        "      run_sp(run_cmd, cwd=scripts_dir, realtime=True)\n",
        "      #run_process(run_cmd, page=page, cwd=scripts_dir, show=True)\n",
        "    except Exception as e:\n",
        "      clear_last()\n",
        "      alert_msg(page, \"Error Running convert_original_stable_diffusion_to_diffusers\", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))\n",
        "      return\n",
        "    if len(os.listdir(custom_path)) == 0:\n",
        "      prt(f\"Problem converting your model. Check console and source checkpoint and try again...\")\n",
        "      os.rmdir(custom_path)\n",
        "      return\n",
        "    clear_last()\n",
        "    clear_last()\n",
        "    clear_last()\n",
        "    prt(f\"Done Converting... Saved locally at {custom_path}\")\n",
        "    if converter_prefs['load_custom_model']:\n",
        "      prefs['custom_model'] = custom_path\n",
        "      prefs['custom_models'].append({'name': model_name, 'path':custom_path})\n",
        "    if(converter_prefs['save_model']):\n",
        "      from huggingface_hub import HfApi, HfFolder, CommitOperationAdd\n",
        "      from huggingface_hub import model_info, create_repo, create_branch, upload_folder\n",
        "      from huggingface_hub.utils import RepositoryNotFoundError, revisionNotFoundError\n",
        "      from diffusers import StableDiffusionPipeline\n",
        "      api = HfApi()\n",
        "      your_username = api.whoami()[\"name\"]\n",
        "      '''dreambooth_pipe = StableDiffusionPipeline.from_pretrained(\n",
        "        custom_path,\n",
        "        torch_dtype=torch.float16,\n",
        "      ).to(\"cuda\")\n",
        "      os.makedirs(\"fp16_model\",exist_ok=True)\n",
        "      dreambooth_pipe.save_pretrained(\"fp16_model\")'''\n",
        "      hf_token = prefs['HuggingFace_api_key']\n",
        "      private = True\n",
        "      if(converter_prefs['where_to_save_model'] == \"Public Library\"):\n",
        "        private = False\n",
        "      #  repo_id = f\"sd-dreambooth-library/{slugify(model_name)}\"\n",
        "      if '/' in model_path and not model_path.startswith('/'):\n",
        "        repo_id = model_path\n",
        "      else:\n",
        "        repo_id = f\"{your_username}/{format_filename(model_name, use_dash=True)}\"\n",
        "      #output_dir = dreambooth_args.output_dir\n",
        "      if(not bool(prefs['HuggingFace_api_key'])):\n",
        "        with open(HfFolder.path_token, 'r') as fin: hf_token = fin.read();\n",
        "      else:\n",
        "        hf_token = prefs['HuggingFace_api_key'] \n",
        "      \n",
        "      description = converter_prefs['readme_description']\n",
        "      if bool(description.strip()):\n",
        "        description = converter_prefs['readme_description'] + '\\n\\n'\n",
        "      readme_text = f'''---\n",
        "license: mit\n",
        "---\n",
        "### {model_name} model on Stable Diffusion using [Stable Diffusion Deluxe](https://colab.research.google.com/github/Skquark/AI-Friends/blob/main/Stable_Diffusion_Deluxe.ipynb)\n",
        "#### model by {api.whoami()[\"name\"]}\n",
        "\n",
        "{description}\n",
        "You can also train your own models and upload them to the library by using [this notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_dreambooth_training.ipynb).\n",
        "And you can run your new concept via `diffusers`: [Colab Notebook for Inference](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_dreambooth_inference.ipynb), [Spaces with the Public Concepts loaded](https://huggingface.co/spaces/sd-dreambooth-library/stable-diffusion-dreambooth-concepts)\n",
        "\n",
        "    '''\n",
        "      #Save the readme to a file\n",
        "      readme_file = open(\"README.md\", \"w\")\n",
        "      readme_file.write(readme_text)\n",
        "      readme_file.close()\n",
        "      #Save the token identifier to a file\n",
        "      '''text_file = open(\"token_identifier.txt\", \"w\")\n",
        "      text_file.write(dreambooth_prefs['instance_prompt'])\n",
        "      text_file.close()'''\n",
        "      operations = [\n",
        "        #CommitOperationAdd(path_in_repo=\"token_identifier.txt\", path_or_fileobj=\"token_identifier.txt\"),\n",
        "        CommitOperationAdd(path_in_repo=\"README.md\", path_or_fileobj=\"README.md\"),\n",
        "      ]\n",
        "      print(repo_id)\n",
        "      print(readme_text)\n",
        "      try:\n",
        "          repo_exists = True\n",
        "          r_info = model_info(repo_id, token=hf_token)\n",
        "      except RepositoryNotFoundError:\n",
        "          repo_exists = False\n",
        "          pass\n",
        "      if not repo_exists:\n",
        "        try:\n",
        "          create_repo(repo_id, private=private, token=hf_token)\n",
        "        except Exception as e:\n",
        "          alert_msg(page, f\"ERROR Creating repo {repo_id}... Make sure your HF token has Write access.\", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip(), selectable=True)]))\n",
        "          return\n",
        "      branch = f\"{converter_prefs['from_format']}-to-{converter_prefs['to_format']}\"\n",
        "      try:\n",
        "          branch_exists = True\n",
        "          b_info = model_info(repo_id, revision=branch, token=hf_token)\n",
        "      except revisionNotFoundError:\n",
        "          branch_exists = False\n",
        "      finally:\n",
        "          if branch_exists:\n",
        "              print(b_info)\n",
        "          else:\n",
        "              create_branch(repo_id, branch=branch, token=hf_token)\n",
        "      api.create_commit(repo_id=repo_id, operations=operations, commit_message=f\"Upload the converted model {model_name} embeds\",token=hf_token)\n",
        "      api.upload_folder(folder_path=custom_path, path_in_repo=\"\", revision=branch, repo_id=repo_id, commit_message=f\"Upload the converted model {model_name} embeds\", token=hf_token)\n",
        "      #api.upload_folder(folder_path=\"fp16_model\", path_in_repo=\"\", repo_id=repo_id,token=hf_token)\n",
        "      #api.upload_folder(folder_path=save_path, path_in_repo=\"concept_images\", repo_id=repo_id, token=hf_token)\n",
        "      prefs['custom_model'] = repo_id\n",
        "      prefs['custom_models'].append({'name': model_name, 'path':repo_id})\n",
        "      page.custom_model.value = repo_id\n",
        "      try:\n",
        "        page.custom_model.update()\n",
        "      except Exception: pass\n",
        "      prt(Markdown(f\"## Your model was saved successfully to _{repo_id}_.<br>[Click here to access it](https://huggingface.co/{repo_id}) and go to _Installers->Model Checkpoint->Custom Model Path_ to use. Include Token in prompts.\", on_tap_link=lambda e: e.page.launch_url(e.data)))\n",
        "    \n",
        "    def push_ckpt(model_to, token, branch):\n",
        "      try:\n",
        "          repo_exists = True\n",
        "          r_info = model_info(model_to, token=token)\n",
        "      except RepositoryNotFoundError:\n",
        "          repo_exists = False\n",
        "      finally:\n",
        "          if repo_exists:\n",
        "              print(r_info)\n",
        "          else:\n",
        "              create_repo(model_to, private=True, token=token)\n",
        "      try:\n",
        "          branch_exists = True\n",
        "          b_info = model_info(model_to, revision=branch, token=token)\n",
        "      except revisionNotFoundError:\n",
        "          branch_exists = False\n",
        "      finally:\n",
        "          if branch_exists:\n",
        "              print(b_info)\n",
        "          else:\n",
        "              create_branch(model_to, branch=branch, token=token)    \n",
        "      upload_folder(folder_path=\"ckpt\", path_in_repo=\"\", revision=branch, repo_id=model_to, commit_message=f\"ckpt\", token=token)\n",
        "      return \"push ckpt done!\"\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "def run_checkpoint_merger(page):\n",
        "    global checkpoint_merger_prefs\n",
        "    def prt(line):\n",
        "      if type(line) == str:\n",
        "        line = Text(line)\n",
        "      page.checkpoint_merger_output.controls.append(line)\n",
        "      page.checkpoint_merger_output.update()\n",
        "    def clear_last():\n",
        "      del page.checkpoint_merger_output.controls[-1]\n",
        "      page.checkpoint_merger_output.update()\n",
        "    if not status['installed_diffusers']:\n",
        "        alert_msg(page, \"You must Install the HuggingFace Diffusers Library first... \")\n",
        "        return\n",
        "    if len(checkpoint_merger_prefs['pretrained_models']) < 2:\n",
        "        alert_msg(page, \"Select 2 or more compatible checkpoint models to the list before running...\")\n",
        "        return\n",
        "    prt(Installing(\"Downloading Required Models and Merging...\"))\n",
        "    try:\n",
        "        from diffusers import DiffusionPipeline\n",
        "        model_path = checkpoint_merger_prefs['pretrained_models'][0]\n",
        "        checkpoint_merger_pipe = DiffusionPipeline.from_pretrained(model_path, custom_pipeline=\"checkpoint_merger.py\", cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)\n",
        "        merged_pipe = checkpoint_merger_pipe.merge(checkpoint_merger_prefs['pretrained_models'], interp = checkpoint_merger_prefs['interp'] if checkpoint_merger_prefs['interp'] != \"weighted_sum\" else None, alpha = checkpoint_merger_prefs['alpha'], force = checkpoint_merger_prefs['force'], cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)\n",
        "        merged_pipe.to(torch_device)\n",
        "    except Exception as e:\n",
        "        clear_last()\n",
        "        alert_msg(page, f\"ERROR: Problem running Merger. Check parameters and try again...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip())]))\n",
        "        with torch.no_grad():\n",
        "            torch.cuda.empty_cache()\n",
        "        return\n",
        "    model_name = format_filename(checkpoint_merger_prefs['name_of_your_model'], force_underscores=True)\n",
        "    output_dir = os.path.join(root_dir, 'my_models', model_name)\n",
        "    repo_id = f\"{prefs['HuggingFace_username']}/{model_name}\"\n",
        "    if bool(checkpoint_merger_prefs['validation_prompt']):\n",
        "        prt(\"Generating Test Validation Image...\")\n",
        "        try:\n",
        "            image = merged_pipe(checkpoint_merger_prefs['validation_prompt']).images[0]\n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            alert_msg(page, f\"ERROR: Problem creating image with merged_pipe...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip())]))\n",
        "            with torch.no_grad():\n",
        "                torch.cuda.empty_cache()\n",
        "            return\n",
        "        fname = format_filename(checkpoint_merger_prefs['validation_prompt'])\n",
        "        fpath = available_file(stable_dir, fname, 0)\n",
        "        image.save(fpath)\n",
        "        clear_last()\n",
        "        prt(Img(src=fpath))\n",
        "    if checkpoint_merger_prefs['save_model']:\n",
        "        private = False if checkpoint_merger_prefs['where_to_save_model'] == \"Public HuggingFace\" else True\n",
        "        if(not prefs['HuggingFace_api_key']):\n",
        "            with open(HfFolder.path_token, 'r') as fin: hf_token = fin.read();\n",
        "        else:\n",
        "            hf_token = prefs['HuggingFace_api_key']\n",
        "        try:\n",
        "            create_repo(repo_id, private=private, exist_ok=True, token=hf_token)\n",
        "            repo = Repository(output_dir, clone_from=repo_id, token=hf_token)\n",
        "        except Exception as e:\n",
        "            alert_msg(page, f\"ERROR Creating repo {repo_id}... Make sure your HF token has Write access.\", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip(), selectable=True)]))\n",
        "            return\n",
        "    else:\n",
        "        if os.path.exists(output_dir):\n",
        "            os.makedirs(output_dir, exist_ok=True)\n",
        "    try:\n",
        "        merged_pipe.save_pretrained(output_dir)\n",
        "    except Exception as e:\n",
        "        clear_last()\n",
        "        alert_msg(page, f\"ERROR: Issue saving pretrained.  Check parameters and try again...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip())]))\n",
        "        with torch.no_grad():\n",
        "            torch.cuda.empty_cache()\n",
        "        return\n",
        "    del checkpoint_merger_pipe\n",
        "    del merged_pipe\n",
        "    if checkpoint_merger_prefs['save_model']:\n",
        "        description = checkpoint_merger_prefs['readme_description']\n",
        "        if bool(description.strip()):\n",
        "            description = checkpoint_merger_prefs['readme_description'] + '\\n\\n'\n",
        "        models_string = \"\"\n",
        "        for m in checkpoint_merger_prefs['pretrained_models']:\n",
        "            models_string += f\"* [{m}](https://huggingface.co/{m})\\n\"\n",
        "        yaml = f\"\"\"\n",
        "---\n",
        "license: creativeml-openrail-m\n",
        "base_model: {model_path}\n",
        "tags:\n",
        "- stable-diffusion\n",
        "- stable-diffusion-diffusers\n",
        "- stable-diffusion-deluxe\n",
        "- text-to-image\n",
        "- diffusers\n",
        "- merge\n",
        "inference: true\n",
        "---\n",
        "\"\"\"\n",
        "        model_card = f\"\"\"\n",
        "# Merged Checkpoint Model - {checkpoint_merger_prefs['name_of_your_model']}\n",
        "These are fine-tuned combined weights of {' + '.join(checkpoint_merger_prefs['pretrained_models'])}.\\n\n",
        "### {repo_id} on Stable Diffusion via Custom Checkpoint using [Stable Diffusion Deluxe](https://colab.research.google.com/github/Skquark/AI-Friends/blob/main/Stable_Diffusion_Deluxe.ipynb)\n",
        "#### Model by {prefs['HuggingFace_username']}\n",
        "{description}\n",
        "\n",
        "Checkpoints used for training this model:\n",
        "{models_string}\n",
        "Alpha Interpolation: {checkpoint_merger_prefs['alpha']}\n",
        "Interpolation Method: {checkpoint_merger_prefs['interp']}\n",
        "\"\"\"\n",
        "        readme_file = open(os.path.join(output_dir, \"README.md\"), \"w\")\n",
        "        readme_file.write(yaml + model_card)#(readme_text)\n",
        "        readme_file.close()\n",
        "        print(repo_id)\n",
        "        print(model_card)        \n",
        "        with open(os.path.join(output_dir, \".gitignore\"), \"w+\") as gitignore:\n",
        "            if \"step_*\" not in gitignore:\n",
        "                gitignore.write(\"step_*\\n\")\n",
        "            if \"epoch_*\" not in gitignore:\n",
        "                gitignore.write(\"epoch_*\\n\")\n",
        "        try:\n",
        "            #api.upload_folder(folder_path=output_dir, path_in_repo=\"\", repo_id=repo_id, token=hf_token)\n",
        "            #api.upload_folder(folder_path=save_path, path_in_repo=\"model_images\", repo_id=repo_id, token=hf_token)\n",
        "            #api.create_commit(repo_id=repo_id, operations=operations, commit_message=f\"Upload the model {name_of_your_model} embeds and token\",token=hf_token)\n",
        "            repo.push_to_hub(commit_message=f\"Upload the Merged model {model_name} embeds and weights\", blocking=False, auto_lfs_prune=True)\n",
        "        except Exception as e:\n",
        "            alert_msg(page, f\"ERROR Pushing {model_name} Repository {repo_id}... Make sure your HF token has Write access.\", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip(), selectable=True)]))\n",
        "            return\n",
        "        prt(Markdown(f\"## Your model was saved successfully to _{repo_id}_.\\n[Click here to access it](https://huggingface.co/{repo_id}). Use it in _Installers->Diffusers Custom Model_ dropdown.\", on_tap_link=lambda e: e.page.launch_url(e.data)))\n",
        "    \n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "def run_tortoise_tts(page):\n",
        "    #https://github.com/neonbjb/tortoise-tts\n",
        "    global tortoise_prefs, pipe_tortoise_tts, prefs\n",
        "    def prt(line):\n",
        "      if type(line) == str:\n",
        "        line = Text(line)\n",
        "      page.tortoise_output.controls.append(line)\n",
        "      page.tortoise_output.update()\n",
        "    def clear_last():\n",
        "      del page.tortoise_output.controls[-1]\n",
        "      page.tortoise_output.update()\n",
        "    def play_audio(e):\n",
        "      e.control.data.play()\n",
        "    if tortoise_prefs['train_custom'] and not bool(tortoise_prefs['custom_voice_name']):\n",
        "      alert_msg(page, \"Provide a Custom Voice Name when training your audio files.\")\n",
        "      return\n",
        "    if not bool(tortoise_prefs['text']):\n",
        "      alert_msg(page, \"Provide Text for the AI voice to read...\")\n",
        "      return\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    state_text = Text(\" Downloading Tortoise-TTS Packages...\", weight=FontWeight.BOLD)\n",
        "    prt(Row([ProgressRing(), state_text]))\n",
        "    tortoise_dir = os.path.join(root_dir, \"tortoise-tts\")\n",
        "    voice_dir = os.path.join(tortoise_dir, 'tortoise', 'voices')\n",
        "    if not os.path.isdir(tortoise_dir):\n",
        "      os.chdir(root_dir)\n",
        "      run_process(\"git clone https://github.com/jnordberg/tortoise-tts.git\", page=page)\n",
        "    os.chdir(tortoise_dir)\n",
        "    try:\n",
        "      import pydub\n",
        "    except Exception:\n",
        "      run_process(\"pip install -q ffmpeg\", page=page)\n",
        "      run_process(\"pip install -q pydub\", page=page)\n",
        "      import pydub\n",
        "      pass\n",
        "    try:\n",
        "      from tortoise.api import TextToSpeech\n",
        "    except Exception:\n",
        "      try:\n",
        "        run_process(\"pip3 install -r requirements.txt\", page=page, cwd=tortoise_dir)\n",
        "        run_process(\"python3 setup.py install\", page=page, cwd=tortoise_dir)\n",
        "      except Exception as e:\n",
        "        clear_last()\n",
        "        alert_msg(page, \"Error Installing Tortoise TextToSpeech requirements\", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip())]))\n",
        "        return\n",
        "      pass\n",
        "    import torch\n",
        "    import torchaudio\n",
        "    import torch.nn as nn\n",
        "    import torch.nn.functional as F\n",
        "    from tortoise.api import TextToSpeech\n",
        "    from tortoise.utils.audio import load_audio, load_voice, load_voices\n",
        "    clear_pipes('tortoise_tts')\n",
        "    # This will download all the models used by Tortoise from the HuggingFace hub.\n",
        "    if pipe_tortoise_tts == None:\n",
        "      try:\n",
        "        pipe_tortoise_tts = TextToSpeech()\n",
        "      except Exception as e:\n",
        "        clear_last()\n",
        "        alert_msg(page, \"Error downloading Tortoise TextToSpeech package\", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))\n",
        "        return\n",
        "    clear_last()\n",
        "    prt(Text(\"  Generating Tortoise Text-to-Speech... (slow, but wins the race)\", weight=FontWeight.BOLD))\n",
        "    prt(progress)\n",
        "    save_dir = os.path.join(root_dir, 'audio_out', tortoise_prefs['batch_folder_name'])\n",
        "    if not os.path.exists(save_dir):\n",
        "      os.makedirs(save_dir, exist_ok=True)\n",
        "    audio_out = os.path.join(prefs['image_output'].rpartition(slash)[0], 'audio_out')\n",
        "    custom_voices = os.path.join(audio_out, 'custom_voices')\n",
        "    if bool(tortoise_prefs['batch_folder_name']):\n",
        "      audio_out = os.path.join(audio_out, tortoise_prefs['batch_folder_name'])\n",
        "    os.makedirs(audio_out, exist_ok=True)\n",
        "    #voice_dirs = os.listdir(os.path.join(root_dir, \"tortoise-tts\", 'tortoise', 'voices'))\n",
        "    #print(str(voice_dirs))\n",
        "    fname = format_filename(tortoise_prefs['text'])\n",
        "    if fname[-1] == '.': fname = fname[:-1]\n",
        "    file_prefix = tortoise_prefs['file_prefix']\n",
        "    tortoise_custom_voices = prefs['tortoise_custom_voices']\n",
        "    tortoise_prefs['voice'] = []\n",
        "    if tortoise_prefs['train_custom']:\n",
        "        if len(tortoise_prefs['custom_wavs']) <2:\n",
        "          alert_msg(page, \"To train a custom voice, provide at least 2 audio files to mimic.\")\n",
        "          return\n",
        "        CUSTOM_VOICE_NAME = format_filename(tortoise_prefs['custom_voice_name'])\n",
        "        custom_voice_folder = os.path.join(root_dir, \"tortoise-tts\", 'tortoise', 'voices', CUSTOM_VOICE_NAME)\n",
        "        os.makedirs(custom_voice_folder, exist_ok=True)\n",
        "        for i, f in enumerate(tortoise_prefs['custom_wavs']):\n",
        "            if f.tolower().endswith('mp3'):\n",
        "              sound = pydub.AudioSpegment.from_mp3(f)\n",
        "              sound.export(os.path.join(custom_voice_folder, f'{i+1}.wav'), format=\"wav\", bitrate=\"22050\")\n",
        "            elif f.tolower().endswith('wav'):\n",
        "              sound = pydub.AudioSpegment.from_wav(f)\n",
        "              sound.export(os.path.join(custom_voice_folder, f'{i+1}.wav'), format=\"wav\", bitrate=\"22050\")\n",
        "            else:\n",
        "              alert_msg(f\"Unknown file type {f.rpartition('.')[2]}... Use only .wav and .mp3 audio clips.\")\n",
        "              return\n",
        "              #shutil.copy(f, os.path.join(custom_voice_folder, f'{i+1}.wav'))\n",
        "        #for i, file_data in enumerate(files.upload().values()):\n",
        "        #    with open(os.path.join(custom_voice_folder, f'{i}.wav'), 'wb') as f:\n",
        "        #        f.write(file_data)\n",
        "        if not CUSTOM_VOICE_NAME in tortoise_prefs['voice']:\n",
        "          #tortoise_prefs['voice'].append(CUSTOM_VOICE_NAME)\n",
        "          page.tortoise_voices.controls.append(Checkbox(label=CUSTOM_VOICE_NAME, value=True, fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, col={'xs':12, 'sm':6, 'md':3, 'lg':3, 'xl': 2}))\n",
        "          page.tortoise_voices.update()\n",
        "          output_voice_folder = os.path.join(custom_voices, CUSTOM_VOICE_NAME)\n",
        "          if os.path.exists(output_voice_folder):\n",
        "            shutil.rmtree(output_voice_folder)\n",
        "            print(f'Output Voice Folder already existed: {output_voice_folder}... Deleting to remake.')\n",
        "          #os.makedirs(output_voice_folder, exist_ok=False)\n",
        "          shutil.copytree(custom_voice_folder, output_voice_folder)\n",
        "          prefs['tortoise_custom_voices'].append({'name':CUSTOM_VOICE_NAME, 'folder': output_voice_folder})\n",
        "          save_settings_file(page)\n",
        "    for v in page.tortoise_voices.controls:\n",
        "        if v.value == True:\n",
        "          tortoise_prefs['voice'].append(v.label)\n",
        "          if not os.path.exists(os.path.join(voice_dir, v.label)):\n",
        "            for custom in tortoise_custom_voices:\n",
        "              if custom['name'] == v.label:\n",
        "                if os.path.exists(custom['folder']):\n",
        "                  #os.makedirs(os.path.join(voice_dir, custom['name'], exist_ok=True))\n",
        "                  shutil.copytree(custom['folder'], os.path.join(voice_dir, custom['name']))\n",
        "                else:\n",
        "                  print(f\"Couldn't find custom folder {custom['folder']}\")\n",
        "    # Load it and send it through Tortoise.\n",
        "    voice = tortoise_prefs['voice']\n",
        "    v_str = voice if isinstance(voice, str) else '+'.join(voice) if isinstance(voice, list) else ''\n",
        "    if len(voice) == 0: v_str = 'random'\n",
        "    audio_name = f'{file_prefix}{v_str}-{fname}'\n",
        "    audio_name = audio_name[:int(prefs['file_max_length'])]\n",
        "    fname = available_file(save_dir, audio_name, 0, ext=\"wav\")\n",
        "    #print(str(voice))\n",
        "    #print(fname)\n",
        "    if len(voice) == 0:\n",
        "        voice_samples = conditioning_latents = None\n",
        "    elif len(voice) == 1:\n",
        "        voice_samples, conditioning_latents = load_voice(voice[0])\n",
        "    else:\n",
        "        voice_samples, conditioning_latents = load_voices(voice)\n",
        "    gen = pipe_tortoise_tts.tts_with_preset(tortoise_prefs['text'], voice_samples=voice_samples, conditioning_latents=conditioning_latents, preset=tortoise_prefs['preset'])\n",
        "    torchaudio.save(fname, gen.squeeze(0).cpu(), 24000)\n",
        "    #IPython.display.Audio('generated.wav')\n",
        "    clear_last()\n",
        "    clear_last()\n",
        "    a_out = Audio(src=fname, autoplay=False)\n",
        "    page.overlay.append(a_out)\n",
        "    page.update()\n",
        "    display_name = fname\n",
        "    #a.tofile(f\"/content/dance-{i}.wav\")\n",
        "    if storage_type == \"Colab Google Drive\":\n",
        "      audio_save = available_file(audio_out, audio_name, 0, ext='wav')\n",
        "      shutil.copy(fname, audio_save)\n",
        "      display_name = audio_save\n",
        "    prt(Row([IconButton(icon=icons.PLAY_CIRCLE_FILLED, icon_size=48, on_click=play_audio, data=a_out), Text(display_name)]))\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "  \n",
        "def run_audio_ldm(page):\n",
        "    global audioLDM_prefs, pipe_audio_ldm, prefs\n",
        "    def prt(line):\n",
        "      if type(line) == str:\n",
        "        line = Text(line)\n",
        "      page.audioLDM_output.controls.append(line)\n",
        "      page.audioLDM_output.update()\n",
        "    def clear_last():\n",
        "      if len(page.audioLDM_output.controls) < 1: return\n",
        "      del page.audioLDM_output.controls[-1]\n",
        "      page.audioLDM_output.update()\n",
        "    def play_audio(e):\n",
        "      e.control.data.play()\n",
        "    if not bool(audioLDM_prefs['text']):\n",
        "      alert_msg(page, \"Provide Text for the AI to create the sound of...\")\n",
        "      return\n",
        "    if not status['installed_diffusers']:\n",
        "      alert_msg(page, \"You must Install the HuggingFace Diffusers Library first... \")\n",
        "      return\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    state_text = Text(\" Downloading Audio LDM Packages...\", weight=FontWeight.BOLD)\n",
        "    prt(Row([ProgressRing(), state_text]))\n",
        "    audioLDM_dir = os.path.join(root_dir, \"audioldm-text-to-audio-generation\")\n",
        "    #voice_dir = os.path.join(audioLDM_dir, 'audioldm', 'voices')\n",
        "    if not os.path.isdir(audioLDM_dir):\n",
        "      os.chdir(root_dir)\n",
        "      run_process(\"git clone https://huggingface.co/spaces/haoheliu/audioldm-text-to-audio-generation\", page=page)\n",
        "    os.chdir(audioLDM_dir)\n",
        "    import sys\n",
        "    sys.path.append(os.path.join(audioLDM_dir, 'audioldm'))\n",
        "    try:\n",
        "        from audioldm import text_to_audio, build_model\n",
        "    except Exception:\n",
        "        try:\n",
        "            run_process(\"pip install einops\", page=page)\n",
        "            run_process(\"pip install -q pyyaml\", page=page)\n",
        "            run_process(\"pip install -q soundfile\", page=page)\n",
        "            run_process(\"pip install -q librosa\", page=page)\n",
        "            run_process(\"pip install -q pandas\", page=page)\n",
        "            run_process(\"pip install -q gradio\", page=page)\n",
        "            run_process(\"pip install -q torchlibrosa\", page=page)\n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            alert_msg(page, \"Error Installing AudioLDM requirements\", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip())]))\n",
        "            return\n",
        "        pass\n",
        "    finally:\n",
        "        from audioldm import text_to_audio, build_model\n",
        "    import soundfile as sf\n",
        "    model_id=\"cvssp/audioldm-s-full-v2\"\n",
        "    clear_pipes('audio_ldm')\n",
        "    # This will download all the models used by Audio LDM from the HuggingFace hub.\n",
        "    if pipe_audio_ldm == None:\n",
        "      try:\n",
        "        pipe_audio_ldm = build_model()\n",
        "      except Exception as e:\n",
        "        clear_last()\n",
        "        alert_msg(page, \"Error downloading Audio LDM package\", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))\n",
        "        return\n",
        "    clear_last()\n",
        "    prt(Text(\"  Generating AudioLDM Sounds...\", weight=FontWeight.BOLD))\n",
        "    prt(progress)\n",
        "    random_seed = int(audioLDM_prefs['seed']) if int(audioLDM_prefs['seed']) > 0 else rnd.randint(0,4294967295)\n",
        "    try:\n",
        "      waveform = text_to_audio(pipe_audio_ldm, audioLDM_prefs['text'], random_seed, duration=audioLDM_prefs['duration'], guidance_scale=audioLDM_prefs['guidance_scale'], n_candidate_gen_per_text=int(audioLDM_prefs['n_candidates']))\n",
        "    except Exception as e:\n",
        "      clear_last()\n",
        "      alert_msg(page, \"Error generating text_to_audio waveform...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))\n",
        "      return\n",
        "    save_dir = os.path.join(root_dir, 'audio_out', audioLDM_prefs['batch_folder_name'])\n",
        "    if not os.path.exists(save_dir):\n",
        "      os.makedirs(save_dir, exist_ok=True)\n",
        "    audio_out = os.path.join(prefs['image_output'].rpartition(slash)[0], 'audio_out')\n",
        "    if bool(audioLDM_prefs['batch_folder_name']):\n",
        "      audio_out = os.path.join(audio_out, audioLDM_prefs['batch_folder_name'])\n",
        "    os.makedirs(audio_out, exist_ok=True)\n",
        "    #voice_dirs = os.listdir(os.path.join(root_dir, \"audioldm-tts\", 'audioldm', 'voices'))\n",
        "    #print(str(voice_dirs))\n",
        "    fname = format_filename(audioLDM_prefs['text'])\n",
        "    if fname[-1] == '.': fname = fname[:-1]\n",
        "    file_prefix = audioLDM_prefs['file_prefix']\n",
        "    audio_name = f'{file_prefix}-{fname}'\n",
        "    audio_name = audio_name[:int(prefs['file_max_length'])]\n",
        "    fname = available_file(save_dir, audio_name, 0, ext=\"wav\")\n",
        "    for i in range(waveform.shape[0]):\n",
        "        sf.write(fname, waveform[i, 0], samplerate=16000)\n",
        "    #torchaudio.save(fname, gen.squeeze(0).cpu(), 24000)\n",
        "    #IPython.display.Audio('generated.wav')\n",
        "    clear_last()\n",
        "    clear_last()\n",
        "    a_out = Audio(src=fname, autoplay=False)\n",
        "    page.overlay.append(a_out)\n",
        "    page.update()\n",
        "    display_name = fname\n",
        "    #a.tofile(f\"/content/dance-{i}.wav\")\n",
        "    if storage_type == \"Colab Google Drive\":\n",
        "      audio_save = available_file(audio_out, audio_name, 0, ext='wav')\n",
        "      shutil.copy(fname, audio_save)\n",
        "      display_name = audio_save\n",
        "    prt(Row([IconButton(icon=icons.PLAY_CIRCLE_FILLED, icon_size=48, on_click=play_audio, data=a_out), Text(display_name)]))\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "def run_bark(page):\n",
        "    global bark_prefs, pipe_bark, prefs\n",
        "    def prt(line):\n",
        "      if type(line) == str:\n",
        "        line = Text(line)\n",
        "      page.bark_output.controls.append(line)\n",
        "      page.bark_output.update()\n",
        "    def clear_last():\n",
        "      if len(page.bark_output.controls) < 1: return\n",
        "      del page.bark_output.controls[-1]\n",
        "      page.bark_output.update()\n",
        "    def play_audio(e):\n",
        "      e.control.data.play()\n",
        "    if not bool(bark_prefs['text']):\n",
        "      alert_msg(page, \"Provide Text for the AI to create the sound of...\")\n",
        "      return\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    prt(Installing(\"Downloading Bark Packages...\"))\n",
        "    try:\n",
        "        import scipy\n",
        "    except ModuleNotFoundError:\n",
        "        run_process(\"pip install -qq --upgrade scipy\", page=page)\n",
        "        pass\n",
        "    from scipy.io.wavfile import write as write_wav\n",
        "    import sys\n",
        "    sys.path.append(os.path.join(root_dir, 'audioldm'))\n",
        "    try:\n",
        "        from bark import SAMPLE_RATE, generate_audio, preload_models\n",
        "        if force_updates: raise ImportError(\"Forcing update\")\n",
        "    except Exception:\n",
        "        try:\n",
        "            run_process(\"pip install git+https://github.com/suno-ai/bark.git\", page=page)\n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            alert_msg(page, \"Error Installing Bark requirements\", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip())]))\n",
        "            return\n",
        "        pass\n",
        "    finally:\n",
        "        from bark import SAMPLE_RATE, generate_audio, preload_models\n",
        "    import soundfile as sf\n",
        "    clear_pipes()\n",
        "    preload_models()\n",
        "    clear_last()\n",
        "    audio_out = os.path.join(prefs['image_output'].rpartition(slash)[0], 'audio_out')\n",
        "    if bool(bark_prefs['batch_folder_name']):\n",
        "      audio_out = os.path.join(audio_out, bark_prefs['batch_folder_name'])\n",
        "    os.makedirs(audio_out, exist_ok=True)\n",
        "    history_prompt = bark_prefs['acoustic_prompt']\n",
        "    if history_prompt == \"Unconditional\":\n",
        "        history_prompt = None\n",
        "    if history_prompt == \"Anouncer\":\n",
        "        history_prompt = \"announcer\"\n",
        "    for i in range(bark_prefs['n_iterations']):\n",
        "        prt(Text(\"  Generating Bark Audio...\", weight=FontWeight.BOLD))\n",
        "        prt(progress)\n",
        "        #random_seed = int(bark_prefs['seed']) if int(bark_prefs['seed']) > 0 else rnd.randint(0,4294967295)\n",
        "        try:\n",
        "            audio_array = generate_audio(bark_prefs['text'], history_prompt=history_prompt, text_temp=bark_prefs['text_temp'], waveform_temp=bark_prefs['waveform_temp'])\n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            alert_msg(page, \"Error generating Bark waveform...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))\n",
        "            return\n",
        "        fname = format_filename(bark_prefs['text'])\n",
        "        if fname[-1] == '.': fname = fname[:-1]\n",
        "        file_prefix = bark_prefs['file_prefix']\n",
        "        audio_name = f'{file_prefix}-{fname}'\n",
        "        audio_name = audio_name[:int(prefs['file_max_length'])]\n",
        "        fname = available_file(audio_out, audio_name, i, ext=\"wav\")\n",
        "        write_wav(fname, SAMPLE_RATE, audio_array)\n",
        "        clear_last()\n",
        "        clear_last()\n",
        "        a_out = Audio(src=fname, autoplay=False)\n",
        "        page.overlay.append(a_out)\n",
        "        page.update()\n",
        "        display_name = fname\n",
        "        prt(Row([IconButton(icon=icons.PLAY_CIRCLE_FILLED, icon_size=48, on_click=play_audio, data=a_out), Text(display_name)]))\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "def run_riffusion(page):\n",
        "    global riffusion_prefs, pipe_riffusion, prefs\n",
        "    def prt(line):\n",
        "      if type(line) == str:\n",
        "        line = Text(line)\n",
        "      page.riffusion_output.controls.append(line)\n",
        "      page.riffusion_output.update()\n",
        "    def clear_last():\n",
        "      if len(page.riffusion_output.controls) < 1: return\n",
        "      del page.riffusion_output.controls[-1]\n",
        "      page.riffusion_output.update()\n",
        "    def play_audio(e):\n",
        "      e.control.data.play()\n",
        "    if not bool(riffusion_prefs['prompt']):\n",
        "      alert_msg(page, \"Provide Text for the AI to create the sound of...\")\n",
        "      return\n",
        "    if not status['installed_diffusers']:\n",
        "      alert_msg(page, \"You must Install the HuggingFace Diffusers Library first... \")\n",
        "      return\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    state_text = Text(\" Downloading Riffusion Packages...\", weight=FontWeight.BOLD)\n",
        "    prt(Row([ProgressRing(), state_text]))\n",
        "    riffusion_dir = os.path.join(root_dir, \"riffusion-inference\")\n",
        "    #voice_dir = os.path.join(riffusion_dir, 'audioldm', 'voices')\n",
        "    if not os.path.isdir(riffusion_dir):\n",
        "      os.chdir(root_dir) # -b v0.3.0 \n",
        "      run_process(\"git clone https://github.com/hmartiro/riffusion-inference\", page=page)\n",
        "    os.chdir(riffusion_dir)\n",
        "    import sys\n",
        "    sys.path.append(os.path.join(riffusion_dir, 'riffusion'))\n",
        "    try:\n",
        "        from riffusion.spectrogram_image_converter import SpectrogramImageConverter\n",
        "    except Exception:\n",
        "        try:\n",
        "            run_process(\"pip install -r requirements.txt\", page=page, cwd=riffusion_dir)\n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            alert_msg(page, \"Error Installing Riffusion requirements\", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip())]))\n",
        "            return\n",
        "        pass\n",
        "    finally:\n",
        "        from riffusion.spectrogram_image_converter import SpectrogramImageConverter\n",
        "        from riffusion.spectrogram_params import SpectrogramParams\n",
        "        try:\n",
        "            from riffusion.audio import spectrogram_from_waveform\n",
        "        except Exception:\n",
        "            print(\"Still can't find riffusion.audio import spectrogram_from_waveform\")\n",
        "            pass\n",
        "        #from IPython.display import Audio\n",
        "        from scipy.io import wavfile\n",
        "    def image_from_spectrogram(spectrogram: np.ndarray, max_volume: float = 50, power_for_image: float = 0.25) -> PILImage.Image:\n",
        "        data = np.power(spectrogram, power_for_image)\n",
        "        data = data * 255 / max_volume\n",
        "        data = 255 - data\n",
        "        image = PILImage.fromarray(data.astype(np.uint8))\n",
        "        image = image.transpose(PILImage.FLIP_TOP_BOTTOM)\n",
        "        image = image.convert(\"RGB\")\n",
        "        return image\n",
        "    model_id=\"riffusion/riffusion-model-v1\"\n",
        "    save_dir = os.path.join(root_dir, 'audio_out', riffusion_prefs['batch_folder_name'])\n",
        "    if not os.path.exists(save_dir):\n",
        "      os.makedirs(save_dir, exist_ok=True)\n",
        "    audio_out = os.path.join(prefs['image_output'].rpartition(slash)[0], 'audio_out')\n",
        "    if bool(riffusion_prefs['batch_folder_name']):\n",
        "      audio_out = os.path.join(audio_out, riffusion_prefs['batch_folder_name'])\n",
        "    os.makedirs(audio_out, exist_ok=True)\n",
        "    #voice_dirs = os.listdir(os.path.join(root_dir, \"audioldm-tts\", 'audioldm', 'voices'))\n",
        "    #print(str(voice_dirs))\n",
        "    fname = format_filename(riffusion_prefs['prompt'])\n",
        "    if fname[-1] == '.': fname = fname[:-1]\n",
        "    file_prefix = riffusion_prefs['file_prefix']\n",
        "    audio_name = f'{file_prefix}-{fname}'\n",
        "    audio_name = audio_name[:int(prefs['file_max_length'])]\n",
        "    \n",
        "    init = riffusion_prefs['audio_file']\n",
        "    if bool(init):\n",
        "        if init.startswith('http'):\n",
        "            init_audio = download_file(init)\n",
        "        else:\n",
        "            if os.path.isfile(init):\n",
        "                init_audio = init\n",
        "            else:\n",
        "                init_audio = None\n",
        "    else:\n",
        "        init_audio = None\n",
        "\n",
        "    if pipe_riffusion == None or (init_audio == None and riffusion_prefs['loaded_pipe'] == \"image\") or (init_audio != None and riffusion_prefs['loaded_pipe'] == \"text\"):\n",
        "      clear_pipes()\n",
        "      try:\n",
        "        if init_audio == None:\n",
        "            from diffusers import DiffusionPipeline #, custom_pipeline=\"AlanB/lpw_stable_diffusion_mod\"\n",
        "            pipe_riffusion = DiffusionPipeline.from_pretrained(model_id, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)\n",
        "            pipe_riffusion = optimize_pipe(pipe_riffusion)\n",
        "            riffusion_prefs['loaded_pipe'] = \"text\"\n",
        "        else:\n",
        "            from diffusers import StableDiffusionImg2ImgPipeline\n",
        "            pipe_riffusion = StableDiffusionImg2ImgPipeline.from_pretrained(model_id, torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)\n",
        "            pipe_riffusion = pipe_riffusion.to(torch_device)\n",
        "            riffusion_prefs['loaded_pipe'] = \"image\"\n",
        "      except Exception as e:\n",
        "        clear_last()\n",
        "        alert_msg(page, \"Error downloading Riffusion package\", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))\n",
        "        return\n",
        "    else:\n",
        "        clear_pipes('riffusion')\n",
        "    clear_last()\n",
        "    prt(Text(\"  Generating Riffusion Sounds...\", weight=FontWeight.BOLD))\n",
        "    prt(progress)\n",
        "    random_seed = int(riffusion_prefs['seed']) if int(riffusion_prefs['seed']) > 0 else rnd.randint(0,4294967295)\n",
        "    generator = torch.Generator(device=torch_device).manual_seed(random_seed)\n",
        "    try:\n",
        "        if init_audio == None:\n",
        "            params = SpectrogramParams()\n",
        "            converter = SpectrogramImageConverter(params)\n",
        "            specs = pipe_riffusion(\n",
        "                riffusion_prefs['prompt'],\n",
        "                negative_prompt=riffusion_prefs['negative_prompt'],\n",
        "                guidance_scale=riffusion_prefs['guidance_scale'],\n",
        "                steps=riffusion_prefs['steps'],\n",
        "                width=riffusion_prefs['max_size'],\n",
        "                height=riffusion_prefs['max_size'],\n",
        "                batch_size=riffusion_prefs['batch_size'],\n",
        "                generator=generator,\n",
        "            ).images\n",
        "        else:\n",
        "            rate, data = wavfile.read(init_audio)\n",
        "            data = np.mean(data, axis=1)\n",
        "            data = data.astype(np.float32)\n",
        "            data = data[rate*7:rate*14]\n",
        "            spectrogram = spectrogram_from_waveform(waveform=data, sample_rate=rate, n_fft=8192, hop_length=512, win_length=8192)\n",
        "            spec = image_from_spectrogram(spectrogram)\n",
        "            specs = pipe_riffusion(\n",
        "                prompt=riffusion_prefs['prompt'],\n",
        "                negative_prompt=riffusion_prefs['negative_prompt'],\n",
        "                image=spec,\n",
        "                strength=riffusion_prefs['strength'],\n",
        "                guidance_scale=riffusion_prefs['guidance_scale'],\n",
        "                steps=riffusion_prefs['steps'],\n",
        "                width=riffusion_prefs['max_size'],\n",
        "                height=riffusion_prefs['max_size'],\n",
        "                batch_size=riffusion_prefs['batch_size'],\n",
        "                generator=generator,\n",
        "            ).images\n",
        "    except Exception as e:\n",
        "      clear_last()\n",
        "      alert_msg(page, \"Error generating Spectrogram Image Converter from Diffusion...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))\n",
        "      return\n",
        "\n",
        "    clear_last()\n",
        "    clear_last()\n",
        "    for spec in specs:\n",
        "        audio_file = available_file(save_dir, audio_name, 0, ext=\"wav\")\n",
        "        image_file = available_file(save_dir, audio_name, 0)\n",
        "        wav = converter.audio_from_spectrogram_image(image=spec)\n",
        "        wav.export(audio_file, format='wav')\n",
        "        spec.save(image_file)\n",
        "        a_out = Audio(src=audio_file, autoplay=False)\n",
        "        page.overlay.append(a_out)\n",
        "        page.update()\n",
        "        display_name = audio_file\n",
        "        if storage_type == \"Colab Google Drive\":\n",
        "          audio_save = available_file(audio_out, fname, 0, ext='wav')\n",
        "          image_save = available_file(audio_out, fname, 0)\n",
        "          shutil.copy(audio_file, audio_save)\n",
        "          shutil.copy(image_file, image_save)\n",
        "          display_name = audio_save\n",
        "        prt(Row([IconButton(icon=icons.PLAY_CIRCLE_FILLED, icon_size=48, on_click=play_audio, data=a_out), Text(display_name)]))\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "\n",
        "def run_mubert(page):\n",
        "    def prt(line):\n",
        "      if type(line) == str:\n",
        "        line = Text(line)\n",
        "      page.tortoise_output.controls.append(line)\n",
        "      page.tortoise_output.update()\n",
        "    def clear_last():\n",
        "      if len(page.tortoise_output.controls) == 0: return\n",
        "      del page.tortoise_output.controls[-1]\n",
        "      page.tortoise_output.update()\n",
        "    def play_audio(e):\n",
        "      e.control.data.play()\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    state_text = Text(\" Downloading Mubert Packages...\", weight=FontWeight.BOLD)\n",
        "    prt(Row([ProgressRing(), state_text]))\n",
        "    mubert_dir = os.path.join(root_dir, \"mubert-songs\")\n",
        "    if bool(mubert_prefs['batch_folder_name']):\n",
        "        mubert_dir = os.path.join(mubert_dir, mubert_prefs['batch_folder_name'])\n",
        "    if not os.path.exists(mubert_dir):\n",
        "        os.makedirs(mubert_dir)\n",
        "    import time\n",
        "    try:\n",
        "        from sentence_transformers import SentenceTransformer\n",
        "    except Exception:\n",
        "        run_process(\"pip install sentence_transformers\", page=page, show=True, print=True)\n",
        "        from sentence_transformers import SentenceTransformer\n",
        "        pass\n",
        "    try:\n",
        "        import httpx\n",
        "    except ModuleNotFoundError:\n",
        "        run_process(\"pip install httpx\", page=page)\n",
        "        import httpx\n",
        "        pass\n",
        "\n",
        "    MUBERT_TAGS_STRING = 'tribal,action,kids,neo-classic,run 130,pumped,jazz / funk,ethnic,dubtechno,reggae,acid jazz,liquidfunk,funk,witch house,tech house,underground,artists,mystical,disco,sensorium,r&b,agender,psychedelic trance / psytrance,peaceful,run 140,piano,run 160,setting,meditation,christmas,ambient,horror,cinematic,electro house,idm,bass,minimal,underscore,drums,glitchy,beautiful,technology,tribal house,country pop,jazz & funk,documentary,space,classical,valentines,chillstep,experimental,trap,new jack swing,drama,post-rock,tense,corporate,neutral,happy,analog,funky,spiritual,sberzvuk special,chill hop,dramatic,catchy,holidays,fitness 90,optimistic,orchestra,acid techno,energizing,romantic,minimal house,breaks,hyper pop,warm up,dreamy,dark,urban,microfunk,dub,nu disco,vogue,keys,hardcore,aggressive,indie,electro funk,beauty,relaxing,trance,pop,hiphop,soft,acoustic,chillrave / ethno-house,deep techno,angry,dance,fun,dubstep,tropical,latin pop,heroic,world music,inspirational,uplifting,atmosphere,art,epic,advertising,chillout,scary,spooky,slow ballad,saxophone,summer,erotic,jazzy,energy 100,kara mar,xmas,atmospheric,indie pop,hip-hop,yoga,reggaeton,lounge,travel,running,folk,chillrave & ethno-house,detective,darkambient,chill,fantasy,minimal techno,special,night,tropical house,downtempo,lullaby,meditative,upbeat,glitch hop,fitness,neurofunk,sexual,indie rock,future pop,jazz,cyberpunk,melancholic,happy hardcore,family / kids,synths,electric guitar,comedy,psychedelic trance & psytrance,edm,psychedelic rock,calm,zen,bells,podcast,melodic house,ethnic percussion,nature,heavy,bassline,indie dance,techno,drumnbass,synth pop,vaporwave,sad,8-bit,chillgressive,deep,orchestral,futuristic,hardtechno,nostalgic,big room,sci-fi,tutorial,joyful,pads,minimal 170,drill,ethnic 108,amusing,sleepy ambient,psychill,italo disco,lofi,house,acoustic guitar,bassline house,rock,k-pop,synthwave,deep house,electronica,gabber,nightlife,sport & fitness,road trip,celebration,electro,disco house,electronic'\n",
        "    MUBERT_TAGS = np.array(MUBERT_TAGS_STRING.split(','))\n",
        "    MUBERT_LICENSE = \"ttmmubertlicense#f0acYBenRcfeFpNT4wpYGaTQIyDI4mJGv5MfIhBFz97NXDwDNFHmMRsBSzmGsJwbTpP1A6i07AXcIeAHo5\"\n",
        "    MUBERT_MODE = \"loop\"\n",
        "    MUBERT_TOKEN = \"4951f6428e83172a4f39de05d5b3ab10d58560b8\"\n",
        "\n",
        "    def get_mubert_tags_embeddings(w2v_model):\n",
        "        return w2v_model.encode(MUBERT_TAGS)\n",
        "\n",
        "    def get_pat(email: str):\n",
        "        r = httpx.post('https://api-b2b.mubert.com/v2/GetServiceAccess', json={\"method\": \"GetServiceAccess\", \"params\": {\"email\": email, \"license\": MUBERT_LICENSE, \"token\": MUBERT_TOKEN, \"mode\": MUBERT_MODE}})\n",
        "        rdata = json.loads(r.text)\n",
        "        if rdata['status'] != 1:\n",
        "            alert_msg(page, \"ERROR Requesting Mubert Service. Probably incorrect e-mail...\")\n",
        "        pat = rdata['data']['pat']\n",
        "        return pat\n",
        "\n",
        "    def find_similar(em, embeddings, method='cosine'):\n",
        "        scores = []\n",
        "        for ref in embeddings:\n",
        "            if method == 'cosine':\n",
        "                scores.append(1 - np.dot(ref, em) / (np.linalg.norm(ref) * np.linalg.norm(em)))\n",
        "            if method == 'norm':\n",
        "                scores.append(np.linalg.norm(ref - em))\n",
        "        return np.array(scores), np.argsort(scores)\n",
        "\n",
        "    def get_tags_for_prompts(w2v_model, mubert_tags_embeddings, prompts, top_n=3, debug=False):\n",
        "        prompts_embeddings = w2v_model.encode(prompts)\n",
        "        ret = []\n",
        "        for i, pe in enumerate(prompts_embeddings):\n",
        "            scores, idxs = find_similar(pe, mubert_tags_embeddings)\n",
        "            top_tags = MUBERT_TAGS[idxs[:top_n]]\n",
        "            top_prob = 1 - scores[idxs[:top_n]]\n",
        "            if debug:\n",
        "                prt(f\"Prompt: {prompts[i]}\\nTags: {', '.join(top_tags)}\\nScores: {top_prob}\\n\\n\\n\")\n",
        "            ret.append((prompts[i], list(top_tags)))\n",
        "        return ret\n",
        "    minilm = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "    mubert_tags_embeddings = get_mubert_tags_embeddings(minilm)\n",
        "\n",
        "    def get_track_by_tags(tags, pat, duration, maxit=20, loop=False):\n",
        "        if loop:\n",
        "            mode = \"loop\"\n",
        "        else:\n",
        "            mode = \"track\"\n",
        "        r = httpx.post('https://api-b2b.mubert.com/v2/RecordTrackTTM', json={\"method\": \"RecordTrackTTM\", \"params\": { \"pat\": pat,\"duration\": duration, \"tags\": tags, \"mode\": mode}})\n",
        "        rdata = json.loads(r.text)\n",
        "        assert rdata['status'] == 1, rdata['error']['text']\n",
        "        trackurl = rdata['data']['tasks'][0]['download_link']\n",
        "        prt('Generating your Mubert track... ')\n",
        "        for i in range(maxit):\n",
        "            r = httpx.get(trackurl)\n",
        "            if r.status_code == 200:\n",
        "                return trackurl\n",
        "            time.sleep(1)\n",
        "\n",
        "    def generate_track_by_prompt(email, prompt, duration, loop=False):\n",
        "        try:\n",
        "            pat = get_pat(email)\n",
        "            _, tags = get_tags_for_prompts(minilm, mubert_tags_embeddings, [prompt, ])[0]\n",
        "            return get_track_by_tags(tags, pat, int(duration), loop=loop), \"Success\", \", \".join(tags)\n",
        "        except Exception as e:\n",
        "            return None, str(e), \"\"\n",
        "    #btn.click(fn=generate_track_by_prompt, inputs=[email, prompt, duration, is_loop], outputs=[out, result_msg, tags])\n",
        "    clear_last()\n",
        "    out, result_msg, tags = generate_track_by_prompt(mubert_prefs['email'], mubert_prefs['prompt'], mubert_prefs['duration'], loop=mubert_prefs['is_loop'])    \n",
        "    if out == None:\n",
        "      alert_msg(page, \"Error generating track by prompt. The API Key problably reached montly limit...\",  content=Text(result_msg))\n",
        "      return\n",
        "    clear_last()\n",
        "    clear_last()\n",
        "    audio_out = os.path.join(prefs['image_output'].rpartition(slash)[0], 'audio_out')\n",
        "    mubert_songs = os.path.join(audio_out, 'mubert_songs')\n",
        "    audio_name = format_filename(mubert_prefs['prompt'])\n",
        "    audio_name = f\"{mubert_prefs['file_prefix']}{audio_name}\"\n",
        "    if bool(mubert_prefs['batch_folder_name']):\n",
        "      mubert_songs = os.path.join(audio_out, mubert_prefs['batch_folder_name'])\n",
        "    os.makedirs(mubert_songs, exist_ok=True)\n",
        "    fname = available_file(mubert_songs, audio_name, 0, ext=\"mp3\")\n",
        "    audio_file = download_file(out)\n",
        "    shutil.copy(audio_file, fname)\n",
        "    a_out = Audio(src=fname, autoplay=False)\n",
        "    page.overlay.append(a_out)\n",
        "    page.update()\n",
        "    display_name = fname\n",
        "    #a.tofile(f\"/content/dance-{i}.wav\")\n",
        "    if storage_type == \"Colab Google Drive\":\n",
        "      audio_save = available_file(audio_out, audio_name, 0, ext='mp3')\n",
        "      shutil.copy(fname, audio_save)\n",
        "      display_name = audio_save\n",
        "    prt(Row([IconButton(icon=icons.PLAY_CIRCLE_FILLED, icon_size=48, on_click=play_audio, data=a_out), Column([Text(display_name), Text(tags, style=TextThemeStyle.DISPLAY_SMALL)])]))\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "    \n",
        "\n",
        "loaded_StableUnCLIP = None\n",
        "def run_unCLIP(page, from_list=False):\n",
        "    global unCLIP_prefs, pipe_unCLIP, loaded_StableUnCLIP\n",
        "    if not status['installed_diffusers']:\n",
        "      alert_msg(page, \"You must Install the HuggingFace Diffusers Library first... \")\n",
        "      return\n",
        "    def prt(line, update=True):\n",
        "      if type(line) == str:\n",
        "        line = Text(line)\n",
        "      if from_list:\n",
        "        page.imageColumn.controls.append(line)\n",
        "        if update:\n",
        "          page.imageColumn.update()\n",
        "      else:\n",
        "        page.unCLIP.controls.append(line)\n",
        "        if update:\n",
        "          page.unCLIP.update()\n",
        "    def clear_last():\n",
        "      if from_list:\n",
        "        del page.imageColumn.controls[-1]\n",
        "        page.imageColumn.update()\n",
        "      else:\n",
        "        del page.unCLIP.controls[-1]\n",
        "        page.unCLIP.update()\n",
        "    def clear_list():\n",
        "      if from_list:\n",
        "        page.imageColumn.controls.clear()\n",
        "      else:\n",
        "        page.unCLIP.controls = page.unCLIP.controls[:1]\n",
        "    def autoscroll(scroll=True):\n",
        "      if from_list:\n",
        "        page.imageColumn.auto_scroll = scroll\n",
        "        page.imageColumn.update()\n",
        "        page.unCLIP.auto_scroll = scroll\n",
        "        page.unCLIP.update()\n",
        "      else:\n",
        "        page.unCLIP.auto_scroll = scroll\n",
        "        page.unCLIP.update()\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    if unCLIP_prefs['use_StableUnCLIP_pipeline']:\n",
        "      total_steps = unCLIP_prefs['prior_num_inference_steps']\n",
        "    else:\n",
        "      total_steps = unCLIP_prefs['prior_num_inference_steps'] + unCLIP_prefs['decoder_num_inference_steps'] + unCLIP_prefs['super_res_num_inference_steps']\n",
        "    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:\n",
        "      callback_fnc.has_been_called = True\n",
        "      nonlocal progress, total_steps\n",
        "      #total_steps = len(latents)\n",
        "      percent = (step +1)/ total_steps\n",
        "      progress.value = percent\n",
        "      progress.tooltip = f\"{step +1} / {total_steps}  Timestep: {timestep}\"\n",
        "      progress.update()\n",
        "    unCLIP_prompts = []\n",
        "    if from_list:\n",
        "      if len(prompts) < 1:\n",
        "        alert_msg(page, \"You need to add Prompts to your List first... \")\n",
        "        return\n",
        "      for p in prompts:\n",
        "        unCLIP_prompts.append(p.prompt)\n",
        "    else:\n",
        "      if not bool(unCLIP_prefs['prompt']):\n",
        "        alert_msg(page, \"You need to add a Text Prompt first... \")\n",
        "        return\n",
        "      unCLIP_prompts.append(unCLIP_prefs['prompt'])\n",
        "    autoscroll(True)\n",
        "    clear_list()\n",
        "    from PIL.PngImagePlugin import PngInfo\n",
        "    clear_pipes('unCLIP')\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.reset_max_memory_allocated()\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "    if from_list:\n",
        "      page.tabs.selected_index = 4\n",
        "      page.tabs.update()\n",
        "    model_id = \"kakaobrain/karlo-v1-alpha\"\n",
        "    stable = \"Stable \" if unCLIP_prefs['use_StableUnCLIP_pipeline'] else \"\"\n",
        "    if pipe_unCLIP != None and ((loaded_StableUnCLIP == True and not unCLIP_prefs['use_StableUnCLIP_pipeline']) or (loaded_StableUnCLIP == False and unCLIP_prefs['use_StableUnCLIP_pipeline'])):\n",
        "        del pipe_unCLIP\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "        pipe_unCLIP = None\n",
        "    if pipe_unCLIP == None:\n",
        "        prt(Installing(f\"  Downloading {stable}unCLIP Kakaobrain Karlo Pipeline... It's a big one, see console for progress.\"))\n",
        "        try:\n",
        "            if unCLIP_prefs['use_StableUnCLIP_pipeline']:\n",
        "              from diffusers import UnCLIPScheduler, DDPMScheduler, StableUnCLIPPipeline\n",
        "              from diffusers.models import PriorTransformer\n",
        "              from transformers import CLIPTokenizer, CLIPTextModelWithProjection\n",
        "              prior = PriorTransformer.from_pretrained(model_id, subfolder=\"prior\", torch_dtype=torch.float16)\n",
        "              prior_text_model_id = \"openai/clip-vit-large-patch14\"\n",
        "              prior_tokenizer = CLIPTokenizer.from_pretrained(prior_text_model_id)\n",
        "              prior_text_model = CLIPTextModelWithProjection.from_pretrained(prior_text_model_id, torch_dtype=torch.float16)\n",
        "              prior_scheduler = UnCLIPScheduler.from_pretrained(model_id, subfolder=\"prior_scheduler\")\n",
        "              #prior_scheduler = DDPMScheduler.from_config(prior_scheduler.config)\n",
        "              prior_scheduler = pipeline_scheduler(prior_scheduler, from_scheduler=False)\n",
        "              stable_unclip_model_id = \"stabilityai/stable-diffusion-2-1-unclip-small\"\n",
        "              pipe_unCLIP = StableUnCLIPPipeline.from_pretrained(\n",
        "                  stable_unclip_model_id,\n",
        "                  torch_dtype=torch.float16,\n",
        "                  variant=\"fp16\",\n",
        "                  prior_tokenizer=prior_tokenizer,\n",
        "                  prior_text_encoder=prior_text_model,\n",
        "                  prior=prior,\n",
        "                  prior_scheduler=prior_scheduler,\n",
        "              )\n",
        "              pipe_unCLIP.to(torch_device)\n",
        "              pipe_unCLIP.enable_attention_slicing()\n",
        "              pipe_unCLIP.enable_sequential_cpu_offload()\n",
        "              #from diffusers import DiffusionPipeline\n",
        "              #pipe_unCLIP = DiffusionPipeline.from_pretrained(model_id, custom_pipeline=\"stable_unclip\", torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, decoder_pipe_kwargs=dict(image_encoder=None))\n",
        "              #pipe_unCLIP.to(torch_device)\n",
        "              #pipe_unCLIP = optimize_pipe(pipe_unCLIP)\n",
        "              loaded_StableUnCLIP = True\n",
        "            else:\n",
        "              from diffusers import UnCLIPPipeline\n",
        "              pipe_unCLIP = UnCLIPPipeline.from_pretrained(model_id, torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)\n",
        "              #pipe_unCLIP.to(torch_device)\n",
        "              pipe_unCLIP = optimize_pipe(pipe_unCLIP)\n",
        "              loaded_StableUnCLIP = False\n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            alert_msg(page, f\"Error Downloading {stable}unCLIP Pipeline\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "            return\n",
        "        pipe_unCLIP.set_progress_bar_config(disable=True)\n",
        "        clear_last()\n",
        "    s = \"s\" if unCLIP_prefs['num_images'] > 1 else \"\"\n",
        "    prt(f\" Generating {stable}unCLIP{s} of your Image...\")\n",
        "    batch_output = os.path.join(stable_dir, unCLIP_prefs['batch_folder_name'])\n",
        "    if not os.path.isdir(batch_output):\n",
        "      os.makedirs(batch_output)\n",
        "    batch_output = os.path.join(prefs['image_output'], unCLIP_prefs['batch_folder_name'])\n",
        "    if not os.path.isdir(batch_output):\n",
        "      os.makedirs(batch_output)\n",
        "    for pr in unCLIP_prompts:\n",
        "        for num in range(unCLIP_prefs['num_images']):\n",
        "            prt(progress)\n",
        "            autoscroll(False)\n",
        "            random_seed = (int(unCLIP_prefs['seed']) + num) if int(unCLIP_prefs['seed']) > 0 else rnd.randint(0,4294967295)\n",
        "            generator = torch.Generator(device=torch_device).manual_seed(random_seed)\n",
        "            try:\n",
        "                if unCLIP_prefs['use_StableUnCLIP_pipeline']:#decoder_num_inference_steps=unCLIP_prefs['decoder_num_inference_steps'], super_res_num_inference_steps=unCLIP_prefs['super_res_num_inference_steps'], decoder_guidance_scale=unCLIP_prefs['decoder_guidance_scale'], \n",
        "                  images = pipe_unCLIP([pr], prior_num_inference_steps=unCLIP_prefs['prior_num_inference_steps'], prior_guidance_scale=unCLIP_prefs['prior_guidance_scale'], num_images_per_prompt=1, width=512, height=512, generator=generator, callback=callback_fnc, callback_steps=1).images\n",
        "                else:\n",
        "                  images = pipe_unCLIP([pr], prior_num_inference_steps=unCLIP_prefs['prior_num_inference_steps'], decoder_num_inference_steps=unCLIP_prefs['decoder_num_inference_steps'], super_res_num_inference_steps=unCLIP_prefs['super_res_num_inference_steps'], prior_guidance_scale=unCLIP_prefs['prior_guidance_scale'], decoder_guidance_scale=unCLIP_prefs['decoder_guidance_scale'], num_images_per_prompt=1, generator=generator, callback=callback_fnc, callback_steps=1).images\n",
        "            except Exception as e:\n",
        "                clear_last()\n",
        "                alert_msg(page, f\"Error running {stable}unCLIP Pipeline\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "                return\n",
        "            autoscroll(True)\n",
        "            clear_last()\n",
        "            fname = format_filename(pr)\n",
        "\n",
        "            if prefs['file_suffix_seed']: fname += f\"-{random_seed}\"\n",
        "            for image in images:\n",
        "                image_path = available_file(os.path.join(stable_dir, unCLIP_prefs['batch_folder_name']), fname, num)\n",
        "                unscaled_path = image_path\n",
        "                output_file = image_path.rpartition(slash)[2]\n",
        "                image.save(image_path)\n",
        "                out_path = image_path.rpartition(slash)[0]\n",
        "                upscaled_path = os.path.join(out_path, output_file)\n",
        "                if not unCLIP_prefs['display_upscaled_image'] or not unCLIP_prefs['apply_ESRGAN_upscale']:\n",
        "                    prt(Row([ImageButton(src=unscaled_path, width=512, height=512, data=upscaled_path, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "                    #prt(Row([Img(src=unscaled_path, fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "                if unCLIP_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:\n",
        "                    w = int(unCLIP_prefs['width'] * unCLIP_prefs[\"enlarge_scale\"])\n",
        "                    h = int(unCLIP_prefs['height'] * unCLIP_prefs[\"enlarge_scale\"])\n",
        "                    prt(Row([Text(f'Enlarging {unCLIP_prefs[\"enlarge_scale\"]}X to {w}x{h}')], alignment=MainAxisAlignment.CENTER))\n",
        "                    os.chdir(os.path.join(dist_dir, 'Real-ESRGAN'))\n",
        "                    upload_folder = 'upload'\n",
        "                    result_folder = 'results'     \n",
        "                    if os.path.isdir(upload_folder):\n",
        "                        shutil.rmtree(upload_folder)\n",
        "                    if os.path.isdir(result_folder):\n",
        "                        shutil.rmtree(result_folder)\n",
        "                    os.mkdir(upload_folder)\n",
        "                    os.mkdir(result_folder)\n",
        "                    short_name = f'{fname[:80]}-{num}.png'\n",
        "                    dst_path = os.path.join(dist_dir, 'Real-ESRGAN', upload_folder, short_name)\n",
        "                    #print(f'Moving {fpath} to {dst_path}')\n",
        "                    #shutil.move(fpath, dst_path)\n",
        "                    shutil.copy(image_path, dst_path)\n",
        "                    #faceenhance = ' --face_enhance' if unCLIP_prefs[\"face_enhance\"] else ''\n",
        "                    faceenhance = ''\n",
        "                    run_sp(f'python inference_realesrgan.py -n realesr-general-x4v3 -i upload --outscale {unCLIP_prefs[\"enlarge_scale\"]}{faceenhance}', cwd=os.path.join(dist_dir, 'Real-ESRGAN'), realtime=False)\n",
        "                    out_file = short_name.rpartition('.')[0] + '_out.png'\n",
        "                    upscaled_path = os.path.join(out_path, output_file)\n",
        "                    shutil.move(os.path.join(dist_dir, 'Real-ESRGAN', result_folder, out_file), upscaled_path)\n",
        "                    image_path = upscaled_path\n",
        "                    os.chdir(stable_dir)\n",
        "                    clear_last()\n",
        "                if prefs['save_image_metadata']:\n",
        "                    img = PILImage.open(image_path)\n",
        "                    metadata = PngInfo()\n",
        "                    metadata.add_text(\"artist\", prefs['meta_ArtistName'])\n",
        "                    metadata.add_text(\"copyright\", prefs['meta_Copyright'])\n",
        "                    metadata.add_text(\"software\", \"Stable Diffusion Deluxe\" + f\", upscaled {unCLIP_prefs['enlarge_scale']}x with ESRGAN\" if unCLIP_prefs['apply_ESRGAN_upscale'] else \"\")\n",
        "                    metadata.add_text(\"pipeline\", f\"{stable}unCLIP\")\n",
        "                    if prefs['save_config_in_metadata']:\n",
        "                      metadata.add_text(\"title\", pr)\n",
        "                      config_json = unCLIP_prefs.copy()\n",
        "                      config_json['model_path'] = model_id\n",
        "                      config_json['seed'] = random_seed\n",
        "                      del config_json['num_images']\n",
        "                      del config_json['display_upscaled_image']\n",
        "                      del config_json['batch_folder_name']\n",
        "                      if not config_json['apply_ESRGAN_upscale']:\n",
        "                        del config_json['enlarge_scale']\n",
        "                        del config_json['apply_ESRGAN_upscale']\n",
        "                      metadata.add_text(\"config_json\", json.dumps(config_json, ensure_ascii=True, indent=4))\n",
        "                    img.save(image_path, pnginfo=metadata)\n",
        "                #TODO: PyDrive\n",
        "                if storage_type == \"Colab Google Drive\":\n",
        "                    new_file = available_file(os.path.join(prefs['image_output'], unCLIP_prefs['batch_folder_name']), fname, num)\n",
        "                    out_path = new_file\n",
        "                    shutil.copy(image_path, new_file)\n",
        "                elif bool(prefs['image_output']):\n",
        "                    new_file = available_file(os.path.join(prefs['image_output'], unCLIP_prefs['batch_folder_name']), fname, num)\n",
        "                    out_path = new_file\n",
        "                    shutil.copy(image_path, new_file)\n",
        "                time.sleep(0.2)\n",
        "                if unCLIP_prefs['display_upscaled_image']:\n",
        "                    time.sleep(0.6)\n",
        "                    prt(Row([ImageButton(src=upscaled_path, data=upscaled_path, width=512 * float(unCLIP_prefs[\"enlarge_scale\"]), height=512 * float(unCLIP_prefs[\"enlarge_scale\"]), page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "                    #prt(Row([Img(src=upscaled_path, fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "                prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))\n",
        "    autoscroll(False)\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "def run_unCLIP_image_variation(page, from_list=False):\n",
        "    global unCLIP_image_variation_prefs, pipe_unCLIP_image_variation\n",
        "    if not status['installed_diffusers']:\n",
        "      alert_msg(page, \"You must Install the HuggingFace Diffusers Library first... \")\n",
        "      return \n",
        "    def prt(line, update=True):\n",
        "      if type(line) == str:\n",
        "        line = Text(line)\n",
        "      if from_list:\n",
        "        page.imageColumn.controls.append(line)\n",
        "        if update:\n",
        "          page.imageColumn.update()\n",
        "      else:\n",
        "        page.UnCLIP_ImageVariation.controls.append(line)\n",
        "        if update:\n",
        "          page.UnCLIP_ImageVariation.update()\n",
        "    def clear_last():\n",
        "      if from_list:\n",
        "        del page.imageColumn.controls[-1]\n",
        "        page.imageColumn.update()\n",
        "      else:\n",
        "        del page.UnCLIP_ImageVariation.controls[-1]\n",
        "        page.UnCLIP_ImageVariation.update()\n",
        "    def clear_list():\n",
        "      if from_list:\n",
        "        page.imageColumn.controls.clear()\n",
        "      else:\n",
        "        page.UnCLIP_ImageVariation.controls = page.UnCLIP_ImageVariation.controls[:1]\n",
        "    def autoscroll(scroll=True):\n",
        "      if from_list:\n",
        "        page.imageColumn.auto_scroll = scroll\n",
        "        page.imageColumn.update()\n",
        "        page.UnCLIP_ImageVariation.auto_scroll = scroll\n",
        "        page.UnCLIP_ImageVariation.update()\n",
        "      else:\n",
        "        page.UnCLIP_ImageVariation.auto_scroll = scroll\n",
        "        page.UnCLIP_ImageVariation.update()\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    total_steps = unCLIP_image_variation_prefs['decoder_num_inference_steps'] + unCLIP_image_variation_prefs['super_res_num_inference_steps']\n",
        "    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:\n",
        "      callback_fnc.has_been_called = True\n",
        "      nonlocal progress, total_steps\n",
        "      #total_steps = len(latents)\n",
        "      percent = (step +1)/ total_steps\n",
        "      progress.value = percent\n",
        "      progress.tooltip = f\"{step +1} / {total_steps}  Timestep: {timestep}\"\n",
        "      progress.update()\n",
        "    unCLIP_image_variation_inits = []\n",
        "    if from_list:\n",
        "      if len(prompts) < 1:\n",
        "        alert_msg(page, \"You need to add Prompts to your List first... \")\n",
        "        return\n",
        "      for p in prompts:\n",
        "        if bool(p['init_image']):\n",
        "          unCLIP_image_variation_inits.append(p.prompt)\n",
        "    else:\n",
        "      if not bool(unCLIP_image_variation_prefs['init_image']):\n",
        "        alert_msg(page, \"You need to add a Initial Image first... \")\n",
        "        return\n",
        "      unCLIP_image_variation_inits.append(unCLIP_image_variation_prefs['init_image'])\n",
        "    autoscroll(True)\n",
        "    clear_list()\n",
        "    from io import BytesIO\n",
        "    from PIL.PngImagePlugin import PngInfo\n",
        "    from PIL import ImageOps\n",
        "    if from_list:\n",
        "      page.tabs.selected_index = 4\n",
        "      page.tabs.update()\n",
        "    clear_pipes('unCLIP_image_variation')\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.reset_max_memory_allocated()\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "    if pipe_unCLIP_image_variation == None:\n",
        "        from diffusers import UnCLIP_ImageVariationPipeline\n",
        "        prt(Installing(\" Downloading unCLIP Image Variation Kakaobrain Karlo Pipeline... It's a big one, see console for progress.\"))\n",
        "        try:\n",
        "            pipe_unCLIP_image_variation = UnCLIP_ImageVariationPipeline.from_pretrained(\"kakaobrain/karlo-v1-alpha-image-variations\", torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)\n",
        "            #pipe_unCLIP_image_variation.to(torch_device)\n",
        "            pipe_unCLIP_image_variation = optimize_pipe(pipe_unCLIP_image_variation)\n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            alert_msg(page, \"Error Downloading unCLIP Image Variation Pipeline\", content=Text(str(e)))\n",
        "            return\n",
        "        pipe_unCLIP_image_variation.set_progress_bar_config(disable=True)\n",
        "        clear_last()\n",
        "    s = \"s\" if unCLIP_image_variation_prefs['num_images'] > 1 else \"\"\n",
        "    prt(f\"Generating unCLIP Image Variation{s} of your Image...\")\n",
        "    batch_output = os.path.join(stable_dir, unCLIP_image_variation_prefs['batch_folder_name'])\n",
        "    if not os.path.isdir(batch_output):\n",
        "      os.makedirs(batch_output)\n",
        "    batch_output = os.path.join(prefs['image_output'], unCLIP_image_variation_prefs['batch_folder_name'])\n",
        "    if not os.path.isdir(batch_output):\n",
        "      os.makedirs(batch_output)\n",
        "    for init in unCLIP_image_variation_inits:\n",
        "        if init.startswith('http'):\n",
        "          init_img = PILImage.open(requests.get(init, stream=True).raw)\n",
        "        else:\n",
        "          if os.path.isfile(init):\n",
        "            init_img = PILImage.open(init)\n",
        "          else:\n",
        "            alert_msg(page, f\"ERROR: Couldn't find your init_image {init}\")\n",
        "            return\n",
        "        width, height = init_img.size\n",
        "        width, height = scale_dimensions(width, height, unCLIP_image_variation_prefs['max_size'])\n",
        "        init_img = init_img.resize((width, height), resample=PILImage.BICUBIC)\n",
        "        init_img = ImageOps.exif_transpose(init_img).convert(\"RGB\")\n",
        "        for num in range(unCLIP_image_variation_prefs['num_images']):\n",
        "            prt(progress)\n",
        "            autoscroll(False)\n",
        "            random_seed = (int(unCLIP_image_variation_prefs['seed']) + num) if int(unCLIP_image_variation_prefs['seed']) > 0 else rnd.randint(0,4294967295)\n",
        "            generator = torch.Generator(device=torch_device).manual_seed(random_seed)\n",
        "            try:\n",
        "                images = pipe_unCLIP_image_variation(image=init_img, decoder_num_inference_steps=unCLIP_image_variation_prefs['decoder_num_inference_steps'], super_res_num_inference_steps=unCLIP_image_variation_prefs['super_res_num_inference_steps'], decoder_guidance_scale=unCLIP_image_variation_prefs['decoder_guidance_scale'], num_images_per_prompt=1, generator=generator).images #, callback=callback_fnc, callback_steps=1\n",
        "            except Exception as e:\n",
        "                clear_last()\n",
        "                alert_msg(page, \"Error running unCLIP Image Variation Pipeline\", content=Text(str(e)))\n",
        "                return\n",
        "            clear_last()\n",
        "            autoscroll(True)\n",
        "            #fname = format_filename(unCLIP_image_variation_prefs['file_name'])\n",
        "            fname = init.rpartition(slash)[2].rpartition('.')[0]\n",
        "            if prefs['file_suffix_seed']: fname += f\"-{random_seed}\"\n",
        "            for image in images:\n",
        "                image_path = available_file(os.path.join(stable_dir, unCLIP_image_variation_prefs['batch_folder_name']), fname, num)\n",
        "                unscaled_path = image_path\n",
        "                output_file = image_path.rpartition(slash)[2]\n",
        "                image.save(image_path)\n",
        "                out_path = image_path.rpartition(slash)[0]\n",
        "                upscaled_path = os.path.join(out_path, output_file)\n",
        "                if not unCLIP_image_variation_prefs['display_upscaled_image'] or not unCLIP_image_variation_prefs['apply_ESRGAN_upscale']:\n",
        "                    prt(Row([ImageButton(src=unscaled_path, data=upscaled_path, width=512, height=512, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "                    #prt(Row([Img(src=unscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "                if unCLIP_image_variation_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:\n",
        "                    os.chdir(os.path.join(dist_dir, 'Real-ESRGAN'))\n",
        "                    upload_folder = 'upload'\n",
        "                    result_folder = 'results'     \n",
        "                    if os.path.isdir(upload_folder):\n",
        "                        shutil.rmtree(upload_folder)\n",
        "                    if os.path.isdir(result_folder):\n",
        "                        shutil.rmtree(result_folder)\n",
        "                    os.mkdir(upload_folder)\n",
        "                    os.mkdir(result_folder)\n",
        "                    short_name = f'{fname[:80]}-{num}.png'\n",
        "                    dst_path = os.path.join(dist_dir, 'Real-ESRGAN', upload_folder, short_name)\n",
        "                    #print(f'Moving {fpath} to {dst_path}')\n",
        "                    #shutil.move(fpath, dst_path)\n",
        "                    shutil.copy(image_path, dst_path)\n",
        "                    #faceenhance = ' --face_enhance' if unCLIP_image_variation_prefs[\"face_enhance\"] else ''\n",
        "                    faceenhance = ''\n",
        "                    run_sp(f'python inference_realesrgan.py -n realesr-general-x4v3 -i upload --outscale {unCLIP_image_variation_prefs[\"enlarge_scale\"]}{faceenhance}', cwd=os.path.join(dist_dir, 'Real-ESRGAN'), realtime=False)\n",
        "                    out_file = short_name.rpartition('.')[0] + '_out.png'\n",
        "                    shutil.move(os.path.join(dist_dir, 'Real-ESRGAN', result_folder, out_file), upscaled_path)\n",
        "                    image_path = upscaled_path\n",
        "                    os.chdir(stable_dir)\n",
        "                    if unCLIP_image_variation_prefs['display_upscaled_image']:\n",
        "                        time.sleep(0.6)\n",
        "                        prt(Row([ImageButton(src=upscaled_path, data=upscaled_path, width=512 * float(unCLIP_image_variation_prefs[\"enlarge_scale\"]), height=512 * float(unCLIP_image_variation_prefs[\"enlarge_scale\"]), page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "                        #prt(Row([Img(src=upscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "                if prefs['save_image_metadata']:\n",
        "                    img = PILImage.open(image_path)\n",
        "                    metadata = PngInfo()\n",
        "                    metadata.add_text(\"artist\", prefs['meta_ArtistName'])\n",
        "                    metadata.add_text(\"copyright\", prefs['meta_Copyright'])\n",
        "                    metadata.add_text(\"software\", \"Stable Diffusion Deluxe\" + f\", upscaled {unCLIP_image_variation_prefs['enlarge_scale']}x with ESRGAN\" if unCLIP_image_variation_prefs['apply_ESRGAN_upscale'] else \"\")\n",
        "                    metadata.add_text(\"pipeline\", \"unCLIP_image_variation\")\n",
        "                    if prefs['save_config_in_metadata']:\n",
        "                      #metadata.add_text(\"title\", unCLIP_image_variation_prefs['file_name'])\n",
        "                      config_json = unCLIP_image_variation_prefs.copy()\n",
        "                      config_json['model_path'] = \"fusing/karlo-image-variations-diffusers\"\n",
        "                      config_json['seed'] = random_seed\n",
        "                      del config_json['num_images']\n",
        "                      del config_json['display_upscaled_image']\n",
        "                      del config_json['batch_folder_name']\n",
        "                      del config_json['file_name']\n",
        "                      if not config_json['apply_ESRGAN_upscale']:\n",
        "                        del config_json['enlarge_scale']\n",
        "                        del config_json['apply_ESRGAN_upscale']\n",
        "                      metadata.add_text(\"config_json\", json.dumps(config_json, ensure_ascii=True, indent=4))\n",
        "                    img.save(image_path, pnginfo=metadata)\n",
        "                #TODO: PyDrive\n",
        "                if storage_type == \"Colab Google Drive\":\n",
        "                    new_file = available_file(os.path.join(prefs['image_output'], unCLIP_image_variation_prefs['batch_folder_name']), fname, num)\n",
        "                    out_path = new_file\n",
        "                    shutil.copy(image_path, new_file)\n",
        "                elif bool(prefs['image_output']):\n",
        "                    new_file = available_file(os.path.join(prefs['image_output'], unCLIP_image_variation_prefs['batch_folder_name']), fname, num)\n",
        "                    out_path = new_file\n",
        "                    shutil.copy(image_path, new_file)\n",
        "                time.sleep(0.2)\n",
        "                prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))\n",
        "    autoscroll(False)\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "def run_unCLIP_interpolation(page, from_list=False):\n",
        "    global unCLIP_interpolation_prefs, pipe_unCLIP_interpolation, loaded_StableUnCLIP\n",
        "    if not status['installed_diffusers']:\n",
        "      alert_msg(page, \"You must Install the HuggingFace Diffusers Library first... \")\n",
        "      return\n",
        "    def prt(line, update=True):\n",
        "      if type(line) == str:\n",
        "        line = Text(line)\n",
        "      if from_list:\n",
        "        page.imageColumn.controls.append(line)\n",
        "        if update:\n",
        "          page.imageColumn.update()\n",
        "      else:\n",
        "        page.unCLIP_interpolation_output.controls.append(line)\n",
        "        if update:\n",
        "          page.unCLIP_interpolation_output.update()\n",
        "    def clear_last():\n",
        "      if from_list:\n",
        "        del page.imageColumn.controls[-1]\n",
        "        page.imageColumn.update()\n",
        "      else:\n",
        "        del page.unCLIP_interpolation_output.controls[-1]\n",
        "        page.unCLIP_interpolation_output.update()\n",
        "    def clear_list():\n",
        "      if from_list:\n",
        "        page.imageColumn.controls.clear()\n",
        "      else:\n",
        "        page.unCLIP_Interpolation.controls = page.unCLIP_Interpolation.controls[:1]\n",
        "    def autoscroll(scroll=True):\n",
        "      if from_list:\n",
        "        page.imageColumn.auto_scroll = scroll\n",
        "        page.imageColumn.update()\n",
        "        page.unCLIP_Interpolation.auto_scroll = scroll\n",
        "        page.unCLIP_Interpolation.update()\n",
        "      else:\n",
        "        page.unCLIP_Interpolation.auto_scroll = scroll\n",
        "        page.unCLIP_Interpolation.update()\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    total_steps = unCLIP_interpolation_prefs['prior_num_inference_steps'] + unCLIP_interpolation_prefs['decoder_num_inference_steps'] + unCLIP_interpolation_prefs['super_res_num_inference_steps']\n",
        "    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:\n",
        "      callback_fnc.has_been_called = True\n",
        "      nonlocal progress, total_steps\n",
        "      #total_steps = len(latents)\n",
        "      percent = (step +1)/ total_steps\n",
        "      progress.value = percent\n",
        "      progress.tooltip = f\"{step +1} / {total_steps}  Timestep: {timestep}\"\n",
        "      progress.update()\n",
        "    unCLIP_interpolation_prompts = []\n",
        "    if from_list:\n",
        "      if len(prompts) < 1:\n",
        "        alert_msg(page, \"You need to add Prompts to your List first... \")\n",
        "        return\n",
        "      for p in prompts:\n",
        "        unCLIP_interpolation_prompts.append({'start_prompt': p.prompt, 'end_prompt':p['prompt2']})\n",
        "    else:\n",
        "      if not bool(unCLIP_interpolation_prefs['prompt']):\n",
        "        alert_msg(page, \"You need to add a Text Prompt first... \")\n",
        "        return\n",
        "      unCLIP_interpolation_prompts.append({'start_prompt': unCLIP_interpolation_prefs['prompt'], 'end_prompt':unCLIP_interpolation_prefs['end_prompt']})\n",
        "    autoscroll(True)\n",
        "    clear_list()\n",
        "    from PIL.PngImagePlugin import PngInfo\n",
        "    clear_pipes()#'unCLIP_interpolation')\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.reset_max_memory_allocated()\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "    if from_list:\n",
        "      page.tabs.selected_index = 4\n",
        "      page.tabs.update()\n",
        "    model_id = \"kakaobrain/karlo-v1-alpha\"\n",
        "    stable = \"Stable \"\n",
        "    if pipe_unCLIP_interpolation != None:\n",
        "        del pipe_unCLIP_interpolation\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "        pipe_unCLIP_interpolation = None\n",
        "    if pipe_unCLIP_interpolation == None:\n",
        "        prt(Installing(f\"Downloading {stable}unCLIP Kakaobrain Karlo Pipeline... It's a big one, see console for progress.\"))\n",
        "        try:\n",
        "            from diffusers import DiffusionPipeline\n",
        "            pipe_unCLIP_interpolation = DiffusionPipeline.from_pretrained(model_id, custom_pipeline=\"AlanB/unclip_text_interpolation_mod\", torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None) #, decoder_pipe_kwargs=dict(image_encoder=None)\n",
        "            pipe_unCLIP_interpolation.to(torch_device)\n",
        "            pipe_unCLIP_interpolation.enable_attention_slicing()\n",
        "            pipe_unCLIP_interpolation.enable_sequential_cpu_offload()\n",
        "            loaded_StableUnCLIP = True\n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            alert_msg(page, f\"Error Downloading {stable}unCLIP Pipeline\", content=Text(str(e)))\n",
        "            return\n",
        "        #pipe_unCLIP_interpolation.set_progress_bar_config(disable=True)\n",
        "        clear_last()\n",
        "    s = \"s\" if unCLIP_interpolation_prefs['num_images'] > 1 else \"\"\n",
        "    prt(f\" Generating {stable}unCLIP{s} of your Image...\")\n",
        "    batch_output = os.path.join(stable_dir, unCLIP_interpolation_prefs['batch_folder_name'])\n",
        "    if not os.path.isdir(batch_output):\n",
        "      os.makedirs(batch_output)\n",
        "    batch_output = os.path.join(prefs['image_output'], unCLIP_interpolation_prefs['batch_folder_name'])\n",
        "    if not os.path.isdir(batch_output):\n",
        "      os.makedirs(batch_output)\n",
        "    for pr in unCLIP_interpolation_prompts:\n",
        "        for num in range(unCLIP_interpolation_prefs['num_images']):\n",
        "            prt(progress)\n",
        "            autoscroll(False)\n",
        "            random_seed = (int(unCLIP_interpolation_prefs['seed']) + num) if int(unCLIP_interpolation_prefs['seed']) > 0 else rnd.randint(0,4294967295)\n",
        "            generator = torch.Generator(device=torch_device).manual_seed(random_seed)\n",
        "            try:\n",
        "                images = pipe_unCLIP_interpolation(start_prompt=pr['start_prompt'], end_prompt=pr['end_prompt'], steps=unCLIP_interpolation_prefs['steps'], prior_num_inference_steps=unCLIP_interpolation_prefs['prior_num_inference_steps'], decoder_num_inference_steps=unCLIP_interpolation_prefs['decoder_num_inference_steps'], super_res_inference_steps=unCLIP_interpolation_prefs['super_res_inference_steps'], prior_guidance_scale=unCLIP_interpolation_prefs['prior_guidance_scale'], decoder_guidance_scale=unCLIP_interpolation_prefs['decoder_guidance_scale'], callback=callback_fnc, generator=generator).images\n",
        "            except Exception as e:\n",
        "                clear_last()\n",
        "                alert_msg(page, f\"Error running {stable}unCLIP Interpolation Pipeline\", content=Text(str(e)))\n",
        "                return\n",
        "            clear_last()\n",
        "            autoscroll(True)\n",
        "            file_max_length = int(prefs['file_max_length']) / 2\n",
        "            fname = f\"{format_filename(pr['start_prompt'], max_length=file_max_length)}-to-{format_filename(pr['end_prompt'], max_length=file_max_length)}\"\n",
        "\n",
        "            if prefs['file_suffix_seed']: fname += f\"-{random_seed}\"\n",
        "            for image in images:\n",
        "                image_path = available_file(os.path.join(stable_dir, unCLIP_interpolation_prefs['batch_folder_name']), fname, num)\n",
        "                unscaled_path = image_path\n",
        "                output_file = image_path.rpartition(slash)[2]\n",
        "                image.save(image_path)\n",
        "                out_path = image_path.rpartition(slash)[0]\n",
        "                upscaled_path = os.path.join(out_path, output_file)\n",
        "                if not unCLIP_interpolation_prefs['display_upscaled_image'] or not unCLIP_interpolation_prefs['apply_ESRGAN_upscale']:\n",
        "                    prt(Row([ImageButton(src=unscaled_path, data=upscaled_path, width=512, height=512, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "                    #prt(Row([Img(src=unscaled_path, fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "                if unCLIP_interpolation_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:\n",
        "                    os.chdir(os.path.join(dist_dir, 'Real-ESRGAN'))\n",
        "                    upload_folder = 'upload'\n",
        "                    result_folder = 'results'     \n",
        "                    if os.path.isdir(upload_folder):\n",
        "                        shutil.rmtree(upload_folder)\n",
        "                    if os.path.isdir(result_folder):\n",
        "                        shutil.rmtree(result_folder)\n",
        "                    os.mkdir(upload_folder)\n",
        "                    os.mkdir(result_folder)\n",
        "                    short_name = f'{fname[:80]}-{num}.png'\n",
        "                    dst_path = os.path.join(dist_dir, 'Real-ESRGAN', upload_folder, short_name)\n",
        "                    #print(f'Moving {fpath} to {dst_path}')\n",
        "                    #shutil.move(fpath, dst_path)\n",
        "                    shutil.copy(image_path, dst_path)\n",
        "                    #faceenhance = ' --face_enhance' if unCLIP_interpolation_prefs[\"face_enhance\"] else ''\n",
        "                    faceenhance = ''\n",
        "                    run_sp(f'python inference_realesrgan.py -n realesr-general-x4v3 -i upload --outscale {unCLIP_interpolation_prefs[\"enlarge_scale\"]}{faceenhance}', cwd=os.path.join(dist_dir, 'Real-ESRGAN'), realtime=False)\n",
        "                    out_file = short_name.rpartition('.')[0] + '_out.png'\n",
        "                    shutil.move(os.path.join(dist_dir, 'Real-ESRGAN', result_folder, out_file), upscaled_path)\n",
        "                    image_path = upscaled_path\n",
        "                    os.chdir(stable_dir)\n",
        "                    \n",
        "                if prefs['save_image_metadata']:\n",
        "                    img = PILImage.open(image_path)\n",
        "                    metadata = PngInfo()\n",
        "                    metadata.add_text(\"artist\", prefs['meta_ArtistName'])\n",
        "                    metadata.add_text(\"copyright\", prefs['meta_Copyright'])\n",
        "                    metadata.add_text(\"software\", \"Stable Diffusion Deluxe\" + f\", upscaled {unCLIP_interpolation_prefs['enlarge_scale']}x with ESRGAN\" if unCLIP_interpolation_prefs['apply_ESRGAN_upscale'] else \"\")\n",
        "                    metadata.add_text(\"pipeline\", f\"{stable}unCLIP Text Interpolation\")\n",
        "                    if prefs['save_config_in_metadata']:\n",
        "                      metadata.add_text(\"title\", f\"{pr['start_prompt']}-to-{pr['end_prompt']}\")\n",
        "                      config_json = unCLIP_interpolation_prefs.copy()\n",
        "                      config_json['model_path'] = model_id\n",
        "                      config_json['seed'] = random_seed\n",
        "                      del config_json['num_images']\n",
        "                      del config_json['display_upscaled_image']\n",
        "                      del config_json['batch_folder_name']\n",
        "                      if not config_json['apply_ESRGAN_upscale']:\n",
        "                        del config_json['enlarge_scale']\n",
        "                        del config_json['apply_ESRGAN_upscale']\n",
        "                      metadata.add_text(\"config_json\", json.dumps(config_json, ensure_ascii=True, indent=4))\n",
        "                    img.save(image_path, pnginfo=metadata)\n",
        "                #TODO: PyDrive\n",
        "                if storage_type == \"Colab Google Drive\":\n",
        "                    new_file = available_file(os.path.join(prefs['image_output'], unCLIP_interpolation_prefs['batch_folder_name']), fname, num)\n",
        "                    out_path = new_file\n",
        "                    shutil.copy(image_path, new_file)\n",
        "                elif bool(prefs['image_output']):\n",
        "                    new_file = available_file(os.path.join(prefs['image_output'], unCLIP_interpolation_prefs['batch_folder_name']), fname, num)\n",
        "                    out_path = new_file\n",
        "                    shutil.copy(image_path, new_file)\n",
        "                time.sleep(0.2)\n",
        "                if unCLIP_interpolation_prefs['display_upscaled_image']:\n",
        "                    time.sleep(0.6)\n",
        "                    prt(Row([ImageButton(src=upscaled_path, data=upscaled_path, width=512 * float(unCLIP_interpolation_prefs[\"enlarge_scale\"]), height=512 * float(unCLIP_interpolation_prefs[\"enlarge_scale\"]), page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "                    #prt(Row([Img(src=upscaled_path, fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "                prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))\n",
        "    autoscroll(False)\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "def run_unCLIP_image_interpolation(page, from_list=False):\n",
        "    global unCLIP_image_interpolation_prefs, pipe_unCLIP_image_interpolation\n",
        "    if not status['installed_diffusers']:\n",
        "      alert_msg(page, \"You must Install the HuggingFace Diffusers Library first... \")\n",
        "      return \n",
        "    def prt(line, update=True):\n",
        "      if type(line) == str:\n",
        "        line = Text(line)\n",
        "      if from_list:\n",
        "        page.imageColumn.controls.append(line)\n",
        "        if update:\n",
        "          page.imageColumn.update()\n",
        "      else:\n",
        "        page.UnCLIP_ImageInterpolation.controls.append(line)\n",
        "        if update:\n",
        "          page.UnCLIP_ImageInterpolation.update()\n",
        "    def clear_last():\n",
        "      if from_list:\n",
        "        del page.imageColumn.controls[-1]\n",
        "        page.imageColumn.update()\n",
        "      else:\n",
        "        del page.UnCLIP_ImageInterpolation.controls[-1]\n",
        "        page.UnCLIP_ImageInterpolation.update()\n",
        "    def clear_list():\n",
        "      if from_list:\n",
        "        page.imageColumn.controls.clear()\n",
        "      else:\n",
        "        page.UnCLIP_ImageInterpolation.controls = page.UnCLIP_ImageInterpolation.controls[:1]\n",
        "    def autoscroll(scroll=True):\n",
        "      if from_list:\n",
        "        page.imageColumn.auto_scroll = scroll\n",
        "        page.imageColumn.update()\n",
        "        page.UnCLIP_ImageInterpolation.auto_scroll = scroll\n",
        "        page.UnCLIP_ImageInterpolation.update()\n",
        "      else:\n",
        "        page.UnCLIP_ImageInterpolation.auto_scroll = scroll\n",
        "        page.UnCLIP_ImageInterpolation.update()\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    total_steps = unCLIP_image_interpolation_prefs['decoder_num_inference_steps'] + unCLIP_image_interpolation_prefs['super_res_num_inference_steps']\n",
        "    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:\n",
        "      callback_fnc.has_been_called = True\n",
        "      nonlocal progress, total_steps\n",
        "      #total_steps = len(latents)\n",
        "      percent = (step +1)/ total_steps\n",
        "      progress.value = percent\n",
        "      progress.tooltip = f\"{step +1} / {total_steps}  Timestep: {timestep}\"\n",
        "      progress.update()\n",
        "    unCLIP_image_interpolation_inits = []\n",
        "    if from_list:\n",
        "      if len(prompts) < 1:\n",
        "        alert_msg(page, \"You need to add Prompts to your List first... \")\n",
        "        return\n",
        "      for p in prompts:\n",
        "        if bool(p['init_image']) and bool(p['mask_image']):\n",
        "          unCLIP_image_interpolation_inits.append({'init_image':p['init_image'], 'end_image':p['mask_image']})\n",
        "    else:\n",
        "      if not bool(unCLIP_image_interpolation_prefs['init_image']) or not bool(unCLIP_image_interpolation_prefs['end_image']):\n",
        "        alert_msg(page, \"You need to add a Initial and Ending Image first... \")\n",
        "        return\n",
        "      unCLIP_image_interpolation_inits.append({'init_image':unCLIP_image_interpolation_prefs['init_image'], 'end_image':unCLIP_image_interpolation_prefs['end_image']})\n",
        "    autoscroll(True)\n",
        "    clear_list()\n",
        "    from io import BytesIO\n",
        "    from PIL.PngImagePlugin import PngInfo\n",
        "    from PIL import ImageOps\n",
        "    if from_list:\n",
        "      page.tabs.selected_index = 4\n",
        "      page.tabs.update()\n",
        "    clear_pipes('unCLIP_image_interpolation')\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.reset_max_memory_allocated()\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "    dtype = torch.float16 if not prefs['higher_vram_mode'] else torch.float32 if torch.cuda.is_available() else torch.bfloat16\n",
        "    if pipe_unCLIP_image_interpolation == None:\n",
        "        from diffusers import DiffusionPipeline\n",
        "        prt(Installing(\"Downloading unCLIP Image Interpolation Kakaobrain Karlo Pipeline... It's a big one, see console for progress.\"))\n",
        "        try:\n",
        "            pipe_unCLIP_image_interpolation = DiffusionPipeline.from_pretrained(\"kakaobrain/karlo-v1-alpha-image-variations\", custom_pipeline=\"unclip_image_interpolation\", torch_dtype=dtype, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)\n",
        "            #pipe_unCLIP_image_interpolation.to(torch_device)\n",
        "            pipe_unCLIP_image_interpolation = optimize_pipe(pipe_unCLIP_image_interpolation)\n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            alert_msg(page, \"Error Downloading unCLIP Image Interpolation Pipeline\", content=Text(str(e)))\n",
        "            return\n",
        "        pipe_unCLIP_image_interpolation.set_progress_bar_config(disable=True)\n",
        "        clear_last()\n",
        "    s = \"s\" if unCLIP_image_interpolation_prefs['num_images'] > 1 else \"\"\n",
        "    prt(f\"Generating unCLIP Image Interpolation{s} of your Image...\")\n",
        "    batch_output = os.path.join(stable_dir, unCLIP_image_interpolation_prefs['batch_folder_name'])\n",
        "    if not os.path.isdir(batch_output):\n",
        "      os.makedirs(batch_output)\n",
        "    batch_output = os.path.join(prefs['image_output'], unCLIP_image_interpolation_prefs['batch_folder_name'])\n",
        "    if not os.path.isdir(batch_output):\n",
        "      os.makedirs(batch_output)\n",
        "    for interpolation_images in unCLIP_image_interpolation_inits:\n",
        "        if interpolation_images['init_image'].startswith('http'):\n",
        "          init_img = PILImage.open(requests.get(interpolation_images['init_image'], stream=True).raw)\n",
        "        else:\n",
        "          if os.path.isfile(interpolation_images['init_image']):\n",
        "            init_img = PILImage.open(interpolation_images['init_image'])\n",
        "          else:\n",
        "            alert_msg(page, f\"ERROR: Couldn't find your init_image {interpolation_images['init_image']}\")\n",
        "            return\n",
        "        width, height = init_img.size\n",
        "        width, height = scale_dimensions(width, height, unCLIP_image_interpolation_prefs['max_size'])\n",
        "        init_img = init_img.resize((width, height), resample=PILImage.BICUBIC)\n",
        "        init_img = ImageOps.exif_transpose(init_img).convert(\"RGB\")\n",
        "        \n",
        "        if interpolation_images['end_image'].startswith('http'):\n",
        "          end_img = PILImage.open(requests.get(interpolation_images['end_image'], stream=True).raw)\n",
        "        else:\n",
        "          if os.path.isfile(interpolation_images['end_image']):\n",
        "            end_img = PILImage.open(interpolation_images['end_image'])\n",
        "          else:\n",
        "            alert_msg(page, f\"ERROR: Couldn't find your init_end_imageimage {interpolation_images['end_image']}\")\n",
        "            return\n",
        "        width, height = end_img.size\n",
        "        width, height = scale_dimensions(width, height, unCLIP_image_interpolation_prefs['max_size'])\n",
        "        end_img = end_img.resize((width, height), resample=PILImage.BICUBIC)\n",
        "        end_img = ImageOps.exif_transpose(end_img).convert(\"RGB\")\n",
        "        prt(progress)\n",
        "        autoscroll(False)\n",
        "        num = 0\n",
        "        random_seed = (int(unCLIP_image_interpolation_prefs['seed']) + num) if int(unCLIP_image_interpolation_prefs['seed']) > 0 else rnd.randint(0,4294967295)\n",
        "        generator = torch.Generator(device=torch_device).manual_seed(random_seed)\n",
        "        try:\n",
        "            images = pipe_unCLIP_image_interpolation(image=[init_img, end_img], steps=unCLIP_image_interpolation_prefs['interpolation_steps'], decoder_num_inference_steps=unCLIP_image_interpolation_prefs['decoder_num_inference_steps'], super_res_num_inference_steps=unCLIP_image_interpolation_prefs['super_res_num_inference_steps'], decoder_guidance_scale=unCLIP_image_interpolation_prefs['decoder_guidance_scale'], num_images_per_prompt=unCLIP_image_interpolation_prefs['num_images'], generator=generator).images #, callback=callback_fnc, callback_steps=1\n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            alert_msg(page, \"Error running unCLIP Image Interpolation Pipeline\", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip(), selectable=True)]))\n",
        "            return\n",
        "        autoscroll(True)\n",
        "        clear_last()\n",
        "        #fname = format_filename(unCLIP_image_interpolation_prefs['file_name'])\n",
        "        fname_init = interpolation_images['init_image'].rpartition(slash)[2].rpartition('.')[0]\n",
        "        fname_end = interpolation_images['end_image'].rpartition(slash)[2].rpartition('.')[0]\n",
        "        fname = f\"{fname_init[:int(prefs['file_max_length']/2)]}-to-{fname_end[:int(prefs['file_max_length']/2)]}\"\n",
        "        if prefs['file_suffix_seed']: fname += f\"-{random_seed}\"\n",
        "        for image in images:\n",
        "            random_seed += num\n",
        "            image_path = available_file(os.path.join(stable_dir, unCLIP_image_interpolation_prefs['batch_folder_name']), fname, num)\n",
        "            unscaled_path = image_path\n",
        "            output_file = image_path.rpartition(slash)[2]\n",
        "            image.save(image_path)\n",
        "            out_path = image_path.rpartition(slash)[0]\n",
        "            upscaled_path = os.path.join(out_path, output_file)\n",
        "            if not unCLIP_image_interpolation_prefs['display_upscaled_image'] or not unCLIP_image_interpolation_prefs['apply_ESRGAN_upscale']:\n",
        "                prt(Row([ImageButton(src=unscaled_path, data=upscaled_path, width=512, height=512, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "                #prt(Row([Img(src=unscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "            if unCLIP_image_interpolation_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:\n",
        "                os.chdir(os.path.join(dist_dir, 'Real-ESRGAN'))\n",
        "                upload_folder = 'upload'\n",
        "                result_folder = 'results'     \n",
        "                if os.path.isdir(upload_folder):\n",
        "                    shutil.rmtree(upload_folder)\n",
        "                if os.path.isdir(result_folder):\n",
        "                    shutil.rmtree(result_folder)\n",
        "                os.mkdir(upload_folder)\n",
        "                os.mkdir(result_folder)\n",
        "                short_name = f'{fname[:80]}-{num}.png'\n",
        "                dst_path = os.path.join(dist_dir, 'Real-ESRGAN', upload_folder, short_name)\n",
        "                #print(f'Moving {fpath} to {dst_path}')\n",
        "                #shutil.move(fpath, dst_path)\n",
        "                shutil.copy(image_path, dst_path)\n",
        "                #faceenhance = ' --face_enhance' if unCLIP_image_interpolation_prefs[\"face_enhance\"] else ''\n",
        "                faceenhance = ''\n",
        "                run_sp(f'python inference_realesrgan.py -n realesr-general-x4v3 -i upload --outscale {unCLIP_image_interpolation_prefs[\"enlarge_scale\"]}{faceenhance}', cwd=os.path.join(dist_dir, 'Real-ESRGAN'), realtime=False)\n",
        "                out_file = short_name.rpartition('.')[0] + '_out.png'\n",
        "                shutil.move(os.path.join(dist_dir, 'Real-ESRGAN', result_folder, out_file), upscaled_path)\n",
        "                image_path = upscaled_path\n",
        "                os.chdir(stable_dir)\n",
        "                if unCLIP_image_interpolation_prefs['display_upscaled_image']:\n",
        "                    time.sleep(0.6)\n",
        "                    prt(Row([ImageButton(src=upscaled_path, data=upscaled_path, width=512 * float(unCLIP_image_interpolation_prefs[\"enlarge_scale\"]), height=512 * float(unCLIP_image_interpolation_prefs[\"enlarge_scale\"]), page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "                    #prt(Row([Img(src=upscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "            if prefs['save_image_metadata']:\n",
        "                img = PILImage.open(image_path)\n",
        "                metadata = PngInfo()\n",
        "                metadata.add_text(\"artist\", prefs['meta_ArtistName'])\n",
        "                metadata.add_text(\"copyright\", prefs['meta_Copyright'])\n",
        "                metadata.add_text(\"software\", \"Stable Diffusion Deluxe\" + f\", upscaled {unCLIP_image_interpolation_prefs['enlarge_scale']}x with ESRGAN\" if unCLIP_image_interpolation_prefs['apply_ESRGAN_upscale'] else \"\")\n",
        "                metadata.add_text(\"pipeline\", \"unCLIP Image Interpolation\")\n",
        "                if prefs['save_config_in_metadata']:\n",
        "                    #metadata.add_text(\"title\", unCLIP_image_interpolation_prefs['file_name'])\n",
        "                    config_json = unCLIP_image_interpolation_prefs.copy()\n",
        "                    config_json['model_path'] = \"kakaobrain/karlo-v1-alpha-image-variations\"\n",
        "                    config_json['seed'] = random_seed\n",
        "                    del config_json['num_images']\n",
        "                    del config_json['display_upscaled_image']\n",
        "                    del config_json['batch_folder_name']\n",
        "                    del config_json['file_name']\n",
        "                    if not config_json['apply_ESRGAN_upscale']:\n",
        "                        del config_json['enlarge_scale']\n",
        "                        del config_json['apply_ESRGAN_upscale']\n",
        "                    metadata.add_text(\"config_json\", json.dumps(config_json, ensure_ascii=True, indent=4))\n",
        "                img.save(image_path, pnginfo=metadata)\n",
        "            #TODO: PyDrive\n",
        "            if storage_type == \"Colab Google Drive\":\n",
        "                new_file = available_file(os.path.join(prefs['image_output'], unCLIP_image_interpolation_prefs['batch_folder_name']), fname, num)\n",
        "                out_path = new_file\n",
        "                shutil.copy(image_path, new_file)\n",
        "            elif bool(prefs['image_output']):\n",
        "                new_file = available_file(os.path.join(prefs['image_output'], unCLIP_image_interpolation_prefs['batch_folder_name']), fname, num)\n",
        "                out_path = new_file\n",
        "                shutil.copy(image_path, new_file)\n",
        "            prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))\n",
        "            num +=1\n",
        "    autoscroll(False)\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "\n",
        "def run_magic_mix(page, from_list=False):\n",
        "    global magic_mix_prefs, pipe_magic_mix\n",
        "    if not status['installed_diffusers']:\n",
        "      alert_msg(page, \"You must Install the HuggingFace Diffusers Library first... \")\n",
        "      return\n",
        "    def prt(line, update=True):\n",
        "      if type(line) == str:\n",
        "        line = Text(line)\n",
        "      if from_list:\n",
        "        page.imageColumn.controls.append(line)\n",
        "        if update:\n",
        "          page.imageColumn.update()\n",
        "      else:\n",
        "        page.MagicMix.controls.append(line)\n",
        "        if update:\n",
        "          page.MagicMix.update()\n",
        "    def clear_last():\n",
        "      if from_list:\n",
        "        del page.imageColumn.controls[-1]\n",
        "        page.imageColumn.update()\n",
        "      else:\n",
        "        del page.MagicMix.controls[-1]\n",
        "        page.MagicMix.update()\n",
        "    def clear_list():\n",
        "      if from_list:\n",
        "        page.imageColumn.controls.clear()\n",
        "      else:\n",
        "        page.MagicMix.controls = page.MagicMix.controls[:1]\n",
        "    def autoscroll(scroll=True):\n",
        "      if from_list:\n",
        "        page.imageColumn.auto_scroll = scroll\n",
        "        page.imageColumn.update()\n",
        "        page.MagicMix.auto_scroll = scroll\n",
        "        page.MagicMix.update()\n",
        "      else:\n",
        "        page.MagicMix.auto_scroll = scroll\n",
        "        page.MagicMix.update()\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    total_steps = magic_mix_prefs['num_inference_steps']\n",
        "    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:\n",
        "      callback_fnc.has_been_called = True\n",
        "      nonlocal progress, total_steps\n",
        "      #total_steps = len(latents)\n",
        "      percent = (step +1)/ total_steps\n",
        "      progress.value = percent\n",
        "      progress.tooltip = f\"{step +1} / {total_steps}  Timestep: {timestep}\"\n",
        "      progress.update()\n",
        "    magic_mix_prompts = []\n",
        "    if from_list:\n",
        "      if len(prompts) < 1:\n",
        "        alert_msg(page, \"You need to add Prompts to your List first... \")\n",
        "        return\n",
        "      for p in prompts:\n",
        "        magic_mix_prompts.append(p.prompt)\n",
        "    else:\n",
        "      if not bool(magic_mix_prefs['prompt']):\n",
        "        alert_msg(page, \"You need to add a Text Prompt first... \")\n",
        "        return\n",
        "      magic_mix_prompts.append(magic_mix_prefs['prompt'])\n",
        "    if from_list:\n",
        "      page.tabs.selected_index = 4\n",
        "      page.tabs.update()\n",
        "    else:\n",
        "      clear_list()\n",
        "    autoscroll(True)\n",
        "    from io import BytesIO\n",
        "    from PIL.PngImagePlugin import PngInfo\n",
        "    from PIL import ImageOps\n",
        "    if magic_mix_prefs['init_image'].startswith('http'):\n",
        "      init_img = PILImage.open(requests.get(magic_mix_prefs['init_image'], stream=True).raw)\n",
        "    else:\n",
        "      if os.path.isfile(magic_mix_prefs['init_image']):\n",
        "        init_img = PILImage.open(magic_mix_prefs['init_image'])\n",
        "      else:\n",
        "        alert_msg(page, f\"ERROR: Couldn't find your init_image {magic_mix_prefs['init_image']}\")\n",
        "        return\n",
        "    width, height = init_img.size\n",
        "    width, height = scale_dimensions(width, height, magic_mix_prefs['max_size'])\n",
        "    init_img = init_img.resize((width, height), resample=PILImage.BICUBIC)\n",
        "    init_img = ImageOps.exif_transpose(init_img).convert(\"RGB\")\n",
        "    '''tform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Resize(\n",
        "            (width, height),\n",
        "            interpolation=transforms.InterpolationMode.BICUBIC,\n",
        "            antialias=False,\n",
        "            ),\n",
        "        transforms.Normalize(\n",
        "          [0.48145466, 0.4578275, 0.40821073],\n",
        "          [0.26862954, 0.26130258, 0.27577711]),\n",
        "    ])\n",
        "    init_img = tform(init_img).to(torch_device)'''\n",
        "    clear_pipes('magic_mix')\n",
        "    #torch.cuda.empty_cache()\n",
        "    #torch.cuda.reset_max_memory_allocated()\n",
        "    #torch.cuda.reset_peak_memory_stats()\n",
        "    model = get_model(prefs['model_ckpt'])['path']\n",
        "    scheduler_mode = magic_mix_prefs['scheduler_mode']\n",
        "    if scheduler_mode == \"LMS Discrete\":\n",
        "      from diffusers import LMSDiscreteScheduler\n",
        "      schedule = LMSDiscreteScheduler.from_pretrained(model, subfolder=\"scheduler\")\n",
        "    elif scheduler_mode == \"PNDM\":\n",
        "      from diffusers import PNDMScheduler\n",
        "      schedule = PNDMScheduler.from_pretrained(model, subfolder=\"scheduler\")\n",
        "    elif scheduler_mode == \"DDIM\":\n",
        "      from diffusers import DDIMScheduler\n",
        "      schedule = DDIMScheduler.from_pretrained(model, subfolder=\"scheduler\")\n",
        "    if pipe_magic_mix == None or magic_mix_prefs['scheduler_mode'] != magic_mix_prefs['scheduler_last']:\n",
        "        from diffusers import DiffusionPipeline\n",
        "        prt(Installing(\"Downloading MagicMix Pipeline... \"))\n",
        "        try:\n",
        "            pipe_magic_mix = DiffusionPipeline.from_pretrained(model, custom_pipeline=\"AlanB/magic_mix_mod\", scheduler=schedule, safety_checker=None, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)\n",
        "            #pipe_magic_mix.to(torch_device)\n",
        "            pipe_magic_mix = optimize_pipe(pipe_magic_mix)\n",
        "            magic_mix_prefs['scheduler_last'] = magic_mix_prefs['scheduler_mode']\n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            alert_msg(page, \"Error Downloading MagicMix Pipeline\", content=Text(str(e)))\n",
        "            return\n",
        "        #pipe_magic_mix.set_progress_bar_config(disable=True)\n",
        "        clear_last()\n",
        "    s = \"es\" if magic_mix_prefs['num_images'] > 1 else \"\"\n",
        "    prt(f\"Generating MagicMix{s} of your Image...\")\n",
        "    batch_output = os.path.join(stable_dir, magic_mix_prefs['batch_folder_name'])\n",
        "    if not os.path.isdir(batch_output):\n",
        "      os.makedirs(batch_output)\n",
        "    batch_output = os.path.join(prefs['image_output'], magic_mix_prefs['batch_folder_name'])\n",
        "    if not os.path.isdir(batch_output):\n",
        "      os.makedirs(batch_output)\n",
        "    for pr in magic_mix_prompts:\n",
        "        for num in range(magic_mix_prefs['num_images']):\n",
        "            prt(progress)\n",
        "            autoscroll(False)\n",
        "            random_seed = (int(magic_mix_prefs['seed']) + num) if int(magic_mix_prefs['seed']) > 0 else rnd.randint(0,4294967295)\n",
        "            #generator = torch.Generator(device=torch_device).manual_seed(random_seed)\n",
        "            try:\n",
        "                image = pipe_magic_mix(img=init_img, prompt=pr, steps=magic_mix_prefs['num_inference_steps'], kmin=magic_mix_prefs['kmin'], kmax=magic_mix_prefs['kmax'], mix_factor=magic_mix_prefs['mix_factor'], guidance_scale=magic_mix_prefs['guidance_scale'], seed=random_seed, callback=callback_fnc, callback_steps=1)#.images \n",
        "            except Exception as e:\n",
        "                clear_last()\n",
        "                alert_msg(page, \"Error running MagicMix Pipeline\", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))\n",
        "                return\n",
        "            autoscroll(True)\n",
        "            clear_last()\n",
        "            fname = format_filename(pr)\n",
        "\n",
        "            if prefs['file_suffix_seed']: fname += f\"-{random_seed}\"\n",
        "            #for image in images:\n",
        "            image_path = available_file(os.path.join(stable_dir, magic_mix_prefs['batch_folder_name']), fname, num)\n",
        "            unscaled_path = image_path\n",
        "            output_file = image_path.rpartition(slash)[2]\n",
        "            image.save(image_path)\n",
        "            out_path = image_path.rpartition(slash)[0]\n",
        "            upscaled_path = os.path.join(out_path, output_file)\n",
        "            if not magic_mix_prefs['display_upscaled_image'] or not magic_mix_prefs['apply_ESRGAN_upscale']:\n",
        "                prt(Row([ImageButton(src=unscaled_path, data=upscaled_path, width=width, height=height, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "                #prt(Row([Img(src=unscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "            if magic_mix_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:\n",
        "                os.chdir(os.path.join(dist_dir, 'Real-ESRGAN'))\n",
        "                upload_folder = 'upload'\n",
        "                result_folder = 'results'     \n",
        "                if os.path.isdir(upload_folder):\n",
        "                    shutil.rmtree(upload_folder)\n",
        "                if os.path.isdir(result_folder):\n",
        "                    shutil.rmtree(result_folder)\n",
        "                os.mkdir(upload_folder)\n",
        "                os.mkdir(result_folder)\n",
        "                short_name = f'{fname[:80]}-{num}.png'\n",
        "                dst_path = os.path.join(dist_dir, 'Real-ESRGAN', upload_folder, short_name)\n",
        "                #print(f'Moving {fpath} to {dst_path}')\n",
        "                #shutil.move(fpath, dst_path)\n",
        "                shutil.copy(image_path, dst_path)\n",
        "                #faceenhance = ' --face_enhance' if magic_mix_prefs[\"face_enhance\"] else ''\n",
        "                faceenhance = ''\n",
        "                run_sp(f'python inference_realesrgan.py -n realesr-general-x4v3 -i upload --outscale {magic_mix_prefs[\"enlarge_scale\"]}{faceenhance}', cwd=os.path.join(dist_dir, 'Real-ESRGAN'), realtime=False)\n",
        "                out_file = short_name.rpartition('.')[0] + '_out.png'\n",
        "                shutil.move(os.path.join(dist_dir, 'Real-ESRGAN', result_folder, out_file), upscaled_path)\n",
        "                image_path = upscaled_path\n",
        "                os.chdir(stable_dir)\n",
        "                if magic_mix_prefs['display_upscaled_image']:\n",
        "                    time.sleep(0.6)\n",
        "                    prt(Row([ImageButton(src=upscaled_path, data=upscaled_path, width=width * float(magic_mix_prefs[\"enlarge_scale\"]), height=height * float(magic_mix_prefs[\"enlarge_scale\"]), page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "                    #prt(Row([Img(src=upscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "            if prefs['save_image_metadata']:\n",
        "                img = PILImage.open(image_path)\n",
        "                metadata = PngInfo()\n",
        "                metadata.add_text(\"artist\", prefs['meta_ArtistName'])\n",
        "                metadata.add_text(\"copyright\", prefs['meta_Copyright'])\n",
        "                metadata.add_text(\"software\", \"Stable Diffusion Deluxe\" + f\", upscaled {magic_mix_prefs['enlarge_scale']}x with ESRGAN\" if magic_mix_prefs['apply_ESRGAN_upscale'] else \"\")\n",
        "                metadata.add_text(\"pipeline\", \"magic_mix\")\n",
        "                if prefs['save_config_in_metadata']:\n",
        "                  metadata.add_text(\"title\", pr)\n",
        "                  config_json = magic_mix_prefs.copy()\n",
        "                  config_json['model_path'] = model\n",
        "                  config_json['seed'] = random_seed\n",
        "                  del config_json['num_images']\n",
        "                  del config_json['display_upscaled_image']\n",
        "                  del config_json['batch_folder_name']\n",
        "                  del config_json['file_name']\n",
        "                  del config_json[\"scheduler_last\"]\n",
        "                  del config_json['max_size']\n",
        "                  if not config_json['apply_ESRGAN_upscale']:\n",
        "                    del config_json['enlarge_scale']\n",
        "                    del config_json['apply_ESRGAN_upscale']\n",
        "                  metadata.add_text(\"config_json\", json.dumps(config_json, ensure_ascii=True, indent=4))\n",
        "                img.save(image_path, pnginfo=metadata)\n",
        "            #TODO: PyDrive\n",
        "            if storage_type == \"Colab Google Drive\":\n",
        "                new_file = available_file(os.path.join(prefs['image_output'], magic_mix_prefs['batch_folder_name']), fname, num)\n",
        "                out_path = new_file\n",
        "                shutil.copy(image_path, new_file)\n",
        "            elif bool(prefs['image_output']):\n",
        "                new_file = available_file(os.path.join(prefs['image_output'], magic_mix_prefs['batch_folder_name']), fname, num)\n",
        "                out_path = new_file\n",
        "                shutil.copy(image_path, new_file)\n",
        "            time.sleep(0.2)\n",
        "            prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))\n",
        "    autoscroll(False)\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "def run_paint_by_example(page):\n",
        "    global paint_by_example_prefs, prefs, status, pipe_paint_by_example\n",
        "    if not status['installed_diffusers']:\n",
        "      alert_msg(page, \"You need to Install HuggingFace Diffusers before using...\")\n",
        "      return\n",
        "    if not bool(paint_by_example_prefs['original_image']) or (not bool(paint_by_example_prefs['alpha_mask']) and not bool(paint_by_example_prefs['mask_image'])):\n",
        "      alert_msg(page, \"You must provide the Original Image and the Mask Image to process...\")\n",
        "      return\n",
        "    if not bool(paint_by_example_prefs['example_image']):\n",
        "      alert_msg(page, \"You must provide an Example Image to Transfer Subject from...\")\n",
        "      return\n",
        "    def prt(line):\n",
        "      if type(line) == str:\n",
        "        line = Text(line, size=17)\n",
        "      page.PaintByExample.controls.append(line)\n",
        "      page.PaintByExample.update()\n",
        "    def clear_last():\n",
        "      del page.PaintByExample.controls[-1]\n",
        "      page.PaintByExample.update()\n",
        "    def autoscroll(scroll=True):\n",
        "      page.PaintByExample.auto_scroll = scroll\n",
        "      page.PaintByExample.update()\n",
        "    def clear_list():\n",
        "      page.PaintByExample.controls = page.PaintByExample.controls[:1]\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    total_steps = paint_by_example_prefs['num_inference_steps']\n",
        "    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:\n",
        "      callback_fnc.has_been_called = True\n",
        "      nonlocal progress, total_steps\n",
        "      #total_steps = len(latents)\n",
        "      percent = (step +1)/ total_steps\n",
        "      progress.value = percent\n",
        "      progress.tooltip = f\"{step +1} / {total_steps}  Timestep: {timestep}\"\n",
        "      progress.update()\n",
        "      #print(f'{type(latents)} {len(latents)}- {str(latents)}')\n",
        "    autoscroll(True)\n",
        "    clear_list()\n",
        "    prt(Installing(\"Installing Paint-by-Example Pipeline...\"))\n",
        "    import requests, random\n",
        "    from io import BytesIO\n",
        "    from PIL import ImageOps\n",
        "    from PIL.PngImagePlugin import PngInfo\n",
        "    if paint_by_example_prefs['original_image'].startswith('http'):\n",
        "      #response = requests.get(paint_by_example_prefs['original_image'])\n",
        "      #original_img = PILImage.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "      original_img = PILImage.open(requests.get(paint_by_example_prefs['original_image'], stream=True).raw)\n",
        "    else:\n",
        "      if os.path.isfile(paint_by_example_prefs['original_image']):\n",
        "        original_img = PILImage.open(paint_by_example_prefs['original_image'])\n",
        "      else:\n",
        "        alert_msg(page, f\"ERROR: Couldn't find your original_image {paint_by_example_prefs['original_image']}\")\n",
        "        return\n",
        "    width, height = original_img.size\n",
        "    width, height = scale_dimensions(width, height, paint_by_example_prefs['max_size'])\n",
        "    if bool(paint_by_example_prefs['alpha_mask']):\n",
        "      original_img = ImageOps.exif_transpose(original_img).convert(\"RGBA\")\n",
        "    else:\n",
        "      original_img = ImageOps.exif_transpose(original_img).convert(\"RGB\")\n",
        "    original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS)\n",
        "    mask_img = None\n",
        "    if not bool(paint_by_example_prefs['mask_image']) and bool(paint_by_example_prefs['alpha_mask']):\n",
        "      red, green, blue, alpha = PILImage.Image.split(original_img)\n",
        "      #mask_img = ImageOps.invert(alpha.convert('RGB'))\n",
        "      mask_img = alpha.convert('L')\n",
        "    else:\n",
        "      if paint_by_example_prefs['mask_image'].startswith('http'):\n",
        "        mask_img = PILImage.open(requests.get(paint_by_example_prefs['mask_image'], stream=True).raw)\n",
        "      else:\n",
        "        if os.path.isfile(paint_by_example_prefs['mask_image']):\n",
        "          mask_img = PILImage.open(paint_by_example_prefs['mask_image'])\n",
        "        else:\n",
        "          alert_msg(page, f\"ERROR: Couldn't find your mask_image {paint_by_example_prefs['mask_image']}\")\n",
        "          return\n",
        "      if paint_by_example_prefs['invert_mask']:\n",
        "        mask_img = ImageOps.invert(mask_img.convert('RGB'))\n",
        "    #mask_img = mask_img.convert(\"L\")\n",
        "    #mask_img = mask_img.convert(\"1\")\n",
        "    mask_img = mask_img.resize((width, height), resample=PILImage.NEAREST)\n",
        "    mask_img = ImageOps.exif_transpose(mask_img).convert(\"RGB\")\n",
        "    #print(f'Resize to {width}x{height}')\n",
        "    if paint_by_example_prefs['example_image'].startswith('http'):\n",
        "      example_img = PILImage.open(requests.get(paint_by_example_prefs['example_image'], stream=True).raw)\n",
        "    else:\n",
        "      if os.path.isfile(paint_by_example_prefs['example_image']):\n",
        "        example_img = PILImage.open(paint_by_example_prefs['example_image'])\n",
        "      else:\n",
        "        alert_msg(page, f\"ERROR: Couldn't find your Example Image {paint_by_example_prefs['example_image']}\")\n",
        "        return\n",
        "    \n",
        "    clear_pipes('paint_by_example')\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.reset_max_memory_allocated()\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "    model_id = \"Fantasy-Studio/Paint-by-Example\"\n",
        "    if pipe_paint_by_example is None:\n",
        "      from diffusers import PaintByExamplePipeline, PNDMScheduler\n",
        "      PNDMscheduler = PNDMScheduler(skip_prk_steps=True)\n",
        "      pipe_paint_by_example = PaintByExamplePipeline.from_pretrained(model_id, scheduler=PNDMscheduler, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)\n",
        "      pipe_paint_by_example = pipe_paint_by_example.to(\"cpu\")\n",
        "    clear_last()\n",
        "    prt(\"Generating Paint-by-Example of your Image... (slow because it uses CPU instead of GPU)\")\n",
        "    prt(progress)\n",
        "    autoscroll(False)\n",
        "    batch_output = os.path.join(stable_dir, paint_by_example_prefs['batch_folder_name'])\n",
        "    if not os.path.isdir(batch_output):\n",
        "      os.makedirs(batch_output)\n",
        "    batch_output = os.path.join(prefs['image_output'], paint_by_example_prefs['batch_folder_name'])\n",
        "    if not os.path.isdir(batch_output):\n",
        "      os.makedirs(batch_output)\n",
        "    random_seed = int(paint_by_example_prefs['seed']) if int(paint_by_example_prefs['seed']) > 0 else random.randint(0,4294967295)\n",
        "    generator = torch.Generator(device=\"cpu\").manual_seed(random_seed)\n",
        "    #generator = torch.manual_seed(random_seed)\n",
        "    try:\n",
        "      images = pipe_paint_by_example(image=original_img, mask_image=mask_img, example_image=example_img, num_inference_steps=paint_by_example_prefs['num_inference_steps'], eta=paint_by_example_prefs['eta'], guidance_scale=paint_by_example_prefs['guidance_scale'], num_images_per_prompt=paint_by_example_prefs['num_images'], generator=generator, callback=callback_fnc, callback_steps=1).images\n",
        "    except Exception as e:\n",
        "      clear_last()\n",
        "      alert_msg(page, f\"ERROR: Couldn't Paint-by-Example your image for some reason.  Possibly out of memory or something wrong with my code...\", content=Text(str(e)))\n",
        "      return\n",
        "    clear_last()\n",
        "    clear_last()\n",
        "    autoscroll(True)\n",
        "    filename = paint_by_example_prefs['original_image'].rpartition(slash)[2].rpartition('.')[0]\n",
        "    #if prefs['file_suffix_seed']: fname += f\"-{random_seed}\"\n",
        "    num = 0\n",
        "    for image in images:\n",
        "        random_seed += num\n",
        "        fname = filename + (f\"-{random_seed}\" if prefs['file_suffix_seed'] else \"\")\n",
        "        image_path = available_file(os.path.join(stable_dir, paint_by_example_prefs['batch_folder_name']), fname, num)\n",
        "        unscaled_path = image_path\n",
        "        output_file = image_path.rpartition(slash)[2]\n",
        "        image.save(image_path)\n",
        "        out_path = image_path.rpartition(slash)[0]\n",
        "        upscaled_path = os.path.join(out_path, output_file)\n",
        "        if not paint_by_example_prefs['display_upscaled_image'] or not paint_by_example_prefs['apply_ESRGAN_upscale']:\n",
        "            prt(Row([ImageButton(src=unscaled_path, data=upscaled_path, width=width, height=height, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "            #prt(Row([Img(src=unscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "        if paint_by_example_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:\n",
        "            os.chdir(os.path.join(dist_dir, 'Real-ESRGAN'))\n",
        "            upload_folder = 'upload'\n",
        "            result_folder = 'results'     \n",
        "            if os.path.isdir(upload_folder):\n",
        "                shutil.rmtree(upload_folder)\n",
        "            if os.path.isdir(result_folder):\n",
        "                shutil.rmtree(result_folder)\n",
        "            os.mkdir(upload_folder)\n",
        "            os.mkdir(result_folder)\n",
        "            short_name = f'{fname[:80]}-{num}.png'\n",
        "            dst_path = os.path.join(dist_dir, 'Real-ESRGAN', upload_folder, short_name)\n",
        "            #print(f'Moving {fpath} to {dst_path}')\n",
        "            #shutil.move(fpath, dst_path)\n",
        "            shutil.copy(image_path, dst_path)\n",
        "            #faceenhance = ' --face_enhance' if paint_by_example_prefs[\"face_enhance\"] else ''\n",
        "            faceenhance = ''\n",
        "            run_sp(f'python inference_realesrgan.py -n realesr-general-x4v3 -i upload --outscale {paint_by_example_prefs[\"enlarge_scale\"]}{faceenhance}', cwd=os.path.join(dist_dir, 'Real-ESRGAN'), realtime=False)\n",
        "            out_file = short_name.rpartition('.')[0] + '_out.png'\n",
        "            shutil.move(os.path.join(dist_dir, 'Real-ESRGAN', result_folder, out_file), upscaled_path)\n",
        "            image_path = upscaled_path\n",
        "            os.chdir(stable_dir)\n",
        "            if paint_by_example_prefs['display_upscaled_image']:\n",
        "                time.sleep(0.6)\n",
        "                prt(Row([ImageButton(src=upscaled_path, data=upscaled_path, width=width * float(paint_by_example_prefs[\"enlarge_scale\"]), height=height * float(paint_by_example_prefs[\"enlarge_scale\"]), page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "                #prt(Row([Img(src=upscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "        if prefs['save_image_metadata']:\n",
        "            img = PILImage.open(image_path)\n",
        "            metadata = PngInfo()\n",
        "            metadata.add_text(\"artist\", prefs['meta_ArtistName'])\n",
        "            metadata.add_text(\"copyright\", prefs['meta_Copyright'])\n",
        "            metadata.add_text(\"software\", \"Stable Diffusion Deluxe\" + f\", upscaled {paint_by_example_prefs['enlarge_scale']}x with ESRGAN\" if paint_by_example_prefs['apply_ESRGAN_upscale'] else \"\")\n",
        "            metadata.add_text(\"pipeline\", \"Paint-by-Example\")\n",
        "            if prefs['save_config_in_metadata']:\n",
        "              config_json = paint_by_example_prefs.copy()\n",
        "              config_json['model_path'] = model_id\n",
        "              config_json['seed'] = random_seed\n",
        "              del config_json['num_images']\n",
        "              del config_json['max_size']\n",
        "              del config_json['display_upscaled_image']\n",
        "              del config_json['batch_folder_name']\n",
        "              del config_json['invert_mask']\n",
        "              del config_json['alpha_mask']\n",
        "              if not config_json['apply_ESRGAN_upscale']:\n",
        "                del config_json['enlarge_scale']\n",
        "                del config_json['apply_ESRGAN_upscale']\n",
        "              metadata.add_text(\"config_json\", json.dumps(config_json, ensure_ascii=True, indent=4))\n",
        "            img.save(image_path, pnginfo=metadata)\n",
        "        #TODO: PyDrive\n",
        "        if storage_type == \"Colab Google Drive\":\n",
        "            new_file = available_file(os.path.join(prefs['image_output'], paint_by_example_prefs['batch_folder_name']), fname, num)\n",
        "            out_path = new_file\n",
        "            shutil.copy(image_path, new_file)\n",
        "        elif bool(prefs['image_output']):\n",
        "            new_file = available_file(os.path.join(prefs['image_output'], paint_by_example_prefs['batch_folder_name']), fname, num)\n",
        "            out_path = new_file\n",
        "            shutil.copy(image_path, new_file)\n",
        "        time.sleep(0.2)\n",
        "        prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))\n",
        "        num += 1\n",
        "    autoscroll(True)\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "\n",
        "def run_instruct_pix2pix(page, from_list=False):\n",
        "    global instruct_pix2pix_prefs, prefs, status, pipe_instruct_pix2pix\n",
        "    if not status['installed_diffusers']:\n",
        "      alert_msg(page, \"You need to Install HuggingFace Diffusers before using...\")\n",
        "      return\n",
        "    if not bool(instruct_pix2pix_prefs['original_image']) and not instruct_pix2pix_prefs['use_init_video']:\n",
        "      alert_msg(page, \"You must provide the Original Image and the Mask Image to process...\")\n",
        "      return\n",
        "    if not bool(instruct_pix2pix_prefs['init_video']) and instruct_pix2pix_prefs['use_init_video']:\n",
        "      alert_msg(page, \"You must provide the Input Initial Video Clip to process...\")\n",
        "      return\n",
        "    if not bool(instruct_pix2pix_prefs['prompt']):\n",
        "      alert_msg(page, \"You must provide a Instructional Image Editing Prompt...\")\n",
        "      return\n",
        "    def prt(line, update=True):\n",
        "      if type(line) == str:\n",
        "        line = Text(line)\n",
        "      if from_list:\n",
        "        page.imageColumn.controls.append(line)\n",
        "        if update:\n",
        "          page.imageColumn.update()\n",
        "      else:\n",
        "        page.InstructPix2Pix.controls.append(line)\n",
        "        if update:\n",
        "          page.InstructPix2Pix.update()\n",
        "    def clear_last():\n",
        "      if from_list:\n",
        "        if len(page.imageColumn.controls) == 0: return\n",
        "        del page.imageColumn.controls[-1]\n",
        "        page.imageColumn.update()\n",
        "      else:\n",
        "        if len(page.InstructPix2Pix.controls) == 1: return\n",
        "        del page.InstructPix2Pix.controls[-1]\n",
        "        page.InstructPix2Pix.update()\n",
        "    def autoscroll(scroll=True):\n",
        "      if from_list:\n",
        "        page.imageColumn.auto_scroll = scroll\n",
        "        page.imageColumn.update()\n",
        "      else:\n",
        "        page.InstructPix2Pix.auto_scroll = scroll\n",
        "        page.InstructPix2Pix.update()\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    total_steps = instruct_pix2pix_prefs['num_inference_steps']\n",
        "    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:\n",
        "      callback_fnc.has_been_called = True\n",
        "      nonlocal progress, total_steps\n",
        "      #total_steps = len(latents)\n",
        "      percent = (step +1)/ total_steps\n",
        "      progress.value = percent\n",
        "      progress.tooltip = f\"{step +1} / {total_steps}  Timestep: {timestep}\"\n",
        "      progress.update()\n",
        "      #print(f'{type(latents)} {len(latents)}- {str(latents)}')\n",
        "    def clear_list():\n",
        "      if from_list:\n",
        "        page.imageColumn.controls.clear()\n",
        "      else:\n",
        "        page.InstructPix2Pix.controls = page.InstructPix2Pix.controls[:1]\n",
        "    instruct_pix2pix_prompts = []\n",
        "    if from_list:\n",
        "      if len(prompts) < 1:\n",
        "        alert_msg(page, \"You need to add Prompts to your List first... \")\n",
        "        return\n",
        "      for p in prompts:\n",
        "        instruct = {'prompt': p.prompt, 'negative_prompt': p['negative_prompt'], 'original_image': p['init_image'] if bool(p['init_image']) else instruct_pix2pix_prefs['original_image'], 'seed': p['seed']}\n",
        "        instruct_pix2pix_prompts.append(instruct)\n",
        "      page.tabs.selected_index = 4\n",
        "      page.tabs.update()\n",
        "    else:\n",
        "      if not bool(instruct_pix2pix_prefs['prompt']):\n",
        "        alert_msg(page, \"You need to add a Text Prompt first... \")\n",
        "        return\n",
        "      instruct = {'prompt':instruct_pix2pix_prefs['prompt'], 'negative_prompt': instruct_pix2pix_prefs['negative_prompt'], 'original_image': instruct_pix2pix_prefs['original_image'], 'seed': instruct_pix2pix_prefs['seed']}\n",
        "      instruct_pix2pix_prompts.append(instruct)\n",
        "    autoscroll(True)\n",
        "    clear_list()\n",
        "    prt(Divider(thickness=2, height=4))\n",
        "    prt(Installing(\"Installing Instruct-Pix2Pix Pipeline...\"))\n",
        "    import requests, random\n",
        "    from io import BytesIO\n",
        "    from PIL import ImageOps\n",
        "    from PIL.PngImagePlugin import PngInfo\n",
        "    \n",
        "    clear_pipes('instruct_pix2pix')\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.reset_max_memory_allocated()\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "    model_id = \"timbrooks/instruct-pix2pix\"\n",
        "    if pipe_instruct_pix2pix is None:\n",
        "      from diffusers import StableDiffusionInstructPix2PixPipeline\n",
        "      from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker\n",
        "      pipe_instruct_pix2pix = StableDiffusionInstructPix2PixPipeline.from_pretrained(model_id, torch_dtype=torch.float16, safety_checker=None if prefs['disable_nsfw_filter'] else StableDiffusionSafetyChecker.from_pretrained(\"CompVis/stable-diffusion-safety-checker\").to(torch_device), requires_safety_checker=not prefs['disable_nsfw_filter'], cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)\n",
        "      pipe_instruct_pix2pix = optimize_pipe(pipe_instruct_pix2pix)\n",
        "      #pipe_instruct_pix2pix = pipe_instruct_pix2pix.to(torch_device)\n",
        "    pipeline_scheduler(pipe_instruct_pix2pix)\n",
        "    clear_last()\n",
        "    prt(\"Generating Instruct-Pix2Pix of your Image...\")\n",
        "    prt(progress)\n",
        "    max_size = instruct_pix2pix_prefs['max_size']\n",
        "    batch_output = os.path.join(stable_dir, instruct_pix2pix_prefs['batch_folder_name'])\n",
        "    output_dir = batch_output\n",
        "    if not os.path.isdir(batch_output):\n",
        "      os.makedirs(batch_output)\n",
        "    batch_output = os.path.join(prefs['image_output'], instruct_pix2pix_prefs['batch_folder_name'])\n",
        "    if not os.path.isdir(batch_output):\n",
        "      os.makedirs(batch_output)\n",
        "    if instruct_pix2pix_prefs['use_init_video']:\n",
        "        init_vid = instruct_pix2pix_prefs['init_video']\n",
        "        try:\n",
        "            start_time = float(instruct_pix2pix_prefs['start_time'])\n",
        "            end_time = float(instruct_pix2pix_prefs['end_time'])\n",
        "            fps = int(instruct_pix2pix_prefs['fps'])\n",
        "        except Exception:\n",
        "            alert_msg(page, \"Make sure your Numbers are actual numbers...\")\n",
        "            return\n",
        "        if init_vid.startswith('http'):\n",
        "            init_vid = download_file(init_vid, output_dir)\n",
        "        else:\n",
        "            if not os.path.isfile(init_vid):\n",
        "              alert_msg(page, f\"ERROR: Couldn't find your init_video {init_vid}\")\n",
        "              return\n",
        "        prt(\"Extracting Frames from Video Clip\")\n",
        "        try:\n",
        "            import cv2\n",
        "        except ModuleNotFoundError:\n",
        "            run_process(\"pip install -q opencv-contrib-python\", page=page)\n",
        "            import cv2\n",
        "            pass\n",
        "        try:\n",
        "            cap = cv2.VideoCapture(init_vid)\n",
        "        except Exception as e:\n",
        "            alert_msg(page, \"ERROR Reading Video File. May be Incompatible Format...\")\n",
        "            clear_last()\n",
        "            return\n",
        "        count = 0\n",
        "        video = []\n",
        "        frames = []\n",
        "        width = height = 0\n",
        "        cap.set(cv2.CAP_PROP_FPS, fps)\n",
        "        video_length = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
        "        start_frame = int(start_time * fps)\n",
        "        if end_time == 0 or end_time == 0.0:\n",
        "            end_frame = int(video_length)\n",
        "        else:\n",
        "            end_frame = int(end_time * fps)\n",
        "        total = end_frame - start_frame\n",
        "        for i in range(start_frame, end_frame):\n",
        "            cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
        "            success, image = cap.read()\n",
        "            if success:\n",
        "                #filename = os.path.join(output_dir, f'{file_prefix}{count}.png')\n",
        "                if width == 0:\n",
        "                    shape = image.shape\n",
        "                    width, height = scale_dimensions(shape[1], shape[0], max=max_size, multiple=16)\n",
        "                image = cv2.resize(image, (width, height), interpolation = cv2.INTER_AREA)\n",
        "                #cv2.imwrite(os.path.join(output_dir, filename), image)\n",
        "                video.append(PILImage.fromarray(image))\n",
        "                count += 1\n",
        "        cap.release()\n",
        "        clear_last()\n",
        "        #reader = imageio.get_reader(instruct_pix2pix_prefs['init_video'], \"ffmpeg\")\n",
        "        #frame_count = instruct_pix2pix_prefs['fps'] #TODO: This isn't frame count, do it right\n",
        "        #video = [Image.fromarray(reader.get_data(i)) for i in range(frame_count)]\n",
        "    else: video = None\n",
        "    for pr in instruct_pix2pix_prompts:\n",
        "      if not instruct_pix2pix_prefs['use_init_video']:\n",
        "        if pr['original_image'].startswith('http'):\n",
        "          #response = requests.get(instruct_pix2pix_prefs['original_image'])\n",
        "          #original_img = PILImage.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "          original_img = PILImage.open(requests.get(pr['original_image'], stream=True).raw)\n",
        "        else:\n",
        "          if os.path.isfile(pr['original_image']):\n",
        "            original_img = PILImage.open(pr['original_image'])\n",
        "          else:\n",
        "            alert_msg(page, f\"ERROR: Couldn't find your original_image {pr['original_image']}\")\n",
        "            return\n",
        "        width, height = original_img.size\n",
        "        width, height = scale_dimensions(width, height, instruct_pix2pix_prefs['max_size'])\n",
        "        original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS)\n",
        "      for num in range(instruct_pix2pix_prefs['num_images']):\n",
        "        prt(progress)\n",
        "        random_seed = (int(pr['seed']) + num) if int(pr['seed']) > 0 else rnd.randint(0,4294967295)\n",
        "        generator = torch.Generator(device=torch_device).manual_seed(random_seed)\n",
        "        #generator = torch.manual_seed(random_seed)\n",
        "        try:\n",
        "          if instruct_pix2pix_prefs['use_init_video']:\n",
        "            from diffusers.pipelines.text_to_video_synthesis.pipeline_text_to_video_zero import CrossFrameAttnProcessor\n",
        "            pipe_instruct_pix2pix.unet.set_attn_processor(CrossFrameAttnProcessor(batch_size=3))\n",
        "            images = pipe_instruct_pix2pix([pr['prompt']] * len(video), image=video, negative_prompt=pr['negative_prompt'] if bool(pr['negative_prompt']) else None, num_inference_steps=instruct_pix2pix_prefs['num_inference_steps'], eta=instruct_pix2pix_prefs['eta'], image_guidance_scale=instruct_pix2pix_prefs['guidance_scale'], num_images_per_prompt=instruct_pix2pix_prefs['num_images'], generator=generator, callback=callback_fnc, callback_steps=1).images\n",
        "          else:\n",
        "            images = pipe_instruct_pix2pix(pr['prompt'], image=original_img, negative_prompt=pr['negative_prompt'] if bool(pr['negative_prompt']) else None, num_inference_steps=instruct_pix2pix_prefs['num_inference_steps'], eta=instruct_pix2pix_prefs['eta'], image_guidance_scale=instruct_pix2pix_prefs['guidance_scale'], num_images_per_prompt=instruct_pix2pix_prefs['num_images'], generator=generator, callback=callback_fnc, callback_steps=1).images\n",
        "        except Exception as e:\n",
        "          clear_last()\n",
        "          alert_msg(page, f\"ERROR: Couldn't run Instruct-Pix2Pix on your image for some reason.  Possibly out of memory or something wrong with my code...\", content=Text(str(e)))\n",
        "          gc.collect()\n",
        "          torch.cuda.empty_cache()\n",
        "          return\n",
        "        clear_last()\n",
        "        clear_last()\n",
        "        filename = pr['original_image'].rpartition(slash)[2].rpartition('.')[0]\n",
        "        filename = f\"-{format_filename(pr['prompt'])}\"\n",
        "        filename = filename[:int(prefs['file_max_length'])]\n",
        "        #if prefs['file_suffix_seed']: fname += f\"-{random_seed}\"\n",
        "        #num = 0\n",
        "        for image in images:\n",
        "            random_seed += num\n",
        "            fname = filename + (f\"-{random_seed}\" if prefs['file_suffix_seed'] else \"\")\n",
        "            image_path = available_file(os.path.join(stable_dir, instruct_pix2pix_prefs['batch_folder_name']), fname, num)\n",
        "            unscaled_path = image_path\n",
        "            output_file = image_path.rpartition(slash)[2]\n",
        "            image.save(image_path)\n",
        "            out_path = image_path.rpartition(slash)[0]\n",
        "            upscaled_path = os.path.join(out_path, output_file)\n",
        "            if not instruct_pix2pix_prefs['display_upscaled_image'] or not instruct_pix2pix_prefs['apply_ESRGAN_upscale']:\n",
        "                prt(Row([ImageButton(src=unscaled_path, data=upscaled_path, width=width, height=height, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "                #prt(Row([Img(src=unscaled_path, fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "            if instruct_pix2pix_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:\n",
        "                os.chdir(os.path.join(dist_dir, 'Real-ESRGAN'))\n",
        "                upload_folder = 'upload'\n",
        "                result_folder = 'results'     \n",
        "                if os.path.isdir(upload_folder):\n",
        "                    shutil.rmtree(upload_folder)\n",
        "                if os.path.isdir(result_folder):\n",
        "                    shutil.rmtree(result_folder)\n",
        "                os.mkdir(upload_folder)\n",
        "                os.mkdir(result_folder)\n",
        "                short_name = f'{fname[:80]}-{num}.png'\n",
        "                dst_path = os.path.join(dist_dir, 'Real-ESRGAN', upload_folder, short_name)\n",
        "                #print(f'Moving {fpath} to {dst_path}')\n",
        "                #shutil.move(fpath, dst_path)\n",
        "                shutil.copy(image_path, dst_path)\n",
        "                #faceenhance = ' --face_enhance' if instruct_pix2pix_prefs[\"face_enhance\"] else ''\n",
        "                faceenhance = ''\n",
        "                run_sp(f'python inference_realesrgan.py -n realesr-general-x4v3 -i upload --outscale {instruct_pix2pix_prefs[\"enlarge_scale\"]}{faceenhance}', cwd=os.path.join(dist_dir, 'Real-ESRGAN'), realtime=False)\n",
        "                out_file = short_name.rpartition('.')[0] + '_out.png'\n",
        "                shutil.move(os.path.join(dist_dir, 'Real-ESRGAN', result_folder, out_file), upscaled_path)\n",
        "                image_path = upscaled_path\n",
        "                os.chdir(stable_dir)\n",
        "                if instruct_pix2pix_prefs['display_upscaled_image']:\n",
        "                    time.sleep(0.6)\n",
        "                    prt(Row([ImageButton(src=upscaled_path, data=upscaled_path, width=width * float(instruct_pix2pix_prefs[\"enlarge_scale\"]), height=height * float(instruct_pix2pix_prefs[\"enlarge_scale\"]), page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "                    #prt(Row([Img(src=upscaled_path, fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "            if prefs['save_image_metadata']:\n",
        "                img = PILImage.open(image_path)\n",
        "                metadata = PngInfo()\n",
        "                metadata.add_text(\"artist\", prefs['meta_ArtistName'])\n",
        "                metadata.add_text(\"copyright\", prefs['meta_Copyright'])\n",
        "                metadata.add_text(\"software\", \"Stable Diffusion Deluxe\" + f\", upscaled {instruct_pix2pix_prefs['enlarge_scale']}x with ESRGAN\" if instruct_pix2pix_prefs['apply_ESRGAN_upscale'] else \"\")\n",
        "                metadata.add_text(\"pipeline\", \"Instruct-Pix2Pix\")\n",
        "                if prefs['save_config_in_metadata']:\n",
        "                  config_json = instruct_pix2pix_prefs.copy()\n",
        "                  config_json['model_path'] = model_id\n",
        "                  config_json['seed'] = random_seed\n",
        "                  config_json['prompt'] = pr['prompt']\n",
        "                  config_json['negative_prompt'] = pr['negative_prompt']\n",
        "                  del config_json['num_images']\n",
        "                  del config_json['max_size']\n",
        "                  del config_json['display_upscaled_image']\n",
        "                  del config_json['batch_folder_name']\n",
        "                  if not config_json['apply_ESRGAN_upscale']:\n",
        "                    del config_json['enlarge_scale']\n",
        "                    del config_json['apply_ESRGAN_upscale']\n",
        "                  metadata.add_text(\"config_json\", json.dumps(config_json, ensure_ascii=True, indent=4))\n",
        "                img.save(image_path, pnginfo=metadata)\n",
        "            #TODO: PyDrive\n",
        "            if storage_type == \"Colab Google Drive\":\n",
        "                new_file = available_file(os.path.join(prefs['image_output'], instruct_pix2pix_prefs['batch_folder_name']), fname, num)\n",
        "                out_path = new_file\n",
        "                shutil.copy(image_path, new_file)\n",
        "            elif bool(prefs['image_output']):\n",
        "                new_file = available_file(os.path.join(prefs['image_output'], instruct_pix2pix_prefs['batch_folder_name']), fname, num)\n",
        "                out_path = new_file\n",
        "                shutil.copy(image_path, new_file)\n",
        "            time.sleep(0.2)\n",
        "            prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))\n",
        "            #num += 1\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "def run_controlnet(page, from_list=False):\n",
        "    global controlnet_prefs, prefs, status, pipe_controlnet, controlnet, controlnet_models\n",
        "    if not status['installed_diffusers']:\n",
        "      alert_msg(page, \"You need to Install HuggingFace Diffusers before using...\")\n",
        "      return\n",
        "    if not bool(controlnet_prefs['original_image']) and len(controlnet_prefs['multi_controlnets']) == 0:\n",
        "      alert_msg(page, \"You must provide the Original Image to process...\")\n",
        "      return\n",
        "    if not bool(controlnet_prefs['prompt']) and not from_list:\n",
        "      alert_msg(page, \"You must provide a Prompt to paint in your image...\")\n",
        "      return\n",
        "    def prt(line, update=True):\n",
        "      if type(line) == str:\n",
        "        line = Text(line)\n",
        "      if from_list:\n",
        "        page.imageColumn.controls.append(line)\n",
        "        if update:\n",
        "          page.imageColumn.update()\n",
        "      else:\n",
        "        page.ControlNet.controls.append(line)\n",
        "        #page.controlnet_output.controls.append(line)\n",
        "        if update:\n",
        "          page.ControlNet.update()\n",
        "          #page.controlnet_output.update()\n",
        "    def clear_last():\n",
        "      if from_list:\n",
        "        if len(page.imageColumn.controls) == 0: return\n",
        "        del page.imageColumn.controls[-1]\n",
        "        page.imageColumn.update()\n",
        "      else:\n",
        "        if len(page.ControlNet.controls) == 1: return\n",
        "        del page.ControlNet.controls[-1]\n",
        "        page.ControlNet.update()\n",
        "        #if len(page.controlnet_output.controls) == 0: return\n",
        "        #del page.controlnet_output.controls[-1]\n",
        "        #page.controlnet_output.update()\n",
        "    def clear_list():\n",
        "      if from_list:\n",
        "        page.imageColumn.controls.clear()\n",
        "      else:\n",
        "        page.ControlNet.controls = page.ControlNet.controls[:1]\n",
        "    def autoscroll(scroll=True):\n",
        "      if from_list:\n",
        "        page.imageColumn.auto_scroll = scroll\n",
        "        page.imageColumn.update()\n",
        "      else:\n",
        "        page.ControlNet.auto_scroll = scroll\n",
        "        page.ControlNet.update()\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    total_steps = controlnet_prefs['steps']\n",
        "    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:\n",
        "      callback_fnc.has_been_called = True\n",
        "      nonlocal progress, total_steps\n",
        "      percent = (step +1)/ total_steps\n",
        "      progress.value = percent\n",
        "      progress.tooltip = f\"{step +1} / {total_steps}  Timestep: {timestep}\"\n",
        "      progress.update()\n",
        "    controlnet_prompts = []\n",
        "    if from_list:\n",
        "      if len(prompts) < 1:\n",
        "        alert_msg(page, \"You need to add Prompts to your List first... \")\n",
        "        return\n",
        "      for p in prompts:\n",
        "        control = {'prompt': p.prompt, 'negative_prompt': p['negative_prompt'] if bool(p['negative_prompt']) else controlnet_prefs['negative_prompt'], 'original_image': p['init_image'] if bool(p['init_image']) else controlnet_prefs['original_image'], 'conditioning_scale': controlnet_prefs['conditioning_scale'], 'seed': p['seed']}\n",
        "        controlnet_prompts.append(control)\n",
        "      page.tabs.selected_index = 4\n",
        "      page.tabs.update()\n",
        "      #page.controlnet_output.controls.clear()\n",
        "    else:\n",
        "      if not bool(controlnet_prefs['prompt']):\n",
        "        alert_msg(page, \"You need to add a Text Prompt first... \")\n",
        "        return\n",
        "      original = controlnet_prefs['original_image']\n",
        "      conditioning_scale = controlnet_prefs['conditioning_scale']\n",
        "      if len(controlnet_prefs['multi_controlnets']) > 0:\n",
        "        original = []\n",
        "        conditioning_scale = []\n",
        "        for c in controlnet_prefs['multi_controlnets']:\n",
        "          original.append(c['original_image'])\n",
        "          conditioning_scale.append(c['conditioning_scale'])\n",
        "      control = {'prompt':controlnet_prefs['prompt'], 'negative_prompt': controlnet_prefs['negative_prompt'], 'original_image': original, 'conditioning_scale': conditioning_scale, 'seed': controlnet_prefs['seed']}\n",
        "      if controlnet_prefs['use_init_video']:\n",
        "        control['init_video'] = controlnet_prefs['init_video']\n",
        "        control['start_time'] = controlnet_prefs['start_time']\n",
        "        control['end_time'] = controlnet_prefs['end_time']\n",
        "        control['fps'] = controlnet_prefs['fps']\n",
        "      controlnet_prompts.append(control)\n",
        "      #page.controlnet_output.controls.clear()\n",
        "    autoscroll(True)\n",
        "    clear_list()\n",
        "    prt(Divider(thickness=2, height=4))\n",
        "    prt(Installing(\"Installing ControlNet Packages...\"))\n",
        "    if status['loaded_controlnet'] == controlnet_prefs[\"control_task\"]:\n",
        "        clear_pipes('controlnet')\n",
        "    else:\n",
        "        clear_pipes()\n",
        "    import requests\n",
        "    from io import BytesIO\n",
        "    from PIL import ImageOps\n",
        "    from PIL.PngImagePlugin import PngInfo\n",
        "    try:\n",
        "        try:\n",
        "          import cv2\n",
        "        except ModuleNotFoundError:\n",
        "          run_sp(\"pip install opencv-contrib-python\", realtime=False)\n",
        "          pass\n",
        "        try:\n",
        "          from controlnet_aux import MLSDdetector\n",
        "        except ModuleNotFoundError:\n",
        "          run_sp(\"pip install --upgrade controlnet-aux\", realtime=False)\n",
        "          #run_sp(\"pip install git+https://github.com/patrickvonplaten/controlnet_aux.git\")\n",
        "          pass\n",
        "        from controlnet_aux import MLSDdetector\n",
        "        from controlnet_aux import OpenposeDetector\n",
        "        from diffusers import StableDiffusionControlNetPipeline, ControlNetModel\n",
        "        #run_sp(\"pip install scikit-image\", realtime=False)\n",
        "    except Exception as e:\n",
        "        clear_last()\n",
        "        alert_msg(page, f\"ERROR Installing Required Packages...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "        return\n",
        "    canny_checkpoint = \"lllyasviel/control_v11p_sd15_canny\"\n",
        "    scribble_checkpoint = \"lllyasviel/control_v11p_sd15_scribble\"\n",
        "    openpose_checkpoint = \"lllyasviel/control_v11p_sd15_openpose\"\n",
        "    depth_checkpoint = \"lllyasviel/control_v11p_sd15_depth\"\n",
        "    HED_checkpoint = \"lllyasviel/control_v11p_sd15_softedge\"\n",
        "    mlsd_checkpoint = \"lllyasviel/control_v11p_sd15_mlsd\"\n",
        "    normal_checkpoint = \"lllyasviel/control_v11p_sd15_normalbae\"\n",
        "    seg_checkpoint = \"lllyasviel/control_v11p_sd15_seg\"\n",
        "    lineart_checkpoint = \"lllyasviel/control_v11p_sd15_lineart\"\n",
        "    ip2p_checkpoint = \"lllyasviel/control_v11e_sd15_ip2p\"\n",
        "    shuffle_checkpoint = \"lllyasviel/control_v11e_sd15_shuffle\"\n",
        "    tile_checkpoint = \"lllyasviel/control_v11f1e_sd15_tile\"\n",
        "    brightness_checkpoint = \"ioclab/control_v1p_sd15_brightness\"\n",
        "    hed = None\n",
        "    openpose = None\n",
        "    depth_estimator = None\n",
        "    mlsd = None\n",
        "    image_processor = None\n",
        "    image_segmentor = None\n",
        "    normal = None\n",
        "    lineart = None\n",
        "    shuffle = None\n",
        "    def get_controlnet(task):\n",
        "        nonlocal hed, openpose, depth_estimator, mlsd, image_processor, image_segmentor, normal, lineart, shuffle\n",
        "        if controlnet_models[task] != None:\n",
        "            return controlnet_models[task]\n",
        "        if task == \"Canny Map Edge\" or task == \"Video Canny Edge\":\n",
        "            task = \"Canny Map Edge\"\n",
        "            controlnet_models[task] = ControlNetModel.from_pretrained(canny_checkpoint, torch_dtype=torch.float16).to(torch_device)\n",
        "        elif task == \"Scribble\":\n",
        "            from controlnet_aux import HEDdetector\n",
        "            hed = HEDdetector.from_pretrained('lllyasviel/Annotators')\n",
        "            controlnet_models[task] = ControlNetModel.from_pretrained(scribble_checkpoint, torch_dtype=torch.float16).to(torch_device)\n",
        "        elif task == \"OpenPose\" or task == \"Video OpenPose\":\n",
        "            task = \"OpenPose\"\n",
        "            from controlnet_aux import OpenposeDetector\n",
        "            openpose = OpenposeDetector.from_pretrained('lllyasviel/ControlNet')\n",
        "            controlnet_models[task] = ControlNetModel.from_pretrained(openpose_checkpoint, torch_dtype=torch.float16).to(torch_device)\n",
        "        elif task == \"Depth\":\n",
        "            from transformers import pipeline\n",
        "            depth_estimator = pipeline('depth-estimation')\n",
        "            controlnet_models[task] = ControlNetModel.from_pretrained(depth_checkpoint, torch_dtype=torch.float16).to(torch_device)\n",
        "        elif task == \"HED\":\n",
        "            from controlnet_aux import HEDdetector\n",
        "            hed = HEDdetector.from_pretrained('lllyasviel/Annotators')\n",
        "            #pidi_net = PidiNetDetector.from_pretrained('lllyasviel/Annotators')\n",
        "            controlnet_models[task] = ControlNetModel.from_pretrained(HED_checkpoint, torch_dtype=torch.float16).to(torch_device)\n",
        "        elif task == \"M-LSD\":\n",
        "            from controlnet_aux import MLSDdetector\n",
        "            mlsd = MLSDdetector.from_pretrained('lllyasviel/ControlNet')\n",
        "            controlnet_models[task] = ControlNetModel.from_pretrained(mlsd_checkpoint, torch_dtype=torch.float16).to(torch_device)\n",
        "        elif task == \"Normal Map\":\n",
        "            #from transformers import pipeline\n",
        "            #depth_estimator = pipeline(\"depth-estimation\", model =\"Intel/dpt-hybrid-midas\")\n",
        "            from controlnet_aux import NormalBaeDetector\n",
        "            normal = NormalBaeDetector.from_pretrained(\"lllyasviel/Annotators\")\n",
        "            controlnet_models[task] = ControlNetModel.from_pretrained(normal_checkpoint, torch_dtype=torch.float16).to(torch_device)\n",
        "        elif task == \"Segmented\":\n",
        "            from transformers import AutoImageProcessor, UperNetForSemanticSegmentation\n",
        "            from controlnet_utils import ade_palette\n",
        "            image_processor = AutoImageProcessor.from_pretrained(\"openmmlab/upernet-convnext-small\")\n",
        "            image_segmentor = UperNetForSemanticSegmentation.from_pretrained(\"openmmlab/upernet-convnext-small\")\n",
        "            controlnet_models[task] = ControlNetModel.from_pretrained(seg_checkpoint, torch_dtype=torch.float16).to(torch_device)\n",
        "        elif task == \"LineArt\":\n",
        "            from controlnet_aux import LineartDetector\n",
        "            lineart = LineartDetector.from_pretrained(\"lllyasviel/Annotators\")\n",
        "            controlnet_models[task] = ControlNetModel.from_pretrained(lineart_checkpoint, torch_dtype=torch.float16).to(torch_device)\n",
        "        elif task == \"Shuffle\":\n",
        "            from controlnet_aux import ContentShuffleDetector\n",
        "            shuffle = ContentShuffleDetector()\n",
        "            controlnet_models[task] = ControlNetModel.from_pretrained(shuffle_checkpoint, torch_dtype=torch.float16).to(torch_device)\n",
        "        elif task == \"Tile\":\n",
        "            controlnet_models[task] = ControlNetModel.from_pretrained(tile_checkpoint, torch_dtype=torch.float16).to(torch_device)\n",
        "        elif task == \"Brightness\":\n",
        "            controlnet_models[task] = ControlNetModel.from_pretrained(brightness_checkpoint, torch_dtype=torch.float16, use_safetensors=True)\n",
        "        elif task == \"Instruct Pix2Pix\":\n",
        "            controlnet_models[task] = ControlNetModel.from_pretrained(ip2p_checkpoint, torch_dtype=torch.float16).to(torch_device)\n",
        "\n",
        "        return controlnet_models[task]\n",
        "    width, height = 0, 0\n",
        "    def resize_for_condition_image(input_image: PILImage, resolution: int):\n",
        "        input_image = input_image.convert(\"RGB\")\n",
        "        W, H = input_image.size\n",
        "        k = float(resolution) / min(H, W)\n",
        "        H *= k\n",
        "        W *= k\n",
        "        H = int(round(H / 64.0)) * 64\n",
        "        W = int(round(W / 64.0)) * 64\n",
        "        img = input_image.resize((W, H), resample=PILImage.LANCZOS)\n",
        "        return img\n",
        "    def prep_image(task, img):\n",
        "        nonlocal hed, openpose, depth_estimator, mlsd, image_processor, image_segmentor, normal, lineart, shuffle\n",
        "        nonlocal width, height\n",
        "        if isinstance(img, str):\n",
        "          if img.startswith('http'):\n",
        "              #response = requests.get(controlnet_prefs['original_image'])\n",
        "              #original_img = PILImage.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "              original_img = PILImage.open(requests.get(img, stream=True).raw)\n",
        "          else:\n",
        "              if os.path.isfile(img):\n",
        "                  original_img = PILImage.open(img)\n",
        "              else:\n",
        "                  alert_msg(page, f\"ERROR: Couldn't find your original_image {img}\")\n",
        "                  return\n",
        "          width, height = original_img.size\n",
        "          width, height = scale_dimensions(width, height, controlnet_prefs['max_size'])\n",
        "          #print(f\"Size: {width}x{height}\")\n",
        "          original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS)\n",
        "        #return original_img\n",
        "        try:\n",
        "            if task == \"Canny Map Edge\" or task == \"Video Canny Edge\":\n",
        "                input_image = np.array(original_img)\n",
        "                input_image = cv2.Canny(input_image, controlnet_prefs['low_threshold'], controlnet_prefs['high_threshold'])\n",
        "                input_image = input_image[:, :, None]\n",
        "                input_image = np.concatenate([input_image, input_image, input_image], axis=2)\n",
        "                original_img = PILImage.fromarray(input_image)\n",
        "            elif task == \"Scribble\":\n",
        "                original_img = hed(original_img, scribble=True)\n",
        "            elif task == \"OpenPose\" or task == \"Video OpenPose\":\n",
        "                original_img = openpose(original_img, hand_and_face=True)\n",
        "            elif task == \"Depth\":\n",
        "                original_img = depth_estimator(original_img)['depth']\n",
        "                input_image = np.array(original_img)\n",
        "                input_image = input_image[:, :, None]\n",
        "                input_image = np.concatenate([input_image, input_image, input_image], axis=2)\n",
        "                original_img = PILImage.fromarray(input_image)\n",
        "            elif task == \"HED\":\n",
        "                original_img = hed(original_img, safe=True)\n",
        "            elif task == \"M-LSD\":\n",
        "                original_img = mlsd(original_img)\n",
        "            elif task == \"Normal Map\":\n",
        "                #depth_estimator = pipeline(\"depth-estimation\", model=\"Intel/dpt-hybrid-midas\" )\n",
        "                '''original_img = depth_estimator(original_img)['predicted_depth'][0]\n",
        "                input_image = original_img.numpy()\n",
        "                image_depth = input_image.copy()\n",
        "                image_depth -= np.min(image_depth)\n",
        "                image_depth /= np.max(image_depth)\n",
        "                bg_threhold = 0.4\n",
        "                x = cv2.Sobel(input_image, cv2.CV_32F, 1, 0, ksize=3)\n",
        "                x[image_depth < bg_threhold] = 0\n",
        "                y = cv2.Sobel(input_image, cv2.CV_32F, 0, 1, ksize=3)\n",
        "                y[image_depth < bg_threhold] = 0\n",
        "                z = np.ones_like(x) * np.pi * 2.0\n",
        "                input_image = np.stack([x, y, z], axis=2)\n",
        "                input_image /= np.sum(input_image ** 2.0, axis=2, keepdims=True) ** 0.5\n",
        "                input_image = (input_image * 127.5 + 127.5).clip(0, 255).astype(np.uint8)\n",
        "                original_img = PILImage.fromarray(input_image)'''\n",
        "                original_img = normal(original_img)\n",
        "            elif task == \"Segmented\":\n",
        "                from controlnet_utils import ade_palette\n",
        "                pixel_values = image_processor(original_img, return_tensors=\"pt\").pixel_values\n",
        "                with torch.no_grad():\n",
        "                  outputs = image_segmentor(pixel_values)\n",
        "                seg = image_processor.post_process_semantic_segmentation(outputs, target_sizes=[original_img.size[::-1]])[0]\n",
        "                color_seg = np.zeros((seg.shape[0], seg.shape[1], 3), dtype=np.uint8) # height, width, 3\n",
        "                palette = np.array(ade_palette())\n",
        "                for label, color in enumerate(palette):\n",
        "                    color_seg[seg == label, :] = color\n",
        "                color_seg = color_seg.astype(np.uint8)\n",
        "                original_img = PILImage.fromarray(color_seg)\n",
        "            elif task == \"LineArt\":\n",
        "                original_img = lineart(original_img)\n",
        "            elif task == \"Shuffle\":\n",
        "                original_img = shuffle(original_img)\n",
        "            elif task == \"Tile\":\n",
        "                original_img = resize_for_condition_image(original_img, 1024)\n",
        "            elif task == \"Brightness\":\n",
        "                original_img = PILImage.fromarray(original_img).convert('L')\n",
        "            return original_img\n",
        "        except Exception as e:\n",
        "            #clear_last()\n",
        "            clear_last()\n",
        "            alert_msg(page, f\"ERROR Preparing ControlNet {controlnet_prefs['control_task']} Input Image...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "            gc.collect()\n",
        "            torch.cuda.empty_cache()\n",
        "            return\n",
        "    def prep_video(vid):\n",
        "        nonlocal width, height\n",
        "        if vid.startswith('http'):\n",
        "            init_vid = download_file(vid, stable_dir)\n",
        "        else:\n",
        "            if os.path.isfile(vid):\n",
        "                init_vid = vid\n",
        "            else:\n",
        "                alert_msg(page, f\"ERROR: Couldn't find your init_video {vid}\")\n",
        "                return\n",
        "        try:\n",
        "            start_time = float(controlnet_prefs['start_time'])\n",
        "            end_time = float(controlnet_prefs['end_time'])\n",
        "            fps = int(controlnet_prefs['fps'])\n",
        "            max_size = controlnet_prefs['max_size']\n",
        "        except Exception:\n",
        "            alert_msg(page, \"Make sure your Numbers are actual numbers...\")\n",
        "            return\n",
        "        prt(\"Extracting Frames from Video Clip\")\n",
        "        try:\n",
        "            cap = cv2.VideoCapture(init_vid)\n",
        "        except Exception as e:\n",
        "            alert_msg(page, \"ERROR Reading Video File. May be Incompatible Format...\")\n",
        "            clear_last()\n",
        "            return\n",
        "        count = 0\n",
        "        video = []\n",
        "        frames = []\n",
        "        width = height = 0\n",
        "        cap.set(cv2.CAP_PROP_FPS, fps)\n",
        "        video_length = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
        "        start_frame = int(start_time * fps)\n",
        "        if end_time == 0 or end_time == 0.0:\n",
        "            end_frame = int(video_length)\n",
        "        else:\n",
        "            end_frame = int(end_time * fps)\n",
        "        total = end_frame - start_frame\n",
        "        for i in range(start_frame, end_frame):\n",
        "            cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
        "            success, image = cap.read()\n",
        "            if success:\n",
        "                #filename = os.path.join(output_dir, f'{file_prefix}{count}.png')\n",
        "                if width == 0:\n",
        "                    shape = image.shape\n",
        "                    width, height = scale_dimensions(shape[1], shape[0], max=max_size, multiple=16)\n",
        "                image = cv2.resize(image, (width, height), interpolation = cv2.INTER_AREA)\n",
        "                #cv2.imwrite(os.path.join(output_dir, filename), image)\n",
        "                image = prep_image(controlnet_prefs['control_task'], PILImage.fromarray(image))\n",
        "                video.append(image)\n",
        "                count += 1\n",
        "        cap.release()\n",
        "        clear_last()\n",
        "        return video\n",
        "    loaded_controlnet = None\n",
        "    if len(controlnet_prefs['multi_controlnets']) > 0 and not from_list and not controlnet_prefs['use_init_video']:\n",
        "        controlnet = []\n",
        "        loaded_controlnet = []\n",
        "        for c in controlnet_prefs['multi_controlnets']:\n",
        "            controlnet.append(get_controlnet(c['control_task']))\n",
        "            loaded_controlnet.append(c['control_task'])\n",
        "    else:\n",
        "        controlnet = get_controlnet(controlnet_prefs['control_task'])\n",
        "        loaded_controlnet = controlnet_prefs['control_task']\n",
        "    for k, v in controlnet_models.items():\n",
        "      if v != None and k in loaded_controlnet:\n",
        "        del v\n",
        "        controlnet_models[k] = None\n",
        "    model = get_model(prefs['model_ckpt'])\n",
        "    model_path = model['path']\n",
        "    if pipe_controlnet == None or status['loaded_controlnet'] != controlnet_prefs[\"control_task\"]:\n",
        "        pipe_controlnet = StableDiffusionControlNetPipeline.from_pretrained(model_path, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)\n",
        "        #pipe_controlnet.enable_model_cpu_offload()\n",
        "        pipe_controlnet = optimize_pipe(pipe_controlnet, vae=True, vae_tiling=True)\n",
        "        status['loaded_controlnet'] = loaded_controlnet #controlnet_prefs[\"control_task\"]\n",
        "    #else:\n",
        "        #pipe_controlnet.controlnet=controlnet\n",
        "    pipe_controlnet = pipeline_scheduler(pipe_controlnet)\n",
        "    if controlnet_prefs['use_init_video']:\n",
        "        from diffusers.pipelines.text_to_video_synthesis.pipeline_text_to_video_zero import CrossFrameAttnProcessor\n",
        "        pipe_controlnet.unet.set_attn_processor(CrossFrameAttnProcessor(batch_size=2))\n",
        "        pipe_controlnet.controlnet.set_attn_processor(CrossFrameAttnProcessor(batch_size=2))\n",
        "    clear_last()\n",
        "    prt(f\"Generating ControlNet {controlnet_prefs['control_task']} of your Image...\")\n",
        "    batch_output = os.path.join(stable_dir, controlnet_prefs['batch_folder_name'])\n",
        "    if not os.path.isdir(batch_output):\n",
        "      os.makedirs(batch_output)\n",
        "    batch_output = os.path.join(prefs['image_output'], controlnet_prefs['batch_folder_name'])\n",
        "    if not os.path.isdir(batch_output):\n",
        "      os.makedirs(batch_output)\n",
        "    for pr in controlnet_prompts:\n",
        "        prt(progress)\n",
        "        autoscroll(False)\n",
        "        if len(controlnet_prefs['multi_controlnets']) > 0 and not from_list and not controlnet_prefs['use_init_video']:\n",
        "            original_img = []\n",
        "            for c in controlnet_prefs['multi_controlnets']:\n",
        "                original_img.append(prep_image(c['control_task'], c['original_image']))\n",
        "        elif not controlnet_prefs['use_init_video']:\n",
        "            original_img = prep_image(controlnet_prefs['control_task'], pr['original_image'])\n",
        "        else:\n",
        "            video_img = prep_video(pr['original_image'])\n",
        "            latents = torch.randn((1, 4, 64, 64), device=\"cuda\", dtype=torch.float16).repeat(len(video_img), 1, 1, 1)\n",
        "        try:\n",
        "            random_seed = int(pr['seed']) if int(pr['seed']) > 0 else rnd.randint(0,4294967295)\n",
        "            generator = torch.Generator(device=torch_device).manual_seed(random_seed)\n",
        "            if not controlnet_prefs['use_init_video']:\n",
        "                images = pipe_controlnet(pr['prompt'], negative_prompt=pr['negative_prompt'], image=original_img, controlnet_conditioning_scale=pr['conditioning_scale'], num_inference_steps=controlnet_prefs['steps'], guidance_scale=controlnet_prefs['guidance_scale'], eta=controlnet_prefs['eta'], num_images_per_prompt=controlnet_prefs['batch_size'], height=height, width=width, generator=generator, callback=callback_fnc, callback_steps=1).images\n",
        "            else:\n",
        "                images = pipe_controlnet(pr['prompt'] * len(video_img), negative_prompt=pr['negative_prompt'] * len(video_img), image=video_img, latents=latents, controlnet_conditioning_scale=pr['conditioning_scale'], num_inference_steps=controlnet_prefs['steps'], guidance_scale=controlnet_prefs['guidance_scale'], eta=controlnet_prefs['eta'], height=height, width=width, generator=generator, callback=callback_fnc, callback_steps=1).images\n",
        "        except Exception as e:\n",
        "            #clear_last()\n",
        "            clear_last()\n",
        "            alert_msg(page, f\"ERROR Generating ControlNet {controlnet_prefs['control_task']}...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "            gc.collect()\n",
        "            torch.cuda.empty_cache()\n",
        "            return\n",
        "        clear_pipes('controlnet')\n",
        "        clear_last()\n",
        "        #clear_last()\n",
        "        autoscroll(True)\n",
        "        #filename = pr['original_image'].rpartition(slash)[2].rpartition('.')[0]\n",
        "        filename = f\"{controlnet_prefs['file_prefix']}{format_filename(pr['prompt'])}\"\n",
        "        filename = filename[:int(prefs['file_max_length'])]\n",
        "        #if prefs['file_suffix_seed']: fname += f\"-{random_seed}\"\n",
        "        num = 0\n",
        "        for image in images:\n",
        "            random_seed += num\n",
        "            fname = filename + (f\"-{random_seed}\" if prefs['file_suffix_seed'] else \"\")\n",
        "            image_path = available_file(os.path.join(stable_dir, controlnet_prefs['batch_folder_name']), fname, num)\n",
        "            unscaled_path = image_path\n",
        "            output_file = image_path.rpartition(slash)[2]\n",
        "            #PILImage.fromarray(image).save(image_path)\n",
        "            image.save(image_path)\n",
        "            out_path = image_path.rpartition(slash)[0]\n",
        "            upscaled_path = os.path.join(out_path, output_file)\n",
        "            new_file = available_file(batch_output, fname, num)\n",
        "            if not controlnet_prefs['display_upscaled_image'] or not controlnet_prefs['apply_ESRGAN_upscale']:\n",
        "                prt(Row([ImageButton(src=unscaled_path, data=new_file, width=width, height=height, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "                #prt(Row([Img(src=unscaled_path, fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "            if controlnet_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:\n",
        "                os.chdir(os.path.join(dist_dir, 'Real-ESRGAN'))\n",
        "                upload_folder = 'upload'\n",
        "                result_folder = 'results'\n",
        "                if os.path.isdir(upload_folder):\n",
        "                    shutil.rmtree(upload_folder)\n",
        "                if os.path.isdir(result_folder):\n",
        "                    shutil.rmtree(result_folder)\n",
        "                os.mkdir(upload_folder)\n",
        "                os.mkdir(result_folder)\n",
        "                short_name = f'{fname[:80]}-{num}.png'\n",
        "                dst_path = os.path.join(dist_dir, 'Real-ESRGAN', upload_folder, short_name)\n",
        "                #print(f'Moving {fpath} to {dst_path}')\n",
        "                #shutil.move(fpath, dst_path)\n",
        "                shutil.copy(image_path, dst_path)\n",
        "                #faceenhance = ' --face_enhance' if controlnet_prefs[\"face_enhance\"] else ''\n",
        "                faceenhance = ''\n",
        "                run_sp(f'python inference_realesrgan.py -n realesr-general-x4v3 -i upload --outscale {controlnet_prefs[\"enlarge_scale\"]}{faceenhance}', cwd=os.path.join(dist_dir, 'Real-ESRGAN'), realtime=False)\n",
        "                out_file = short_name.rpartition('.')[0] + '_out.png'\n",
        "                shutil.move(os.path.join(dist_dir, 'Real-ESRGAN', result_folder, out_file), upscaled_path)\n",
        "                image_path = upscaled_path\n",
        "                os.chdir(stable_dir)\n",
        "                \n",
        "            if prefs['save_image_metadata']:\n",
        "                task = and_list(controlnet_prefs['control_task']) if isinstance(controlnet_prefs['control_task'], list) else controlnet_prefs['control_task']\n",
        "                img = PILImage.open(image_path)\n",
        "                metadata = PngInfo()\n",
        "                metadata.add_text(\"artist\", prefs['meta_ArtistName'])\n",
        "                metadata.add_text(\"copyright\", prefs['meta_Copyright'])\n",
        "                metadata.add_text(\"software\", \"Stable Diffusion Deluxe\" + f\", upscaled {controlnet_prefs['enlarge_scale']}x with ESRGAN\" if controlnet_prefs['apply_ESRGAN_upscale'] else \"\")\n",
        "                metadata.add_text(\"pipeline\", \"ControlNet \" + task)\n",
        "                if prefs['save_config_in_metadata']:\n",
        "                  config_json = controlnet_prefs.copy()\n",
        "                  config_json['model_path'] = model_path\n",
        "                  config_json['seed'] = random_seed\n",
        "                  config_json['prompt'] = pr['prompt']\n",
        "                  config_json['negative_prompt'] = pr['negative_prompt']\n",
        "                  del config_json['batch_size']\n",
        "                  del config_json['max_size']\n",
        "                  del config_json['display_upscaled_image']\n",
        "                  del config_json['batch_folder_name']\n",
        "                  if not config_json['apply_ESRGAN_upscale']:\n",
        "                    del config_json['enlarge_scale']\n",
        "                    del config_json['apply_ESRGAN_upscale']\n",
        "                  metadata.add_text(\"config_json\", json.dumps(config_json, ensure_ascii=True, indent=4))\n",
        "                img.save(image_path, pnginfo=metadata)\n",
        "            #TODO: PyDrive\n",
        "            if storage_type == \"Colab Google Drive\":\n",
        "                #new_file = available_file(output_path, fname, num)\n",
        "                #out_path = new_file\n",
        "                shutil.copy(image_path, new_file)\n",
        "            elif bool(prefs['image_output']):\n",
        "                #new_file = available_file(output_path, fname, num)\n",
        "                #out_path = new_file\n",
        "                shutil.copy(image_path, new_file)\n",
        "            if controlnet_prefs['display_upscaled_image']:\n",
        "                prt(Row([ImageButton(src=new_file, data=new_file, width=width * float(controlnet_prefs[\"enlarge_scale\"]), height=height * float(controlnet_prefs[\"enlarge_scale\"]), page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "                #prt(Row([Img(src=upscaled_path, fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "            prt(Row([Text(new_file)], alignment=MainAxisAlignment.CENTER))\n",
        "            num += 1\n",
        "    autoscroll(False)\n",
        "    del hed, openpose, depth_estimator, mlsd, image_processor, image_segmentor, normal, lineart, shuffle\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "def run_deepfloyd(page, from_list=False):\n",
        "    global deepfloyd_prefs, prefs, status, pipe_deepfloyd\n",
        "    import torch\n",
        "    #if not status['installed_diffusers']:\n",
        "    #  alert_msg(page, \"You need to Install HuggingFace Diffusers before using...\")\n",
        "    #  return\n",
        "    #if not bool(deepfloyd_prefs['init_image']):\n",
        "    #  alert_msg(page, \"You must provide the Original Image and the Mask Image to process...\")\n",
        "    #  return\n",
        "    if not bool(deepfloyd_prefs['prompt']):\n",
        "      alert_msg(page, \"You must provide a Text-to-Image Prompt...\")\n",
        "      return\n",
        "    def prt(line, update=True):\n",
        "      if type(line) == str:\n",
        "        line = Text(line)\n",
        "      if from_list:\n",
        "        page.imageColumn.controls.append(line)\n",
        "        if update:\n",
        "          page.imageColumn.update()\n",
        "      else:\n",
        "        page.DeepFloyd.controls.append(line)\n",
        "        if update:\n",
        "          page.DeepFloyd.update()\n",
        "    def clear_last(update=True):\n",
        "      if from_list:\n",
        "        if len(page.imageColumn.controls) == 0: return\n",
        "        del page.imageColumn.controls[-1]\n",
        "        if update: page.imageColumn.update()\n",
        "      else:\n",
        "        if len(page.DeepFloyd.controls) == 1: return\n",
        "        del page.DeepFloyd.controls[-1]\n",
        "        if update: page.DeepFloyd.update()\n",
        "    def autoscroll(scroll=True):\n",
        "      if from_list:\n",
        "        page.imageColumn.auto_scroll = scroll\n",
        "        page.imageColumn.update()\n",
        "      else:\n",
        "        page.DeepFloyd.auto_scroll = scroll\n",
        "        page.DeepFloyd.update()\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    total_steps = deepfloyd_prefs['num_inference_steps']\n",
        "    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:\n",
        "      callback_fnc.has_been_called = True\n",
        "      nonlocal progress, total_steps\n",
        "      #total_steps = len(latents)\n",
        "      percent = (step +1)/ total_steps\n",
        "      progress.value = percent\n",
        "      progress.tooltip = f\"{step +1} / {total_steps}  Timestep: {timestep}\"\n",
        "      progress.update()\n",
        "      #print(f'{type(latents)} {len(latents)}- {str(latents)}')\n",
        "    def clear_list():\n",
        "      if from_list:\n",
        "        page.imageColumn.controls.clear()\n",
        "      else:\n",
        "        page.DeepFloyd.controls = page.DeepFloyd.controls[:1]\n",
        "    deepfloyd_prompts = []\n",
        "    if from_list:\n",
        "      if len(prompts) < 1:\n",
        "        alert_msg(page, \"You need to add Prompts to your List first... \")\n",
        "        return\n",
        "      for p in prompts:\n",
        "        df_prompt = {'prompt': p.prompt, 'negative_prompt': p['negative_prompt'], 'init_image': p['init_image'] if bool(p['init_image']) else deepfloyd_prefs['init_image'], 'mask_image': p['mask_image'] if bool(p['mask_image']) else deepfloyd_prefs['mask_image'], 'image_strength': p['init_image_strength'], 'guidance_scale':p['guidance_scale'], 'num_inference_steps': p['steps'], 'seed': p['seed']}\n",
        "        deepfloyd_prompts.append(df_prompt)\n",
        "      page.tabs.selected_index = 4\n",
        "      page.tabs.update()\n",
        "    else:\n",
        "      if not bool(deepfloyd_prefs['prompt']):\n",
        "        alert_msg(page, \"You need to add a Text Prompt first... \")\n",
        "        return\n",
        "      df_prompt = {'prompt':deepfloyd_prefs['prompt'], 'negative_prompt': deepfloyd_prefs['negative_prompt'], 'init_image': deepfloyd_prefs['init_image'], 'mask_image': deepfloyd_prefs['mask_image'], 'image_strength': deepfloyd_prefs['image_strength'], 'guidance_scale':deepfloyd_prefs['guidance_scale'], 'num_inference_steps': deepfloyd_prefs['num_inference_steps'], 'seed': deepfloyd_prefs['seed']}\n",
        "      deepfloyd_prompts.append(df_prompt)\n",
        "    autoscroll(True)\n",
        "    clear_list()\n",
        "    prt(Divider(thickness=2, height=4))\n",
        "    #if not status['installed_diffusers']:\n",
        "    install = Installing(\"Installing Diffusers & Required Packages...\")\n",
        "    prt(install)\n",
        "    try:\n",
        "        import diffusers\n",
        "        if force_updates: raise ModuleNotFoundError(\"Forcing update\")\n",
        "    except ModuleNotFoundError:\n",
        "        install.set_details(\"...HuggingFace Diffusers v0.16\")\n",
        "        run_process(\"pip install --upgrade diffusers~=0.16\", page=page)\n",
        "        #run_process(\"pip install --upgrade git+https://github.com/Skquark/diffusers.git\", page=page)\n",
        "        #run_process(\"pip install --upgrade git+https://github.com/Skquark/diffusers.git@main#egg=diffusers[torch]\", page=page)\n",
        "        pass\n",
        "    try:\n",
        "        import transformers\n",
        "        if force_updates: raise ModuleNotFoundError(\"Forcing update\")\n",
        "    except ModuleNotFoundError:\n",
        "        install.set_details(\"...Transformers v4.28\")\n",
        "        #run_process(\"pip install -qq --upgrade git+https://github.com/huggingface/transformers\", page=page)\n",
        "        run_process(\"pip install --upgrade transformers~=4.28\", page=page)\n",
        "        pass\n",
        "    try:\n",
        "        import safetensors\n",
        "        from safetensors import safe_open\n",
        "    except ModuleNotFoundError:\n",
        "        install.set_details(\"...SafeTensors v0.3\")\n",
        "        run_process(\"pip install --upgrade safetensors~=0.3\", page=page)\n",
        "        import safetensors\n",
        "        from safetensors import safe_open\n",
        "        pass\n",
        "    try:\n",
        "        import sentencepiece\n",
        "    except ModuleNotFoundError:\n",
        "        install.set_details(\"...SentencePiece v0.1\")\n",
        "        run_sp(\"pip install --upgrade sentencepiece~=0.1\", realtime=False)\n",
        "        import sentencepiece\n",
        "        pass\n",
        "    try:\n",
        "        import accelerate\n",
        "    except ModuleNotFoundError:\n",
        "        install.set_details(\"...Accelerate v0.18\")\n",
        "        run_process(\"pip install --upgrade accelerate~=0.18\", page=page)\n",
        "        pass\n",
        "    install.set_message(\"Installing DeepFloyd IF Required Packages...\")\n",
        "    if deepfloyd_prefs['low_memory']:\n",
        "        #clear_last(update=False)\n",
        "        #prt(Installing(\"Installing DeepFloyd IF Required Packages...\"))\n",
        "        #try:\n",
        "        #  import bitsandbytes\n",
        "        #except Exception:\n",
        "        install.set_details(\"...BitsandBytes v0.38\")\n",
        "        os.environ['LD_LIBRARY_PATH'] += \"/usr/lib/wsl/lib:$LD_LIBRARY_PATH\"\n",
        "        #run_sp(\"export LD_LIBRARY_PATH=/usr/lib/wsl/lib:$LD_LIBRARY_PATH\", realtime=False)\n",
        "        if sys.platform.startswith(\"win\"):\n",
        "            run_sp(\"pip install bitsandbytes-windows\", realtime=False)\n",
        "        else:\n",
        "            run_sp(\"pip install --upgrade bitsandbytes~=0.38\", realtime=False)\n",
        "        # import bitsandbytes\n",
        "        #  pass\n",
        "    try:\n",
        "        import torch\n",
        "    except ModuleNotFoundError:\n",
        "        install.set_details(\"...Torch v2.0\")\n",
        "        run_process(\"pip install --upgrade torch~=2.0\", page=page)\n",
        "        import torch\n",
        "        pass\n",
        "    try:\n",
        "        from huggingface_hub import notebook_login, HfFolder, login\n",
        "    except ModuleNotFoundError:\n",
        "        install.set_details(\"...HuggingFace Hub\")\n",
        "        run_process(\"pip install huggingface_hub --upgrade\", page=page)\n",
        "        import torch\n",
        "        pass\n",
        "    if not os.path.exists(HfFolder.path_token):\n",
        "        try:\n",
        "          login(token=prefs['HuggingFace_api_key'], add_to_git_credential=True)\n",
        "        except Exception:\n",
        "          alert_msg(page, \"ERROR Logging into HuggingFace... Check your API Key or Internet conenction.\")\n",
        "          return\n",
        "\n",
        "    import requests, random\n",
        "    from io import BytesIO\n",
        "    from PIL import ImageOps\n",
        "    from PIL.PngImagePlugin import PngInfo\n",
        "    from diffusers import DiffusionPipeline\n",
        "    from diffusers.utils import pt_to_pil\n",
        "    from diffusers import IFPipeline, IFImg2ImgPipeline, IFInpaintingPipeline, IFSuperResolutionPipeline\n",
        "    #from diffusers.pipelines.deepfloyd_if.safety_checker IFSafetyChecker\n",
        "    from transformers import T5EncoderModel, T5Tokenizer\n",
        "    #run_sp(\"accelerate config default\", realtime=False)\n",
        "    clear_pipes()\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.reset_max_memory_allocated()\n",
        "    #torch.cuda.reset_peak_memory_stats()\n",
        "    model_id = \"DeepFloyd/IF-I-XL-v1.0\"\n",
        "    clear_last(update=False)\n",
        "    \n",
        "    max_size = deepfloyd_prefs['max_size']\n",
        "    batch_output = os.path.join(stable_dir, deepfloyd_prefs['batch_folder_name'])\n",
        "    output_dir = batch_output\n",
        "    if not os.path.isdir(batch_output):\n",
        "      os.makedirs(batch_output)\n",
        "    batch_output = os.path.join(prefs['image_output'], deepfloyd_prefs['batch_folder_name'])\n",
        "    if not os.path.isdir(batch_output):\n",
        "      os.makedirs(batch_output)\n",
        "\n",
        "    for pr in deepfloyd_prompts:\n",
        "        init_img = None\n",
        "        mask_img = None\n",
        "        if bool(pr['init_image']):\n",
        "            if pr['init_image'].startswith('http'):\n",
        "                init_img = PILImage.open(requests.get(pr['init_image'], stream=True).raw)\n",
        "            else:\n",
        "                if os.path.isfile(pr['init_image']):\n",
        "                    init_img = PILImage.open(pr['init_image'])\n",
        "                else:\n",
        "                    alert_msg(page, f\"ERROR: Couldn't find your init_image {pr['init_image']}\")\n",
        "                    return\n",
        "            width, height = init_img.size\n",
        "            width, height = scale_dimensions(width, height, deepfloyd_prefs['max_size'])\n",
        "            if bool(deepfloyd_prefs['alpha_mask']):\n",
        "                init_img = init_img.convert(\"RGBA\")\n",
        "            else:\n",
        "                init_img = init_img.convert(\"RGB\")\n",
        "            init_img = init_img.resize((width, height), resample=PILImage.Resampling.LANCZOS)\n",
        "            if not bool(pr['mask_image']) and bool(deepfloyd_prefs['alpha_mask']):\n",
        "                mask_img = init_img.convert('RGBA')\n",
        "                red, green, blue, alpha = PILImage.Image.split(init_img)\n",
        "                mask_img = alpha.convert('L')\n",
        "            elif bool(pr['mask_image']):\n",
        "                if pr['mask_image'].startswith('http'):\n",
        "                    #response = requests.get(deepfloyd_prefs['init_image'])\n",
        "                    #init_img = PILImage.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "                    mask_img = PILImage.open(requests.get(pr['mask_image'], stream=True).raw)\n",
        "                else:\n",
        "                    if os.path.isfile(pr['mask_image']):\n",
        "                        mask_img = PILImage.open(pr['mask_image'])\n",
        "                    else:\n",
        "                        alert_msg(page, f\"ERROR: Couldn't find your mask_image {pr['mask_image']}\")\n",
        "                        return\n",
        "                width, height = mask_img.size\n",
        "                width, height = scale_dimensions(width, height, deepfloyd_prefs['max_size'])\n",
        "                mask_img = mask_img.resize((width, height), resample=PILImage.Resampling.LANCZOS)\n",
        "        if deepfloyd_prefs['invert_mask'] and not deepfloyd_prefs['alpha_mask']:\n",
        "            from PIL import ImageOps\n",
        "            mask_img = ImageOps.invert(mask_img.convert('RGB'))\n",
        "        for num in range(deepfloyd_prefs['num_images']):\n",
        "            random_seed = (int(pr['seed']) + num) if int(pr['seed']) > 0 else rnd.randint(0,4294967295)\n",
        "            generator = torch.Generator().manual_seed(random_seed)\n",
        "            try:\n",
        "                install = Installing(\"Running DeepFloyd-IF Text Encoder...\")\n",
        "                prt(install)\n",
        "                if deepfloyd_prefs['low_memory']:\n",
        "                    #, load_in_8bit=True\n",
        "                    install.set_details(\"...text_encoder T5EncoderModel\")\n",
        "                    text_encoder = T5EncoderModel.from_pretrained(model_id, subfolder=\"text_encoder\", device_map=\"auto\", load_in_8bit=True, variant=\"8bit\", cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)\n",
        "                    install.set_details(\"...DiffusionPipeline\")\n",
        "                    pipe_deepfloyd = DiffusionPipeline.from_pretrained(model_id, text_encoder=text_encoder, unet=None, device_map=None, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)\n",
        "                    # Still getting errors here! WTF?\n",
        "                    #images = pipe_deepfloyd(pr['prompt'], image=init_img, negative_prompt=pr['negative_prompt'] if bool(pr['negative_prompt']) else None, num_inference_steps=deepfloyd_prefs['num_inference_steps'], eta=deepfloyd_prefs['eta'], image_guidance_scale=deepfloyd_prefs['guidance_scale'], num_images_per_prompt=deepfloyd_prefs['num_images'], generator=generator, callback=callback_fnc, callback_steps=1).images\n",
        "                    install.set_details(\"...encode_prompts\")\n",
        "                    prompt_embeds, negative_embeds = pipe_deepfloyd.encode_prompt(pr['prompt'], negative_prompt=pr['negative_prompt'] if bool(pr['negative_prompt']) else None)\n",
        "                    del text_encoder\n",
        "                    del pipe_deepfloyd\n",
        "                else:\n",
        "                    install.set_details(\"...DiffusionPipeline\")\n",
        "                    pipe_deepfloyd = DiffusionPipeline.from_pretrained(model_id, variant=\"fp16\", torch_dtype=torch.float16, safety_checker=None, requires_safety_checker=not prefs['disable_nsfw_filter'], cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)\n",
        "                    pipe_deepfloyd.enable_model_cpu_offload()\n",
        "                    install.set_details(\"...encode_prompts\")\n",
        "                    prompt_embeds, negative_embeds = pipe_deepfloyd.encode_prompt(pr['prompt'], negative_prompt=pr['negative_prompt'] if bool(pr['negative_prompt']) else None)\n",
        "                install.set_details(\"...clearing pipes\")\n",
        "                gc.collect()\n",
        "                torch.cuda.empty_cache()\n",
        "                clear_last(update=False)\n",
        "                safety_modules = {}\n",
        "                if init_img == None:\n",
        "                    prt(Installing(\"Stage 1: Installing DeepFloyd-IF Pipeline...\"))\n",
        "                    # if prefs['disable_nsfw_filter'] else IFSafetyChecker.from_pretrained(\"CompVis/stable-diffusion-safety-checker\").to(torch_device)\n",
        "                    total_steps = pr['num_inference_steps']\n",
        "                    clear_last()\n",
        "                    prt(progress)\n",
        "                    if deepfloyd_prefs['low_memory']:\n",
        "                        pipe_deepfloyd = IFPipeline.from_pretrained(model_id, text_encoder=None, variant=\"fp16\", torch_dtype=torch.float16, device_map=\"auto\", safety_checker=None, requires_safety_checker=not prefs['disable_nsfw_filter'], cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)\n",
        "                        #pipe_deepfloyd.enable_model_cpu_offload()\n",
        "                    images = pipe_deepfloyd(\n",
        "                        prompt_embeds=prompt_embeds,\n",
        "                        negative_prompt_embeds=negative_embeds,\n",
        "                        num_inference_steps = pr['num_inference_steps'],\n",
        "                        guidance_scale = pr['guidance_scale'],\n",
        "                        output_type=\"pt\",\n",
        "                        generator=generator,\n",
        "                        callback=callback_fnc, callback_steps=1,\n",
        "                    ).images\n",
        "                    safety_modules = {\n",
        "                        \"feature_extractor\": pipe_deepfloyd.feature_extractor,\n",
        "                        \"safety_checker\": pipe_deepfloyd.safety_checker,\n",
        "                        \"watermarker\": pipe_deepfloyd.watermarker,\n",
        "                    }\n",
        "                    del pipe_deepfloyd\n",
        "                    gc.collect()\n",
        "                    torch.cuda.empty_cache()\n",
        "                    total_steps = deepfloyd_prefs['superres_num_inference_steps']\n",
        "                    clear_last()\n",
        "                    prt(Installing(\"Stage 2: Installing DeepFloyd Super Resolution Pipeline...\"))\n",
        "                    #IFSuperResolutionPipeline\n",
        "                    pipe_deepfloyd = DiffusionPipeline.from_pretrained(\"DeepFloyd/IF-II-L-v1.0\", text_encoder=None, variant=\"fp16\", torch_dtype=torch.float16)\n",
        "                    if not deepfloyd_prefs['low_memory']:\n",
        "                        pipe_deepfloyd.enable_model_cpu_offload()\n",
        "                    clear_last()\n",
        "                    prt(progress)\n",
        "                    images = pipe_deepfloyd(\n",
        "                        image=images,\n",
        "                        prompt_embeds=prompt_embeds,\n",
        "                        negative_prompt_embeds=negative_embeds,\n",
        "                        num_inference_steps = deepfloyd_prefs['superres_num_inference_steps'],\n",
        "                        guidance_scale = deepfloyd_prefs['superres_guidance_scale'],\n",
        "                        output_type=\"pt\",\n",
        "                        generator=generator,\n",
        "                        callback=callback_fnc, callback_steps=1,\n",
        "                    ).images\n",
        "                elif init_img != None and mask_img == None:\n",
        "                    prt(Installing(\"Stage 1: Installing DeepFloyd-IF Image2Image Pipeline...\"))\n",
        "                    # if prefs['disable_nsfw_filter'] else IFSafetyChecker.from_pretrained(\"CompVis/stable-diffusion-safety-checker\").to(torch_device)\n",
        "                    pipe_deepfloyd = IFImg2ImgPipeline.from_pretrained(model_id, variant=\"fp16\", torch_dtype=torch.float16, safety_checker=None, requires_safety_checker=not prefs['disable_nsfw_filter'], cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)\n",
        "                    #pipe_deepfloyd.enable_model_cpu_offload()\n",
        "                    total_steps = pr['num_inference_steps']\n",
        "                    clear_last()\n",
        "                    prt(progress)\n",
        "                    images = pipe_deepfloyd(\n",
        "                        image=init_img,\n",
        "                        strength=pr['image_strength'],\n",
        "                        prompt_embeds=prompt_embeds,\n",
        "                        negative_prompt_embeds=negative_embeds,\n",
        "                        num_inference_steps = pr['num_inference_steps'],\n",
        "                        guidance_scale = pr['guidance_scale'],\n",
        "                        output_type=\"pt\",\n",
        "                        generator=generator,\n",
        "                        callback=callback_fnc, callback_steps=1,\n",
        "                    ).images\n",
        "                    safety_modules = {\n",
        "                        \"feature_extractor\": pipe_deepfloyd.feature_extractor,\n",
        "                        \"safety_checker\": pipe_deepfloyd.safety_checker,\n",
        "                        \"watermarker\": pipe_deepfloyd.watermarker,\n",
        "                    }\n",
        "                    del pipe_deepfloyd\n",
        "                    gc.collect()\n",
        "                    torch.cuda.empty_cache()\n",
        "                    total_steps = deepfloyd_prefs['superres_num_inference_steps']\n",
        "                    clear_last()\n",
        "                    prt(Installing(\"Stage 2: Installing DeepFloyd Img2Img Super Resolution Pipeline...\"))\n",
        "                    from diffusers import IFImg2ImgSuperResolutionPipeline\n",
        "                    pipe_deepfloyd = IFImg2ImgSuperResolutionPipeline.from_pretrained(\"DeepFloyd/IF-II-L-v1.0\", text_encoder=None, variant=\"fp16\", torch_dtype=torch.float16, device_map=\"auto\")\n",
        "                    #pipe_deepfloyd.enable_model_cpu_offload()\n",
        "                    clear_last()\n",
        "                    prt(progress)\n",
        "                    images = pipe_deepfloyd(\n",
        "                        image=images,\n",
        "                        origional_image=init_img,\n",
        "                        strength=pr['image_strength'],\n",
        "                        prompt_embeds=prompt_embeds,\n",
        "                        negative_prompt_embeds=negative_embeds,\n",
        "                        num_inference_steps = deepfloyd_prefs['superres_num_inference_steps'],\n",
        "                        guidance_scale = deepfloyd_prefs['superres_guidance_scale'],\n",
        "                        output_type=\"pt\",\n",
        "                        generator=generator,\n",
        "                        callback=callback_fnc, callback_steps=1,\n",
        "                    ).images\n",
        "                elif init_img != None and mask_img != None:\n",
        "                    prt(Installing(\"Stage 1: Installing DeepFloyd-IF Inpainting Pipeline...\"))\n",
        "                    # if prefs['disable_nsfw_filter'] else IFSafetyChecker.from_pretrained(\"CompVis/stable-diffusion-safety-checker\").to(torch_device)\n",
        "                    pipe_deepfloyd = IFInpaintingPipeline.from_pretrained(model_id, variant=\"fp16\", torch_dtype=torch.float16, safety_checker=None, requires_safety_checker=not prefs['disable_nsfw_filter'], cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)\n",
        "                    pipe_deepfloyd.enable_model_cpu_offload()\n",
        "                    total_steps = pr['num_inference_steps']\n",
        "                    clear_last()\n",
        "                    prt(progress)\n",
        "                    images = pipe_deepfloyd(\n",
        "                        image=init_img,\n",
        "                        mask_image=mask_img,\n",
        "                        strength=pr['image_strength'],\n",
        "                        prompt_embeds=prompt_embeds,\n",
        "                        negative_prompt_embeds=negative_embeds,\n",
        "                        num_inference_steps = pr['num_inference_steps'],\n",
        "                        guidance_scale = pr['guidance_scale'],\n",
        "                        output_type=\"pt\",\n",
        "                        generator=generator,\n",
        "                        callback=callback_fnc, callback_steps=1,\n",
        "                    ).images\n",
        "                    safety_modules = {\n",
        "                        \"feature_extractor\": pipe_deepfloyd.feature_extractor,\n",
        "                        \"safety_checker\": pipe_deepfloyd.safety_checker,\n",
        "                        \"watermarker\": pipe_deepfloyd.watermarker,\n",
        "                    }\n",
        "                    del pipe_deepfloyd\n",
        "                    gc.collect()\n",
        "                    torch.cuda.empty_cache()\n",
        "                    clear_last()\n",
        "                    prt(Installing(\"Stage 2: Installing DeepFloyd Inpainting Super Resolution Pipeline...\"))\n",
        "                    from diffusers import IFInpaintingSuperResolutionPipeline\n",
        "                    pipe_deepfloyd = IFInpaintingSuperResolutionPipeline.from_pretrained(\"DeepFloyd/IF-II-L-v1.0\", text_encoder=None, variant=\"fp16\", torch_dtype=torch.float16, device_map=None)\n",
        "                    pipe_deepfloyd.enable_model_cpu_offload()\n",
        "                    total_steps = deepfloyd_prefs['superres_num_inference_steps']\n",
        "                    clear_last()\n",
        "                    prt(progress)\n",
        "                    images = pipe_deepfloyd(\n",
        "                        image=images,\n",
        "                        origional_image=init_img,\n",
        "                        mask_image=mask_img,\n",
        "                        strength=pr['image_strength'],\n",
        "                        prompt_embeds=prompt_embeds,\n",
        "                        negative_prompt_embeds=negative_embeds,\n",
        "                        num_inference_steps = deepfloyd_prefs['superres_num_inference_steps'],\n",
        "                        guidance_scale = deepfloyd_prefs['superres_guidance_scale'],\n",
        "                        output_type=\"pt\",\n",
        "                        generator=generator,\n",
        "                        callback=callback_fnc, callback_steps=1,\n",
        "                    ).images\n",
        "                \n",
        "                del pipe_deepfloyd\n",
        "                gc.collect()\n",
        "                torch.cuda.empty_cache()\n",
        "                prt(Installing(\"Stage 3: Installing Stable Diffusion X4 Upscaler Pipeline...\"))\n",
        "                pipe_deepfloyd = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-x4-upscaler\", **safety_modules, torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)\n",
        "                pipe_deepfloyd.enable_model_cpu_offload()\n",
        "                total_steps = deepfloyd_prefs['upscale_num_inference_steps']\n",
        "                clear_last()\n",
        "                prt(progress)\n",
        "                images = pipe_deepfloyd(prompt=pr['prompt'], negative_prompt=pr['negative_prompt'] if bool(pr['negative_prompt']) else None, image=images, noise_level=100, num_inference_steps=deepfloyd_prefs['upscale_num_inference_steps'], guidance_scale=deepfloyd_prefs['upscale_guidance_scale'], generator=generator, callback=callback_fnc, callback_steps=1).images\n",
        "                del pipe_deepfloyd\n",
        "                gc.collect()\n",
        "                torch.cuda.empty_cache()\n",
        "                if deepfloyd_prefs['apply_watermark']:\n",
        "                    from diffusers.pipelines.deepfloyd_if import IFWatermarker\n",
        "                    watermarker = IFWatermarker.from_pretrained(model_id, subfolder=\"watermarker\")\n",
        "                    watermarker.apply_watermark(images, pipe.unet.config.sample_size)\n",
        "                    del watermarker\n",
        "\n",
        "            except EnvironmentError as e:\n",
        "                clear_last()\n",
        "                alert_msg(page, f\"ERROR: You must accept the license on the DeepFloyd model card first.\", content=Text(str(e)))\n",
        "                #del pipe_deepfloyd\n",
        "                gc.collect()\n",
        "                torch.cuda.empty_cache()\n",
        "                return\n",
        "            except Exception as e:\n",
        "                clear_last()\n",
        "                alert_msg(page, f\"ERROR: Couldn't run IF-DeepFloyd on your image for some reason.  Possibly out of memory or something wrong with my code...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "                #del pipe_deepfloyd\n",
        "                gc.collect()\n",
        "                torch.cuda.empty_cache()\n",
        "                return\n",
        "        \n",
        "            #clear_last()\n",
        "            clear_last()\n",
        "            filename = f\"{deepfloyd_prefs['file_prefix']}{format_filename(pr['prompt'])}\"\n",
        "            #if prefs['file_suffix_seed']: fname += f\"-{random_seed}\"\n",
        "            #num = 0\n",
        "            for image in images:\n",
        "                random_seed += num\n",
        "                fname = filename + (f\"-{random_seed}\" if prefs['file_suffix_seed'] else \"\")\n",
        "                image_path = available_file(os.path.join(stable_dir, deepfloyd_prefs['batch_folder_name']), fname, num)\n",
        "                unscaled_path = image_path\n",
        "                output_file = image_path.rpartition(slash)[2]\n",
        "                image = pt_to_pil(image)\n",
        "                image.save(image_path)\n",
        "                out_path = image_path.rpartition(slash)[0]\n",
        "                upscaled_path = os.path.join(out_path, output_file)\n",
        "                if not deepfloyd_prefs['display_upscaled_image'] or not deepfloyd_prefs['apply_ESRGAN_upscale']:\n",
        "                    prt(Row([ImageButton(src=unscaled_path, data=upscaled_path, width=width, height=height, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "                    #prt(Row([Img(src=unscaled_path, fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "                if deepfloyd_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:\n",
        "                    #w = int(arg['width'] * prefs[\"enlarge_scale\"])\n",
        "                    #h = int(arg['height'] * prefs[\"enlarge_scale\"])\n",
        "                    #prt(Row([Text(f'Enlarging {prefs[\"enlarge_scale\"]}X to {w}x{h}')], alignment=MainAxisAlignment.CENTER))\n",
        "                    prt(Row([Text(f'Enlarging Real-ESRGAN {prefs[\"enlarge_scale\"]}X')], alignment=MainAxisAlignment.CENTER))\n",
        "                    os.chdir(os.path.join(dist_dir, 'Real-ESRGAN'))\n",
        "                    upload_folder = 'upload'\n",
        "                    result_folder = 'results'     \n",
        "                    if os.path.isdir(upload_folder):\n",
        "                        shutil.rmtree(upload_folder)\n",
        "                    if os.path.isdir(result_folder):\n",
        "                        shutil.rmtree(result_folder)\n",
        "                    os.mkdir(upload_folder)\n",
        "                    os.mkdir(result_folder)\n",
        "                    short_name = f'{fname[:80]}-{num}.png'\n",
        "                    dst_path = os.path.join(dist_dir, 'Real-ESRGAN', upload_folder, short_name)\n",
        "                    #shutil.move(fpath, dst_path)\n",
        "                    shutil.copy(image_path, dst_path)\n",
        "                    #faceenhance = ' --face_enhance' if deepfloyd_prefs[\"face_enhance\"] else ''\n",
        "                    faceenhance = ''\n",
        "                    run_sp(f'python inference_realesrgan.py -n realesr-general-x4v3 -i upload --outscale {deepfloyd_prefs[\"enlarge_scale\"]}{faceenhance}', cwd=os.path.join(dist_dir, 'Real-ESRGAN'), realtime=False)\n",
        "                    out_file = short_name.rpartition('.')[0] + '_out.png'\n",
        "                    shutil.move(os.path.join(dist_dir, 'Real-ESRGAN', result_folder, out_file), upscaled_path)\n",
        "                    image_path = upscaled_path\n",
        "                    os.chdir(stable_dir)\n",
        "                    clear_last()\n",
        "                    if deepfloyd_prefs['display_upscaled_image']:\n",
        "                        time.sleep(0.6)\n",
        "                        prt(Row([ImageButton(src=upscaled_path, data=upscaled_path, width=width * float(deepfloyd_prefs[\"enlarge_scale\"]), height=height * float(deepfloyd_prefs[\"enlarge_scale\"]), page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "                        #prt(Row([Img(src=upscaled_path, fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "                if prefs['save_image_metadata']:\n",
        "                    img = PILImage.open(image_path)\n",
        "                    metadata = PngInfo()\n",
        "                    metadata.add_text(\"artist\", prefs['meta_ArtistName'])\n",
        "                    metadata.add_text(\"copyright\", prefs['meta_Copyright'])\n",
        "                    metadata.add_text(\"software\", \"Stable Diffusion Deluxe\" + f\", upscaled {deepfloyd_prefs['enlarge_scale']}x with ESRGAN\" if deepfloyd_prefs['apply_ESRGAN_upscale'] else \"\")\n",
        "                    metadata.add_text(\"pipeline\", \"DeepFloyd-IF\") #TODO: Img2Img or Inpainting\n",
        "                    if prefs['save_config_in_metadata']:\n",
        "                        config_json = deepfloyd_prefs.copy()\n",
        "                        config_json['model_path'] = model_id\n",
        "                        config_json['seed'] = random_seed\n",
        "                        config_json['prompt'] = pr['prompt']\n",
        "                        config_json['negative_prompt'] = pr['negative_prompt']\n",
        "                        del config_json['num_images']\n",
        "                        del config_json['max_size']\n",
        "                        del config_json['display_upscaled_image']\n",
        "                        del config_json['batch_folder_name']\n",
        "                        if not config_json['apply_ESRGAN_upscale']:\n",
        "                            del config_json['enlarge_scale']\n",
        "                            del config_json['apply_ESRGAN_upscale']\n",
        "                        metadata.add_text(\"config_json\", json.dumps(config_json, ensure_ascii=True, indent=4))\n",
        "                    img.save(image_path, pnginfo=metadata)\n",
        "                #TODO: PyDrive\n",
        "                if storage_type == \"Colab Google Drive\":\n",
        "                    new_file = available_file(os.path.join(prefs['image_output'], deepfloyd_prefs['batch_folder_name']), fname, num)\n",
        "                    out_path = new_file\n",
        "                    shutil.copy(image_path, new_file)\n",
        "                elif bool(prefs['image_output']):\n",
        "                    new_file = available_file(os.path.join(prefs['image_output'], deepfloyd_prefs['batch_folder_name']), fname, num)\n",
        "                    out_path = new_file\n",
        "                    shutil.copy(image_path, new_file)\n",
        "                time.sleep(0.2)\n",
        "                prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))\n",
        "                #num += 1\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "\n",
        "def run_text_to_video(page):\n",
        "    global text_to_video_prefs, prefs, status, pipe_text_to_video, model_path\n",
        "    if not status['installed_diffusers']:\n",
        "      alert_msg(page, \"You need to Install HuggingFace Diffusers before using...\")\n",
        "      return\n",
        "    def prt(line):\n",
        "      if type(line) == str:\n",
        "        line = Text(line, size=17)\n",
        "      page.TextToVideo.controls.append(line)\n",
        "      page.TextToVideo.update()\n",
        "    def clear_last():\n",
        "      del page.TextToVideo.controls[-1]\n",
        "      page.TextToVideo.update()\n",
        "    def clear_list():\n",
        "      page.TextToVideo.controls = page.TextToVideo.controls[:1]\n",
        "    def autoscroll(scroll=True):\n",
        "      page.TextToVideo.auto_scroll = scroll\n",
        "      page.TextToVideo.update()\n",
        "      page.TextToVideo.auto_scroll = scroll\n",
        "      page.TextToVideo.update()\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    total_steps = text_to_video_prefs['num_inference_steps']\n",
        "    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:\n",
        "      callback_fnc.has_been_called = True\n",
        "      nonlocal progress, total_steps\n",
        "      #total_steps = len(latents)\n",
        "      percent = (step +1)/ total_steps\n",
        "      progress.value = percent\n",
        "      progress.tooltip = f\"{step +1} / {total_steps}  Timestep: {timestep}\"\n",
        "      progress.update()\n",
        "      #print(f'{type(latents)} {len(latents)}- {str(latents)}')\n",
        "    clear_list()\n",
        "    autoscroll(True)\n",
        "    prt(Installing(\"Installing Text-To-Video Pipeline...\"))\n",
        "    model_id = \"damo-vilab/text-to-video-ms-1.7b\"\n",
        "    clear_pipes()\n",
        "    #clear_pipes('text_to_video')\n",
        "    if pipe_text_to_video is None:\n",
        "        from diffusers import TextToVideoSDPipeline, DPMSolverMultistepScheduler\n",
        "        pipe_text_to_video = TextToVideoSDPipeline.from_pretrained(model_id, torch_dtype=torch.float16, variant=\"fp16\", cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)\n",
        "        #pipe_text_to_video = pipeline_scheduler(pipe_text_to_video)\n",
        "        pipe_text_to_video.scheduler = DPMSolverMultistepScheduler.from_config(pipe_text_to_video.scheduler.config)\n",
        "        if text_to_video_prefs['lower_memory']:\n",
        "            pipe_text_to_video.enable_sequential_cpu_offload()\n",
        "            #pipe_text_to_video.enable_model_cpu_offload()\n",
        "            pipe_text_to_video.enable_vae_tiling()\n",
        "            #pipe_text_to_video.enable_vae_slicing()\n",
        "        else:\n",
        "            pipe_text_to_video = pipe_text_to_video.to(torch_device)\n",
        "        #pipe_text_to_video = optimize_pipe(pipe_text_to_video, vae_tiling=True, vae=True, to_gpu=False)\n",
        "        pipe_text_to_video.set_progress_bar_config(disable=True)\n",
        "    else:\n",
        "        pipe_text_to_video = pipeline_scheduler(pipe_text_to_video)\n",
        "    clear_last()\n",
        "    prt(\"Generating Text-To-Video of your Prompt...\")\n",
        "    prt(progress)\n",
        "    autoscroll(False)\n",
        "    batch_output = os.path.join(stable_dir, text_to_video_prefs['batch_folder_name'])\n",
        "    if not os.path.isdir(batch_output):\n",
        "      os.makedirs(batch_output)\n",
        "    local_output = batch_output\n",
        "    batch_output = os.path.join(prefs['image_output'], text_to_video_prefs['batch_folder_name'])\n",
        "    if not os.path.isdir(batch_output):\n",
        "      os.makedirs(batch_output)\n",
        "    random_seed = int(text_to_video_prefs['seed']) if int(text_to_video_prefs['seed']) > 0 else rnd.randint(0,4294967295)\n",
        "    generator = torch.Generator(device=\"cpu\").manual_seed(random_seed)\n",
        "    #generator = torch.manual_seed(random_seed)\n",
        "    width = text_to_video_prefs['width']\n",
        "    height = text_to_video_prefs['height']\n",
        "    try:\n",
        "      #print(f\"prompt={text_to_video_prefs['prompt']}, negative_prompt={text_to_video_prefs['negative_prompt']}, editing_prompt={editing_prompt}, edit_warmup_steps={edit_warmup_steps}, edit_guidance_scale={edit_guidance_scale}, edit_threshold={edit_threshold}, edit_weights={edit_weights}, reverse_editing_direction={reverse_editing_direction}, edit_momentum_scale={text_to_video_prefs['edit_momentum_scale']}, edit_mom_beta={text_to_video_prefs['edit_mom_beta']}, num_inference_steps={text_to_video_prefs['num_inference_steps']}, eta={text_to_video_prefs['eta']}, guidance_scale={text_to_video_prefs['guidance_scale']}\")\n",
        "      #, output_type = \"pt\", width=width, height=height\n",
        "      frames = pipe_text_to_video(prompt=text_to_video_prefs['prompt'], negative_prompt=text_to_video_prefs['negative_prompt'], num_frames=text_to_video_prefs['num_frames'], num_inference_steps=text_to_video_prefs['num_inference_steps'], eta=text_to_video_prefs['eta'], guidance_scale=text_to_video_prefs['guidance_scale'], generator=generator, callback=callback_fnc, callback_steps=1).frames\n",
        "    except Exception as e:\n",
        "      clear_last()\n",
        "      clear_last()\n",
        "      alert_msg(page, f\"ERROR: Couldn't Text-To-Video your image for some reason. Possibly out of memory or something wrong with my code...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "      return\n",
        "    clear_last()\n",
        "    clear_last()\n",
        "    autoscroll(True)\n",
        "    save_path = os.path.join(prefs['image_output'], text_to_video_prefs['batch_folder_name'])\n",
        "    filename = f\"{prefs['file_prefix']}{format_filename(text_to_video_prefs['prompt'])}\"\n",
        "    filename = filename[:int(prefs['file_max_length'])]\n",
        "    #if prefs['file_suffix_seed']: filename += f\"-{random_seed}\"\n",
        "    autoscroll(True)\n",
        "    video_path = \"\"\n",
        "    if text_to_video_prefs['export_to_video']:\n",
        "        from diffusers.utils import export_to_video\n",
        "        video_path = export_to_video(frames)\n",
        "        shutil.copy(video_path, available_file(local_output, filename, 0, ext=\"mp4\", no_num=True))\n",
        "        shutil.copy(video_path, available_file(batch_output, filename, 0, ext=\"mp4\", no_num=True))\n",
        "        #print(f\"video_path: {video_path}\")\n",
        "    #video = frames.cpu().numpy()\n",
        "    #print(f\"video: {video}\")\n",
        "    import cv2\n",
        "    from PIL.PngImagePlugin import PngInfo\n",
        "    num = 0\n",
        "    for image in frames:\n",
        "        random_seed += num\n",
        "        fname = filename + (f\"-{random_seed}\" if prefs['file_suffix_seed'] else \"\")\n",
        "        image_path = available_file(batch_output, fname, num)\n",
        "        unscaled_path = image_path\n",
        "        output_file = image_path.rpartition(slash)[2]\n",
        "        #uint8_image = (image * 255).round().astype(\"uint8\")\n",
        "        #np_image = image.cpu().numpy()\n",
        "        #print(f\"image: {type(image)}, np_image: {type(np_image)}\")\n",
        "        #print(f\"image: {type(image)} to {image_path}\")\n",
        "        cv2.imwrite(image_path, image)\n",
        "        #PILImage.fromarray(np_image).save(image_path)\n",
        "        out_path = image_path.rpartition(slash)[0]\n",
        "        upscaled_path = os.path.join(out_path, output_file)\n",
        "        if not text_to_video_prefs['display_upscaled_image'] or not text_to_video_prefs['apply_ESRGAN_upscale']:\n",
        "            prt(Row([ImageButton(src=unscaled_path, data=upscaled_path, width=width, height=height, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "        if text_to_video_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:\n",
        "            os.chdir(os.path.join(dist_dir, 'Real-ESRGAN'))\n",
        "            upload_folder = 'upload'\n",
        "            result_folder = 'results'     \n",
        "            if os.path.isdir(upload_folder):\n",
        "                shutil.rmtree(upload_folder)\n",
        "            if os.path.isdir(result_folder):\n",
        "                shutil.rmtree(result_folder)\n",
        "            os.mkdir(upload_folder)\n",
        "            os.mkdir(result_folder)\n",
        "            short_name = f'{fname[:80]}-{num}.png'\n",
        "            dst_path = os.path.join(dist_dir, 'Real-ESRGAN', upload_folder, short_name)\n",
        "            #print(f'Moving {fpath} to {dst_path}')\n",
        "            #shutil.move(fpath, dst_path)\n",
        "            shutil.copy(image_path, dst_path)\n",
        "            #faceenhance = ' --face_enhance' if text_to_video_prefs[\"face_enhance\"] else ''\n",
        "            faceenhance = ''\n",
        "            run_sp(f'python inference_realesrgan.py -n realesr-general-x4v3 -i upload --outscale {text_to_video_prefs[\"enlarge_scale\"]}{faceenhance}', cwd=os.path.join(dist_dir, 'Real-ESRGAN'), realtime=False)\n",
        "            out_file = short_name.rpartition('.')[0] + '_out.png'\n",
        "            shutil.move(os.path.join(dist_dir, 'Real-ESRGAN', result_folder, out_file), upscaled_path)\n",
        "            image_path = upscaled_path\n",
        "            os.chdir(stable_dir)\n",
        "            if text_to_video_prefs['display_upscaled_image']:\n",
        "                time.sleep(0.6)\n",
        "                prt(Row([ImageButton(src=upscaled_path, data=upscaled_path, width=width * float(text_to_video_prefs[\"enlarge_scale\"]), height=height * float(text_to_video_prefs[\"enlarge_scale\"]), page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "                #prt(Row([Img(src=upscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "        if prefs['save_image_metadata']:\n",
        "            img = PILImage.open(image_path)\n",
        "            metadata = PngInfo()\n",
        "            metadata.add_text(\"artist\", prefs['meta_ArtistName'])\n",
        "            metadata.add_text(\"copyright\", prefs['meta_Copyright'])\n",
        "            metadata.add_text(\"software\", \"Stable Diffusion Deluxe\" + f\", upscaled {text_to_video_prefs['enlarge_scale']}x with ESRGAN\" if text_to_video_prefs['apply_ESRGAN_upscale'] else \"\")\n",
        "            metadata.add_text(\"pipeline\", \"Text-To-Video\")\n",
        "            if prefs['save_config_in_metadata']:\n",
        "              config_json = text_to_video_prefs.copy()\n",
        "              config_json['model_path'] = model_id\n",
        "              config_json['scheduler_mode'] = prefs['scheduler_mode']\n",
        "              config_json['seed'] = random_seed\n",
        "              del config_json['num_frames']\n",
        "              del config_json['width']\n",
        "              del config_json['height']\n",
        "              del config_json['display_upscaled_image']\n",
        "              del config_json['batch_folder_name']\n",
        "              del config_json['lower_memory']\n",
        "              if not config_json['apply_ESRGAN_upscale']:\n",
        "                del config_json['enlarge_scale']\n",
        "                del config_json['apply_ESRGAN_upscale']\n",
        "              metadata.add_text(\"config_json\", json.dumps(config_json, ensure_ascii=True, indent=4))\n",
        "            img.save(image_path, pnginfo=metadata)\n",
        "        #TODO: PyDrive\n",
        "        if storage_type == \"Colab Google Drive\":\n",
        "            new_file = available_file(os.path.join(prefs['image_output'], text_to_video_prefs['batch_folder_name']), fname, num)\n",
        "            out_path = new_file\n",
        "            shutil.copy(image_path, new_file)\n",
        "        elif bool(prefs['image_output']):\n",
        "            new_file = available_file(os.path.join(prefs['image_output'], text_to_video_prefs['batch_folder_name']), fname, num)\n",
        "            out_path = new_file\n",
        "            shutil.copy(image_path, new_file)\n",
        "        prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))\n",
        "        num += 1\n",
        "    autoscroll(False)\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "def run_text_to_video_zero(page):\n",
        "    global text_to_video_zero_prefs, prefs, status, pipe_text_to_video_zero, model_path\n",
        "    if not status['installed_diffusers']:\n",
        "      alert_msg(page, \"You need to Install HuggingFace Diffusers before using...\")\n",
        "      return\n",
        "    def prt(line):\n",
        "      if type(line) == str:\n",
        "        line = Text(line, size=17)\n",
        "      page.TextToVideo.controls.append(line)\n",
        "      page.TextToVideo.update()\n",
        "    def clear_last():\n",
        "      del page.TextToVideo.controls[-1]\n",
        "      page.TextToVideo.update()\n",
        "    def clear_list():\n",
        "      page.TextToVideo.controls = page.TextToVideo.controls[:1]\n",
        "    def autoscroll(scroll=True):\n",
        "      page.TextToVideo.auto_scroll = scroll\n",
        "      page.TextToVideo.update()\n",
        "      page.TextToVideo.auto_scroll = scroll\n",
        "      page.TextToVideo.update()\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    total_steps = text_to_video_zero_prefs['num_inference_steps']\n",
        "    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:\n",
        "      callback_fnc.has_been_called = True\n",
        "      nonlocal progress, total_steps\n",
        "      #total_steps = len(latents)\n",
        "      percent = (step +1)/ total_steps\n",
        "      progress.value = percent\n",
        "      progress.tooltip = f\"{step +1} / {total_steps}  Timestep: {timestep}\"\n",
        "      progress.update()\n",
        "      #print(f'{type(latents)} {len(latents)}- {str(latents)}')\n",
        "    clear_list()\n",
        "    autoscroll(True)\n",
        "    prt(Installing(\"Installing Text-To-Video Zero Pipeline...\"))\n",
        "    import cv2\n",
        "    #model_id = \"damo-vilab/text-to-video-ms-1.7b\"\n",
        "    clear_pipes()\n",
        "    #clear_pipes('text_to_video_zero')\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.reset_max_memory_allocated()\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "    if pipe_text_to_video_zero is None:\n",
        "        from diffusers import TextToVideoZeroPipeline, DPMSolverMultistepScheduler\n",
        "        pipe_text_to_video_zero = TextToVideoZeroPipeline.from_pretrained(model_path, torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)\n",
        "        #pipe_text_to_video_zero = pipeline_scheduler(pipe_text_to_video_zero)\n",
        "        pipe_text_to_video_zero.scheduler = DPMSolverMultistepScheduler.from_config(pipe_text_to_video_zero.scheduler.config)\n",
        "        pipe_text_to_video_zero = pipe_text_to_video_zero.to(torch_device)\n",
        "        #pipe_text_to_video_zero = optimize_pipe(pipe_text_to_video_zero, vae_tiling=True, vae=True, to_gpu=False)\n",
        "        pipe_text_to_video_zero.set_progress_bar_config(disable=True)\n",
        "    else:\n",
        "        pipe_text_to_video_zero = pipeline_scheduler(pipe_text_to_video_zero)\n",
        "    try:\n",
        "        import imageio\n",
        "    except ModuleNotFoundError:\n",
        "        run_sp(\"pip install imageio\", realtime=False)\n",
        "        import imageio\n",
        "        pass\n",
        "    clear_last()\n",
        "    prt(\"Generating Text-To-Video Zero from your Prompt...\")\n",
        "    prt(progress)\n",
        "    autoscroll(False)\n",
        "    batch_output = os.path.join(stable_dir, text_to_video_zero_prefs['batch_folder_name'])\n",
        "    if not os.path.isdir(batch_output):\n",
        "      os.makedirs(batch_output)\n",
        "    local_output = batch_output\n",
        "    batch_output = os.path.join(prefs['image_output'], text_to_video_zero_prefs['batch_folder_name'])\n",
        "    if not os.path.isdir(batch_output):\n",
        "      os.makedirs(batch_output)\n",
        "    random_seed = int(text_to_video_zero_prefs['seed']) if int(text_to_video_zero_prefs['seed']) > 0 else rnd.randint(0,4294967295)\n",
        "    generator = torch.Generator(device=\"cuda\").manual_seed(random_seed)\n",
        "    #generator = torch.manual_seed(random_seed)\n",
        "    width = text_to_video_zero_prefs['width']\n",
        "    height = text_to_video_zero_prefs['height']\n",
        "    try:\n",
        "      #print(f\"prompt={text_to_video_zero_prefs['prompt']}, negative_prompt={text_to_video_zero_prefs['negative_prompt']}, editing_prompt={editing_prompt}, edit_warmup_steps={edit_warmup_steps}, edit_guidance_scale={edit_guidance_scale}, edit_threshold={edit_threshold}, edit_weights={edit_weights}, reverse_editing_direction={reverse_editing_direction}, edit_momentum_scale={text_to_video_zero_prefs['edit_momentum_scale']}, edit_mom_beta={text_to_video_zero_prefs['edit_mom_beta']}, num_inference_steps={text_to_video_zero_prefs['num_inference_steps']}, eta={text_to_video_zero_prefs['eta']}, guidance_scale={text_to_video_zero_prefs['guidance_scale']}\")\n",
        "      #, output_type = \"pt\", width=width, height=height\n",
        "      frames = pipe_text_to_video_zero(prompt=text_to_video_zero_prefs['prompt'], negative_prompt=text_to_video_zero_prefs['negative_prompt'], video_length=text_to_video_zero_prefs['num_frames'], num_inference_steps=text_to_video_zero_prefs['num_inference_steps'], eta=text_to_video_zero_prefs['eta'], guidance_scale=text_to_video_zero_prefs['guidance_scale'], motion_field_strength_x=text_to_video_zero_prefs['motion_field_strength_x'], motion_field_strength_y=text_to_video_zero_prefs['motion_field_strength_y'], t0=text_to_video_zero_prefs['t0'], t1=text_to_video_zero_prefs['t1'], generator=generator, callback=callback_fnc, callback_steps=1).images\n",
        "    except Exception as e:\n",
        "      clear_last()\n",
        "      clear_last()\n",
        "      alert_msg(page, f\"ERROR: Couldn't Text-To-Video Zero your image for some reason. Possibly out of memory or something wrong with my code...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "      return\n",
        "    clear_last()\n",
        "    clear_last()\n",
        "    save_path = os.path.join(prefs['image_output'], text_to_video_zero_prefs['batch_folder_name'])\n",
        "    filename = f\"{prefs['file_prefix']}{format_filename(text_to_video_zero_prefs['prompt'])}\"\n",
        "    filename = filename[:int(prefs['file_max_length'])]\n",
        "    #if prefs['file_suffix_seed']: filename += f\"-{random_seed}\"\n",
        "    autoscroll(True)\n",
        "    video_path = \"\"\n",
        "    if text_to_video_zero_prefs['export_to_video']:\n",
        "        #from diffusers.utils import export_to_video\n",
        "        #video_path = export_to_video(frames)\n",
        "        local_file = available_file(local_output, filename, 0, ext=\"mp4\", no_num=True)\n",
        "        save_file = available_file(batch_output, filename, 0, ext=\"mp4\", no_num=True)\n",
        "        imageio.mimsave(local_file, frames, fps=4)\n",
        "        shutil.copy(local_file, save_file)\n",
        "        #print(f\"video_path: {video_path}\")\n",
        "    #video = frames.cpu().numpy()\n",
        "    #print(f\"video: {video}\")\n",
        "    #print(f\"frames type: {type(frames)} len: {len(frames)}\")\n",
        "    #result = [(r * 255).astype(\"uint8\") for r in result]\n",
        "    from PIL.PngImagePlugin import PngInfo\n",
        "    num = 0\n",
        "    for image in frames:\n",
        "        random_seed += num\n",
        "        fname = filename + (f\"-{random_seed}\" if prefs['file_suffix_seed'] else \"\")\n",
        "        image_path = available_file(batch_output, fname, num)\n",
        "        unscaled_path = image_path\n",
        "        output_file = image_path.rpartition(slash)[2]\n",
        "        #uint8_image = (image * 255).round().astype(\"uint8\")\n",
        "        img = PILImage.fromarray((image * 255).astype(\"uint8\"))\n",
        "        #print(f\"img type: {type(img)}\")\n",
        "        #np_image = image.cpu().numpy()\n",
        "        #print(f\"image: {type(image)}, np_image: {type(np_image)}\")\n",
        "        #print(f\"image type: {type(image)} to {image_path}\")\n",
        "        #cv2.imwrite(image_path, image)\n",
        "        #PILImage.fromarray(image).save(image_path)\n",
        "        img.save(image_path)\n",
        "        #image.save(image_path)\n",
        "        #imageio.imwrite(local_file, image, extension=\".png\")\n",
        "        #img = pipe_text_to_video_zero.numpy_to_pil(image)\n",
        "        #img.save(image_path)\n",
        "        #imageio.imsave(local_file, img, extension=\".png\")\n",
        "        #PILImage.fromarray(img).save(image_path)\n",
        "        out_path = image_path.rpartition(slash)[0]\n",
        "        upscaled_path = os.path.join(out_path, output_file)\n",
        "        if not text_to_video_zero_prefs['display_upscaled_image'] or not text_to_video_zero_prefs['apply_ESRGAN_upscale']:\n",
        "            prt(Row([ImageButton(src=unscaled_path, data=upscaled_path, width=width, height=height, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "        if text_to_video_zero_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:\n",
        "            os.chdir(os.path.join(dist_dir, 'Real-ESRGAN'))\n",
        "            upload_folder = 'upload'\n",
        "            result_folder = 'results'     \n",
        "            if os.path.isdir(upload_folder):\n",
        "                shutil.rmtree(upload_folder)\n",
        "            if os.path.isdir(result_folder):\n",
        "                shutil.rmtree(result_folder)\n",
        "            os.mkdir(upload_folder)\n",
        "            os.mkdir(result_folder)\n",
        "            short_name = f'{fname[:80]}-{num}.png'\n",
        "            dst_path = os.path.join(dist_dir, 'Real-ESRGAN', upload_folder, short_name)\n",
        "            #print(f'Moving {fpath} to {dst_path}')\n",
        "            #shutil.move(fpath, dst_path)\n",
        "            shutil.copy(image_path, dst_path)\n",
        "            #faceenhance = ' --face_enhance' if text_to_video_zero_prefs[\"face_enhance\"] else ''\n",
        "            faceenhance = ''\n",
        "            run_sp(f'python inference_realesrgan.py -n realesr-general-x4v3 -i upload --outscale {text_to_video_zero_prefs[\"enlarge_scale\"]}{faceenhance}', cwd=os.path.join(dist_dir, 'Real-ESRGAN'), realtime=False)\n",
        "            out_file = short_name.rpartition('.')[0] + '_out.png'\n",
        "            shutil.move(os.path.join(dist_dir, 'Real-ESRGAN', result_folder, out_file), upscaled_path)\n",
        "            image_path = upscaled_path\n",
        "            os.chdir(stable_dir)\n",
        "            if text_to_video_zero_prefs['display_upscaled_image']:\n",
        "                time.sleep(0.2)\n",
        "                prt(Row([ImageButton(src=upscaled_path, data=upscaled_path, width=width * float(text_to_video_zero_prefs[\"enlarge_scale\"]), height=height * float(text_to_video_zero_prefs[\"enlarge_scale\"]), page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "                #prt(Row([Img(src=upscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "        if prefs['save_image_metadata']:\n",
        "            img = PILImage.open(image_path)\n",
        "            metadata = PngInfo()\n",
        "            metadata.add_text(\"artist\", prefs['meta_ArtistName'])\n",
        "            metadata.add_text(\"copyright\", prefs['meta_Copyright'])\n",
        "            metadata.add_text(\"software\", \"Stable Diffusion Deluxe\" + f\", upscaled {text_to_video_zero_prefs['enlarge_scale']}x with ESRGAN\" if text_to_video_zero_prefs['apply_ESRGAN_upscale'] else \"\")\n",
        "            metadata.add_text(\"pipeline\", \"Text-To-Video Zero\")\n",
        "            if prefs['save_config_in_metadata']:\n",
        "              config_json = text_to_video_zero_prefs.copy()\n",
        "              config_json['model_path'] = model_path\n",
        "              config_json['scheduler_mode'] = prefs['scheduler_mode']\n",
        "              config_json['seed'] = random_seed\n",
        "              del config_json['num_frames']\n",
        "              del config_json['width']\n",
        "              del config_json['height']\n",
        "              del config_json['display_upscaled_image']\n",
        "              del config_json['batch_folder_name']\n",
        "              if not config_json['apply_ESRGAN_upscale']:\n",
        "                del config_json['enlarge_scale']\n",
        "                del config_json['apply_ESRGAN_upscale']\n",
        "              metadata.add_text(\"config_json\", json.dumps(config_json, ensure_ascii=True, indent=4))\n",
        "            img.save(image_path, pnginfo=metadata)\n",
        "        #TODO: PyDrive\n",
        "        if storage_type == \"Colab Google Drive\":\n",
        "            new_file = available_file(os.path.join(prefs['image_output'], text_to_video_zero_prefs['batch_folder_name']), fname, num)\n",
        "            out_path = new_file\n",
        "            shutil.copy(image_path, new_file)\n",
        "        elif bool(prefs['image_output']):\n",
        "            new_file = available_file(os.path.join(prefs['image_output'], text_to_video_zero_prefs['batch_folder_name']), fname, num)\n",
        "            out_path = new_file\n",
        "            shutil.copy(image_path, new_file)\n",
        "        prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))\n",
        "        num += 1\n",
        "    autoscroll(False)\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "    \n",
        "def run_stable_animation(page):\n",
        "    global stable_animation_prefs, prefs, status\n",
        "    if not bool(prefs['Stability_api_key']):\n",
        "        alert_msg(e.page, \"You must have your DreamStudio.ai Stability-API Key to use Stability.  Note that it will cost you tokens.\")\n",
        "        return\n",
        "    def prt(line):\n",
        "      if type(line) == str:\n",
        "        line = Text(line, size=17)\n",
        "      page.StableAnimation.controls.append(line)\n",
        "      page.StableAnimation.update()\n",
        "    def clear_last():\n",
        "      del page.StableAnimation.controls[-1]\n",
        "      page.StableAnimation.update()\n",
        "    def clear_list():\n",
        "      page.StableAnimation.controls = page.StableAnimation.controls[:1]\n",
        "    def autoscroll(scroll=True):\n",
        "      page.StableAnimation.auto_scroll = scroll\n",
        "      page.StableAnimation.update()\n",
        "    abort_run = False\n",
        "    def abort_diffusion(e):\n",
        "      nonlocal abort_run\n",
        "      abort_run = True\n",
        "      page.snd_error.play()\n",
        "      page.snd_delete.play()\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    total_steps = stable_animation_prefs['num_inference_steps']\n",
        "    def callback_fnc(step: int) -> None:\n",
        "      callback_fnc.has_been_called = True\n",
        "      nonlocal progress, total_steps\n",
        "      percent = (step +1)/ total_steps\n",
        "      progress.value = percent\n",
        "      progress.tooltip = f\"{step +1} / {total_steps}\"\n",
        "      progress.update()\n",
        "    clear_list()\n",
        "    autoscroll(True)\n",
        "    prt(Installing(\"Installing Stable Animation Pipeline...\"))\n",
        "    #import cv2\n",
        "    #model_id = \"damo-vilab/text-to-video-ms-1.7b\"\n",
        "    try:\n",
        "        clear_pipes()\n",
        "        #clear_pipes('stable_animation')\n",
        "        torch.cuda.empty_cache()\n",
        "        #torch.cuda.reset_max_memory_allocated()\n",
        "        torch.cuda.reset_peak_memory_stats()\n",
        "    except Exception:\n",
        "        pass\n",
        "    try:\n",
        "        from stability_sdk import api\n",
        "    except ModuleNotFoundError:\n",
        "        run_sp(\"pip install stability_sdk[anim_ui]\", realtime=True)\n",
        "        from stability_sdk import api\n",
        "        pass\n",
        "    from tqdm import tqdm\n",
        "    from PIL.PngImagePlugin import PngInfo\n",
        "    \n",
        "    #batch_output = os.path.join(stable_dir, stable_animation_prefs['batch_folder_name'])\n",
        "    #if not os.path.isdir(batch_output):\n",
        "    #  os.makedirs(batch_output)\n",
        "    #local_output = batch_output\n",
        "    batch_output = os.path.join(prefs['image_output'], stable_animation_prefs['batch_folder_name'])\n",
        "    if not os.path.isdir(batch_output):\n",
        "      os.makedirs(batch_output)\n",
        "    random_seed = int(stable_animation_prefs['seed']) if int(stable_animation_prefs['seed']) > 0 else rnd.randint(0,4294967295)\n",
        "    width = stable_animation_prefs['width']\n",
        "    height = stable_animation_prefs['height']\n",
        "    animation_prompts = stable_animation_prefs['animation_prompt'].strip()\n",
        "    try:\n",
        "        prompts = json.loads(animation_prompts)\n",
        "    except json.JSONDecodeError:\n",
        "        try:\n",
        "            prompts = eval(animation_prompts)\n",
        "        except Exception as e:\n",
        "            alert_msg(page, \"Invalid JSON or Python code for animation_prompts.\")\n",
        "            return\n",
        "    prompts = {int(k): v for k, v in prompts.items()}\n",
        "    from stability_sdk.api import (ClassifierException, Context, OutOfCreditsException)\n",
        "    context = Context(\"grpc.stability.ai:443\", prefs['Stability_api_key'])\n",
        "    from stability_sdk.animation import (\n",
        "        AnimationArgs,\n",
        "        Animator,\n",
        "        interpolate_frames\n",
        "    )\n",
        "    from stability_sdk.utils import (create_video_from_frames, extract_frames_from_video, interpolate_mode_from_string)\n",
        "    args = AnimationArgs()\n",
        "    args.width = width\n",
        "    args.height = height\n",
        "    args.sampler = stable_animation_prefs['sampler']\n",
        "    #args_generation.custom_model = stable_animation_prefs['custom_model']\n",
        "    args.model = stable_animation_prefs['model'].lower()\n",
        "    args.seed = random_seed\n",
        "    args.cfg_scale = stable_animation_prefs['guidance_scale']\n",
        "    args.clip_guidance = stable_animation_prefs['clip_guidance']\n",
        "    args.init_image = stable_animation_prefs['init_image']\n",
        "    args.init_sizing = stable_animation_prefs['init_sizing'].lower()\n",
        "    args.mask_path = stable_animation_prefs['mask_image']\n",
        "    args.mask_invert = stable_animation_prefs['mask_invert']\n",
        "    args.preset = stable_animation_prefs['style_preset']\n",
        "    args.animation_mode = stable_animation_prefs['animation_mode']\n",
        "    args.max_frames = stable_animation_prefs['max_frames']\n",
        "    args.border = stable_animation_prefs['border'].lower()\n",
        "    args.noise_add_curve = stable_animation_prefs['noise_add_curve']\n",
        "    args.noise_scale_curve = stable_animation_prefs['noise_scale_curve']\n",
        "    args.strength_curve = stable_animation_prefs['strength_curve']\n",
        "    args.steps_curve = stable_animation_prefs['steps_curve']\n",
        "    args.steps_strength_adj = stable_animation_prefs['steps_strength_adj']\n",
        "    args.interpolate_prompts = stable_animation_prefs['interpolate_prompts']\n",
        "    args.locked_seed = stable_animation_prefs['locked_seed']\n",
        "    args.angle = stable_animation_prefs['angle']\n",
        "    args.zoom = stable_animation_prefs['zoom']\n",
        "    args.translation_x = stable_animation_prefs['translation_x']\n",
        "    args.translation_y = stable_animation_prefs['translation_y']\n",
        "    args.translation_z = stable_animation_prefs['translation_z']\n",
        "    args.rotation_x = stable_animation_prefs['rotation_x']\n",
        "    args.rotation_y = stable_animation_prefs['rotation_y']\n",
        "    args.rotation_z = stable_animation_prefs['rotation_z']\n",
        "    args.diffusion_cadence_curve = stable_animation_prefs['diffusion_cadence_curve']\n",
        "    args.cadence_interp = stable_animation_prefs['cadence_interp'].lower()\n",
        "    args.cadence_spans = stable_animation_prefs['cadence_spans']\n",
        "    args.color_coherence = stable_animation_prefs['color_coherence']\n",
        "    args.brightness_curve = stable_animation_prefs['brightness_curve']\n",
        "    args.contrast_curve = stable_animation_prefs['contrast_curve']\n",
        "    args.hue_curve = stable_animation_prefs['hue_curve']\n",
        "    args.saturation_curve = stable_animation_prefs['saturation_curve']\n",
        "    args.lightness_curve = stable_animation_prefs['lightness_curve']\n",
        "    args.color_match_animate = stable_animation_prefs['color_match_animate']\n",
        "    args.depth_model_weight = stable_animation_prefs['depth_model_weight']\n",
        "    args.near_plane = stable_animation_prefs['near_plane']\n",
        "    args.far_plane = stable_animation_prefs['far_plane']\n",
        "    args.fov_curve = stable_animation_prefs['fov_curve']\n",
        "    args.depth_blur_curve = stable_animation_prefs['depth_blur_curve']\n",
        "    args.depth_warp_curve = stable_animation_prefs['depth_warp_curve']\n",
        "    #args_depth.save_depth_maps = stable_animation_prefs['save_depth_maps']\n",
        "    args.camera_type = stable_animation_prefs['camera_type'].lower()\n",
        "    args.render_mode = stable_animation_prefs['render_mode'].lower()\n",
        "    args.mask_power = stable_animation_prefs['mask_power']\n",
        "    args.use_inpainting_model = stable_animation_prefs['use_inpainting_model']\n",
        "    args.inpaint_border = stable_animation_prefs['inpaint_border']\n",
        "    args.mask_min_value = stable_animation_prefs['mask_min_value']\n",
        "    args.mask_binarization_thr = stable_animation_prefs['mask_binarization_thr']\n",
        "    #args_inpaint.save_inpaint_masks = False\n",
        "    args.video_init_path = stable_animation_prefs['video_init_path']\n",
        "    args.extract_nth_frame = int(stable_animation_prefs['extract_nth_frame'])\n",
        "    args.video_mix_in_curve = stable_animation_prefs['video_mix_in_curve']\n",
        "    args.video_flow_warp = stable_animation_prefs['video_flow_warp']\n",
        "    args.fps = stable_animation_prefs['output_fps']\n",
        "    args.reverse = False\n",
        "    #arg_objs = AnimationArgs(args_generation, args_animation, args_camera, args_coherence, args_color, args_depth, args_render_3d, args_inpaint, args_vid_in, args_vid_out)\n",
        "    try:\n",
        "        animator = Animator(\n",
        "            api_context=context,\n",
        "            animation_prompts=prompts,\n",
        "            negative_prompt=stable_animation_prefs['negative_prompt'],\n",
        "            args=args,\n",
        "            #out_dir=batch_output\n",
        "            #negative_prompt_weight=negative_prompt_weight,\n",
        "            #resume=resume,\n",
        "        )\n",
        "    except ClassifierException as e:\n",
        "        alert_msg(page, \"Animation terminated early due to NSFW classifier.\")\n",
        "        return\n",
        "    except OutOfCreditsException as e:\n",
        "        alert_msg(page, f\"Animation terminated early, out of credits.\\n{e.details}\")\n",
        "        return\n",
        "    except Exception as e:\n",
        "        alert_msg(page, f\"Animation terminated early due to exception:\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "        return\n",
        "    total_steps = args.max_frames\n",
        "    filename = f\"{prefs['file_prefix']}{format_filename(prompts[0])}\"\n",
        "    filename = filename[:int(prefs['file_max_length'])]\n",
        "    #fname = filename + (f\"-{random_seed}\" if prefs['file_suffix_seed'] else \"\")\n",
        "    clear_last()\n",
        "    prt(Row([Text(\"Generating Stable Animation from your Prompts...\"), Container(content=None, expand=True), IconButton(icon=icons.CANCEL, tooltip=\"Abort Current Run\", on_click=abort_diffusion)]))\n",
        "    prt(progress)\n",
        "    autoscroll(False)\n",
        "    try:\n",
        "      #frames = pipe_stable_animation(prompt=stable_animation_prefs['prompt'], negative_prompt=stable_animation_prefs['negative_prompt'], video_length=stable_animation_prefs['max_frames'], num_inference_steps=stable_animation_prefs['num_inference_steps'], eta=stable_animation_prefs['eta'], guidance_scale=stable_animation_prefs['guidance_scale'], motion_field_strength_x=stable_animation_prefs['motion_field_strength_x'], motion_field_strength_y=stable_animation_prefs['motion_field_strength_y'], t0=stable_animation_prefs['t0'], t1=stable_animation_prefs['t1'], generator=generator, callback=callback_fnc, callback_steps=1).images\n",
        "      for num, image in enumerate(tqdm(animator.render(), initial=animator.start_frame_idx, total=args.max_frames), start=animator.start_frame_idx):\n",
        "        if abort_run:\n",
        "            clear_last()\n",
        "            clear_last()\n",
        "            prt(\"üõë  Aborted Current Animation Run\")\n",
        "            return\n",
        "        callback_fnc(num)\n",
        "        fname = f\"frame_{num:05d}\"\n",
        "        #image_path = available_file(batch_output, fname, num, no_num=True)\n",
        "        image_path = os.path.join(batch_output, f\"{fname}.png\")\n",
        "        unscaled_path = image_path\n",
        "        output_file = image_path.rpartition(slash)[2]\n",
        "        image.save(image_path)\n",
        "        out_path = image_path.rpartition(slash)[0]\n",
        "        upscaled_path = os.path.join(out_path, output_file)\n",
        "        \n",
        "        if stable_animation_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:\n",
        "            os.chdir(os.path.join(dist_dir, 'Real-ESRGAN'))\n",
        "            upload_folder = 'upload'\n",
        "            result_folder = 'results'     \n",
        "            if os.path.isdir(upload_folder):\n",
        "                shutil.rmtree(upload_folder)\n",
        "            if os.path.isdir(result_folder):\n",
        "                shutil.rmtree(result_folder)\n",
        "            os.mkdir(upload_folder)\n",
        "            os.mkdir(result_folder)\n",
        "            short_name = f'{fname[:80]}-{num}.png'\n",
        "            dst_path = os.path.join(dist_dir, 'Real-ESRGAN', upload_folder, short_name)\n",
        "            shutil.copy(image_path, dst_path)\n",
        "            faceenhance = ''\n",
        "            run_sp(f'python inference_realesrgan.py -n realesr-general-x4v3 -i upload --outscale {stable_animation_prefs[\"enlarge_scale\"]}{faceenhance}', cwd=os.path.join(dist_dir, 'Real-ESRGAN'), realtime=False)\n",
        "            out_file = short_name.rpartition('.')[0] + '_out.png'\n",
        "            shutil.move(os.path.join(dist_dir, 'Real-ESRGAN', result_folder, out_file), upscaled_path)\n",
        "            image_path = upscaled_path\n",
        "            os.chdir(stable_dir)\n",
        "            if stable_animation_prefs['display_upscaled_image']:\n",
        "                time.sleep(0.2)\n",
        "                prt(Row([ImageButton(src=upscaled_path, data=upscaled_path, width=width * float(stable_animation_prefs[\"enlarge_scale\"]), height=height * float(stable_animation_prefs[\"enlarge_scale\"]), page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "                #prt(Row([Img(src=upscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "        if prefs['save_image_metadata']:\n",
        "            img = PILImage.open(image_path)\n",
        "            metadata = PngInfo()\n",
        "            metadata.add_text(\"artist\", prefs['meta_ArtistName'])\n",
        "            metadata.add_text(\"copyright\", prefs['meta_Copyright'])\n",
        "            metadata.add_text(\"software\", \"Stable Diffusion Deluxe\" + f\", upscaled {stable_animation_prefs['enlarge_scale']}x with ESRGAN\" if stable_animation_prefs['apply_ESRGAN_upscale'] else \"\")\n",
        "            metadata.add_text(\"pipeline\", \"Stable Animation\")\n",
        "            if prefs['save_config_in_metadata']:\n",
        "                config_json = stable_animation_prefs.copy()\n",
        "                config_json['model_path'] = model_path\n",
        "                config_json['scheduler_mode'] = prefs['scheduler_mode']\n",
        "                config_json['seed'] = random_seed\n",
        "                del config_json['max_frames']\n",
        "                del config_json['width']\n",
        "                del config_json['height']\n",
        "                del config_json['display_upscaled_image']\n",
        "                del config_json['batch_folder_name']\n",
        "                if not config_json['apply_ESRGAN_upscale']:\n",
        "                    del config_json['enlarge_scale']\n",
        "                    del config_json['apply_ESRGAN_upscale']\n",
        "                metadata.add_text(\"config_json\", json.dumps(config_json, ensure_ascii=True, indent=4))\n",
        "            img.save(image_path, pnginfo=metadata)\n",
        "        if not stable_animation_prefs['display_upscaled_image'] or not stable_animation_prefs['apply_ESRGAN_upscale']:\n",
        "            prt(Row([ImageButton(src=unscaled_path, data=upscaled_path, width=width, height=height, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "        '''if storage_type == \"Colab Google Drive\":\n",
        "            new_file = available_file(os.path.join(prefs['image_output'], stable_animation_prefs['batch_folder_name']), fname, num, no_num=True)\n",
        "            out_path = new_file\n",
        "            shutil.copy(image_path, new_file)\n",
        "        elif bool(prefs['image_output']):\n",
        "            new_file = available_file(os.path.join(prefs['image_output'], stable_animation_prefs['batch_folder_name']), fname, num, no_num=True)\n",
        "            out_path = new_file\n",
        "            shutil.copy(image_path, new_file)'''\n",
        "        prt(Row([Text(image_path)], alignment=MainAxisAlignment.CENTER))\n",
        "    except Exception as e:\n",
        "      #clear_last()\n",
        "      #clear_last()\n",
        "      alert_msg(page, f\"ERROR: Couldn't Stable Animation your image for some reason. Possibly out of memory or something wrong with my code...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "      return\n",
        "    #clear_last()\n",
        "    #clear_last()\n",
        "    save_path = os.path.join(prefs['image_output'], stable_animation_prefs['batch_folder_name'])\n",
        "    #filename = f\"{prefs['file_prefix']}{format_filename(prompts[0])}\"\n",
        "    #filename = filename[:int(prefs['file_max_length'])]\n",
        "    autoscroll(True)\n",
        "    \n",
        "    if stable_animation_prefs['export_to_video']:\n",
        "        prt(\"Exporting Frames to Video\")\n",
        "        #from diffusers.utils import export_to_video\n",
        "        #video_path = export_to_video(frames)\n",
        "        #local_file = available_file(local_output, filename, 0, ext=\"mp4\", no_num=True)\n",
        "        save_file = available_file(batch_output, filename, 0, ext=\"mp4\", no_num=True)\n",
        "        create_video_from_frames(batch_output, save_file, fps=stable_animation_prefs['output_fps'])\n",
        "        #imageio.mimsave(local_file, frames, fps=4)\n",
        "        #shutil.copy(local_file, save_file)\n",
        "        clear_last()\n",
        "    autoscroll(False)\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "    \n",
        "\n",
        "def run_materialdiffusion(page):\n",
        "    global materialdiffusion_prefs, prefs\n",
        "    if not bool(materialdiffusion_prefs['material_prompt']):\n",
        "      alert_msg(page, \"You must provide a text prompt to process your material...\")\n",
        "      return\n",
        "    if not bool(prefs['Replicate_api_key']):\n",
        "      alert_msg(page, \"You must provide your Replicate API Token in Settings to process your material...\")\n",
        "      return\n",
        "    def prt(line):\n",
        "      if type(line) == str:\n",
        "        line = Text(line, size=17)\n",
        "      page.MaterialDiffusion.controls.append(line)\n",
        "      page.MaterialDiffusion.update()\n",
        "    def clear_last():\n",
        "      del page.MaterialDiffusion.controls[-1]\n",
        "      page.MaterialDiffusion.update()\n",
        "    def clear_list():\n",
        "      page.MaterialDiffusion.controls = page.MaterialDiffusion.controls[:1]\n",
        "    def autoscroll(scroll=True):\n",
        "      page.MaterialDiffusion.auto_scroll = scroll\n",
        "      page.MaterialDiffusion.update()\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:\n",
        "      callback_fnc.has_been_called = True\n",
        "      nonlocal progress\n",
        "      total_steps = len(latents)\n",
        "      percent = (step +1)/ total_steps\n",
        "      progress.value = percent\n",
        "      progress.tooltip = f\"{step +1} / {total_steps}  Timestep: {timestep}\"\n",
        "      progress.update()\n",
        "      #print(f'{type(latents)} {len(latents)}- {str(latents)}')\n",
        "    clear_list()\n",
        "    autoscroll(True)\n",
        "    try:\n",
        "      run_sp(\"pip install git+https://github.com/TomMoore515/material_stable_diffusion.git@main#egg=predict\", realtime=True)\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        #alert_msg(page, f\"Error installing Material Diffusion from TomMoore515...\", content=Text(str(e)))\n",
        "        pass\n",
        "    prt(Installing(\"Installing Replicate Material Diffusion Pipeline...\"))\n",
        "    try:\n",
        "        import replicate\n",
        "    except ModuleNotFoundError as e:\n",
        "        run_process(\"pip install replicate -qq\", realtime=True)\n",
        "        import replicate\n",
        "        pass\n",
        "    os.environ[\"REPLICATE_API_TOKEN\"] = prefs['Replicate_api_key']\n",
        "    #export REPLICATE_API_TOKEN=\n",
        "    try:\n",
        "        model = replicate.models.get(\"tommoore515/material_stable_diffusion\")\n",
        "        version = model.versions.get(\"3b5c0242f8925a4ab6c79b4c51e9b4ce6374e9b07b5e8461d89e692fd0faa449\")\n",
        "    except Exception as e:\n",
        "        alert_msg(page, f\"Seems like your Replicate API Token is Invalid. Check it again...\", content=Text(str(e)))\n",
        "        return\n",
        "    import requests\n",
        "    from io import BytesIO\n",
        "    from PIL import ImageOps\n",
        "    init_img = None\n",
        "    if bool(materialdiffusion_prefs['init_image']):\n",
        "        if materialdiffusion_prefs['init_image'].startswith('http'):\n",
        "            init_img = PILImage.open(requests.get(materialdiffusion_prefs['init_image'], stream=True).raw)\n",
        "        else:\n",
        "            if os.path.isfile(materialdiffusion_prefs['init_image']):\n",
        "                init_img = PILImage.open(materialdiffusion_prefs['init_image'])\n",
        "            else:\n",
        "                alert_msg(page, f\"ERROR: Couldn't find your init_image {materialdiffusion_prefs['init_image']}\")\n",
        "                return\n",
        "        #width, height = init_img.size\n",
        "        #width, height = scale_dimensions(materialdiffusion_prefs['width'], materialdiffusion_prefs['height'])\n",
        "        init_img = init_img.resize((materialdiffusion_prefs['width'], materialdiffusion_prefs['height']), resample=PILImage.Resampling.LANCZOS)\n",
        "        init_img = ImageOps.exif_transpose(init_img).convert(\"RGB\")\n",
        "    mask_img = None\n",
        "    if bool(materialdiffusion_prefs['mask_image']):\n",
        "        if materialdiffusion_prefs['mask_image'].startswith('http'):\n",
        "            mask_img = PILImage.open(requests.get(materialdiffusion_prefs['mask_image'], stream=True).raw)\n",
        "        else:\n",
        "            if os.path.isfile(materialdiffusion_prefs['mask_image']):\n",
        "                mask_img = PILImage.open(materialdiffusion_prefs['mask_image'])\n",
        "            else:\n",
        "                alert_msg(page, f\"ERROR: Couldn't find your mask_image {materialdiffusion_prefs['mask_image']}\")\n",
        "                return\n",
        "            if materialdiffusion_prefs['invert_mask']:\n",
        "                mask_img = ImageOps.invert(mask_img.convert('RGB'))\n",
        "                mask_img = mask_img.resize((materialdiffusion_prefs['width'], materialdiffusion_prefs['height']), resample=PILImage.NEAREST)\n",
        "                mask_img = ImageOps.exif_transpose(mask_img).convert(\"RGB\")\n",
        "    #print(f'Resize to {width}x{height}')\n",
        "    clear_pipes()\n",
        "    clear_last()\n",
        "    prt(\"Generating your Material Diffusion Image...\")\n",
        "    prt(progress)\n",
        "    autoscroll(False)\n",
        "    random_seed = int(materialdiffusion_prefs['seed']) if int(materialdiffusion_prefs['seed']) > 0 else rnd.randint(0,4294967295)\n",
        "    try:\n",
        "        images = version.predict(prompt=materialdiffusion_prefs['material_prompt'], width=materialdiffusion_prefs['width'], height=materialdiffusion_prefs['height'], init_image=init_img, mask=mask_img, prompt_strength=materialdiffusion_prefs['prompt_strength'], num_outputs=materialdiffusion_prefs['num_outputs'], num_inference_steps=materialdiffusion_prefs['steps'], guidance_scale=materialdiffusion_prefs['guidance_scale'], seed=random_seed)\n",
        "    except Exception as e:\n",
        "        clear_last()\n",
        "        clear_last()\n",
        "        alert_msg(page, f\"ERROR: Couldn't create your image for some reason.  Possibly out of memory or something wrong with my code...\", content=Text(str(e)))\n",
        "        return\n",
        "    clear_last()\n",
        "    clear_last()\n",
        "    autoscroll(True)\n",
        "    txt2img_output = stable_dir\n",
        "    batch_output = prefs['image_output']\n",
        "    #print(str(images))\n",
        "    if images is None:\n",
        "        prt(f\"ERROR: Problem generating images, check your settings and run above blocks again, or report the error to Skquark if it really seems broken.\")\n",
        "        return\n",
        "    idx = 0\n",
        "    for image in images:\n",
        "        random_seed += idx\n",
        "        fname = format_filename(materialdiffusion_prefs['material_prompt'])\n",
        "        seed_suffix = f\"-{random_seed}\" if bool(prefs['file_suffix_seed']) else ''\n",
        "        fname = f'{materialdiffusion_prefs[\"file_prefix\"]}{fname}{seed_suffix}'\n",
        "        txt2img_output = stable_dir\n",
        "        if bool(materialdiffusion_prefs['batch_folder_name']):\n",
        "            txt2img_output = os.path.join(stable_dir, materialdiffusion_prefs['batch_folder_name'])\n",
        "        if not os.path.exists(txt2img_output):\n",
        "            os.makedirs(txt2img_output)\n",
        "        image_path = available_file(txt2img_output, fname, 1)\n",
        "        #image.save(image_path)\n",
        "        response = requests.get(image, stream=True)\n",
        "        with open(image_path, \"wb\") as f:\n",
        "          f.write(response.content)\n",
        "        new_file = image_path.rpartition(slash)[2]\n",
        "        if not materialdiffusion_prefs['display_upscaled_image'] or not materialdiffusion_prefs['apply_ESRGAN_upscale']:\n",
        "            #prt(Row([Img(src=image_path, width=materialdiffusion_prefs['width'], height=materialdiffusion_prefs['height'], fit=ImageFit.FILL, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "            prt(Row([ImageButton(src=image_path, width=materialdiffusion_prefs['width'], height=materialdiffusion_prefs['height'], page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "\n",
        "        if save_to_GDrive:\n",
        "            batch_output = os.path.join(prefs['image_output'], materialdiffusion_prefs['batch_folder_name'])\n",
        "            if not os.path.exists(batch_output):\n",
        "                os.makedirs(batch_output)\n",
        "        elif storage_type == \"PyDrive Google Drive\":\n",
        "            newFolder = gdrive.CreateFile({'title': materialdiffusion_prefs['batch_folder_name'], \"parents\": [{\"kind\": \"drive#fileLink\", \"id\": prefs['image_output']}],\"mimeType\": \"application/vnd.google-apps.folder\"})\n",
        "            newFolder.Upload()\n",
        "            batch_output = newFolder\n",
        "        out_path = batch_output if save_to_GDrive else txt2img_output\n",
        "        \n",
        "        if materialdiffusion_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:\n",
        "            os.chdir(os.path.join(dist_dir, 'Real-ESRGAN'))\n",
        "            upload_folder = 'upload'\n",
        "            result_folder = 'results'     \n",
        "            if os.path.isdir(upload_folder):\n",
        "                shutil.rmtree(upload_folder)\n",
        "            if os.path.isdir(result_folder):\n",
        "                shutil.rmtree(result_folder)\n",
        "            os.mkdir(upload_folder)\n",
        "            os.mkdir(result_folder)\n",
        "            short_name = f'{fname[:80]}-{idx}.png'\n",
        "            dst_path = os.path.join(dist_dir, 'Real-ESRGAN', upload_folder, short_name)\n",
        "            #print(f'Moving {fpath} to {dst_path}')\n",
        "            #shutil.move(fpath, dst_path)\n",
        "            shutil.copy(image_path, dst_path)\n",
        "            #faceenhance = ' --face_enhance' if materialdiffusion_prefs[\"face_enhance\"] else ''\n",
        "            faceenhance = ''\n",
        "            #python inference_realesrgan.py -n RealESRGAN_x4plus -i upload --outscale {enlarge_scale}{faceenhance}\n",
        "            run_sp(f'python inference_realesrgan.py -n realesr-general-x4v3 -i upload --outscale {materialdiffusion_prefs[\"enlarge_scale\"]}{faceenhance}', cwd=os.path.join(dist_dir, 'Real-ESRGAN'), realtime=False)\n",
        "            out_file = short_name.rpartition('.')[0] + '_out.png'\n",
        "            #print(f'move {root_dir}Real-ESRGAN/{result_folder}/{out_file} to {fpath}')\n",
        "            #shutil.move(f'{root_dir}Real-ESRGAN/{result_folder}/{out_file}', fpath)\n",
        "            upscaled_path = os.path.join(out_path, new_file)\n",
        "            shutil.move(os.path.join(dist_dir, 'Real-ESRGAN', result_folder, out_file), upscaled_path)\n",
        "            # !python inference_realesrgan.py --model_path experiments/pretrained_models/RealESRGAN_x4plus.pth --input upload --netscale 4 --outscale 3.5 --half --face_enhance\n",
        "            os.chdir(stable_dir)\n",
        "            if materialdiffusion_prefs['display_upscaled_image']:\n",
        "                time.sleep(0.6)\n",
        "                prt(Row([Img(src=upscaled_path, width=materialdiffusion_prefs['width'] * float(materialdiffusion_prefs[\"enlarge_scale\"]), height=materialdiffusion_prefs['height'] * float(materialdiffusion_prefs[\"enlarge_scale\"]), fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "        else:\n",
        "            shutil.copy(image_path, os.path.join(out_path, new_file))\n",
        "        # TODO: Add Metadata\n",
        "        prt(Row([Text(new_file)], alignment=MainAxisAlignment.CENTER))\n",
        "    autoscroll(False)\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "def run_DiT(page, from_list=False):\n",
        "    global DiT_prefs, pipe_DiT\n",
        "    if not status['installed_diffusers']:\n",
        "      alert_msg(page, \"You must Install the HuggingFace Diffusers Library first... \")\n",
        "      return\n",
        "    def prt(line, update=True):\n",
        "      if type(line) == str:\n",
        "        line = Text(line)\n",
        "      page.DiT.controls.append(line)\n",
        "      if update:\n",
        "        page.DiT.update()\n",
        "    def clear_last():\n",
        "      del page.DiT.controls[-1]\n",
        "      page.DiT.update()\n",
        "    def clear_list():\n",
        "      page.DiT.controls = page.DiT.controls[:1]\n",
        "    def autoscroll(scroll=True):\n",
        "      page.DiT.auto_scroll = scroll\n",
        "      page.DiT.update()\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    total_steps = DiT_prefs['num_inference_steps']\n",
        "    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:\n",
        "      callback_fnc.has_been_called = True\n",
        "      nonlocal progress, total_steps\n",
        "      #total_steps = len(latents)\n",
        "      percent = (step +1)/ total_steps\n",
        "      progress.value = percent\n",
        "      progress.tooltip = f\"{step +1} / {total_steps}  Timestep: {timestep}\"\n",
        "      progress.update()\n",
        "    DiT_prompts = []\n",
        "    if from_list:\n",
        "      if len(prompts) < 1:\n",
        "        alert_msg(page, \"You need to add Prompts to your List first... \")\n",
        "        return\n",
        "      for p in prompts:\n",
        "        DiT_prompts.append(p.prompt)\n",
        "    else:\n",
        "      if not bool(DiT_prefs['prompt']):\n",
        "        alert_msg(page, \"You need to add a Text Prompt first... \")\n",
        "        return\n",
        "      DiT_prompts.append(DiT_prefs['prompt'])\n",
        "    clear_list()\n",
        "    autoscroll(True)\n",
        "    from PIL.PngImagePlugin import PngInfo\n",
        "    clear_pipes('DiT')\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.reset_max_memory_allocated()\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "    if pipe_DiT == None:\n",
        "        from diffusers import DiTPipeline\n",
        "        prt(Installing(\"Downloading DiT Pipeline...\"))\n",
        "        try:\n",
        "            pipe_DiT = DiTPipeline.from_pretrained(\"facebook/DiT-XL-2-512\", torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)\n",
        "            pipe_DiT.to(torch_device)\n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            alert_msg(page, \"Error Downloading DiT Pipeline\", content=Text(str(e)))\n",
        "            return\n",
        "        pipe_DiT.set_progress_bar_config(disable=True)\n",
        "        clear_last()\n",
        "    s = \"s\" if DiT_prefs['num_images'] > 1 else \"\"\n",
        "    prt(f\"Generating DiT{s} of your Image...\")\n",
        "    batch_output = os.path.join(stable_dir, DiT_prefs['batch_folder_name'])\n",
        "    if not os.path.isdir(batch_output):\n",
        "      os.makedirs(batch_output)\n",
        "    batch_output = os.path.join(prefs['image_output'], DiT_prefs['batch_folder_name'])\n",
        "    if not os.path.isdir(batch_output):\n",
        "      os.makedirs(batch_output)\n",
        "    for pr in DiT_prompts:\n",
        "        for num in range(DiT_prefs['num_images']):\n",
        "            prt(progress)\n",
        "            autoscroll(False)\n",
        "            random_seed = (int(DiT_prefs['seed']) + num) if int(DiT_prefs['seed']) > 0 else rnd.randint(0,4294967295)\n",
        "            generator = torch.Generator(device=torch_device).manual_seed(random_seed)\n",
        "            words = [w.strip() for w in pr.split(',')]\n",
        "            try:\n",
        "                ids = pipe_DiT.get_label_ids(words)\n",
        "            except ValueError as e:\n",
        "                clear_last()\n",
        "                alert_msg(page, \"ImageNet Token not found...\", content=Text(str(e)))\n",
        "                pass\n",
        "            if len(ids) == 0:\n",
        "                clear_last()\n",
        "                alert_msg(\"No ImageNet class phases found in your prompt list. See full 1000 list for classifications.\", page)\n",
        "                return\n",
        "            try:\n",
        "                images = pipe_DiT(ids, num_inference_steps=DiT_prefs['num_inference_steps'], guidance_scale=DiT_prefs['guidance_scale'], generator=generator).images #, callback=callback_fnc, callback_steps=1\n",
        "            except Exception as e:\n",
        "                clear_last()\n",
        "                alert_msg(page, \"Error running DiT Pipeline\", content=Text(str(e)))\n",
        "                return\n",
        "            clear_last()\n",
        "            autoscroll(True)\n",
        "            fname = format_filename(pr)\n",
        "\n",
        "            if prefs['file_suffix_seed']: fname += f\"-{random_seed}\"\n",
        "            for image in images:\n",
        "                image_path = available_file(os.path.join(stable_dir, DiT_prefs['batch_folder_name']), fname, num)\n",
        "                unscaled_path = image_path\n",
        "                output_file = image_path.rpartition(slash)[2]\n",
        "                image.save(image_path)\n",
        "                out_path = image_path.rpartition(slash)[0]\n",
        "                upscaled_path = os.path.join(out_path, output_file)\n",
        "                if not DiT_prefs['display_upscaled_image'] or not DiT_prefs['apply_ESRGAN_upscale']:\n",
        "                    prt(Row([ImageButton(src=unscaled_path, data=upscaled_path, width=512, height=512, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "                    #prt(Row([Img(src=unscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "                if DiT_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:\n",
        "                    os.chdir(os.path.join(dist_dir, 'Real-ESRGAN'))\n",
        "                    upload_folder = 'upload'\n",
        "                    result_folder = 'results'     \n",
        "                    if os.path.isdir(upload_folder):\n",
        "                        shutil.rmtree(upload_folder)\n",
        "                    if os.path.isdir(result_folder):\n",
        "                        shutil.rmtree(result_folder)\n",
        "                    os.mkdir(upload_folder)\n",
        "                    os.mkdir(result_folder)\n",
        "                    short_name = f'{fname[:80]}-{num}.png'\n",
        "                    dst_path = os.path.join(dist_dir, 'Real-ESRGAN', upload_folder, short_name)\n",
        "                    #print(f'Moving {fpath} to {dst_path}')\n",
        "                    #shutil.move(fpath, dst_path)\n",
        "                    shutil.copy(image_path, dst_path)\n",
        "                    #faceenhance = ' --face_enhance' if DiT_prefs[\"face_enhance\"] else ''\n",
        "                    faceenhance = ''\n",
        "                    run_sp(f'python inference_realesrgan.py -n realesr-general-x4v3 -i upload --outscale {DiT_prefs[\"enlarge_scale\"]}{faceenhance}', cwd=os.path.join(dist_dir, 'Real-ESRGAN'), realtime=False)\n",
        "                    out_file = short_name.rpartition('.')[0] + '_out.png'\n",
        "                    shutil.move(os.path.join(dist_dir, 'Real-ESRGAN', result_folder, out_file), upscaled_path)\n",
        "                    image_path = upscaled_path\n",
        "                    os.chdir(stable_dir)\n",
        "                    if DiT_prefs['display_upscaled_image']:\n",
        "                        time.sleep(0.6)\n",
        "                        prt(Row([ImageButton(src=upscaled_path, data=upscaled_path, width=512 * float(DiT_prefs[\"enlarge_scale\"]), height=512 * float(DiT_prefs[\"enlarge_scale\"]), page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "                        #prt(Row([Img(src=upscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "                if prefs['save_image_metadata']:\n",
        "                    img = PILImage.open(image_path)\n",
        "                    metadata = PngInfo()\n",
        "                    metadata.add_text(\"artist\", prefs['meta_ArtistName'])\n",
        "                    metadata.add_text(\"copyright\", prefs['meta_Copyright'])\n",
        "                    metadata.add_text(\"software\", \"Stable Diffusion Deluxe\" + f\", upscaled {DiT_prefs['enlarge_scale']}x with ESRGAN\" if DiT_prefs['apply_ESRGAN_upscale'] else \"\")\n",
        "                    metadata.add_text(\"pipeline\", \"DiT\")\n",
        "                    if prefs['save_config_in_metadata']:\n",
        "                      metadata.add_text(\"title\", pr)\n",
        "                      config_json = DiT_prefs.copy()\n",
        "                      config_json['model_path'] = \"facebook/DiT-XL-2-512\"\n",
        "                      config_json['seed'] = random_seed\n",
        "                      del config_json['num_images']\n",
        "                      del config_json['display_upscaled_image']\n",
        "                      del config_json['batch_folder_name']\n",
        "                      if not config_json['apply_ESRGAN_upscale']:\n",
        "                        del config_json['enlarge_scale']\n",
        "                        del config_json['apply_ESRGAN_upscale']\n",
        "                      metadata.add_text(\"config_json\", json.dumps(config_json, ensure_ascii=True, indent=4))\n",
        "                    img.save(image_path, pnginfo=metadata)\n",
        "                #TODO: PyDrive\n",
        "                if storage_type == \"Colab Google Drive\":\n",
        "                    new_file = available_file(os.path.join(prefs['image_output'], DiT_prefs['batch_folder_name']), fname, num)\n",
        "                    out_path = new_file\n",
        "                    shutil.copy(image_path, new_file)\n",
        "                elif bool(prefs['image_output']):\n",
        "                    new_file = available_file(os.path.join(prefs['image_output'], DiT_prefs['batch_folder_name']), fname, num)\n",
        "                    out_path = new_file\n",
        "                    shutil.copy(image_path, new_file)\n",
        "                time.sleep(0.2)\n",
        "                prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))\n",
        "    autoscroll(False)\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "def get_dreamfusion(page):\n",
        "    os.chdir(root_dir)\n",
        "    run_process(\"git clone https://github.com/ashawkey/stable-dreamfusion.git -q\", page=page)\n",
        "    os.chdir(os.path.join(root_dir, \"stable-dreamfusion\"))\n",
        "    run_process(\"pip install -r requirements.txt -q\", page=page)\n",
        "    run_process(\"pip install git+https://github.com/NVlabs/nvdiffrast/ -q\", page=page)\n",
        "    os.chdir(root_dir)\n",
        "    \n",
        "def run_dreamfusion(page):\n",
        "    global dreamfusion_prefs, status\n",
        "    def add_to_dreamfusion_output(o):\n",
        "      page.dreamfusion_output.controls.append(o)\n",
        "      page.dreamfusion_output.update()\n",
        "    def clear_last():\n",
        "      #page.dreamfusion_output.controls = []\n",
        "      del page.dreamfusion_output.controls[-1]\n",
        "      page.dreamfusion_output.update()\n",
        "    if not status['installed_diffusers']:\n",
        "      alert_msg(page, \"You must Install HuggingFace Diffusers Pipeline before running...\")\n",
        "      return\n",
        "    if not bool(dreamfusion_prefs[\"prompt_text\"].strip()):\n",
        "      alert_msg(page, \"You must enter a simple prompt to generate 3D model from...\")\n",
        "      return\n",
        "    page.dreamfusion_output.controls = []\n",
        "    page.dreamfusion_output.update()\n",
        "    if not status['installed_dreamfusion']:\n",
        "      add_to_dreamfusion_output(Installing(\"Installing Stable DreamFusion 3D Pipeline...\"))\n",
        "      get_dreamfusion(page)\n",
        "      status['installed_dreamfusion'] = True\n",
        "      clear_last()\n",
        "    def convert(seconds):\n",
        "      seconds = seconds % (24 * 3600)\n",
        "      hour = seconds // 3600\n",
        "      seconds %= 3600\n",
        "      minutes = seconds // 60\n",
        "      seconds %= 60\n",
        "      return \"%d:%02d\" % (hour, minutes)\n",
        "    estimate = convert(int(dreamfusion_prefs[\"training_iters\"] * 0.7))\n",
        "    add_to_dreamfusion_output(Text(\"Generating your 3D model, this'll take a while...  Estimating \" + estimate))\n",
        "    add_to_dreamfusion_output(ProgressBar())\n",
        "    df_path = os.path.join(root_dir, \"stable-dreamfusion\")\n",
        "    os.chdir(df_path)\n",
        "    run_str = f'python main.py -O --text \"{dreamfusion_prefs[\"prompt_text\"]}\" --workspace {dreamfusion_prefs[\"workspace\"]} --iters {dreamfusion_prefs[\"training_iters\"]} --lr {dreamfusion_prefs[\"learning_rate\"]} --w {dreamfusion_prefs[\"training_nerf_resolution\"]} --h {dreamfusion_prefs[\"training_nerf_resolution\"]} --seed {dreamfusion_prefs[\"seed\"]} --lambda_entropy {dreamfusion_prefs[\"lambda_entropy\"]} --ckpt {dreamfusion_prefs[\"checkpoint\"]} --save_mesh --max_steps {dreamfusion_prefs[\"max_steps\"]}'\n",
        "    print(run_str)\n",
        "    torch.cuda.empty_cache()\n",
        "    try:\n",
        "      run_process(run_str, page=page)\n",
        "    except:\n",
        "      clear_last()\n",
        "      alert_msg(page, \"Error running DreamFusion, probably Out of Memory. Adjust settings & try again.\")\n",
        "      return\n",
        "    clear_last()\n",
        "    add_to_dreamfusion_output(Text(\"Finished generating obj model, texture and video... Hope it's good.\"))\n",
        "    df_out = os.path.join(df_path, dreamfusion_prefs[\"workspace\"])\n",
        "    if storage_type == \"Colab Google Drive\":\n",
        "      dreamfusion_out = os.path.join(prefs['image_output'].rpartition(slash)[0], 'dreamfusion_out', dreamfusion_prefs[\"workspace\"])\n",
        "      #os.makedirs(dreamfusion_out, exist_ok=True)\n",
        "      if os.path.exists(dreamfusion_out):\n",
        "        dreamfusion_out = available_folder(os.path.join(prefs['image_output'].rpartition(slash)[0], 'dreamfusion_out'), dreamfusion_prefs[\"workspace\"], 1)\n",
        "      shutil.copytree(df_out, dreamfusion_out)\n",
        "      add_to_dreamfusion_output(Text(f\"Saved to {dreamfusion_out}\"))\n",
        "    else:\n",
        "      add_to_dreamfusion_output(Text(f\"Saved to {df_out}\"))\n",
        "    # TODO: PyDrive2\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "    os.chdir(root_dir)\n",
        "\n",
        "base_diffusion = upsampler_diffusion = point_sampler = None\n",
        "\n",
        "def run_point_e(page):\n",
        "    global point_e_prefs, status, base_diffusion, upsampler_diffusion, point_sampler\n",
        "    def add_to_point_e_output(o):\n",
        "      page.point_e_output.controls.append(o)\n",
        "      page.point_e_output.update()\n",
        "    def prt(line):\n",
        "      if type(line) == str:\n",
        "        line = Text(line)\n",
        "      page.point_e_output.controls.append(line)\n",
        "      page.point_e_output.update()\n",
        "    def prt_status(text):\n",
        "        nonlocal status_txt\n",
        "        status_txt.value = text\n",
        "        status_txt.update()\n",
        "    def clear_last():\n",
        "      #page.point_e_output.controls = []\n",
        "      del page.point_e_output.controls[-1]\n",
        "      page.point_e_output.update()\n",
        "    if not status['installed_diffusers']:\n",
        "      alert_msg(page, \"You must Install HuggingFace Diffusers Pipeline before running...\")\n",
        "      return\n",
        "    if not bool(point_e_prefs[\"prompt_text\"].strip()):\n",
        "      alert_msg(page, \"You must enter a simple prompt to generate 3D model from...\")\n",
        "      return\n",
        "    page.point_e_output.controls = []\n",
        "    page.point_e_output.update()\n",
        "    point_e_dir = os.path.join(root_dir, \"point-e\")\n",
        "    if not os.path.exists(point_e_dir):\n",
        "        add_to_point_e_output(Installing(\"Installing OpenAI Point-E 3D Library...\"))\n",
        "        try:\n",
        "            run_process(\"pip install -U scikit-image\")\n",
        "            run_process(\"git clone https://github.com/openai/point-e.git\", cwd=root_dir)\n",
        "            run_process(\"pip install .\", cwd=point_e_dir)\n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            alert_msg(page, \"Error Installing Point-E Requirements\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "            return\n",
        "        clear_last()\n",
        "    clear_pipes()\n",
        "    from tqdm.auto import tqdm\n",
        "    from point_e.diffusion.configs import DIFFUSION_CONFIGS, diffusion_from_config\n",
        "    from point_e.util.pc_to_mesh import marching_cubes_mesh\n",
        "    from point_e.diffusion.sampler import PointCloudSampler\n",
        "    from point_e.models.download import load_checkpoint\n",
        "    from point_e.models.configs import MODEL_CONFIGS, model_from_config\n",
        "    from point_e.util.plotting import plot_point_cloud\n",
        "    from point_e.util.point_cloud import PointCloud\n",
        "    import skimage.measure\n",
        "    from PIL import ImageOps\n",
        "    \n",
        "    if bool(point_e_prefs['batch_folder_name']):\n",
        "        fname = format_filename(point_e_prefs['batch_folder_name'], force_underscore=True)\n",
        "    else:\n",
        "        if bool(point_e_prefs['prompt_text']):\n",
        "            fname = format_filename(point_e_prefs['prompt_text'])\n",
        "        else:\n",
        "            alert_msg(page, \"If you're not using Prompt Text, provide a name for your 3D Model.\")\n",
        "            return\n",
        "    filename = format_filename(point_e_prefs['prompt_text'])\n",
        "    #fname = f\"{point_e_prefs['file_prefix']}{fname}\"\n",
        "    if bool(point_e_prefs['batch_folder_name']):\n",
        "        point_e_out = os.path.join(point_e_dir, point_e_prefs['batch_folder_name'])\n",
        "    else:\n",
        "        point_e_out = point_e_dir\n",
        "    if not os.path.exists(point_e_out):\n",
        "        os.makedirs(point_e_out)\n",
        "    #point_e_out = os.path.join(point_e_out, fname)\n",
        "    #estimate = convert(int(point_e_prefs[\"training_iters\"] * 0.7))\n",
        "    init_img = None\n",
        "    if bool(point_e_prefs['init_image']):\n",
        "        if point_e_prefs['init_image'].startswith('http'):\n",
        "            init_img = PILImage.open(requests.get(point_e_prefs['init_image'], stream=True).raw)\n",
        "        else:\n",
        "            if os.path.isfile(point_e_prefs['init_image']):\n",
        "                init_img = PILImage.open(point_e_prefs['init_image'])\n",
        "            else:\n",
        "                alert_msg(page, f\"ERROR: Couldn't find your init_image {point_e_prefs['init_image']}\")\n",
        "                if not bool(point_e_prefs['prompt_text']):\n",
        "                    return\n",
        "        if init_img != None:\n",
        "            width, height = init_img.size\n",
        "            width, height = scale_dimensions(width, height, 960)\n",
        "            init_img = init_img.resize((width, height), resample=PILImage.Resampling.LANCZOS)\n",
        "            init_img = ImageOps.exif_transpose(init_img).convert(\"RGB\")\n",
        "    \n",
        "    status_txt = Text(\"Generating your 3D model, may take a while...\")\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    total_steps = 130\n",
        "    def callback_fnc(step: int) -> None:\n",
        "      callback_fnc.has_been_called = True\n",
        "      nonlocal progress, total_steps\n",
        "      #total_steps = len(latents)\n",
        "      percent = (step +1)/ total_steps\n",
        "      progress.value = percent\n",
        "      progress.tooltip = f\"{step +1} / {total_steps}\"\n",
        "      progress.update()\n",
        "    add_to_point_e_output(status_txt)\n",
        "    add_to_point_e_output(progress)\n",
        "    base_name = point_e_prefs['base_model']\n",
        "    base_model = model_from_config(MODEL_CONFIGS[base_name], torch_device)\n",
        "    base_model.eval()\n",
        "    base_diffusion = diffusion_from_config(DIFFUSION_CONFIGS[base_name])\n",
        "\n",
        "    prt_status('Creating Upsample model...')\n",
        "    upsampler_model = model_from_config(MODEL_CONFIGS['upsample'], torch_device)\n",
        "    upsampler_model.eval()\n",
        "    upsampler_diffusion = diffusion_from_config(DIFFUSION_CONFIGS['upsample'])\n",
        "\n",
        "    prt_status('Downloading Base Checkpoint...')\n",
        "    base_model.load_state_dict(load_checkpoint(base_name, torch_device))\n",
        "\n",
        "    prt_status('Downloading Upsampler Checkpoint...')\n",
        "    upsampler_model.load_state_dict(load_checkpoint('upsample', torch_device))\n",
        "    point_sampler = PointCloudSampler(\n",
        "        device=torch_device,\n",
        "        models=[base_model, upsampler_model],\n",
        "        diffusions=[base_diffusion, upsampler_diffusion],\n",
        "        num_points=[1024, 4096 - 1024],\n",
        "        aux_channels=['R', 'G', 'B'],\n",
        "        guidance_scale=[point_e_prefs['guidance_scale'], 0.0],\n",
        "        model_kwargs_key_filter=('texts', ''), # Do not condition the upsampler at all\n",
        "    )\n",
        "    samples = None\n",
        "    prt_status(\"Generating Point-E Samples...\") #images=[img]\n",
        "    step = 0\n",
        "    if init_img != None:\n",
        "        for x in tqdm(point_sampler.sample_batch_progressive(batch_size=point_e_prefs['batch_size'], model_kwargs=dict(images=[init_img]))):\n",
        "            samples = x\n",
        "            step += 1\n",
        "            callback_fnc(step)\n",
        "    else:\n",
        "        for x in tqdm(point_sampler.sample_batch_progressive(batch_size=point_e_prefs['batch_size'], model_kwargs=dict(texts=[point_e_prefs['prompt_text']]))):\n",
        "            samples = x\n",
        "            step += 1\n",
        "            callback_fnc(step)\n",
        "    #print(f\"Total steps: {step}\")\n",
        "    pc = point_sampler.output_to_point_clouds(samples)[0]\n",
        "    fig = plot_point_cloud(pc, grid_size=3, fixed_bounds=((-0.75, -0.75, -0.75),(0.75, 0.75, 0.75)))\n",
        "    \n",
        "    prt_status('Saving PointCloud NPZ file...')\n",
        "    pc_file = os.path.join(point_e_out, f'{filename}.npz')\n",
        "    PointCloud.save(pc, pc_file)\n",
        "    sdf = 'sdf'\n",
        "    model = model_from_config(MODEL_CONFIGS[sdf], torch_device)\n",
        "    model.eval()\n",
        "\n",
        "    prt_status('Loading SDF model...')\n",
        "    model.load_state_dict(load_checkpoint(sdf, torch_device))\n",
        "    torch.cuda.empty_cache()\n",
        "    #pc = PointCloud.load('example_data/pc_corgi.npz')\n",
        "    # Plot the point cloud as a sanity check.\n",
        "    #fig = plot_point_cloud(pc, grid_size=2)\n",
        "    #fig = plot_point_cloud(pc, grid_size=3, fixed_bounds=((-0.75, -0.75, -0.75),(0.75, 0.75, 0.75)))\n",
        "    # Produce a mesh (with vertex colors)\n",
        "    try:\n",
        "        mesh = marching_cubes_mesh(\n",
        "            pc=pc,\n",
        "            model=model,\n",
        "            batch_size=4096,\n",
        "            grid_size=32, # increase to 128 for resolution used in evals\n",
        "            progress=True,\n",
        "        )\n",
        "    except Exception as e:\n",
        "      clear_last()\n",
        "      alert_msg(page, \"Error running Point-E marching_cubes_mesh.\", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))\n",
        "      return\n",
        "    # Write the mesh to a PLY file to import into some other program.\n",
        "    ply_file = os.path.join(point_e_out, f'{filename}.ply')\n",
        "    with open(ply_file, 'wb') as f:\n",
        "        mesh.write_ply(f)\n",
        "    \n",
        "    #with open(\"mesh.ply\", 'r') as file:\n",
        "    #    print(file.name)\n",
        "    #https://colab.research.google.com/drive/1Ok3ye2xWsuYOcmbAU3INN7AHy5gvvq5m\n",
        "    del point_sampler\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    point_sampler = None\n",
        "    clear_last()\n",
        "    clear_last()\n",
        "    prt(\"Finished generating Point Cloud and Mesh... Hope it's good.\")\n",
        "    if storage_type == \"Colab Google Drive\":\n",
        "      point_e_save = os.path.join(prefs['image_output'].rpartition(slash)[0], 'point_e', point_e_prefs['batch_folder_name'])\n",
        "      #os.makedirs(point_e_out, exist_ok=True)\n",
        "      #if os.path.exists(point_e_save):\n",
        "      #  point_e_save = available_folder(os.path.join(prefs['image_output'].rpartition(slash)[0], 'point_e'), fname, 1)\n",
        "      shutil.copytree(point_e_out, point_e_save)\n",
        "      prt(Text(f\"Saved npz & ply files to {point_e_save}\"))\n",
        "    else:\n",
        "      prt(Text(f\"Saved npz & ply files to {point_e_out}\"))\n",
        "    # TODO: PyDrive2\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "    os.chdir(root_dir)\n",
        "\n",
        "def run_shap_e(page):\n",
        "    global shap_e_prefs, status\n",
        "    def prt(line):\n",
        "      if type(line) == str:\n",
        "        line = Text(line)\n",
        "      page.shap_e_output.controls.append(line)\n",
        "      page.shap_e_output.update()\n",
        "    def prt_status(text):\n",
        "        nonlocal status_txt\n",
        "        status_txt.value = text\n",
        "        status_txt.update()\n",
        "    def clear_last(update=True):\n",
        "      #page.shap_e_output.controls = []\n",
        "      del page.shap_e_output.controls[-1]\n",
        "      if update: page.shap_e_output.update()\n",
        "    if not bool(shap_e_prefs[\"prompt_text\"].strip()):\n",
        "      alert_msg(page, \"You must enter a simple prompt to generate 3D model from...\")\n",
        "      return\n",
        "    page.shap_e_output.controls = []\n",
        "    page.shap_e_output.update()\n",
        "    shap_e_dir = os.path.join(root_dir, \"shap-e\")\n",
        "    prt(Installing(\"Installing OpenAI Shap-E 3D Libraries...\"))\n",
        "    if not os.path.exists(shap_e_dir):\n",
        "        try:\n",
        "            #run_process(\"pip install -U scikit-image\")\n",
        "            run_process(\"git clone https://github.com/openai/shap-e.git\", cwd=root_dir)\n",
        "            run_process(\"pip install .\", cwd=shap_e_dir)\n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            alert_msg(page, \"Error Installing Shap-E Requirements\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "            return\n",
        "    clear_pipes()\n",
        "    from shap_e.diffusion.sample import sample_latents\n",
        "    from shap_e.diffusion.gaussian_diffusion import diffusion_from_config\n",
        "    from shap_e.models.download import load_model, load_config\n",
        "    from shap_e.util.notebooks import create_pan_cameras, decode_latent_images, gif_widget\n",
        "    from shap_e.util.image_util import load_image\n",
        "    from PIL import ImageOps\n",
        "    try:\n",
        "      import imageio\n",
        "    except Exception:\n",
        "      run_sp(\"pip install imageio\", realtime=False)\n",
        "      import imageio\n",
        "      pass\n",
        "    try:\n",
        "        xm = load_model('transmitter', device=torch_device)\n",
        "        shap_e_model = load_model('image300M' if bool(shap_e_prefs['init_image']) else 'text300M' , device=torch_device)\n",
        "        diffusion = diffusion_from_config(load_config('diffusion'))\n",
        "    except Exception as e:\n",
        "        clear_last()\n",
        "        alert_msg(page, \"Error downloading Shap-E models.\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "        return\n",
        "    if bool(shap_e_prefs['prompt_text']):\n",
        "        fname = format_filename(shap_e_prefs['prompt_text'])\n",
        "    elif bool(shap_e_prefs['init_image']):\n",
        "        fname = format_filename(shap_e_prefs['init_image'].rpartition(slash)[1].rparition('.')[0])\n",
        "    elif bool(shap_e_prefs['batch_folder_name']):\n",
        "        fname = format_filename(shap_e_prefs['batch_folder_name'], force_underscore=True)\n",
        "    else:\n",
        "        alert_msg(page, \"If you're not using Prompt Text, provide a name for your 3D Model.\")\n",
        "        return\n",
        "        \n",
        "    #filename = format_filename(shap_e_prefs['prompt_text'])\n",
        "    #fname = f\"{shap_e_prefs['file_prefix']}{fname}\"\n",
        "    shap_e_out = os.path.join(prefs['image_output'].rpartition(slash)[0], 'shap_e', shap_e_prefs['batch_folder_name'])\n",
        "    #if bool(shap_e_prefs['batch_folder_name']):\n",
        "    #    shap_e_out = os.path.join(shap_e_dir, shap_e_prefs['batch_folder_name'])\n",
        "    #else:\n",
        "    #    shap_e_out = shap_e_dir\n",
        "    if not os.path.exists(shap_e_out):\n",
        "        os.makedirs(shap_e_out)\n",
        "    #shap_e_out = os.path.join(shap_e_out, fname)\n",
        "    #estimate = convert(int(shap_e_prefs[\"training_iters\"] * 0.7))\n",
        "    init_img = None\n",
        "    if bool(shap_e_prefs['init_image']):\n",
        "        if shap_e_prefs['init_image'].startswith('http'):\n",
        "            init_img = PILImage.open(requests.get(shap_e_prefs['init_image'], stream=True).raw)\n",
        "        else:\n",
        "            if os.path.isfile(shap_e_prefs['init_image']):\n",
        "                init_img = PILImage.open(shap_e_prefs['init_image'])\n",
        "            else:\n",
        "                alert_msg(page, f\"ERROR: Couldn't find your init_image {shap_e_prefs['init_image']}\")\n",
        "                if not bool(shap_e_prefs['prompt_text']):\n",
        "                    return\n",
        "        if init_img != None:\n",
        "            width, height = init_img.size\n",
        "            width, height = scale_dimensions(width, height, 512)\n",
        "            init_img = init_img.resize((width, height), resample=PILImage.Resampling.LANCZOS)\n",
        "            init_img = ImageOps.exif_transpose(init_img).convert(\"RGB\")\n",
        "    \n",
        "    status_txt = Text(\"Generating your 3D model... See console for progress.\")\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    total_steps = shap_e_prefs['karras_steps']\n",
        "    def callback_fnc(step: int) -> None:\n",
        "      callback_fnc.has_been_called = True\n",
        "      nonlocal progress, total_steps\n",
        "      #total_steps = len(latents)\n",
        "      percent = (step +1)/ total_steps\n",
        "      progress.value = percent\n",
        "      progress.tooltip = f\"{step +1} / {total_steps}\"\n",
        "      progress.update()\n",
        "    clear_last(update=False)\n",
        "    prt(status_txt)\n",
        "    prt(progress)\n",
        "    if init_img == None:\n",
        "        model_kwargs = dict(texts=[shap_e_prefs['prompt_text']] * shap_e_prefs['batch_size'])\n",
        "    else:\n",
        "        model_kwargs = dict(images=[init_img] * shap_e_prefs['batch_size'])\n",
        "    try:\n",
        "        latents = sample_latents(\n",
        "            batch_size=shap_e_prefs['batch_size'],\n",
        "            model=shap_e_model,\n",
        "            diffusion=diffusion,\n",
        "            guidance_scale=shap_e_prefs['guidance_scale'],\n",
        "            model_kwargs=model_kwargs,\n",
        "            progress=True,\n",
        "            clip_denoised=True,\n",
        "            use_fp16=True,\n",
        "            use_karras=shap_e_prefs['use_karras'],\n",
        "            karras_steps=shap_e_prefs['karras_steps'],\n",
        "            sigma_min=1e-3,\n",
        "            sigma_max=160,\n",
        "            s_churn=0,\n",
        "        )\n",
        "    except Exception as e:\n",
        "        clear_last()\n",
        "        alert_msg(page, \"Error running Shap-E sample_latents.\", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))\n",
        "        return\n",
        "    prt_status(\"Generating Shap-E 3D Models...\") #images=[img]\n",
        "    step = 0\n",
        "    try:\n",
        "        cameras = create_pan_cameras(shap_e_prefs['size'], torch_device)\n",
        "        for i, latent in enumerate(latents):\n",
        "            img_file = os.path.join(shap_e_out, f'{fname}_{i}.png')\n",
        "            images = decode_latent_images(xm, latent, cameras, rendering_mode=shap_e_prefs['render_mode'].lower())\n",
        "            #images.save(img_file)\n",
        "            display(gif_widget(images))\n",
        "            #callback_fnc(i)\n",
        "    except Exception as e:\n",
        "        clear_last()\n",
        "        alert_msg(page, \"Error running Shap-E decode_latent_images.\", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))\n",
        "        return\n",
        "    if shap_e_prefs['save_frames']:\n",
        "        imgs = []\n",
        "        for i, img in enumerate(images):\n",
        "            img_file = os.path.join(shap_e_out, f'{fname}_{i}.png')\n",
        "            img.save(img_file)\n",
        "            #imgs.append(imageio.imread(np.asarray(img)))\n",
        "    gif_file = os.path.join(shap_e_out, f'{fname}.gif')\n",
        "    #imageio.mimsave(gif_file, imgs, 'GIF', duration=100, loop=0)\n",
        "    images[0].save(gif_file, save_all=True, append_images=images[1:], duration=100, loop=0)\n",
        "\n",
        "    prt_status('Saving PLY mesh file...')\n",
        "    from shap_e.util.notebooks import decode_latent_mesh\n",
        "    try:\n",
        "        for i, latent in enumerate(latents):\n",
        "            pc_file = os.path.join(shap_e_out, f'{fname}_{i}.ply')\n",
        "            with open(pc_file, 'wb') as f:\n",
        "                decode_latent_mesh(xm, latent).tri_mesh().write_ply(f)\n",
        "    except Exception as e:\n",
        "      clear_last()\n",
        "      alert_msg(page, \"Error running Shap-E decode_latent_mesh.\", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))\n",
        "      return\n",
        "    del xm\n",
        "    del shap_e_model\n",
        "    del diffusion\n",
        "    del latents\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    clear_last(update=False)\n",
        "    clear_last(update=False)\n",
        "    prt(ImageButton(src=gif_file, width=shap_e_prefs['size'], height=shap_e_prefs['size'], data=gif_file, subtitle=pc_file, page=page))\n",
        "    prt(\"Finished generating Shap-E Mesh... Hope it's good.\")\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "    os.chdir(root_dir)\n",
        "\n",
        "def run_instant_ngp(page):\n",
        "    global instant_ngp_prefs, prefs\n",
        "    def prt(line):\n",
        "      if type(line) == str:\n",
        "        line = Text(line)\n",
        "      page.instant_ngp_output.controls.append(line)\n",
        "      page.instant_ngp_output.update()\n",
        "    def clear_last():\n",
        "      del page.instant_ngp_output.controls[-1]\n",
        "      page.instant_ngp_output.update()\n",
        "    if not status['installed_diffusers']:\n",
        "      alert_msg(page, \"You must Install the HuggingFace Diffusers Library first... \")\n",
        "      return\n",
        "    save_path = os.path.join(root_dir, \"my_ngp\")\n",
        "    error = False\n",
        "    if not os.path.exists(save_path):\n",
        "      error = True\n",
        "    elif len(os.listdir(save_path)) == 0:\n",
        "      error = True\n",
        "    if len(page.instant_ngp_file_list.controls) == 0:\n",
        "      error = True\n",
        "    if error:\n",
        "      alert_msg(page, \"Couldn't find a list of images to train model. Add image files to the list...\")\n",
        "      return\n",
        "    page.instant_ngp_output.controls.clear()\n",
        "    page.instant_ngp_output.update()\n",
        "    prt(Installing(\"Downloading Instant-NGP Packages...\"))\n",
        "    instant_ngp_dir = os.path.join(root_dir, 'instant-ngp')\n",
        "    '''if not os.path.exists(diffusers_dir):\n",
        "      os.chdir(root_dir)\n",
        "      run_process(\"git clone https://github.com/Skquark/diffusers.git\", realtime=False, cwd=root_dir)\n",
        "    run_process('pip install git+https://github.com/Skquark/diffusers.git#egg=diffusers[training]', cwd=root_dir, realtime=False)\n",
        "    os.chdir(diffusers_dir)\n",
        "    run_sp('pip install -e \".[training]\"', cwd=diffusers_dir, realtime=False)\n",
        "    '''\n",
        "    try:\n",
        "        run_sp(\"apt-get install \\\n",
        "            cmake \\\n",
        "            libgoogle-glog-dev \\\n",
        "            libgflags-dev \\\n",
        "            libatlas-base-dev \\\n",
        "            libeigen3-dev \\\n",
        "            libsuitesparse-dev \\\n",
        "            libboost-program-options-dev \\\n",
        "            libboost-filesystem-dev \\\n",
        "            libboost-graph-dev \\\n",
        "            libboost-system-dev \\\n",
        "            libboost-test-dev \\\n",
        "            libfreeimage-dev \\\n",
        "            libmetis-dev \\\n",
        "            libglew-dev \\\n",
        "            qtbase5-dev \\\n",
        "            libqt5opengl5-dev \\\n",
        "            libcgal-dev\")\n",
        "    except Exception as e:\n",
        "        clear_last()\n",
        "        alert_msg(page, f\"ERROR Installing Instant-NGP Packages...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip())]))\n",
        "        return\n",
        "    if not os.path.exists(root_dir, 'ceres-solver'):\n",
        "      run_sp(\"wget https://github.com/camenduru/instant-ngp-colab/releases/download/v1.0/ceres-solver.zip\", realtime=False)\n",
        "      run_sp(f\"unzip {os.path.join(root_dir, 'ceres-solver.zip')} -d ceres-solver\", realtime=False)\n",
        "      os.remove(os.path.join(root_dir, 'ceres-solver.zip'))\n",
        "    run_sp(f\"yes | cp -r {os.path.join(root_dir, 'ceres-solver', 'lib', '.')} {os.path.join('/usr', 'local', 'lib')}\", realtime=False)\n",
        "    run_sp(f\"chmod 755 {os.path.join(root_dir, 'ceres-solver', 'bin', 'colmap')}\", realtime=False)\n",
        "    run_sp(f\"yes | cp -r {os.path.join(root_dir, 'ceres-solver', 'bin', '.')} {os.path.join('/usr', 'local', 'bin')}\", realtime=False)\n",
        "    #run_sp(f\"cp -r /content/ceres-solver/bin/. /usr/local/bin\")\n",
        "    if not os.path.exists(root_dir, 'instant-ngp'):\n",
        "      run_sp(\"wget https://github.com/camenduru/instant-ngp-colab/releases/download/v1.0/instant-ngp.zip\", realtime=False)\n",
        "      #run_sp(\"unzip /content/instant-ngp.zip -d instant-ngp\")\n",
        "      run_sp(f\"unzip {os.path.join(root_dir, 'instant-ngp.zip')} -d instant-ngp\", realtime=False)\n",
        "      os.remove(os.path.join(root_dir, 'instant-ngp.zip'))\n",
        "    #run_sp(\"pip install commentjson huggingface-hub\", realtime=False)\n",
        "    os.chdir(instant_ngp_dir)\n",
        "    #from huggingface_hub import create_repo, upload_folder\n",
        "    #scene_path = \"/content/drive/MyDrive/fox\"\n",
        "    train_path = os.path.join(root_dir, 'my_ngp')\n",
        "    #train_path = \"/content/train\"\n",
        "    #if not os.path.isdir(scene_path):\n",
        "    #    raise NotADirectoryError(scene_path)\n",
        "    '''\n",
        "    rm -rf {train_path}\n",
        "    mkdir {train_path}\n",
        "    cp -r {scene_path}/. /content/train\n",
        "    '''\n",
        "    clear_pipes()\n",
        "    clear_last()\n",
        "    prt(Text(\"Running training on Images... This'll take a while, see console...\", weight=FontWeight.BOLD))\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    prt(progress)\n",
        "    name_of_your_model = instant_ngp_prefs['name_of_your_model']\n",
        "    transforms_path = os.path.join(train_path, f\"transforms.json\")\n",
        "    train_steps = instant_ngp_prefs['train_steps']\n",
        "    snapshot_path = os.path.join(train_path, f\"{train_steps}.msgpack\")\n",
        "    mesh_path = os.path.join(train_path, f\"{name_of_your_model or train_steps}.ply\")\n",
        "    os.chdir(instant_ngp_dir)\n",
        "    run_args = f\"--scene {train_path} --mode nerf --n_steps {train_steps} --save_snapshot {snapshot_path} --save_mesh {mesh_path} --screenshot_dir {train_path}\"\n",
        "    if instant_ngp_prefs[\"vr_mode\"]: run_args += \" --vr\"\n",
        "    if instant_ngp_prefs[\"sharpen\"] != 0.0: run_args += f\" --sharpen {instant_ngp_prefs['sharpen']}\"\n",
        "    if instant_ngp_prefs[\"exposure\"] != 0.0: run_args += f\" --exposure {instant_ngp_prefs['exposure']}\"\n",
        "    try:\n",
        "        run_sp(f\"python ./scripts/colmap2nerf.py --colmap_matcher exhaustive --run_colmap --aabb_scale 4 --images {train_path} --out {transforms_path}\", cwd=instant_ngp_dir)\n",
        "        run_sp(f\"python ./scripts/run.py {run_args}\", cwd=instant_ngp_dir)\n",
        "    except Exception as e:\n",
        "        clear_last()\n",
        "        alert_msg(page, f\"ERROR Running InstantNGP Training...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip())]))\n",
        "        return\n",
        "    #instant_ngp_dir = os.path.join(diffusers_dir, \"examples\", \"dreambooth\")\n",
        "    #instant_ngp_dir = os.path.join(diffusers_dir, \"examples\", \"text_to_image\")\n",
        "    #save_path = \"./my_model\"\n",
        "    #if not os.path.exists(save_path):\n",
        "    #  os.mkdir(save_path)\n",
        "    output_dir = prefs['image_output'].rpartition(slash)[0] + slash + '3D_out'\n",
        "    if bool(name_of_your_model):\n",
        "      output_dir = os.path.join(output_dir, name_of_your_model)\n",
        "    if not os.path.exists(output_dir): os.makedirs(output_dir)\n",
        "    shutil.copytree(train_path, output_dir, dirs_exist_ok=True)\n",
        "    '''video_camera_path = os.path.join(scene_path, \"base_cam.json\")\n",
        "    if not os.path.isfile(video_camera_path):\n",
        "      raise FileNotFoundError(video_camera_path)\n",
        "    video_n_seconds = 5\n",
        "    video_fps = 25\n",
        "    width = 720\n",
        "    height = 720\n",
        "    output_video_path = os.path.join(scene_path, \"output_video.mp4\")\n",
        "    python scripts/run.py {snapshot_path} --video_camera_path {video_camera_path} --video_n_seconds 2 --video_fps 25 --width 720 --height 720 --video_output {output_video_path}\n",
        "    print(f\"Generated video saved to:\\n{output_video_path}\")'''\n",
        "    clear_last()\n",
        "    clear_last()\n",
        "    prt(Markdown(f\"## Your model was saved successfully to _{output_dir}_.\\nNow take those files and load then locally on Windows following [instant-ngp gui instructions](https://github.com/NVlabs/instant-ngp#testbed-controls) to export videos and meshes or try MeshLab... (wish we can do that for ya here)\", on_tap_link=lambda e: e.page.launch_url(e.data)))\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "def run_dall_e(page, from_list=False):\n",
        "    global dall_e_prefs, prefs, prompts\n",
        "    if (not bool(dall_e_prefs['prompt']) and not from_list) or (from_list and (len(prompts) == 0)):\n",
        "      alert_msg(page, \"You must provide a text prompt to process your image generation...\")\n",
        "      return\n",
        "    if not bool(prefs['OpenAI_api_key']):\n",
        "      alert_msg(page, \"You must provide your OpenAI API Key in Settings to process your Dall-e 2 Creation...\")\n",
        "      return\n",
        "    def prt(line):\n",
        "      if type(line) == str:\n",
        "        line = Text(line, size=17)\n",
        "      page.DallE2.controls.append(line)\n",
        "      page.DallE2.update()\n",
        "    def clear_last():\n",
        "      del page.DallE2.controls[-1]\n",
        "      page.DallE2.update()\n",
        "    def autoscroll(scroll=True):\n",
        "      page.DallE2.auto_scroll = scroll\n",
        "      page.DallE2.update()\n",
        "    def clear_list():\n",
        "      page.DallE2.controls = page.DallE2.controls[:1]\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    try:\n",
        "        import openai\n",
        "    except:\n",
        "        prt(Installing(\"Installing OpenAi Dall-E 2 API...\"))\n",
        "        run_process(\"pip install -q openai\", realtime=False)\n",
        "        clear_last()\n",
        "        import openai\n",
        "        pass\n",
        "    try:\n",
        "        openai.api_key = prefs['OpenAI_api_key']\n",
        "    except Exception as e:\n",
        "        alert_msg(page, f\"Seems like your OpenAI API Key is Invalid. Check it again...\", content=Text(str(e)))\n",
        "        return\n",
        "    import requests\n",
        "    from io import BytesIO\n",
        "    from PIL import ImageOps\n",
        "    \n",
        "    save_dir = os.path.join(root_dir, 'dalle_inputs')\n",
        "    init_img = None\n",
        "    dall_e_list = []\n",
        "    if from_list:\n",
        "        if len(prompts) > 0:\n",
        "            for p in prompts:\n",
        "                dall_e_list.append({'prompt': p.prompt, 'init_image': p.arg['init_image'], 'mask_image': p.arg['mask_image']})\n",
        "        else:\n",
        "            alert_msg(page, f\"Your Prompts List is empty. Add to your batch list to use feature.\")\n",
        "            return\n",
        "    else:\n",
        "        dall_e_list.append({'prompt': dall_e_prefs['prompt'], 'init_image': dall_e_prefs['init_image'], 'mask_image': dall_e_prefs['mask_image']})\n",
        "    clear_list()\n",
        "    autoscroll(True)\n",
        "    for p in dall_e_list:\n",
        "        init_image = p['init_image']\n",
        "        mask_image = p['mask_image']\n",
        "        if bool(init_image):\n",
        "            fname = init_image.rpartition(slash)[2]\n",
        "            init_file = os.path.join(save_dir, fname)\n",
        "            if init_image.startswith('http'):\n",
        "                init_img = PILImage.open(requests.get(init_image, stream=True).raw)\n",
        "            else:\n",
        "                if os.path.isfile(init_image):\n",
        "                    init_img = PILImage.open(init_image)\n",
        "                else:\n",
        "                    alert_msg(page, f\"ERROR: Couldn't find your init_image {init_image}\")\n",
        "                    return\n",
        "            init_img = init_img.resize((dall_e_prefs['size'], dall_e_prefs['size']), resample=PILImage.Resampling.LANCZOS)\n",
        "            init_img = ImageOps.exif_transpose(init_img).convert(\"RGB\")\n",
        "            init_img.save(init_file)\n",
        "        mask_img = None\n",
        "        if bool(mask_image):\n",
        "            fname = init_image.rpartition(slash)[2]\n",
        "            mask_file = os.path.join(save_dir, fname)\n",
        "            if mask_image.startswith('http'):\n",
        "                mask_img = PILImage.open(requests.get(mask_image, stream=True).raw)\n",
        "            else:\n",
        "                if os.path.isfile(mask_image):\n",
        "                    mask_img = PILImage.open(mask_image)\n",
        "                else:\n",
        "                    alert_msg(page, f\"ERROR: Couldn't find your mask_image {mask_image}\")\n",
        "                    return\n",
        "                if dall_e_prefs['invert_mask']:\n",
        "                    mask_img = ImageOps.invert(mask_img.convert('RGB'))\n",
        "            mask_img = mask_img.resize((dall_e_prefs['size'], dall_e_prefs['size']), resample=PILImage.NEAREST)\n",
        "            mask_img = ImageOps.exif_transpose(init_img).convert(\"RGB\")\n",
        "            mask_img.save(mask_file)\n",
        "        #print(f'Resize to {width}x{height}')\n",
        "        #clear_pipes()\n",
        "        prt(\"Generating your Dall-E 2 Image...\")\n",
        "        prt(progress)\n",
        "        autoscroll(False)\n",
        "\n",
        "        try:\n",
        "            if bool(init_image) and bool(dall_e_prefs['variation']):\n",
        "                response = openai.Image.create_variation(image=open(init_file, 'rb'), size=dall_e_prefs['size'], n=dall_e_prefs['num_images'])\n",
        "            elif bool(init_image) and not bool(mask_image):\n",
        "                response = openai.Image.create_edit(prompt=p['prompt'], size=dall_e_prefs['size'], n=dall_e_prefs['num_images'], image=open(init_file, 'rb'))\n",
        "            elif bool(init_image) and bool(mask_image):\n",
        "                response = openai.Image.create_edit(prompt=p['prompt'], size=dall_e_prefs['size'], n=dall_e_prefs['num_images'], image=open(init_file, 'rb'), mask=open(mask_file, 'rb'))\n",
        "            else:\n",
        "                response = openai.Image.create(prompt=p['prompt'], size=dall_e_prefs['size'], n=dall_e_prefs['num_images'])\n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            clear_last()\n",
        "            alert_msg(page, f\"ERROR: Something went wrong generating image form API...\", content=Text(str(e)))\n",
        "            return\n",
        "        clear_last()\n",
        "        clear_last()\n",
        "        autoscroll(True)\n",
        "        txt2img_output = stable_dir\n",
        "        batch_output = prefs['image_output']\n",
        "        #print(str(images))\n",
        "        if response is None:\n",
        "            prt(f\"ERROR: Problem generating images, check your settings and run above blocks again, or report the error to Skquark if it really seems broken.\")\n",
        "            return\n",
        "        #print(str(response))\n",
        "        idx = 0\n",
        "        for i in response['data']:\n",
        "            image = i['url']\n",
        "            #random_seed += idx\n",
        "            fname = format_filename(p['prompt'])\n",
        "            #seed_suffix = f\"-{random_seed}\" if bool(prefs['file_suffix_seed']) else ''\n",
        "            fname = f'{dall_e_prefs[\"file_prefix\"]}{fname}'\n",
        "            txt2img_output = stable_dir\n",
        "            if bool(dall_e_prefs['batch_folder_name']):\n",
        "                txt2img_output = os.path.join(stable_dir, dall_e_prefs['batch_folder_name'])\n",
        "            if not os.path.exists(txt2img_output):\n",
        "                os.makedirs(txt2img_output)\n",
        "            image_path = available_file(txt2img_output, fname, 1)\n",
        "            #image.save(image_path)\n",
        "            response = requests.get(image, stream=True)\n",
        "            with open(image_path, \"wb\") as f:\n",
        "                f.write(response.content)\n",
        "            #img = i['url']\n",
        "            new_file = image_path.rpartition(slash)[2].rpartition('-')[0]\n",
        "            size = int(dall_e_prefs['size'].rpartition('x')[0])\n",
        "            out_path = batch_output if save_to_GDrive else txt2img_output\n",
        "            new_path = available_file(out_path, new_file, idx)\n",
        "            if not dall_e_prefs['display_upscaled_image'] or not dall_e_prefs['apply_ESRGAN_upscale']:\n",
        "                prt(Row([ImageButton(src=image_path, data=new_file, width=size, height=size, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "                #prt(Row([Img(src=image_path, width=size, height=size, fit=ImageFit.FILL, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "\n",
        "            if save_to_GDrive:\n",
        "                batch_output = os.path.join(prefs['image_output'], dall_e_prefs['batch_folder_name'])\n",
        "                if not os.path.exists(batch_output):\n",
        "                    os.makedirs(batch_output)\n",
        "            elif storage_type == \"PyDrive Google Drive\":\n",
        "                newFolder = gdrive.CreateFile({'title': dall_e_prefs['batch_folder_name'], \"parents\": [{\"kind\": \"drive#fileLink\", \"id\": prefs['image_output']}],\"mimeType\": \"application/vnd.google-apps.folder\"})\n",
        "                newFolder.Upload()\n",
        "                batch_output = newFolder\n",
        "            \n",
        "            if dall_e_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:\n",
        "                os.chdir(os.path.join(dist_dir, 'Real-ESRGAN'))\n",
        "                upload_folder = 'upload'\n",
        "                result_folder = 'results'     \n",
        "                if os.path.isdir(upload_folder):\n",
        "                    shutil.rmtree(upload_folder)\n",
        "                if os.path.isdir(result_folder):\n",
        "                    shutil.rmtree(result_folder)\n",
        "                os.mkdir(upload_folder)\n",
        "                os.mkdir(result_folder)\n",
        "                short_name = f'{fname[:80]}-{idx}.png'\n",
        "                dst_path = os.path.join(dist_dir, 'Real-ESRGAN', upload_folder, short_name)\n",
        "                #print(f'Moving {fpath} to {dst_path}')\n",
        "                #shutil.move(fpath, dst_path)\n",
        "                shutil.copy(image_path, dst_path)\n",
        "                faceenhance = ' --face_enhance' if dall_e_prefs[\"face_enhance\"] else ''\n",
        "                run_sp(f'python inference_realesrgan.py -n realesr-general-x4v3 -i upload --outscale {dall_e_prefs[\"enlarge_scale\"]}{faceenhance}', cwd=os.path.join(dist_dir, 'Real-ESRGAN'), realtime=False)\n",
        "                out_file = short_name.rpartition('.')[0] + '_out.png'\n",
        "                upscaled_path = new_path\n",
        "                shutil.move(os.path.join(dist_dir, 'Real-ESRGAN', result_folder, out_file), upscaled_path)\n",
        "                # python inference_realesrgan.py --model_path experiments/pretrained_models/RealESRGAN_x4plus.pth --input upload --netscale 4 --outscale 3.5 --half --face_enhance\n",
        "                os.chdir(stable_dir)\n",
        "                if dall_e_prefs['display_upscaled_image']:\n",
        "                    time.sleep(0.6)\n",
        "                    prt(Row([ImageButton(src=upscaled_path, data=upscaled_path, width=size * float(dall_e_prefs[\"enlarge_scale\"]), height=size * float(dall_e_prefs[\"enlarge_scale\"]), page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "                    #prt(Row([Img(src=upscaled_path,fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "            else:\n",
        "                shutil.copy(image_path, new_path)#os.path.join(out_path, new_file))\n",
        "            # TODO: Add Metadata\n",
        "            prt(Row([Text(new_path)], alignment=MainAxisAlignment.CENTER))\n",
        "    autoscroll(False)\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "loaded_kandinsky_task = \"\"\n",
        "def run_kandinsky(page):\n",
        "    global kandinsky_prefs, pipe_kandinsky, prefs, loaded_kandinsky_task\n",
        "    #if status['installed_diffusers']:\n",
        "    #  alert_msg(page, \"Sorry, currently incompatible with Diffusers installed...\", content=Text(\"To run Kandinsky, restart runtime fresh and DO NOT install HuggingFace Diffusers library first, but you can install ESRGAN to use. Kandinsky is currently using an older version of Transformers and we haven't figured out how to easily downgrade version yet to run models together.. Sorry, trying to fix.\"))\n",
        "    #  return\n",
        "    if not bool(kandinsky_prefs['prompt']):\n",
        "      alert_msg(page, \"You must provide a text prompt to process your image generation...\")\n",
        "      return\n",
        "    def prt(line):\n",
        "      if type(line) == str:\n",
        "        line = Text(line, size=17)\n",
        "      page.Kandinsky.controls.append(line)\n",
        "      page.Kandinsky.update()\n",
        "    def clear_last():\n",
        "      del page.Kandinsky.controls[-1]\n",
        "      page.Kandinsky.update()\n",
        "    def autoscroll(scroll=True):\n",
        "      page.Kandinsky.auto_scroll = scroll\n",
        "      page.Kandinsky.update()\n",
        "    def clear_list():\n",
        "      page.Kandinsky.controls = page.Kandinsky.controls[:1]\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    clear_list()\n",
        "    autoscroll(True)\n",
        "    prt(Installing(\"Installing Kandinsky 2.1 Engine & Models... See console log for progress.\"))\n",
        "    clear_pipes(\"kandinsky\")\n",
        "    '''try:\n",
        "        if transformers.__version__ != \"4.23.1\": # Kandinsky conflict\n",
        "          run_sp(\"pip uninstall -y transformers\", realtime=True)\n",
        "          run_process(\"pip uninstall -y git+https://github.com/huggingface/transformers\", realtime=False)\n",
        "    except Exception:\n",
        "        pass\n",
        "    finally:\n",
        "        run_sp(\"pip install --target lib --upgrade transformers==4.23.1 -q\", realtime=True)\n",
        "        #print(f\"Installed transformers v{transformers.__version__}\")\n",
        "    run_process(\"pip install -q sentencepiece\", realtime=False)'''\n",
        "    try:\n",
        "        import accelerate\n",
        "    except ModuleNotFoundError:\n",
        "        run_sp(\"pip install -q --upgrade git+https://github.com/huggingface/accelerate.git\", realtime=True)\n",
        "    try:\n",
        "        import clip\n",
        "    except ModuleNotFoundError:\n",
        "        run_sp('pip install git+https://github.com/openai/CLIP.git', realtime=True)\n",
        "    try:\n",
        "        from kandinsky2 import get_kandinsky2\n",
        "    except ModuleNotFoundError:\n",
        "        #run_process(\"pip install transformers==4.23.1 --upgrade --force-reinstall -q\", realtime=False)\n",
        "        #run_process(\"pip install -q git+https://github.com/ai-forever/Kandinsky-2.0.git\", realtime=False)\n",
        "        #run_sp('pip install -q \"git+https://github.com/ai-forever/Kandinsky-2.0.git\"', realtime=True)\n",
        "        run_sp('pip install \"git+https://github.com/Skquark/Kandinsky-2.git\"', realtime=True)\n",
        "        from kandinsky2 import get_kandinsky2\n",
        "        pass\n",
        "    import requests\n",
        "    from io import BytesIO\n",
        "    from PIL import ImageOps\n",
        "    #save_dir = os.path.join(root_dir, 'kandinsky_inputs')\n",
        "    init_img = None\n",
        "    if bool(kandinsky_prefs['init_image']):\n",
        "        fname = kandinsky_prefs['init_image'].rpartition(slash)[2]\n",
        "        #init_file = os.path.join(save_dir, fname)\n",
        "        if kandinsky_prefs['init_image'].startswith('http'):\n",
        "            init_img = PILImage.open(requests.get(kandinsky_prefs['init_image'], stream=True).raw)\n",
        "        else:\n",
        "            if os.path.isfile(kandinsky_prefs['init_image']):\n",
        "                init_img = PILImage.open(kandinsky_prefs['init_image'])\n",
        "            else:\n",
        "                alert_msg(page, f\"ERROR: Couldn't find your init_image {kandinsky_prefs['init_image']}\")\n",
        "                return\n",
        "        init_img = init_img.resize((kandinsky_prefs['width'], kandinsky_prefs['height']), resample=PILImage.Resampling.LANCZOS)\n",
        "        init_img = ImageOps.exif_transpose(init_img).convert(\"RGB\")\n",
        "        #init_img.save(init_file)\n",
        "    mask_img = None\n",
        "    if bool(kandinsky_prefs['mask_image']):\n",
        "        fname = kandinsky_prefs['init_image'].rpartition(slash)[2]\n",
        "        #mask_file = os.path.join(save_dir, fname)\n",
        "        if kandinsky_prefs['mask_image'].startswith('http'):\n",
        "            mask_img = PILImage.open(requests.get(kandinsky_prefs['mask_image'], stream=True).raw)\n",
        "        else:\n",
        "            if os.path.isfile(kandinsky_prefs['mask_image']):\n",
        "                mask_img = PILImage.open(kandinsky_prefs['mask_image'])\n",
        "            else:\n",
        "                alert_msg(page, f\"ERROR: Couldn't find your mask_image {kandinsky_prefs['mask_image']}\")\n",
        "                return\n",
        "            if kandinsky_prefs['invert_mask']:\n",
        "                mask_img = ImageOps.invert(mask_img.convert('RGB'))\n",
        "        mask_img = mask_img.resize((kandinsky_prefs['width'], kandinsky_prefs['height']), resample=PILImage.NEAREST)\n",
        "        mask_img = ImageOps.exif_transpose(mask_img).convert(\"RGB\")\n",
        "        mask_img = numpy.asarray(mask_img)\n",
        "        #mask_img.save(mask_file)\n",
        "    #print(f'Resize to {width}x{height}')\n",
        "    task_type = \"inpainting\" if bool(kandinsky_prefs['init_image']) and bool(kandinsky_prefs['mask_image']) else \"text2img\"\n",
        "    if pipe_kandinsky == None or loaded_kandinsky_task != task_type:\n",
        "        clear_pipes()\n",
        "        try:\n",
        "            #if bool(kandinsky_prefs['init_image']) and not bool(kandinsky_prefs['mask_image']):\n",
        "            #    pipe_kandinsky = get_kandinsky2('cuda', task_type='img2img', model_version='2.1', use_flash_attention=False, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)\n",
        "            pipe_kandinsky = get_kandinsky2('cuda', task_type=task_type, model_version='2.1', use_flash_attention=False, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)\n",
        "            loaded_kandinsky_task = task_type\n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            alert_msg(page, f\"ERROR Initializing Kandinsky, try running without installing Diffusers first...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))\n",
        "            return\n",
        "    else:\n",
        "        clear_pipes('kandinsky')\n",
        "    clear_last()\n",
        "    prt(\"Generating your Kandinsky 2.1 Image...\")\n",
        "    prt(progress)\n",
        "    autoscroll(False)\n",
        "\n",
        "    try:\n",
        "        if bool(kandinsky_prefs['init_image']) and not bool(kandinsky_prefs['mask_image']):\n",
        "            images_texts = [kandinsky_prefs['prompt'], init_img]\n",
        "            weights = [0.5, kandinsky_prefs['strength']]\n",
        "            #images = pipe_kandinsky.generate_img2img(kandinsky_prefs['prompt'], init_img, strength=kandinsky_prefs['strength'], batch_size=kandinsky_prefs['num_images'], w=kandinsky_prefs['width'], h=kandinsky_prefs['height'], num_steps=kandinsky_prefs['steps'], denoised_type=kandinsky_prefs['denoised_type'], dynamic_threshold_v=kandinsky_prefs['dynamic_threshold_v'], sampler=kandinsky_prefs['sampler'], ddim_eta=kandinsky_prefs['ddim_eta'], guidance_scale=kandinsky_prefs['guidance_scale'])\n",
        "            #images = pipe_kandinsky.generate_img2img(kandinsky_prefs['prompt'], init_img, strength=kandinsky_prefs['strength'], batch_size=kandinsky_prefs['num_images'], w=kandinsky_prefs['width'], h=kandinsky_prefs['height'], num_steps=kandinsky_prefs['steps'], prior_cf_scale=kandinsky_prefs['prior_cf_scale'], prior_steps=str(kandinsky_prefs['prior_steps']), sampler=kandinsky_prefs['sampler'], guidance_scale=kandinsky_prefs['guidance_scale'])\n",
        "            images = pipe_kandinsky.mix_images(images_texts, weights, batch_size=kandinsky_prefs['num_images'], w=kandinsky_prefs['width'], h=kandinsky_prefs['height'], num_steps=kandinsky_prefs['steps'], prior_cf_scale=kandinsky_prefs['prior_cf_scale'], prior_steps=str(kandinsky_prefs['prior_steps']), sampler=kandinsky_prefs['sampler'], guidance_scale=kandinsky_prefs['guidance_scale'])\n",
        "        elif bool(kandinsky_prefs['init_image']) and bool(kandinsky_prefs['mask_image']):\n",
        "            #images = pipe_kandinsky.generate_inpainting(kandinsky_prefs['prompt'], init_img, mask_img, batch_size=kandinsky_prefs['num_images'], w=kandinsky_prefs['width'], h=kandinsky_prefs['height'], num_steps=kandinsky_prefs['steps'], denoised_type=kandinsky_prefs['denoised_type'], dynamic_threshold_v=kandinsky_prefs['dynamic_threshold_v'], sampler=kandinsky_prefs['sampler'], ddim_eta=kandinsky_prefs['ddim_eta'], guidance_scale=kandinsky_prefs['guidance_scale'])\n",
        "            images = pipe_kandinsky.generate_inpainting(kandinsky_prefs['prompt'], init_img, mask_img, batch_size=kandinsky_prefs['num_images'], w=kandinsky_prefs['width'], h=kandinsky_prefs['height'], num_steps=kandinsky_prefs['steps'], prior_cf_scale=kandinsky_prefs['prior_cf_scale'], prior_steps=str(kandinsky_prefs['prior_steps']), sampler=kandinsky_prefs['sampler'], guidance_scale=kandinsky_prefs['guidance_scale'])\n",
        "        else:\n",
        "            #images = pipe_kandinsky.generate_text2img(kandinsky_prefs['prompt'], batch_size=kandinsky_prefs['num_images'], w=kandinsky_prefs['width'], h=kandinsky_prefs['height'], num_steps=kandinsky_prefs['steps'], denoised_type=kandinsky_prefs['denoised_type'], dynamic_threshold_v=kandinsky_prefs['dynamic_threshold_v'], sampler=kandinsky_prefs['sampler'], ddim_eta=kandinsky_prefs['ddim_eta'], guidance_scale=kandinsky_prefs['guidance_scale'])\n",
        "            images = pipe_kandinsky.generate_text2img(kandinsky_prefs['prompt'], batch_size=kandinsky_prefs['num_images'], w=kandinsky_prefs['width'], h=kandinsky_prefs['height'], num_steps=kandinsky_prefs['steps'], prior_cf_scale=kandinsky_prefs['prior_cf_scale'], prior_steps=str(kandinsky_prefs['prior_steps']), sampler=kandinsky_prefs['sampler'], guidance_scale=kandinsky_prefs['guidance_scale'])\n",
        "    except Exception as e:\n",
        "        clear_last()\n",
        "        clear_last()\n",
        "        alert_msg(page, f\"ERROR: Something went wrong generating images...\", content=Text(str(e)))\n",
        "        return\n",
        "    clear_last()\n",
        "    clear_last()\n",
        "    autoscroll(True)\n",
        "    txt2img_output = stable_dir\n",
        "    batch_output = prefs['image_output']\n",
        "    #print(str(images))\n",
        "    if images is None:\n",
        "        prt(f\"ERROR: Problem generating images, check your settings and run again, or report the error to Skquark if it really seems broken.\")\n",
        "        return\n",
        "    idx = 0\n",
        "    for image in images:\n",
        "        fname = format_filename(kandinsky_prefs['prompt'])\n",
        "        #seed_suffix = f\"-{random_seed}\" if bool(prefs['file_suffix_seed']) else ''\n",
        "        fname = f'{kandinsky_prefs[\"file_prefix\"]}{fname}'\n",
        "        txt2img_output = stable_dir\n",
        "        if bool(kandinsky_prefs['batch_folder_name']):\n",
        "            txt2img_output = os.path.join(stable_dir, kandinsky_prefs['batch_folder_name'])\n",
        "        if not os.path.exists(txt2img_output):\n",
        "            os.makedirs(txt2img_output)\n",
        "        image_path = available_file(txt2img_output, fname, 1)\n",
        "        image.save(image_path)\n",
        "        new_file = image_path.rpartition(slash)[2]\n",
        "        if not kandinsky_prefs['display_upscaled_image'] or not kandinsky_prefs['apply_ESRGAN_upscale']:\n",
        "            #prt(Row([Img(src=image_path, width=kandinsky_prefs['width'], height=kandinsky_prefs['height'], fit=ImageFit.FILL, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "            prt(Row([ImageButton(src=image_path, width=kandinsky_prefs['width'], height=kandinsky_prefs['height'], data=image_path, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "        if save_to_GDrive:\n",
        "            batch_output = os.path.join(prefs['image_output'], kandinsky_prefs['batch_folder_name'])\n",
        "            if not os.path.exists(batch_output):\n",
        "                os.makedirs(batch_output)\n",
        "        elif storage_type == \"PyDrive Google Drive\":\n",
        "            newFolder = gdrive.CreateFile({'title': kandinsky_prefs['batch_folder_name'], \"parents\": [{\"kind\": \"drive#fileLink\", \"id\": prefs['image_output']}],\"mimeType\": \"application/vnd.google-apps.folder\"})\n",
        "            newFolder.Upload()\n",
        "            batch_output = newFolder\n",
        "        out_path = batch_output if save_to_GDrive else txt2img_output\n",
        "        \n",
        "        if kandinsky_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:\n",
        "            os.chdir(os.path.join(dist_dir, 'Real-ESRGAN'))\n",
        "            upload_folder = 'upload'\n",
        "            result_folder = 'results'     \n",
        "            if os.path.isdir(upload_folder):\n",
        "                shutil.rmtree(upload_folder)\n",
        "            if os.path.isdir(result_folder):\n",
        "                shutil.rmtree(result_folder)\n",
        "            os.mkdir(upload_folder)\n",
        "            os.mkdir(result_folder)\n",
        "            short_name = f'{fname[:80]}-{idx}.png'\n",
        "            dst_path = os.path.join(dist_dir, 'Real-ESRGAN', upload_folder, short_name)\n",
        "            #print(f'Moving {fpath} to {dst_path}')\n",
        "            #shutil.move(fpath, dst_path)\n",
        "            shutil.copy(image_path, dst_path)\n",
        "            faceenhance = ' --face_enhance' if kandinsky_prefs[\"face_enhance\"] else ''\n",
        "            run_sp(f'python inference_realesrgan.py -n realesr-general-x4v3 -i upload --outscale {kandinsky_prefs[\"enlarge_scale\"]}{faceenhance}', cwd=os.path.join(dist_dir, 'Real-ESRGAN'), realtime=False)\n",
        "            out_file = short_name.rpartition('.')[0] + '_out.png'\n",
        "            upscaled_path = os.path.join(out_path, new_file)\n",
        "            shutil.move(os.path.join(dist_dir, 'Real-ESRGAN', result_folder, out_file), upscaled_path)\n",
        "            # python inference_realesrgan.py --model_path experiments/pretrained_models/RealESRGAN_x4plus.pth --input upload --netscale 4 --outscale 3.5 --half --face_enhance\n",
        "            os.chdir(stable_dir)\n",
        "            if kandinsky_prefs['display_upscaled_image']:\n",
        "                time.sleep(0.6)\n",
        "                prt(Row([Img(src=upscaled_path, width=kandinsky_prefs['width'] * float(kandinsky_prefs[\"enlarge_scale\"]), height=kandinsky_prefs['height'] * float(kandinsky_prefs[\"enlarge_scale\"]), fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "        else:\n",
        "            shutil.copy(image_path, os.path.join(out_path, new_file))\n",
        "        # TODO: Add Metadata\n",
        "        prt(Row([Text(new_file)], alignment=MainAxisAlignment.CENTER))\n",
        "    autoscroll(False)\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "def run_kandinsky_fuse(page):\n",
        "    global kandinsky_fuse_prefs, pipe_kandinsky, prefs, loaded_kandinsky_task\n",
        "    #if status['installed_diffusers']:\n",
        "    #  alert_msg(page, \"Sorry, currently incompatible with Diffusers installed...\", content=Text(\"To run Kandinsky, restart runtime fresh and DO NOT install HuggingFace Diffusers library first, but you can install ESRGAN to use. Kandinsky is currently using an older version of Transformers and we haven't figured out how to easily downgrade version yet to run models together.. Sorry, trying to fix.\"))\n",
        "    #  return\n",
        "    if len(kandinsky_fuse_prefs['mixes']) < 1:\n",
        "      alert_msg(page, \"You must provide layers to fuse to process your image generation...\")\n",
        "      return\n",
        "    def prt(line):\n",
        "      if type(line) == str:\n",
        "        line = Text(line, size=17)\n",
        "      page.KandinskyFuse.controls.append(line)\n",
        "      page.KandinskyFuse.update()\n",
        "    def clear_last():\n",
        "      del page.KandinskyFuse.controls[-1]\n",
        "      page.KandinskyFuse.update()\n",
        "    def autoscroll(scroll=True):\n",
        "      page.KandinskyFuse.auto_scroll = scroll\n",
        "      page.KandinskyFuse.update()\n",
        "    def clear_list():\n",
        "      page.KandinskyFuse.controls = page.KandinskyFuse.controls[:1]\n",
        "    progress = ProgressBar(bar_height=8)\n",
        "    clear_list()\n",
        "    autoscroll(True)\n",
        "    prt(Installing(\"Installing Kandinsky 2.1 Engine & Models... See console log for progress.\"))\n",
        "    clear_pipes(\"kandinsky\")\n",
        "    try:\n",
        "        import clip\n",
        "    except ModuleNotFoundError:\n",
        "        run_sp('pip install git+https://github.com/openai/CLIP.git', realtime=False)\n",
        "    try:\n",
        "        from kandinsky2 import get_kandinsky2\n",
        "    except ModuleNotFoundError:\n",
        "        run_sp('pip install -q \"git+https://github.com/Skquark/Kandinsky-2.git\"', realtime=False)\n",
        "        from kandinsky2 import get_kandinsky2\n",
        "        pass\n",
        "    import requests\n",
        "    from io import BytesIO\n",
        "    from PIL import ImageOps\n",
        "    #save_dir = os.path.join(root_dir, 'kandinsky_fuse_inputs')\n",
        "    images_texts = []\n",
        "    weights = []\n",
        "    mix_names = []\n",
        "    for mix in kandinsky_fuse_prefs['mixes']:\n",
        "        if 'prompt' in mix:\n",
        "            images_texts.append(mix['prompt'])\n",
        "            mix_names.append(mix['prompt'])\n",
        "        else:\n",
        "            init_img = None\n",
        "            fname = mix['init_image'].rpartition(slash)[2]\n",
        "            #init_file = os.path.join(save_dir, fname)\n",
        "            if mix['init_image'].startswith('http'):\n",
        "                init_img = PILImage.open(requests.get(mix['init_image'], stream=True).raw)\n",
        "            else:\n",
        "                if os.path.isfile(mix['init_image']):\n",
        "                    init_img = PILImage.open(mix['init_image'])\n",
        "                else:\n",
        "                    alert_msg(page, f\"ERROR: Couldn't find your init_image {mix['init_image']}\")\n",
        "                    return\n",
        "            init_img = init_img.resize((kandinsky_fuse_prefs['width'], kandinsky_fuse_prefs['height']), resample=PILImage.Resampling.LANCZOS)\n",
        "            init_img = ImageOps.exif_transpose(init_img).convert(\"RGB\")\n",
        "            images_texts.append(init_img)\n",
        "        weights.append(mix['weight'])\n",
        "    mix_name = \" - \".join(mix_names)\n",
        "    #print(f'Resize to {width}x{height}')\n",
        "    task_type = \"text2img\"\n",
        "    if pipe_kandinsky == None or loaded_kandinsky_task != task_type:\n",
        "        clear_pipes()\n",
        "        try:\n",
        "            pipe_kandinsky = get_kandinsky2('cuda', task_type='text2img', model_version='2.1', use_flash_attention=False, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)\n",
        "            loaded_kandinsky_task = \"text2img\"\n",
        "        except Exception as e:\n",
        "            clear_last()\n",
        "            alert_msg(page, f\"ERROR Initializing Kandinsky, try running without installing Diffusers first...\", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))\n",
        "            return\n",
        "    else:\n",
        "        clear_pipes(\"kandinsky\")\n",
        "    clear_last()\n",
        "    prt(\"Generating your Kandinsky 2.1 Fused Image...\")\n",
        "    prt(progress)\n",
        "    autoscroll(False)\n",
        "\n",
        "    try:\n",
        "        images = pipe_kandinsky.mix_images(images_texts, weights, batch_size=kandinsky_fuse_prefs['num_images'], w=kandinsky_fuse_prefs['width'], h=kandinsky_fuse_prefs['height'], num_steps=kandinsky_fuse_prefs['steps'], prior_cf_scale=kandinsky_fuse_prefs['prior_cf_scale'], prior_steps=str(kandinsky_fuse_prefs['prior_steps']), sampler=kandinsky_fuse_prefs['sampler'], guidance_scale=kandinsky_fuse_prefs['guidance_scale'])\n",
        "    except Exception as e:\n",
        "        clear_last()\n",
        "        clear_last()\n",
        "        alert_msg(page, f\"ERROR: Something went wrong generating images...\", content=Text(str(e)))\n",
        "        return\n",
        "    clear_last()\n",
        "    clear_last()\n",
        "    autoscroll(True)\n",
        "    txt2img_output = stable_dir\n",
        "    batch_output = prefs['image_output']\n",
        "    #print(str(images))\n",
        "    if images is None:\n",
        "        prt(f\"ERROR: Problem generating images, check your settings and run again, or report the error to Skquark if it really seems broken.\")\n",
        "        return\n",
        "    idx = 0\n",
        "    for image in images:\n",
        "        fname = format_filename(mix_name)\n",
        "        #seed_suffix = f\"-{random_seed}\" if bool(prefs['file_suffix_seed']) else ''\n",
        "        fname = f'{kandinsky_fuse_prefs[\"file_prefix\"]}{fname}'\n",
        "        txt2img_output = stable_dir\n",
        "        if bool(kandinsky_fuse_prefs['batch_folder_name']):\n",
        "            txt2img_output = os.path.join(stable_dir, kandinsky_fuse_prefs['batch_folder_name'])\n",
        "        if not os.path.exists(txt2img_output):\n",
        "            os.makedirs(txt2img_output)\n",
        "        image_path = available_file(txt2img_output, fname, 1)\n",
        "        image.save(image_path)\n",
        "        new_file = image_path.rpartition(slash)[2]\n",
        "        if not kandinsky_fuse_prefs['display_upscaled_image'] or not kandinsky_fuse_prefs['apply_ESRGAN_upscale']:\n",
        "            #prt(Row([Img(src=image_path, width=kandinsky_fuse_prefs['width'], height=kandinsky_fuse_prefs['height'], fit=ImageFit.FILL, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "            prt(Row([ImageButton(src=image_path, width=kandinsky_fuse_prefs['width'], height=kandinsky_fuse_prefs['height'], data=image_path, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "\n",
        "        if save_to_GDrive:\n",
        "            batch_output = os.path.join(prefs['image_output'], kandinsky_fuse_prefs['batch_folder_name'])\n",
        "            if not os.path.exists(batch_output):\n",
        "                os.makedirs(batch_output)\n",
        "        elif storage_type == \"PyDrive Google Drive\":\n",
        "            newFolder = gdrive.CreateFile({'title': kandinsky_fuse_prefs['batch_folder_name'], \"parents\": [{\"kind\": \"drive#fileLink\", \"id\": prefs['image_output']}],\"mimeType\": \"application/vnd.google-apps.folder\"})\n",
        "            newFolder.Upload()\n",
        "            batch_output = newFolder\n",
        "        out_path = batch_output if save_to_GDrive else txt2img_output\n",
        "        \n",
        "        if kandinsky_fuse_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:\n",
        "            os.chdir(os.path.join(dist_dir, 'Real-ESRGAN'))\n",
        "            upload_folder = 'upload'\n",
        "            result_folder = 'results'     \n",
        "            if os.path.isdir(upload_folder):\n",
        "                shutil.rmtree(upload_folder)\n",
        "            if os.path.isdir(result_folder):\n",
        "                shutil.rmtree(result_folder)\n",
        "            os.mkdir(upload_folder)\n",
        "            os.mkdir(result_folder)\n",
        "            short_name = f'{fname[:80]}-{idx}.png'\n",
        "            dst_path = os.path.join(dist_dir, 'Real-ESRGAN', upload_folder, short_name)\n",
        "            #print(f'Moving {fpath} to {dst_path}')\n",
        "            #shutil.move(fpath, dst_path)\n",
        "            shutil.copy(image_path, dst_path)\n",
        "            faceenhance = ' --face_enhance' if kandinsky_fuse_prefs[\"face_enhance\"] else ''\n",
        "            run_sp(f'python inference_realesrgan.py -n realesr-general-x4v3 -i upload --outscale {kandinsky_fuse_prefs[\"enlarge_scale\"]}{faceenhance}', cwd=os.path.join(dist_dir, 'Real-ESRGAN'), realtime=False)\n",
        "            out_file = short_name.rpartition('.')[0] + '_out.png'\n",
        "            upscaled_path = os.path.join(out_path, new_file)\n",
        "            shutil.move(os.path.join(dist_dir, 'Real-ESRGAN', result_folder, out_file), upscaled_path)\n",
        "            # python inference_realesrgan.py --model_path experiments/pretrained_models/RealESRGAN_x4plus.pth --input upload --netscale 4 --outscale 3.5 --half --face_enhance\n",
        "            os.chdir(stable_dir)\n",
        "            if kandinsky_fuse_prefs['display_upscaled_image']:\n",
        "                time.sleep(0.6)\n",
        "                prt(Row([Img(src=upscaled_path, width=kandinsky_fuse_prefs['width'] * float(kandinsky_fuse_prefs[\"enlarge_scale\"]), height=kandinsky_fuse_prefs['height'] * float(kandinsky_fuse_prefs[\"enlarge_scale\"]), fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "        else:\n",
        "            shutil.copy(image_path, os.path.join(out_path, new_file))\n",
        "        # TODO: Add Metadata\n",
        "        prt(Row([Text(new_file)], alignment=MainAxisAlignment.CENTER))\n",
        "    autoscroll(False)\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "def run_deep_daze(page):\n",
        "    global deep_daze_prefs, status\n",
        "    #https://colab.research.google.com/drive/1_YOHdORb0Fg1Q7vWZ_KlrtFe9Ur3pmVj?usp=sharing#scrollTo=6IPQ_rdA2Sa7\n",
        "    def add_to_deep_daze_output(o):\n",
        "      page.DeepDaze.controls.append(o)\n",
        "      page.DeepDaze.update()\n",
        "    def prt(line):\n",
        "      if type(line) == str:\n",
        "        line = Text(line)\n",
        "      page.DeepDaze.controls.append(line)\n",
        "      page.DeepDaze.update()\n",
        "    def clear_last():\n",
        "      #page.deep_daze_output.controls = []\n",
        "      del page.DeepDaze.controls[-1]\n",
        "      page.DeepDaze.update()\n",
        "    def autoscroll(scroll=True):\n",
        "      page.DeepDaze.auto_scroll = scroll\n",
        "      page.DeepDaze.update()\n",
        "    def clear_list():\n",
        "      page.DeepDaze.controls = page.DeepDaze.controls[:1]\n",
        "    clear_list()\n",
        "    autoscroll(True)\n",
        "    progress = ProgressBar(bar_height=8, expand=True)\n",
        "    total_steps = deep_daze_prefs['iterations']\n",
        "    def callback_fnc(step: int) -> None:\n",
        "      callback_fnc.has_been_called = True\n",
        "      nonlocal progress, total_steps\n",
        "      #total_steps = len(latents)\n",
        "      percent = (step +1)/ total_steps\n",
        "      progress.value = percent\n",
        "      progress.tooltip = f\"{step +1} / {total_steps}\"\n",
        "      progress.update()\n",
        "    prt(Installing(\"Initializing Deep Daze Pipeline...\"))\n",
        "    from PIL.PngImagePlugin import PngInfo\n",
        "    from tqdm.notebook import trange\n",
        "    try:\n",
        "        from deep_daze import Imagine\n",
        "    except Exception:\n",
        "        run_process(\"pip install deep-daze --upgrade\", page=page)\n",
        "        from deep_daze import Imagine\n",
        "    \n",
        "    output_dir = os.path.join(root_dir, \"deep_daze\")\n",
        "    if bool(deep_daze_prefs['batch_folder_name']):\n",
        "        output_dir = os.path.join(output_dir, deep_daze_prefs['batch_folder_name'])\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "    os.chdir(output_dir)\n",
        "    abort_run = False\n",
        "    def abort_daze(e):\n",
        "        nonlocal abort_run\n",
        "        abort_run = True\n",
        "        autoscroll(False)\n",
        "        page.snd_error.play()\n",
        "        page.snd_delete.play()\n",
        "    pipe_deep_daze = Imagine(\n",
        "        text = deep_daze_prefs['prompt'],\n",
        "        num_layers = deep_daze_prefs['num_layers'],\n",
        "        save_every = deep_daze_prefs['save_every'],\n",
        "        image_width = deep_daze_prefs['max_size'],\n",
        "        lr = deep_daze_prefs['learning_rate'],\n",
        "        iterations = deep_daze_prefs['iterations'],\n",
        "        save_progress = True,#deep_daze_prefs['save_progress']\n",
        "    )\n",
        "    clear_last()\n",
        "    prt(\"    Dreaming up your DeepDaze Image...\")\n",
        "    autoscroll(False)\n",
        "    prt(Row([progress, IconButton(icon=icons.CANCEL, tooltip=\"Abort Daze & Save Progress\", on_click=abort_daze)], alignment=MainAxisAlignment.SPACE_BETWEEN))\n",
        "    #filename = format_filename(deep_daze_prefs['prompt'])\n",
        "    filename = deep_daze_prefs['prompt'].replace(' ', '_')\n",
        "    img = None\n",
        "    image_files = []\n",
        "    s = 0\n",
        "    for epoch in trange(20, desc = 'epochs'):\n",
        "        for i in trange(deep_daze_prefs['iterations'], desc = 'iteration'):\n",
        "            pipe_deep_daze.train_step(epoch, i)\n",
        "            callback_fnc(i)\n",
        "            if abort_run: break\n",
        "            if i % pipe_deep_daze.save_every != 0:\n",
        "                continue\n",
        "            num_str = '' if s==0 else f'.{str(s).zfill(6)}'\n",
        "            fname = f\"{filename}{num_str}.jpg\"\n",
        "            output_file = os.path.join(output_dir, fname)\n",
        "            image_files.append(output_file)\n",
        "            if s == 0:\n",
        "                img = Img(src=output_file, fit=ImageFit.FIT_WIDTH, gapless_playback=True)\n",
        "                prt(Row([img], alignment=MainAxisAlignment.CENTER))\n",
        "            else:\n",
        "                img.src=output_file\n",
        "                img.update()\n",
        "            s += 1\n",
        "        if abort_run: break\n",
        "            #image = Image(f'./{filename}.jpg')\n",
        "            #display(image)\n",
        "    output_filename = deep_daze_prefs['file_prefix'] + format_filename(deep_daze_prefs['prompt'])\n",
        "    unscaled_path = fname\n",
        "    image_path = available_file(output_dir, output_filename, 0)\n",
        "    unscaled_path = image_path\n",
        "    output_file = image_path.rpartition(slash)[2]\n",
        "    out_path = image_path.rpartition(slash)[0]\n",
        "    upscaled_path = os.path.join(out_path, output_file)\n",
        "    input = PILImage.open(fname)\n",
        "    input.save(image_path)\n",
        "    #shutil.copy(fname, image_path)\n",
        "    clear_last()\n",
        "    clear_last()\n",
        "    clear_last()\n",
        "    autoscroll(True)\n",
        "    if not deep_daze_prefs['display_upscaled_image'] or not deep_daze_prefs['apply_ESRGAN_upscale']:\n",
        "        prt(Row([ImageButton(src=unscaled_path, data=upscaled_path, width=512, height=512, page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "        #prt(Row([Img(src=unscaled_path, fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "    if deep_daze_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:\n",
        "        os.chdir(os.path.join(dist_dir, 'Real-ESRGAN'))\n",
        "        upload_folder = 'upload'\n",
        "        result_folder = 'results'     \n",
        "        if os.path.isdir(upload_folder):\n",
        "            shutil.rmtree(upload_folder)\n",
        "        if os.path.isdir(result_folder):\n",
        "            shutil.rmtree(result_folder)\n",
        "        os.mkdir(upload_folder)\n",
        "        os.mkdir(result_folder)\n",
        "        short_name = f'{fname[:80]}.png'\n",
        "        dst_path = os.path.join(dist_dir, 'Real-ESRGAN', upload_folder, short_name)\n",
        "        #print(f'Moving {fpath} to {dst_path}')\n",
        "        #shutil.move(fpath, dst_path)\n",
        "        shutil.copy(image_path, dst_path)\n",
        "        #faceenhance = ' --face_enhance' if deep_daze_prefs[\"face_enhance\"] else ''\n",
        "        faceenhance = ''\n",
        "        run_sp(f'python inference_realesrgan.py -n realesr-general-x4v3 -i upload --outscale {deep_daze_prefs[\"enlarge_scale\"]}{faceenhance}', cwd=os.path.join(dist_dir, 'Real-ESRGAN'), realtime=False)\n",
        "        out_file = short_name.rpartition('.')[0] + '_out.png'\n",
        "        upscaled_path = os.path.join(out_path, output_file)\n",
        "        shutil.move(os.path.join(dist_dir, 'Real-ESRGAN', result_folder, out_file), upscaled_path)\n",
        "        image_path = upscaled_path\n",
        "        os.chdir(stable_dir)\n",
        "    if prefs['save_image_metadata']:\n",
        "        img = PILImage.open(image_path)\n",
        "        metadata = PngInfo()\n",
        "        metadata.add_text(\"artist\", prefs['meta_ArtistName'])\n",
        "        metadata.add_text(\"copyright\", prefs['meta_Copyright'])\n",
        "        metadata.add_text(\"software\", \"Stable Diffusion Deluxe\" + f\", upscaled {deep_daze_prefs['enlarge_scale']}x with ESRGAN\" if deep_daze_prefs['apply_ESRGAN_upscale'] else \"\")\n",
        "        metadata.add_text(\"pipeline\", f\"Deep Daze\")\n",
        "        if prefs['save_config_in_metadata']:\n",
        "            metadata.add_text(\"title\", deep_daze_prefs['prompt'])\n",
        "            config_json = deep_daze_prefs.copy()\n",
        "            #config_json['model_path'] = model_id\n",
        "            del config_json['num_images']\n",
        "            del config_json['display_upscaled_image']\n",
        "            del config_json['batch_folder_name']\n",
        "            if not config_json['apply_ESRGAN_upscale']:\n",
        "                del config_json['enlarge_scale']\n",
        "            del config_json['apply_ESRGAN_upscale']\n",
        "            metadata.add_text(\"config_json\", json.dumps(config_json, ensure_ascii=True, indent=4))\n",
        "        img.save(image_path, pnginfo=metadata)\n",
        "    #TODO: PyDrive\n",
        "    if storage_type == \"Colab Google Drive\":\n",
        "        new_file = available_file(os.path.join(prefs['image_output'], deep_daze_prefs['batch_folder_name']), output_filename, 0)\n",
        "        out_path = new_file\n",
        "        shutil.copy(image_path, new_file)\n",
        "    elif bool(prefs['image_output']):\n",
        "        new_file = available_file(os.path.join(prefs['image_output'], deep_daze_prefs['batch_folder_name']), output_filename, 0)\n",
        "        out_path = new_file\n",
        "        shutil.copy(image_path, new_file)\n",
        "    if deep_daze_prefs['save_progress']:\n",
        "        for f in image_files:\n",
        "            if os.path.exists(f):\n",
        "                shutil.copy(f, os.path.join(prefs['image_output'], deep_daze_prefs['batch_folder_name'], f.rpartition(slash)[2]))\n",
        "    else:\n",
        "        for f in image_files[:-1]:\n",
        "            if os.path.exists(f):\n",
        "                os.remove(f)\n",
        "    if deep_daze_prefs['display_upscaled_image']:\n",
        "        prt(Row([ImageButton(src=upscaled_path, data=upscaled_path, width=512 * float(deep_daze_prefs[\"enlarge_scale\"]), height=512 * float(deep_daze_prefs[\"enlarge_scale\"]), page=page)], alignment=MainAxisAlignment.CENTER))\n",
        "        #prt(Row([Img(src=upscaled_path, fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))\n",
        "    prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))\n",
        "    \n",
        "    del pipe_deep_daze\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    autoscroll(False)\n",
        "    if prefs['enable_sounds']: page.snd_alert.play()\n",
        "\n",
        "\n",
        "def main(page: Page):\n",
        "    page.title = \"Stable Diffusion Deluxe - FletUI\"\n",
        "    #page.scroll=ScrollMode.AUTO\n",
        "    #page.auto_scroll=True\n",
        "    def open_url(e):\n",
        "        page.launch_url(e.data)\n",
        "    def exit_disconnect(e):\n",
        "        save_settings_file(page)\n",
        "        if is_Colab:\n",
        "          #run_sp(\"install pyautogui\", realtime=False)\n",
        "          #import pyautogui\n",
        "          #pyautogui.hotkey('ctrl', 'w')\n",
        "          #import keyboard\n",
        "          #keyboard.press_and_release('ctrl+w')\n",
        "          #time.sleep(1.5)\n",
        "          #close_tab()\n",
        "          page.window_close()\n",
        "          from google.colab import runtime\n",
        "          runtime.unassign()\n",
        "          #import time\n",
        "        else:\n",
        "          page.window_close()\n",
        "    def minimize_window(e):\n",
        "        page.window_minimized = True\n",
        "        page.update()\n",
        "    def maximize_window(e):\n",
        "        if page.window_maximized:\n",
        "          page.window_maximized = False\n",
        "          appbar.actions[2].icon=icons.CHECK_BOX_OUTLINE_BLANK\n",
        "          appbar.actions[2].tooltip = \"Maximize Window\"\n",
        "        else:\n",
        "          page.window_maximized = True\n",
        "          appbar.actions[2].icon=icons.MAXIMIZE\n",
        "          appbar.actions[2].tooltip = \"Restore Window\"\n",
        "        page.update()\n",
        "    def open_help_dlg(e):\n",
        "        page.dialog = help_dlg\n",
        "        help_dlg.open = True\n",
        "        page.update()\n",
        "    def close_help_dlg(e):\n",
        "        help_dlg.open = False\n",
        "        page.update()\n",
        "    help_dlg = AlertDialog(\n",
        "        title=Text(\"üíÅ   Help/Information - Stable Diffusion Deluxe \" + SDD_version), content=Column([Text(\"If you don't know what Stable Diffusion is, you're in for a pleasant surprise.. If you're already familiar, you're gonna love how easy it is to be an artist with the help of our AI Friends using our pretty interface.\"),\n",
        "              Text(\"Simply go through the self-explanitory tabs step-by-step and set your preferences to get started. The default values are good for most, but you can have some fun experimenting. All values are automatically saved as you make changes and change tabs.\"),\n",
        "              Text(\"Each time you open the app, you should start in the Installers section, turn on all the components you plan on using in you session, then Run the Installers and let them download. You can multitask and work in other tabs while it's installing.\"),\n",
        "              Text(\"In the Prompts List, add as many text prompts as you can think of, and edit any prompt to override any default Image Parameter.  Once you're ready, Run Diffusion on your Prompts List and watch it fill your Drive with beauty..\"),\n",
        "              Text(\"Try out any and all of our Prompt Helpers to use practical text AIs to make unique descriptive prompts fast, with our Prompt Generator, Remixer, Brainstormer and Advanced Writer.  You'll never run out of inspiration again...\"),\n",
        "              Text(\"Use the other Stable Diffusers to get unique results with advanced AI tools to refine your art to the next level with powerful surpises.. There's a lot of specialized Diffusers that are pure magic when you challenge it. Have fun, and get creative...\"),\n",
        "        ], scroll=ScrollMode.AUTO),\n",
        "        actions=[TextButton(\"üëç  Thanks! \", on_click=close_help_dlg)], actions_alignment=MainAxisAlignment.END,\n",
        "    )\n",
        "    def open_credits_dlg(e):\n",
        "        page.dialog = credits_dlg\n",
        "        credits_dlg.open = True\n",
        "        page.update()\n",
        "    def close_credits_dlg(e):\n",
        "        credits_dlg.open = False\n",
        "        page.update()\n",
        "    credits_markdown = '''This toolkit is an Open-Source side project by [Skquark, Inc.](https://Skquark.com), primarily created by Alan Bedian for fun and full-feature functionality.\n",
        "\n",
        "The real credit goes to the team at [Stability.ai](https://Stability.ai) for making Stable Diffusion so great, and [HuggingFace](https://HuggingFace.co) for their work on the [Diffusers Pipelines](https://github.com/huggingface/diffusers). The HuggingFace Diffusers team includes Patrick von Platen, Suraj Patil, Anton Lozhkov, Pedro Cuenca, Nathan Lambert, Kashif Rasul, Mishig Davaadorj & Thomas Wolf.\n",
        "\n",
        "For the great app UI framework, we thank [Flet](https://Flet.dev) with the amazing Flutter based Python library with a very functional dev platform that made this possible.\n",
        "\n",
        "For the brains behind our Prompt Helpers, we thank our friends [OpenAI](https://beta.OpenAI.com), [Bloom-AI](https://huggingface.co/bigscience/bloom), [TextSynth](https://TextSynth.com) and others for making an AIs so fun to talk to and use.\n",
        "\n",
        "Shoutouts to the Discord Community of [Disco Diffusion](https://discord.gg/d5ZVbAfm), [Stable Diffusion](https://discord.gg/stablediffusion), [HuggingFace](https://discord.gg/hugging-face-879548962464493619) and [Flet](https://discord.gg/nFqy742h) for their support and user contributions.'''\n",
        "    credits_dlg = AlertDialog(\n",
        "        title=Text(\"üôå   Credits/Acknowledgments\"), content=Column([Markdown(credits_markdown, extension_set=\"gitHubWeb\", on_tap_link=open_url)\n",
        "        ], scroll=ScrollMode.AUTO),\n",
        "        actions=[TextButton(\"üëä   Good Stuff... \", on_click=close_credits_dlg)], actions_alignment=MainAxisAlignment.END,\n",
        "    )\n",
        "    def open_contact_dlg(e):\n",
        "        page.dialog = contact_dlg\n",
        "        contact_dlg.open = True\n",
        "        page.update()\n",
        "    def close_contact_dlg(e):\n",
        "        contact_dlg.open = False\n",
        "        page.update()\n",
        "    contact_dlg = AlertDialog(\n",
        "        title=Text(\"üì®  Contact Us\"), content=Column([\n",
        "          Text(\"If you want to reach Alan Bedian/Skquark, Inc. for any reason, feel free to send me a message (as long as it's not spam) by Email or on Discord @Skquark#0394 where I'm usually available.\"),\n",
        "          Text(\"If you're a developer and want to help with this project (or one of my other almost done apps) then you can Join my Discord Channel and get involved. It's fairly quiet in there though...\"),\n",
        "          Row([ft.FilledButton(\"Email Skquark\", on_click=lambda _:page.launch_url(\"mailto:Alan@Skquark.com\")), ft.FilledButton(\"Skquark Discord\", on_click=lambda _:page.launch_url(\"https://discord.gg/fTraJ96Z\"))], alignment=MainAxisAlignment.CENTER),\n",
        "        ], scroll=ScrollMode.AUTO),\n",
        "        actions=[TextButton(\"üëÅÔ∏è‚Äçüó®Ô∏è  Maybe...\", on_click=close_contact_dlg)], actions_alignment=MainAxisAlignment.END,\n",
        "    )\n",
        "    def open_donate_dlg(e):\n",
        "        page.dialog = donate_dlg\n",
        "        donate_dlg.open = True\n",
        "        page.update()\n",
        "    def close_donate_dlg(e):\n",
        "        donate_dlg.open = False\n",
        "        page.update()\n",
        "    donate_dlg = AlertDialog(\n",
        "        title=Text(\"üéÅ  Donate to the Cause\"), content=Column([\n",
        "          Text(\"This app has been a one-man labour of love with a lot of effort poured into it over several months. It's powerful enough to be a paid commercial application, however it's all open-source code behind it created by many contributers to make the tools as cool as they are.\"),\n",
        "          Text(\"While we offer this software free of charge, your support would be greatly appreciated to continue the efforts to keep making it better. If you find value in this software and are able to show some love, any amount you can offer is welcomed...\"),\n",
        "          Text(\"If you're technical enough, you can also donate by making contributions to the code, offering suggestions for improvements, adding to the Community Fine-Tuned Models list, or whatever enhancements to the project you can provide...\"),\n",
        "          Row([ft.FilledButton(\"Donate with PayPal\", on_click=lambda _:page.launch_url(\"https://paypal.me/StarmaTech\")), ft.FilledButton(\"Donate with Venmo\", on_click=lambda _:page.launch_url(\"https://venmo.com/u/Alan-Bedian\"))], alignment=MainAxisAlignment.CENTER),\n",
        "        ], scroll=ScrollMode.AUTO),\n",
        "        actions=[TextButton(\"üí∏  Much appreciated\", on_click=close_donate_dlg)], actions_alignment=MainAxisAlignment.END,\n",
        "    )\n",
        "    page.etas = []\n",
        "    page.theme_mode = prefs['theme_mode'].lower()\n",
        "    if prefs['theme_mode'] == 'Dark':\n",
        "      page.dark_theme = theme.Theme(color_scheme_seed=prefs['theme_color'].lower())#, use_material3=True)\n",
        "    else:\n",
        "      page.theme = theme.Theme(color_scheme_seed=prefs['theme_color'].lower())\n",
        "    app_icon_color = colors.AMBER_800\n",
        "    space = \" \"  if (page.window_width or page.width) >= 1024 else \"\"\n",
        "    appbar=AppBar(title=ft.WindowDragArea(Row([Container(Text(f\"üë®‚Äçüé®Ô∏è{space}  Stable Diffusion - Deluxe Edition  {space}üß∞\" if (page.window_width or page.width) >= 768 else \"Stable Diffusion Deluxe  üñåÔ∏è\", weight=FontWeight.BOLD, color=colors.ON_SURFACE))], alignment=MainAxisAlignment.CENTER), expand=True), elevation=20,\n",
        "      center_title=True,\n",
        "      bgcolor=colors.SURFACE,\n",
        "      leading=IconButton(icon=icons.LOCAL_FIRE_DEPARTMENT_OUTLINED, icon_color=app_icon_color, icon_size=32, tooltip=\"Save Settings File\", on_click=lambda _: app_icon_save()),\n",
        "      #leading_width=40,\n",
        "      actions=[PopupMenuButton(items=[\n",
        "          PopupMenuItem(text=\"ü§î  Help/Info\", on_click=open_help_dlg),\n",
        "          PopupMenuItem(text=\"üëè  Credits\", on_click=open_credits_dlg),\n",
        "          PopupMenuItem(text=\"ü§ß  Issues/Suggestions\", on_click=lambda _:page.launch_url(\"https://github.com/Skquark/AI-Friends/issues\")),\n",
        "          PopupMenuItem(text=\"üòã  Contact Skquark\", on_click=open_contact_dlg),\n",
        "          #PopupMenuItem(text=\"üì®  Email Skquark\", on_click=lambda _:page.launch_url(\"mailto:Alan@Skquark.com\")),\n",
        "          PopupMenuItem(text=\"ü§ë  Offer Donation\", on_click=open_donate_dlg),\n",
        "          #PopupMenuItem(text=\"‚ùé  Exit/Disconnect Runtime\", on_click=exit_disconnect) if is_Colab else PopupMenuItem(),\n",
        "      ])])\n",
        "    if is_Colab:\n",
        "      appbar.actions[0].items.append(PopupMenuItem())\n",
        "      appbar.actions[0].items.append(PopupMenuItem(text=\"‚ùé  Exit/Disconnect Runtime\", on_click=exit_disconnect))\n",
        "    else:\n",
        "      appbar.actions.append(IconButton(icon=icons.MINIMIZE, tooltip=\"Minimize Window\", on_click=minimize_window))\n",
        "      appbar.actions.append(IconButton(icon=icons.CHECK_BOX_OUTLINE_BLANK, tooltip=\"Maximize Window\", on_click=maximize_window))\n",
        "      appbar.actions.append(IconButton(icon=icons.CLOSE, tooltip=\"‚ùé  Exit Application\", on_click=exit_disconnect))\n",
        "      page.window_title_bar_hidden = True\n",
        "    page.appbar = appbar\n",
        "    def app_icon_save():\n",
        "      app_icon_color = colors.GREEN_800\n",
        "      appbar.leading = IconButton(icon=icons.LOCAL_FIRE_DEPARTMENT_OUTLINED, icon_color=app_icon_color, icon_size=32, tooltip=\"Saving Settings File\")\n",
        "      appbar.update()\n",
        "      time.sleep(0.6)\n",
        "      app_icon_color = colors.AMBER_800\n",
        "      appbar.leading = IconButton(icon=icons.LOCAL_FIRE_DEPARTMENT_OUTLINED, icon_color=app_icon_color, icon_size=32, tooltip=\"Save Settings File\", on_click=lambda _: app_icon_save())\n",
        "      appbar.update()\n",
        "    page.app_icon_save = app_icon_save\n",
        "    page.vertical_alignment = MainAxisAlignment.START\n",
        "    page.horizontal_alignment = CrossAxisAlignment.START\n",
        "    t = buildTabs(page)\n",
        "    t.on_change = tab_on_change\n",
        "    #(t,page)\n",
        "    def close_banner(e):\n",
        "        page.banner.open = False\n",
        "        page.update()\n",
        "    #leading=Icon(icons.DOWNLOADING, color=colors.AMBER, size=40), \n",
        "    page.banner = Banner(bgcolor=colors.SECONDARY_CONTAINER, content=Column([]), actions=[TextButton(\"Close\", on_click=close_banner)])\n",
        "    def show_banner_click(e):\n",
        "        page.banner.open = True\n",
        "        page.update()\n",
        "\n",
        "    page.add(t)\n",
        "    initState(page)\n",
        "    if not status['initialized']:\n",
        "        status['initialized'] = True\n",
        "    #page.add(ElevatedButton(\"Show Banner\", on_click=show_banner_click))\n",
        "    #page.add (Text (\"Enhanced Stable Diffusion Deluxe by Skquark, Inc.\"))\n",
        "\n",
        "class Header(UserControl):\n",
        "    def __init__(self, title=\"\", subtitle=\"\", actions=[], divider=True):\n",
        "        super().__init__()\n",
        "        self.title = title\n",
        "        self.subtitle = subtitle\n",
        "        self.actions = actions\n",
        "        self.divider = divider\n",
        "        self.build()\n",
        "    def build(self):\n",
        "        #self.column = Column([Row([Text(self.title, style=TextThemeStyle.TITLE_LARGE, color=colors.SECONDARY, weight=FontWeight.BOLD), Row(self.actions) if bool(self.actions) else Container(content=None)], alignment=MainAxisAlignment.SPACE_BETWEEN, spacing=0, vertical_alignment=CrossAxisAlignment.END)], spacing=4)\n",
        "        self.column = Column([Row([Column([Text(self.title, style=TextThemeStyle.TITLE_LARGE, color=colors.SECONDARY, weight=FontWeight.BOLD), Text(self.subtitle, style=\"titleSmall\", color=colors.TERTIARY) if bool(self.subtitle) else Container(content=None)], spacing=4, expand=True), Row(self.actions) if bool(self.actions) else Container(content=None)], alignment=MainAxisAlignment.SPACE_BETWEEN, spacing=1, vertical_alignment=CrossAxisAlignment.END), Divider(thickness=3, height=5, color=colors.SURFACE_VARIANT) if self.divider else Container(content=None), Container(content=None, height=3)], spacing=2)\n",
        "        return self.column\n",
        "        \n",
        "class ImageButton(UserControl):\n",
        "    def __init__(self, src=\"\", subtitle=\"\", actions=[], center=True, width=None, height=None, data=None, fit=ImageFit.SCALE_DOWN, show_subtitle=False, page=None):\n",
        "        super().__init__()\n",
        "        self.src = src\n",
        "        self.subtitle = subtitle\n",
        "        self.actions = actions\n",
        "        self.center = center\n",
        "        self.width = width\n",
        "        self.height = height\n",
        "        self.data = data or src\n",
        "        self.fit = fit\n",
        "        self.show_subtitle = show_subtitle\n",
        "        self.page = page\n",
        "        self.build()\n",
        "    def build(self):\n",
        "        #self.column = Column([Row([Text(self.title, style=TextThemeStyle.TITLE_LARGE, color=colors.SECONDARY, weight=FontWeight.BOLD), Row(self.actions) if bool(self.actions) else Container(content=None)], alignment=MainAxisAlignment.SPACE_BETWEEN, spacing=0, vertical_alignment=CrossAxisAlignment.END)], spacing=4)\n",
        "        #Img(src=self.src, width=self.width, height=self.height, fit=self.fit, gapless_playback=True),\n",
        "        def download_image(e):\n",
        "          if is_Colab:\n",
        "            print(f\"{type(self.data)} {self.data}\")\n",
        "            from google.colab import files\n",
        "            if os.path.isfile(self.data):\n",
        "              files.download(self.data)\n",
        "            else:\n",
        "              time.sleep(5)\n",
        "              files.download(self.data)\n",
        "          self.page.snack_bar = SnackBar(content=Text(f\"üì≤  Downloading {self.data}... May have to Stop Script for Downloads to start in Colab.\"))\n",
        "          self.page.snack_bar.open = True\n",
        "          self.page.update()\n",
        "        def copy_path(e):\n",
        "            self.page.set_clipboard(self.data)\n",
        "            self.page.snack_bar = SnackBar(content=Text(f\"üìã  {self.data} copied to clipboard... Paste as Init Image.\"))\n",
        "            self.page.snack_bar.open = True\n",
        "            self.page.update()\n",
        "        def image_details(e):\n",
        "          #TODO: Get size & meta\n",
        "            alert_msg(self.page, \"Image Details\", content=Column([Text(self.subtitle or self.data, selectable=True), Img(src=self.data, gapless_playback=True)], horizontal_alignment=CrossAxisAlignment.CENTER), sound=False)\n",
        "        def delete_image(e):\n",
        "            #self.image = Container(content=None)\n",
        "            if os.path.exists(self.src):\n",
        "              os.remove(self.src)\n",
        "            if self.data != self.src:\n",
        "              if os.path.exists(self.data):\n",
        "                os.remove(self.data)\n",
        "            self.image.visible = False\n",
        "            self.image.update()\n",
        "            self.visible = False\n",
        "            self.update()\n",
        "            self.page.snack_bar = SnackBar(content=Text(f\"üóëÔ∏è  Deleted {self.data}...\"))\n",
        "            self.page.snack_bar.open = True\n",
        "            self.page.update()\n",
        "        def scale_width(width, height, max_width):\n",
        "            if width > max_width: return max_width, int(height / (width / max_width))\n",
        "            else: return width, height\n",
        "        #self.image = Row([Img(src=self.src, width=self.width, height=self.height, fit=self.fit, gapless_playback=True)], alignment=MainAxisAlignment.CENTER)\n",
        "        if self.width == None:\n",
        "            self.image = Img(src=self.src, fit=self.fit, gapless_playback=True)\n",
        "        else:\n",
        "            self.width, self.height = scale_width(self.width, self.height, (self.page.width or self.page.window_width) - 28)\n",
        "            self.image = Img(src=self.src, width=self.width, height=self.height, fit=self.fit, gapless_playback=True)\n",
        "        self.column = Column([Row([PopupMenuButton(\n",
        "            items = [\n",
        "                PopupMenuItem(text=\"View Image\", icon=icons.FULLSCREEN, on_click=image_details),\n",
        "                PopupMenuItem(text=\"Copy Path\", icon=icons.CONTENT_PASTE, on_click=copy_path),\n",
        "                PopupMenuItem(text=\"Download Locally\", icon=icons.DOWNLOAD, on_click=download_image),\n",
        "                PopupMenuItem(text=\"Delete Image\", icon=icons.DELETE, on_click=delete_image),\n",
        "            ], tooltip=\"üëÅÔ∏è\", expand=True,\n",
        "            content=Row([self.image], expand=True),\n",
        "            #content=Row([Img(src=self.src, width=self.width, height=self.height, fit=self.fit, gapless_playback=True)], alignment=MainAxisAlignment.CENTER if self.center else MainAxisAlignment.START),\n",
        "            data=self.src if self.data==None else self.data, width=self.width, height=self.height)],\n",
        "            alignment=MainAxisAlignment.CENTER if self.center else MainAxisAlignment.START)], horizontal_alignment=CrossAxisAlignment.CENTER if self.center else CrossAxisAlignment.START)\n",
        "        if self.show_subtitle:\n",
        "            self.column.controls.append(Row([Text(self.subtitle)], alignment=MainAxisAlignment.CENTER if self.center else MainAxisAlignment.START))\n",
        "        return self.column\n",
        "    \n",
        "\n",
        "class NumberPicker(UserControl):\n",
        "    def __init__(self, label=\"\", value=1, min=0, max=20, step=1, height=50, tooltip=None, on_change=None):\n",
        "        super().__init__()\n",
        "        self.value = value\n",
        "        self.min = min\n",
        "        self.max = max\n",
        "        self.step = step\n",
        "        self.label = label\n",
        "        self.height = height\n",
        "        self.tooltip = tooltip\n",
        "        self.on_change = on_change\n",
        "        self.build()\n",
        "    def build(self):\n",
        "        def changed(e):\n",
        "            self.value = int(e.control.value)\n",
        "            if self.value < self.min:\n",
        "              self.value = self.min\n",
        "              self.txt_number.value = self.value\n",
        "              self.txt_number.update()\n",
        "            if self.value > self.max:\n",
        "              self.value = self.max\n",
        "              self.txt_number.value = self.value\n",
        "              self.txt_number.update()\n",
        "            if self.on_change is not None:\n",
        "              e.control = self\n",
        "              self.on_change(e)\n",
        "        def minus_click(e):\n",
        "            self.value = int(self.value)\n",
        "            if self.value > self.min:\n",
        "              self.value -= self.step\n",
        "              self.txt_number.value = self.value\n",
        "              self.txt_number.update()\n",
        "              e.control = self\n",
        "              if self.on_change is not None:\n",
        "                self.on_change(e)\n",
        "        def plus_click(e):\n",
        "            self.value = int(self.value)\n",
        "            if self.value < self.max:\n",
        "              self.value += self.step\n",
        "              self.txt_number.value = self.value\n",
        "              self.txt_number.update()\n",
        "              e.control = self\n",
        "              if self.on_change is not None:\n",
        "                self.on_change(e)\n",
        "        self.txt_number = TextField(value=str(self.value), text_align=TextAlign.CENTER, width=55, height=self.height, content_padding=padding.only(top=4), keyboard_type=KeyboardType.NUMBER, on_change=changed)\n",
        "        label_text = Text(self.label) if not self.tooltip else Tooltip(message=self.tooltip, content=Text(self.label))\n",
        "        return Row([label_text, IconButton(icons.REMOVE, on_click=minus_click), self.txt_number, IconButton(icons.ADD, on_click=plus_click)], spacing=1)\n",
        "\n",
        "class SliderRow(UserControl):\n",
        "    def __init__(self, label=\"\", value=None, min=0, max=20, divisions=20, multiple=1, step=1, round=0, suffix=\"\", left_text=None, right_text=None, visible=True, tooltip=\"\", pref=None, key=None, expand=None, on_change=None):\n",
        "        super().__init__()\n",
        "        self.value = value or pref[key]\n",
        "        self.min = min\n",
        "        self.max = max\n",
        "        self.divisions = divisions\n",
        "        self.multiple = multiple\n",
        "        self.label = label\n",
        "        self.round = round\n",
        "        self.suffix = suffix\n",
        "        self.left_text = left_text\n",
        "        self.right_text = right_text\n",
        "        self._visible = visible\n",
        "        self.slider_row = Container(content=None)\n",
        "        self.tooltip = tooltip\n",
        "        self.expand = expand\n",
        "        self.pref = pref\n",
        "        self.key = key\n",
        "        self.on_change = on_change\n",
        "        self.build()\n",
        "    def build(self):\n",
        "        def change_slider(e):\n",
        "            self.value = round(e.control.value, self.round)\n",
        "            v = int(self.value) if self.round == 0 else float(self.value)\n",
        "            # TODO: Figure out how to update label text on web\n",
        "            #slider.label = f\"{v}{self.suffix}\"\n",
        "            #slider.update()\n",
        "            self.slider_value.value = f\"{v}{self.suffix}\"\n",
        "            self.slider_value.update()\n",
        "            self.slider_edit.value = f\"{v}\"\n",
        "            self.slider_edit.update()\n",
        "            if self.on_change is not None:\n",
        "              e.control = self\n",
        "              self.on_change(e)\n",
        "            self.pref[self.key] = int(self.value) if self.round == 0 else float(self.value)\n",
        "        def changed(e):\n",
        "            try:\n",
        "              self.value = int(e.control.value) if self.round == 0 else round(float(e.control.value), self.round)\n",
        "            except ValueError:\n",
        "              e.control.value = str(self.value)\n",
        "              e.control.update()\n",
        "              return\n",
        "            if self.value < self.min:\n",
        "              self.value = self.min\n",
        "              self.slider_edit.value = self.value\n",
        "              self.slider_value.value = self.value\n",
        "              self.slider_value.update()\n",
        "            if self.value > self.max:\n",
        "              self.value = self.max\n",
        "              self.slider_edit.value = self.value\n",
        "              self.slider_value.value = self.value\n",
        "              self.slider_value.update()\n",
        "            if self.multiple != 1:\n",
        "              v = int(self.multiple * round(self.value / self.multiple))\n",
        "              if v != self.value:\n",
        "                self.slider_edit.value = v\n",
        "                self.slider_edit.update()\n",
        "                self.value = v\n",
        "            if self.on_change is not None:\n",
        "              e.control = self\n",
        "              self.on_change(e)\n",
        "            slider.value = self.value\n",
        "            slider.label = f\"{self.value}{self.suffix}\"\n",
        "            slider.update()\n",
        "            self.slider_value.value = f\"{self.value}{self.suffix}\"\n",
        "            self.slider_value.update()\n",
        "            self.pref[self.key] = int(self.value) if self.round == 0 else float(self.value)\n",
        "        def blur(e):\n",
        "            self.slider_edit.visible = False\n",
        "            slider_text.visible = True\n",
        "            self.slider_edit.update()\n",
        "            slider_text.update()\n",
        "            slider_label.value = f\"{self.label}: \"\n",
        "            slider_label.update()\n",
        "        def edit(e):\n",
        "            self.slider_edit.visible = True\n",
        "            slider_text.visible = False\n",
        "            slider_text.update()\n",
        "            self.slider_edit.update()\n",
        "            slider_label.value = f\"{self.label}\"\n",
        "            slider_label.update()\n",
        "            #self.slider_number = TextField(value=str(self.value), on_blur=blur, text_align=TextAlign.CENTER, width=55, height=50, content_padding=padding.only(top=4), keyboard_type=KeyboardType.NUMBER, on_change=changed)\n",
        "            #self.update()\n",
        "            #\n",
        "            #e.control.update()\n",
        "            #e.page.update()\n",
        "        self.slider_edit = TextField(value=str(self.value), on_blur=blur, autofocus=True, visible=False, text_align=TextAlign.CENTER, width=51, height=45, content_padding=padding.only(top=6), keyboard_type=KeyboardType.NUMBER, on_change=changed)\n",
        "        slider = Slider(min=float(self.min), max=float(self.max), divisions=int(self.divisions), label=\"{value}\" + self.suffix, value=float(self.pref[self.key]), tooltip=self.tooltip, expand=True, on_change=change_slider)\n",
        "        self.slider = slider\n",
        "        self.slider_value = Text(f\" {self.pref[self.key]}{self.suffix}\", weight=FontWeight.BOLD)\n",
        "        slider_text = GestureDetector(self.slider_value, on_tap=edit, mouse_cursor=ft.MouseCursor.PRECISE)\n",
        "        slider_label = Text(f\"{self.label}: \")\n",
        "        left = Text(\"\", visible=False)\n",
        "        right = Text(\"\", visible=False)\n",
        "        if bool(self.left_text): left.value = self.left_text\n",
        "        if bool(self.right_text): right.value = self.right_text\n",
        "        self.slider_number = slider_text\n",
        "        self.slider_row = Container(Row([slider_label, slider_text, self.slider_edit, left, slider, right]), expand=self.expand, visible=self._visible)\n",
        "        return self.slider_row\n",
        "    def set_value(self, value):\n",
        "        self.value = value\n",
        "        self.slider.value = value\n",
        "        self.slider_edit.value = value\n",
        "        self.pref[self.key] = value\n",
        "        self.slider_value.value = f\" {self.pref[self.key]}{self.suffix}\"\n",
        "        self.slider_value.update()\n",
        "        self.slider.update()\n",
        "    def set_min(self, value):\n",
        "        self.min = value\n",
        "        self.slider.min = value\n",
        "    def set_max(self, value):\n",
        "        self.max = value\n",
        "        self.slider.max = value\n",
        "    def set_divisions(self, value):\n",
        "        self.divisions = value\n",
        "        self.slider.divisions = value\n",
        "    @property\n",
        "    def show(self):\n",
        "        return self._visible\n",
        "    @show.setter\n",
        "    def show(self, value):\n",
        "        self._visible = value\n",
        "        self.slider_row.visible = value\n",
        "        self.slider_row.update()\n",
        "    def update_slider(self):\n",
        "        self.slider.update()\n",
        "\n",
        "class Installing(UserControl):\n",
        "    def __init__(self, message=\"\"):\n",
        "        super().__init__()\n",
        "        self.message = message\n",
        "        self.build()\n",
        "    def build(self):\n",
        "        self.message_txt = Text(self.message, style=ft.TextThemeStyle.BODY_LARGE, color=colors.SECONDARY, weight=FontWeight.BOLD, max_lines=3)\n",
        "        self.details = Text(\"\")\n",
        "        return Container(content=Row([ProgressRing(), Container(content=None, width=1), self.message_txt, Container(content=None, expand=True), self.details]), padding=padding.only(left=9, bottom=4))\n",
        "    def set_message(self, msg):\n",
        "        self.message_txt.value = msg\n",
        "        self.message_txt.update()\n",
        "    def set_details(self, msg):\n",
        "        self.details.value = msg\n",
        "        self.details.update()\n",
        "\n",
        "''' Sample alt Object format\n",
        "class Component(UserControl):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.build()\n",
        "    def search(self, e):\n",
        "        pass\n",
        "    def build(self):\n",
        "        self.expand = True\n",
        "        #self.table\n",
        "        self.parameter = Ref[TextField]()\n",
        "        return Column(controls=[])\n",
        "class Main:\n",
        "    def __init__(self):\n",
        "        self.page = None\n",
        "    def __call__(self, page: Page):\n",
        "        self.page = page\n",
        "        page.title = \"Alternative Boot experiment\"\n",
        "        self.add_stuff()\n",
        "    def add_stuff(self):\n",
        "        self.page.add(Text(\"Some text\", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45)\n",
        "        self.page.update()\n",
        "main = Main()'''\n",
        "\n",
        "port = 80\n",
        "if tunnel_type == \"ngrok\":\n",
        "  #if bool(url):\n",
        "  #  public_url = url\n",
        "  #else:\n",
        "    from pyngrok import conf, ngrok\n",
        "    conf.get_default().ngrok_version = \"v3\"\n",
        "    #public_url = ngrok.connect(port = str(port)).public_url\n",
        "    public_url = ngrok.connect(port).public_url\n",
        "elif tunnel_type == \"localtunnel\":\n",
        "  if False:\n",
        "  #if bool(url):\n",
        "    public_url = url\n",
        "  else:\n",
        "    import re\n",
        "    #print(run_sp('lt -p 80', realtime=False))\n",
        "    localtunnel = subprocess.Popen(['lt', '--port', '80', 'http'], stdout=subprocess.PIPE)\n",
        "    url = str(localtunnel.stdout.readline())\n",
        "    public_url = (re.search(\"(?P<url>https?:\\/\\/[^\\s]+loca.lt)\", url).group(\"url\"))\n",
        "else: public_url=\"\"\n",
        "from IPython.display import Javascript\n",
        "if bool(public_url):\n",
        "    if auto_launch_website:\n",
        "        display(Javascript('window.open(\"{url}\");'.format(url=public_url)))\n",
        "        time.sleep(0.7)\n",
        "        clear_output()\n",
        "    print(\"\\nOpen URL in browser to launch app in tab: \" + str(public_url))\n",
        "def close_tab():\n",
        "  display(Javascript(\"window.close('', '_parent', '');\"))\n",
        "#await google.colab.kernel.proxyPort(%s)\n",
        "# Still not working to display app in Colab console, but tried.\n",
        "def show_port(adr, height=500):\n",
        "  display(Javascript(\"\"\"\n",
        "  (async ()=>{\n",
        "    fm = document.createElement('iframe')\n",
        "    fm.src = '%s'\n",
        "    fm.width = '100%%'\n",
        "    fm.height = '%d'\n",
        "    fm.frameBorder = 0\n",
        "    document.body.append(fm)\n",
        "  })();\n",
        "  \"\"\" % (adr, height) ))\n",
        "#import requests\n",
        "#r = requests.get('http://localhost:4040/api/tunnels')\n",
        "#url = r.json()['tunnels'][0]['public_url']\n",
        "#print(url)\n",
        "#await google.colab.kernel.proxyPort(%s)\n",
        "#get_ipython().system_raw('python3 -m http.server 8888 &') \n",
        "#get_ipython().system_raw('./ngrok http 8501 &')\n",
        "#show_port(public_url.public_url, port)0\n",
        "#show_port(public_url.public_url)\n",
        "#run_sp(f'python -m webbrowser -t \"{public_url.public_url}\"')\n",
        "#webbrowser.open(public_url.public_url, new=0, autoraise=True)\n",
        "#webbrowser.open_new_tab(public_url.public_url)\n",
        "#flet.app(target=main, view=flet.WEB_BROWSER, port=port, host=socket_host)\n",
        "#flet.app(target=main, view=flet.WEB_BROWSER, port=port, host=host_address)\n",
        "#flet.app(target=main, view=flet.WEB_BROWSER, port=80, host=public_url.public_url)\n",
        "#flet.app(target=main, view=flet.WEB_BROWSER, port=port, host=\"0.0.0.0\")\n",
        "if tunnel_type == \"desktop\":\n",
        "  ft.app(target=main, assets_dir=root_dir, upload_dir=root_dir)\n",
        "else:\n",
        "  #ft.app(target=main, view=ft.WEB_BROWSER, port=80, assets_dir=root_dir, upload_dir=root_dir, web_renderer=\"html\")\n",
        "  ft.app(target=main, view=ft.WEB_BROWSER, port=80, assets_dir=root_dir, upload_dir=root_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "gfi-vDp6A8it"
      },
      "outputs": [],
      "source": [
        "#@markdown ---\n",
        "raise(RuntimeError(\"Stopping Execution for Run All to end...\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "zE0dN7OiVBz0"
      },
      "outputs": [],
      "source": [
        "#@title ## ‚ñ∂Ô∏è **Inpaint Dream Mask Maker** - GradIO WebUI (temporary until I make better Mask Painter in Flet UI)\n",
        "#@markdown Prepared your B&W ***mask_image*** and resizes ***init_image*** to add to your batch prompts list easily.\n",
        "max_image_resolution = 960 #@param {'type':'slider', min:256, max:1024, step:64}\n",
        "init_image_strength_override = False #param {type:'boolean'}\n",
        "import string\n",
        "#import PIL.ImageOps\n",
        "#@title ## üì• **Install GradIO Web Server** (_if_ you want to use Inpaint Helper interface)\n",
        "try:\n",
        "  import gradio as gr\n",
        "except ImportError:\n",
        "  run_sp(\"pip install gradio datasets tqdm\")\n",
        "  pass\n",
        "finally:\n",
        "  import gradio as gr\n",
        "\n",
        "#import numpy as np\n",
        "import torch\n",
        "from torch import autocast\n",
        "import requests\n",
        "import PIL\n",
        "from PIL import Image, ImageOps\n",
        "from io import BytesIO\n",
        "clear_output()\n",
        "#print(f'{Color.BLUE}Successfully Installed GradIO...{Color.END}')\n",
        "\n",
        "image = None\n",
        "def multiple_of_64(x):\n",
        "    return int(round(x/64)*64)\n",
        "def multiple_of_8(x):\n",
        "    return int(round(x/8)*8)\n",
        "def scale_dimensions(width, height):\n",
        "  max = int(max_image_resolution)\n",
        "  r_width = width\n",
        "  r_height = height\n",
        "  if width < max and height < max:\n",
        "    if width >= height:\n",
        "      ratio = max / width\n",
        "      r_width = max\n",
        "      r_height = int(height * ratio)\n",
        "    else:\n",
        "      ratio = max / height\n",
        "      r_height = max\n",
        "      r_width = int(width * ratio)\n",
        "    width = r_width\n",
        "    height = r_height\n",
        "  if width >= height:\n",
        "    if width > max:\n",
        "      r_width = max\n",
        "      r_height = int(height * (max/width))\n",
        "    else:\n",
        "      r_width = width\n",
        "      r_height = height\n",
        "  else:\n",
        "    if height > max:\n",
        "      r_height = max\n",
        "      r_width = int(width * (max/height))\n",
        "    else:\n",
        "      r_width = width\n",
        "      r_height = height\n",
        "  return multiple_of_64(r_width), multiple_of_64(r_height)\n",
        "\n",
        "def format_filename(s):\n",
        "    valid_chars = \"-_.() %s%s\" % (string.ascii_letters, string.digits)\n",
        "    filename = ''.join(c for c in s if c in valid_chars)\n",
        "    filename = filename.replace(' ','_')\n",
        "    return filename[:24]\n",
        "\n",
        "def available_file(folder, name, idx, mask=False):\n",
        "  available = False\n",
        "  mask_str = '-mask' if mask else ''\n",
        "  while not available:\n",
        "    if os.path.isfile(os.path.join(folder, f'{name}-{idx}{mask_str}.png')):\n",
        "      idx += 1\n",
        "    else: available = True\n",
        "  return os.path.join(folder, f'{name}-{idx}{mask_str}.png')\n",
        "\n",
        "def available_folder(folder, name, idx):\n",
        "  available = False\n",
        "  while not available:\n",
        "    if os.path.isdir(os.path.join(folder, f'{name}-{idx}')):\n",
        "      idx += 1\n",
        "    else: available = True\n",
        "  return os.path.join(folder, f'{name}-{idx}')\n",
        "  \n",
        "def create(prompt, img, option):\n",
        "    global prefs\n",
        "    mask_img = img[\"mask\"]\n",
        "    init_img = img[\"image\"]\n",
        "    w, h = init_img.size\n",
        "    w, h = scale_dimensions(w, h)\n",
        "    mask_img = mask_img.convert(\"1\")\n",
        "    mask_img = mask_img.resize((w, h), resample=PIL.Image.LANCZOS)\n",
        "    if option == \"Replace everything else\":\n",
        "       mask_img = ImageOps.invert(mask_img.convert('RGB'))\n",
        "       #mask = mask.convert('1')\n",
        "    init_img = init_img.convert(\"RGB\")\n",
        "    init_img = init_img.resize((w, h))\n",
        "\n",
        "    fname = format_filename(prompt)\n",
        "    mask_name = f'{root_dir}{fname}-mask.png'\n",
        "    img_name = f'{root_dir}{fname}.png'\n",
        "    if os.path.isfile(img_name):\n",
        "      img_name = available_file(root_dir, fname, 1, False)\n",
        "    if os.path.isfile(mask_name):\n",
        "      mask_name = available_file(root_dir, fname, 1, True)\n",
        "    mask_img.save(mask_name)\n",
        "    init_img.save(img_name)\n",
        "    #print(str(img.name))\n",
        "    #print(str(img.filename))\n",
        "    image_strength = f'init_image_strength=0.4, ' if init_image_strength_override else ''\n",
        "    print(f'    Dream(\"{prompt}\", init_image=\"{img_name}\", mask_image=\"{mask_name}\", {image_strength}width={w}, height={h}),')\n",
        "    if prefs is not None:\n",
        "      prompt_pref = prefs['prompt_list']\n",
        "      new_dream = {'prompt': prompt, 'init_image': img_name, 'mask_image': mask_name, 'width': w, 'height': h}\n",
        "      if init_image_strength_override:\n",
        "        new_dream['init_image_strength':]\n",
        "      prompt_pref.append(new_dream)\n",
        "      prefs['prompt_list'] = prompt_pref\n",
        "      print(\"Added to your Stable Diffusion Deluxe Prompts List (in saved preferences)\")\n",
        "    #display(mask)\n",
        "    return f'Dream(\"{prompt}\", init_image=\"{img_name}\", mask_image=\"{mask_name}\", {image_strength}width={w}, height={h}),'\n",
        "\n",
        "block = gr.Blocks(css=\".container { max-width: 1200px; margin: auto; }\")\n",
        "with block as demo:\n",
        "    gr.Markdown(\"<h1><center>Stable Diffusion Inpaint Dream Mask Maker</center></h1>\")\n",
        "    with gr.Group():\n",
        "        with gr.Row().style(mobile_collapse=False, equal_height=False):\n",
        "                text = gr.Textbox(\n",
        "                    label=\"Enter your Inpaint prompt\", show_label=True, max_lines=1\n",
        "                ).style(\n",
        "                    border=(True, False, True, True),\n",
        "                    rounded=(True, False, False, True),\n",
        "                    container=False,\n",
        "                )\n",
        "        with gr.Row().style(mobile_collapse=False, equal_height=True):\n",
        "          option = gr.Radio(label=\"Masking\", choices=[\"Replace selection\", \"Replace everything else\"], value=\"Replace selection\")\n",
        "          btn = gr.Button(\"Prepare Dream\").style(\n",
        "                    margin=True,\n",
        "                    rounded=(True, True, True, True),\n",
        "                )\n",
        "        image = gr.Image(\n",
        "            tool=\"sketch\",\n",
        "            source=\"upload\",\n",
        "            #image_mode=\"L\", shape=(42, 42), invert_colors=True,\n",
        "            label=\"Input Init Image\",\n",
        "            type=\"pil\"\n",
        "            #type=\"numpy\"\n",
        "        )\n",
        "        gallery = gr.Markdown('')\n",
        "        #gallery = gr.Gallery(label=\"Generated mask_image\", show_label=False).style(grid=[2], height=ScrollMode.AUTO)\n",
        "        text.submit(create, inputs=[text,image,option], outputs=gallery)\n",
        "        btn.click(create, inputs=[text,image,option], outputs=gallery)\n",
        "\n",
        "demo.launch(debug=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "za8wL7yUJJYI"
      ],
      "include_colab_link": true,
      "private_outputs": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit (microsoft store)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "aecaad1160391d286dea8adfafabc0106aa931e7deb6a40f00e2f89f1c4ee412"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
