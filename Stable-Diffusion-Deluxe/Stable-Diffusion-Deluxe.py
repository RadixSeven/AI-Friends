#@title ## ⚙️ Install Python Desktop Flet Framework & Initiallize Settings
#markdown If you're running on Google Colab, authorize Google Drive with the popup to connect storage. If you're on a local or cloud Jupyter Notebook, you must get a OAuth json using instructions below to save images to GDrive, however the feature is not currently working and in progress. When set, continue to run the Web UI and experiment away..
#@markdown We'll connect to your Google Drive and save all of your preferences in realtime there, as well as your created images. This is the folder location and file name we recommend in your mounted gdrive, will be created if you're new, but you can save elsewhere.  Launches webpage with localtunnel, but in case it's down you can use ngrok instead.
#pyinstaller Stable-Diffusion-Deluxe.py --hidden-import=requests --hidden-import=torch --hidden-import=huggingface-hub --hidden-import=transformers[dev] --hidden-import=tqdm --hidden-import=regex  --hidden-import=git+https://github.com/Skquark/diffusers.git@main#egg=diffusers[torch] --collect-all=tqdm --collect-all=git+https://github.com/Skquark/diffusers.git@main#egg=diffusers[torch]  --collect-all=transformers --collect-all=regex
#pyinstaller Stable-Diffusion-Deluxe.spec -y

storage_type = "Local Drive" #@param ["Colab Google Drive", "PyDrive Google Drive", "Local Drive"]
Google_OAuth_client_secret_json = "/content/client_secrets.json" #param {'type': 'string'}
save_to_GDrive = False #param {'type': 'boolean'}
saved_settings_json = '.\sdd-settings.json' #@param {'type': 'string'}
tunnel_type = "desktop" #@param ["localtunnel", "ngrok"] 
#, "cloudflared"
auto_launch_website = False #@param {'type': 'boolean'}
force_updates = False
SDD_version = "v1.9.0"
import os, subprocess, sys, shutil
import random as rnd
root_dir = '/content/'
dist_dir = root_dir
is_Colab = True
newest_flet = True
try:
  import google.colab
  root_dir = '/content/'
except:
  root_dir = os.getcwd()
  dist_dir = os.path.join(root_dir, 'dist', 'Stable-Diffusion-Deluxe')
  if not os.path.isdir(dist_dir):
    dist_dir = root_dir
  #print(f'Root: {root_dir} Dist:{dist_dir}')
  is_Colab = False
  pass
stable_dir = root_dir
env = os.environ.copy()
def run_sp(cmd_str, cwd=None, realtime=True):
  cmd_list = cmd_str if type(cmd_str) is list else cmd_str.split()
  if realtime:
    if cwd is None:
      process = subprocess.Popen(cmd_str, shell = True, env=env, bufsize = 1, stdout=subprocess.PIPE, stderr = subprocess.STDOUT, encoding='utf-8', errors = 'replace' ) 
    else:
      process = subprocess.Popen(cmd_str, shell = True, cwd=cwd, env=env, bufsize = 1, stdout=subprocess.PIPE, stderr = subprocess.STDOUT, encoding='utf-8', errors = 'replace' ) 
    while True:
      realtime_output = process.stdout.readline()
      if realtime_output == '' and process.poll() is not None:
        break
      if realtime_output:
        print(realtime_output.strip(), flush=False)
        sys.stdout.flush()
  else:
    if cwd is None:
      return subprocess.run(cmd_list, stdout=subprocess.PIPE, env=env).stdout.decode('utf-8')
    else:
      return subprocess.run(cmd_list, stdout=subprocess.PIPE, env=env, cwd=cwd).stdout.decode('utf-8')
save_to_GDrive = storage_type == "Colab Google Drive"
if save_to_GDrive:
  if not os.path.isdir(os.path.join(root_dir, 'drive')):
    from google.colab import drive
    drive.mount(os.path.join(root_dir, 'drive'))
elif storage_type == "PyDrive Google Drive":
  "pip install PyDrive2"
stable_dir = os.path.join(root_dir, 'Stable_Diffusion')
if not os.path.exists(stable_dir):
  os.makedirs(stable_dir)
#if not os.path.exists(image_output):
#  os.makedirs(image_output)
sample_data = '/content/sample_data'
if os.path.exists(sample_data):
  for f in os.listdir(sample_data):
    os.remove(os.path.join(sample_data, f))
  os.rmdir(sample_data)
os.chdir(stable_dir)
#loaded_Stability_api = False
#loaded_img2img = False
#use_Stability_api = False
def version_checker():
  try:
    response = requests.get("https://raw.githubusercontent.com/Skquark/AI-Friends/main/DSD_version.txt")
    current_v = response.text.strip()
    if current_v != SDD_version:
      print(f'A new update is available. You are running {SDD_version} and {current_v} is up. We recommended refreshing Stable Diffusion Deluxe for the latest cool features or fixes.\nhttps://colab.research.google.com/github/Skquark/AI-Friends/blob/main/Stable_Diffusion_Deluxe.ipynb\nChangelog if interested: https://github.com/Skquark/AI-Friends/commits/main/Stable_Diffusion_Deluxe.ipynb')
  except: pass #Probably offline
def ng():
  response = requests.get("https://raw.githubusercontent.com/Skquark/AI-Friends/main/_ng")
  ng_list = response.text.strip().split('\n')
  _ng = rnd.choice(ng_list).partition('_')
  return _ng[2]+_ng[1]+_ng[0]

def download_file(url, to=None, filename=None, raw=True):
    if filename != None:
        local_filename = filename
    else:
        local_filename = url.split('/')[-1]
        if '?' in local_filename:
            local_filename = local_filename.rpartition('?')[0]
    local_filename = os.path.join(to if to != None else root_dir, local_filename)
    if to != None:
        if not os.path.exists(to):
            os.makedirs(to)
    if os.path.isfile(local_filename):
        return local_filename
    with requests.get(url, stream=True) as r:
        with open(local_filename, 'wb') as f:
            if raw:
              shutil.copyfileobj(r.raw, f)
            else:
              f.write(r.content)
    return local_filename
def wget(url, to):
    res = subprocess.run(['wget', '-q', url, '-O', to], stdout=subprocess.PIPE).stdout.decode('utf-8')

try:
  import flet
except ImportError as e:
  run_sp("pip install flet --upgrade --quiet --pre")
  #run_sp("pip install -i https://test.pypi.org/simple/ flet")
  #run_sp("pip install --upgrade git+https://github.com/flet-dev/flet.git@controls-s3#egg=flet-dev")
  pass
'''try:
  from flet_ivid import VideoContainer
except ModuleNotFoundError:
  run_sp("pip install --upgrade flet_ivid")
  from flet_ivid import VideoContainer
  pass'''
try:
  import requests
except Exception:
  run_sp("pip install -q requests", realtime=False)
  import requests
try:
  from IPython.display import clear_output
except Exception:
  run_sp("pip install -q ipython")
  from IPython.display import clear_output
  pass
try:
  from emoji import emojize
except ImportError as e:
  run_sp("pip install emoji --quiet")
  from emoji import emojize
  pass
if 'url' not in locals():
  url=""
if tunnel_type == "ngrok":
  try:
    import pyngrok
  except ImportError as e:
    run_sp("pip install pyngrok --quiet", realtime=False)
    run_sp(f"ngrok authtoken {ng()}", realtime=False)
    import pyngrok
    pass
elif tunnel_type == "localtunnel":
  if not bool(url):
    import re
    run_sp("npm install -g -q localtunnel")
    localtunnel = subprocess.Popen(['lt', '--port', '80', 'http'], stdout=subprocess.PIPE)
    url = str(localtunnel.stdout.readline())
    url = (re.search("(?P<url>https?:\/\/[^\s]+loca.lt)", url).group("url"))
    print(url)

gdrive = None
if storage_type == "PyDrive Google Drive":
  if not os.path.isfile(Google_OAuth_client_secret_json):
    raise ValueError("Couldn't locate your client_secret.json file to authenticate. Follow instructions below then copy certificate to your root dir.")
  try:
    from pydrive2.auth import GoogleAuth, ServiceAccountCredentials
    from pydrive2.drive import GoogleDrive
    from oauth2client.contrib.gce import AppAssertionCredentials
  except ImportError as e:
    run_sp("pip install PyDrive2 -q")
    from pydrive2.auth import GoogleAuth, ServiceAccountCredentials
    from pydrive2.drive import GoogleDrive
    from oauth2client.contrib.gce import AppAssertionCredentials
    pass
  import httplib2

  old_local_webserver_auth = GoogleAuth.LocalWebserverAuth
  def LocalWebServerAuth(self, *args, **kwargs):
      if isinstance(self.credentials, AppAssertionCredentials):
          self.credentials.refresh(httplib2.Http())
          return
      return old_local_webserver_auth(self, *args, **kwargs)
  GoogleAuth.LocalWebserverAuth = LocalWebServerAuth

  #scope = 'https://www.googleapis.com/auth/drive'
  #credentials = ServiceAccountCredentials.from_json_keyfile_name(Google_OAuth_client_secret_json, scope)
  gauth = GoogleAuth()
  gauth.LoadCredentialsFile(Google_OAuth_client_secret_json)
  #gauth.LocalWebserverAuth()
  if is_Colab: gauth.CommandLineAuth()
  else:
    gauth.LocalWebserverAuth()
    gauth.SaveCredentialsFile(Google_OAuth_client_secret_json)
  gdrive = GoogleDrive(gauth)
slash = '/'
from pathlib import Path
if not is_Colab:
    image_output = os.path.join(Path.home(), "Pictures", "Stable_Diffusion")
    if "\\" in image_output:
        slash = '\\'
else:
    image_output = '/content/drive/MyDrive/AI/Stable_Diffusion/images_out'
saved_settings_json = os.path.join(root_dir, "sdd-settings.json")
favicon = os.path.join(root_dir, "favicon.png")
loading_animation = os.path.join(root_dir, "loading_animation.png")
assets = os.path.join(root_dir, "assets")
if not os.path.isfile(favicon):
    download_file("https://github.com/Skquark/AI-Friends/blob/main/assets/favicon.png?raw=true")
if not os.path.isfile(loading_animation):
    download_file("https://github.com/Skquark/AI-Friends/blob/main/assets/loading_animation.png?raw=true")
if not os.path.exists(assets):
    os.makedirs(assets)
    download_file("https://github.com/Skquark/AI-Friends/blob/main/assets/snd-alert.mp3?raw=true", to=assets)
    download_file("https://github.com/Skquark/AI-Friends/blob/main/assets/snd-delete.mp3?raw=true", to=assets)
    download_file("https://github.com/Skquark/AI-Friends/blob/main/assets/snd-error.mp3?raw=true", to=assets)
    download_file("https://github.com/Skquark/AI-Friends/blob/main/assets/snd-done.mp3?raw=true", to=assets)
    download_file("https://github.com/Skquark/AI-Friends/blob/main/assets/snd-drop.mp3?raw=true", to=assets)
sdd_utils_py = os.path.join(root_dir, "sdd_utils.py")
if not os.path.exists(sdd_utils_py):
    download_file("https://raw.githubusercontent.com/Skquark/AI-Friends/main/sdd_utils.py", to=root_dir, raw=False)
#sys.path.append(sdd_utils_py)
import sdd_utils
clear_output()

import json
if 'prefs' not in locals():
  prefs = {}
def load_settings_file():
  global prefs
  if os.path.isfile(saved_settings_json):
    with open(saved_settings_json) as settings:
      prefs = json.load(settings)
    print("Successfully loaded settings json...")
  else:
    print("Settings file not found, starting with defaults...")
    prefs = {
      'save_to_GDrive': True,
      'image_output': image_output,
      'file_prefix': 'sd-',
      'file_suffix_seed': False,
      'file_max_length': 220,
      'file_allowSpace': False,
      'file_datetime': False,
      'save_image_metadata': True,
      'meta_ArtistName':'',
      'meta_Copyright': '',
      'save_config_in_metadata': True,
      'save_config_json': False,
      'theme_mode': 'Dark',
      'theme_color': 'Green',
      'enable_sounds': True,
      'start_in_installation': False,
      'disable_nsfw_filter': True,
      'retry_attempts': 3,
      'HuggingFace_api_key': "",
      'Stability_api_key': "",
      'OpenAI_api_key': "",
      'TextSynth_api_key': "",
      'Replicate_api_key': "",
      'AIHorde_api_key': "0000000000",
      'HuggingFace_username': "",
      'scheduler_mode': "DDIM",
      'higher_vram_mode': False,
      'enable_attention_slicing': True,
      'memory_optimization': "None",
      'sequential_cpu_offload': False,
      'vae_slicing': True,
      'vae_tiling': False,
      'enable_torch_compile': False,
      'enable_tome': False,
      'tome_ratio': 0.5,
      'cache_dir': '',
      'install_diffusers': True,
      'install_interpolation': False,
      'install_text2img': True,
      'install_img2img': False,
      'install_SDXL': False,
      'install_megapipe': True,
      'install_CLIP_guided': False,
      'install_OpenAI': False,
      'install_TextSynth': False,
      'install_dreamfusion': False,
      'install_repaint': False,
      'install_imagic': False,
      'install_composable': False,
      'install_safe': False,
      'install_versatile': False,
      'install_depth2img': False,
      'install_alt_diffusion': False,
      'install_attend_and_excite': False,
      'install_SAG': False,
      'install_panorama': False,
      'install_upscale': False,
      'safety_config': 'Strong',
      'use_imagic': False,
      'use_SDXL': False,
      'SDXL_high_noise_frac': 0.7,
      'SDXL_compel': False,
      'use_composable': False,
      'use_safe': False,
      'use_versatile': False,
      'use_alt_diffusion': False,
      'use_attend_and_excite': False,
      'max_iter_to_alter': 25,
      'use_SAG': False,
      'sag_scale': 0.75,
      'use_panorama': False,
      'panorama_width': 2048,
      'use_upscale': False,
      'upscale_noise_level': 20,
      'install_conceptualizer': False,
      'use_conceptualizer': False,
      'concepts_model': 'cat-toy',
      'model_ckpt': 'Stable Diffusion v1.5',
      'finetuned_model': 'Midjourney v4 style',
      'dreambooth_model': 'disco-diffusion-style',
      'custom_model': '',
      'custom_models': [],
      'tortoise_custom_voices': [],
      'custom_dance_diffusion_models': [],
      'clip_model_id': "laion/CLIP-ViT-B-32-laion2B-s34B-b79K",
      'install_Stability_api': False,
      'use_Stability_api': False,
      'model_checkpoint': "stable-diffusion-768-v2-1",
      'generation_sampler': "K_EULER_ANCESTRAL",
      'clip_guidance_preset': "FAST_BLUE",
      'install_AIHorde_api': False,
      'use_AIHorde_api': False,
      'AIHorde_model': 'stable_diffusion',
      'AIHorde_sampler': 'k_euler_a',
      'AIHorde_post_processing': "None",
      'install_ESRGAN': True,
      'batch_folder_name': "",
      'batch_size': 1,
      'n_iterations': 1,
      'steps': 50,
      'eta': 0.4,
      'seed': 0,
      'guidance_scale': 8,
      'width': 960,
      'height': 512,
      'init_image': "",
      'mask_image': "",
      'init_image_strength': 0.25,
      'alpha_mask': False,
      'invert_mask': False,
      'precision': 'autocast',
      'use_inpaint_model': False,
      'centipede_prompts_as_init_images': False,
      'use_depth2img': False,
      'use_LoRA_model': False,
      'LoRA_model': 'Von Platen LoRA',
      'custom_LoRA_models': [],
      'custom_LoRA_model': "",
      'use_interpolation': False,
      'num_interpolation_steps': 22,
      'use_clip_guided_model': False,
      'clip_guidance_scale': 571,
      'use_cutouts': True,
      'num_cutouts': 4,
      'unfreeze_unet': True,
      'unfreeze_vae': True,
      'apply_ESRGAN_upscale': True,
      'enlarge_scale': 1.5,
      'face_enhance':False,
      'display_upscaled_image': False,
      'negatives': ["Blurry"],
      'custom_negatives': "",
      'prompt_styler': '',
      'prompt_style': 'cinematic-default',
      'prompt_list': [],
      'prompt_generator': {
          'phrase': '',
          'subject_detail': '',
          'phrase_as_subject': False,
          'amount': 10,
          'random_artists': 2,
          'random_styles': 1,
          'permutate_artists': False,
          'request_mode': 3,
          'AI_temperature': 0.9,
          'AI_engine': "OpenAI GPT-3",
          'economy_mode': True,
      },
      'prompt_remixer': {
          'seed_prompt': '',
          'optional_about_influencer': '',
          'amount': 10,
          'random_artists': 2,
          'random_styles': 1,
          'permutate_artists': False,
          'request_mode': 3,
          'AI_temperature': 0.9,
          'AI_engine': "OpenAI GPT-3",
      },
      'prompt_brainstormer': {
          'AI_engine': 'OpenAI GPT-3',
          'about_prompt': '',
          'request_mode': 'Brainstorm',
          'AI_temperature': 0.9,
      },
      'prompt_writer': {
          'art_Subjects': '',
          'negative_prompt': '',
          'by_Artists': '',
          'art_Styles': '',
          'amount': 10,
          'random_artists': 2,
          'random_styles': 1,
          'permutate_artists': False,
      },
    }
if prefs == {}:
  load_settings_file()
version_checker()



#@title ## **▶️ Run Stable Diffusion Deluxe** - Flet/Flutter WebUI App
import flet as ft
#from flet import *
from flet import Page, View, Column, Row, ResponsiveRow, Container, Text, Stack, TextField, Checkbox, Switch, Image, ElevatedButton, FilledButton, IconButton, Markdown, Tab, Tabs, AppBar, Divider, VerticalDivider, GridView, Tooltip, SnackBar, AnimatedSwitcher, ButtonStyle, FloatingActionButton, Audio, Theme, Dropdown, Slider, ListTile, ListView, TextButton, PopupMenuButton, PopupMenuItem, AlertDialog, Banner, Icon, ProgressBar, ProgressRing, GestureDetector, KeyboardEvent, FilePicker, FilePickerResultEvent, FilePickerUploadFile, FilePickerUploadEvent, UserControl, Ref
from flet import icons, dropdown, colors, padding, margin, alignment, border_radius, theme, animation, KeyboardType, TextThemeStyle, AnimationCurve
from flet import TextAlign, FontWeight, ClipBehavior, MainAxisAlignment, CrossAxisAlignment, ScrollMode, ImageFit, ThemeMode
from flet import BlendMode
from flet import Image as Img
from sdd_utils import finetuned_models, dreambooth_models, styles, artists
try:
    import PIL
except ModuleNotFoundError:
    run_sp("pip install Pillow", realtime=False)
    run_sp("pip install image", realtime=False)
    import PIL
    pass
from PIL import Image as PILImage # Avoids flet conflict
if not hasattr(PILImage, 'Resampling'):  # Allow Pillow<9.0
   PILImage.Resampling = PILImage
import random as rnd
import io, shutil, traceback, string, datetime, threading
from packaging import version
from contextlib import redirect_stdout
try:
  import numpy as np
except ModuleNotFoundError:
  run_sp("pip install numpy", realtime=False)
  import numpy as np
  pass

if 'prefs' not in locals():
    raise ValueError("Setup not initialized. Run the previous code block first and authenticate your Drive storage.")
status = {
    'installed_diffusers': False,
    'installed_txt2img': False,
    'installed_img2img': False,
    'installed_SDXL': False,
    'installed_stability': False,
    'installed_AIHorde': False,
    'installed_megapipe': False,
    'installed_interpolation': False,
    'installed_clip': False,
    'installed_ESRGAN': False,
    'installed_OpenAI': False,
    'installed_TextSynth': False,
    'installed_conceptualizer': False,
    'installed_dreamfusion': False,
    'installed_repaint': False,
    'installed_imagic': False,
    'installed_composable': False,
    'installed_safe': False,
    'installed_versatile': False,
    'installed_depth2img': False,
    'installed_attend_and_excite': False,
    'installed_SAG': False,
    'installed_panorama': False,
    'installed_alt_diffusion': False,
    'installed_upscale': False,
    'installed_xformers': False,
    'finetuned_model': False,
    'loaded_scheduler': '',
    'loaded_model': '',
    'loaded_controlnet': '',
    'loaded_SDXL': '',
    'changed_settings': False,
    'changed_installers': False,
    'changed_parameters': False,
    'changed_prompts': False,
    'changed_prompt_generator': False,
    'changed_prompt_remixer': False,
    'changed_prompt_brainstormer': False,
    'changed_prompt_writer': False,
    'initialized': False,
}

def save_settings_file(page, change_icon=True):
  if change_icon:
    page.app_icon_save()
  if not os.path.isfile(saved_settings_json):
    settings_path = saved_settings_json.rpartition(slash)[0]
    os.makedirs(settings_path, exist_ok=True)
  with open(saved_settings_json, "w") as write_file:
    json.dump(prefs, write_file, indent=4)

current_tab = 0
def tab_on_change (e):
    t = e.control
    global current_tab, status
    #print (f"tab changed from {current_tab} to: {t.selected_index}")
    #print(str(t.tabs[t.selected_index].text))
    if current_tab == 0:
      #if not status['initialized']:
      #  initState(e.page)
      #  status['initialized'] = True
      if status['changed_settings']:
        save_settings_file(e.page)
        status['changed_settings'] = False
        #print(len(e.page.Settings.content.controls))
        #save_settings(e.page.Settings.content.controls)
    if current_tab == 1:
      if status['changed_installers']:
        save_settings_file(e.page)
        status['changed_installers'] = False
        #print("Saving Installers")
      e.page.show_install_fab(False)
    if current_tab == 2:
      if status['changed_parameters']:
        update_args()
        e.page.update_prompts()
        save_settings_file(e.page)
        status['changed_parameters'] = False
      e.page.show_apply_fab(False)
    if current_tab == 3:
      if status['changed_prompts']:
        e.page.save_prompts()
        save_settings_file(e.page)
        status['changed_prompts'] = False
      e.page.show_run_diffusion_fab(False, p=e.page)
    if current_tab == 5:
      if status['changed_prompt_generator']:
        save_settings_file(e.page)
        status['changed_prompt_generator'] = False

    current_tab = t.selected_index
    if current_tab == 1:
      refresh_installers(e.page.Installers.controls[0].content.controls)
      e.page.show_install_fab(True)
      #page.Installers.init_boxes()
    if current_tab == 2:
      update_parameters(e.page)
      #for p in e.page.Parameters.content.controls:
      e.page.Parameters.controls[0].content.update()
      e.page.Parameters.update()
      e.page.show_apply_fab(len(prompts) > 0 and status['changed_parameters'])
    if current_tab == 3:
      e.page.show_run_diffusion_fab(True, p=e.page) #len(prompts) > 0
    e.page.update()

def buildTabs(page):
    page.Settings = buildSettings(page)
    page.Installers = buildInstallers(page)
    page.Parameters = buildParameters(page)
    page.PromptsList = buildPromptsList(page)
    page.PromptHelpers = buildPromptHelpers(page)
    page.Images = buildImages(page)
    page.ImageAIs = buildImageAIs(page)
    page.Trainers = buildTrainers(page)
    page.VideoAIs = buildVideoAIs(page)
    page.AudioAIs = buildAudioAIs(page)
    page.Text3DAIs = build3DAIs(page)
    page.Extras = buildExtras(page)

    t = Tabs(selected_index=0, animation_duration=300, expand=1,
        tabs=[
            Tab(text="Settings", content=page.Settings, icon=icons.SETTINGS_OUTLINED),
            Tab(text="Installation", content=page.Installers, icon=icons.INSTALL_DESKTOP),
            Tab(text="Image Parameters", content=page.Parameters, icon=icons.DISPLAY_SETTINGS),
            Tab(text="Prompts List", content=page.PromptsList, icon=icons.FORMAT_LIST_BULLETED),
            Tab(text="Generate Images", content=page.Images, icon=icons.IMAGE_OUTLINED),
            Tab(text="Prompt Helpers", content=page.PromptHelpers, icon=icons.BUBBLE_CHART_OUTLINED),
            Tab(text="Image AIs", content=page.ImageAIs, icon=icons.PALETTE),
            Tab(text="Video AIs", content=page.VideoAIs, icon=icons.VIDEO_CAMERA_BACK),
            Tab(text="3D AIs", content=page.Text3DAIs, icon=icons.VIEW_IN_AR),
            Tab(text="Audio AIs", content=page.AudioAIs, icon=icons.EQUALIZER),
            Tab(text="AI Trainers", content=page.Trainers, icon=icons.TSUNAMI),
            Tab(text="Extras", content=page.Extras, icon=icons.ALL_INBOX),
        ],
    )
    page.tabs = t
    return t

def buildPromptHelpers(page):
    page.generator = buildPromptGenerator(page)
    page.remixer = buildPromptRemixer(page)
    page.brainstormer = buildPromptBrainstormer(page)
    page.writer = buildPromptWriter(page)
    page.negatives = buildNegatives(page)
    page.styler = buildPromptStyler(page)
    page.Image2Text = buildImage2Text(page)
    page.MagicPrompt = buildMagicPrompt(page)
    page.DistilGPT2 = buildDistilGPT2(page)
    page.RetrievePrompts = buildRetrievePrompts(page)
    page.InitFolder = buildInitFolder(page)
    page.InitVideo = buildInitVideo(page)
    promptTabs = Tabs(selected_index=0, animation_duration=300, expand=1,
        tabs=[
            Tab(text="Prompt Writer", content=page.writer, icon=icons.CLOUD_CIRCLE),
            Tab(text="Prompt Generator", content=page.generator, icon=icons.CLOUD),
            Tab(text="Prompt Remixer", content=page.remixer, icon=icons.CLOUD_SYNC_ROUNDED),
            Tab(text="Prompt Brainstormer", content=page.brainstormer, icon=icons.CLOUDY_SNOWING),
            Tab(text="Prompt Styler", content=page.styler, icon=icons.FORMAT_COLOR_FILL),
            Tab(text="Negatives", content=page.negatives, icon=icons.REMOVE_CIRCLE),
            Tab(text="Image2Text", content=page.Image2Text, icon=icons.WRAP_TEXT),
            Tab(text="Magic Prompt", content=page.MagicPrompt, icon=icons.AUTO_FIX_HIGH),
            Tab(text="Distil GPT-2", content=page.DistilGPT2, icon=icons.FILTER_ALT),
            Tab(text="Retrieve Prompt from Image", content=page.RetrievePrompts, icon=icons.PHOTO_LIBRARY_OUTLINED),
            Tab(text="Init Images from Folder", content=page.InitFolder, icon=icons.FOLDER_SPECIAL),
            Tab(text="Init Images from Video", content=page.InitVideo, icon=icons.SWITCH_VIDEO),
        ],
    )
    return promptTabs

def buildImageAIs(page):
    page.RePainter = buildRepainter(page)
    page.unCLIP = buildUnCLIP(page)
    page.unCLIP_Interpolation = buildUnCLIP_Interpolation(page)
    page.unCLIP_ImageInterpolation = buildUnCLIP_ImageInterpolation(page)
    page.UnCLIP_ImageVariation = buildUnCLIP_ImageVariation(page)
    page.Reference = buildReference(page)
    page.ControlNetQR = buildControlNetQR(page)
    page.ControlNetSegmentAnything = buildControlNetSegmentAnything(page)
    page.EDICT = buildEDICT(page)
    page.DiffEdit = buildDiffEdit(page)
    page.ImageVariation = buildImageVariation(page)
    page.CLIPstyler = buildCLIPstyler(page)
    page.MagicMix = buildMagicMix(page)
    page.SemanticGuidance = buildSemanticGuidance(page)
    page.PaintByExample = buildPaintByExample(page)
    page.InstructPix2Pix = buildInstructPix2Pix(page)
    page.ControlNet = buildControlNet(page)
    page.ControlNetXL = buildControlNetXL(page)
    page.DeepFloyd = buildDeepFloyd(page)
    page.MaterialDiffusion = buildMaterialDiffusion(page)
    page.DallE2 = buildDallE2(page)
    page.Kandinsky = buildKandinsky(page)
    page.KandinskyFuse = buildKandinskyFuse(page)
    page.KandinskyControlNet = buildKandinskyControlNet(page)
    page.DiT = buildDiT(page)
    page.DreamFusion = buildDreamFusion(page)
    page.Point_E = buildPoint_E(page)
    page.Shap_E = buildShap_E(page)
    page.InstantNGP = buildInstantNGP(page)
    diffusersTabs = Tabs(selected_index=0, animation_duration=300, expand=1,
        tabs=[
            Tab(text="Instruct Pix2Pix", content=page.InstructPix2Pix, icon=icons.SOLAR_POWER),
            Tab(text="ControlNet", content=page.ControlNet, icon=icons.HUB),
            Tab(text="ControlNet SDXL", content=page.ControlNetXL, icon=icons.PEST_CONTROL),
            Tab(text="Kandinsky", content=page.Kandinsky, icon=icons.TOLL),
            Tab(text="Kandinsky Fuse", content=page.KandinskyFuse, icon=icons.FIREPLACE),
            Tab(text="Kandinsky ControlNet", content=page.KandinskyControlNet, icon=icons.CAMERA_ENHANCE),
            Tab(text="DeepFloyd-IF", content=page.DeepFloyd, icon=icons.LOOKS),
            Tab(text="unCLIP", content=page.unCLIP, icon=icons.ATTACHMENT_SHARP),
            Tab(text="unCLIP Interpolation", content=page.unCLIP_Interpolation, icon=icons.TRANSFORM),
            Tab(text="unCLIP Image Interpolation", content=page.unCLIP_ImageInterpolation, icon=icons.ANIMATION),
            Tab(text="unCLIP Image Variation", content=page.UnCLIP_ImageVariation, icon=icons.AIRLINE_STOPS),
            Tab(text="Image Variation", content=page.ImageVariation, icon=icons.FORMAT_COLOR_FILL),
            Tab(text="Reference", content=page.Reference, icon=icons.CRISIS_ALERT),
            Tab(text="QRCode", content=page.ControlNetQR, icon=icons.QR_CODE_2),
            Tab(text="Re-Segment-Anything", content=page.ControlNetSegmentAnything, icon=icons.SEND_TIME_EXTENSION),
            Tab(text="EDICT Edit", content=page.EDICT, icon=icons.AUTO_AWESOME),
            Tab(text="DiffEdit", content=page.DiffEdit, icon=icons.AUTO_GRAPH),
            Tab(text="RePainter", content=page.RePainter, icon=icons.FORMAT_PAINT),
            Tab(text="MagicMix", content=page.MagicMix, icon=icons.BLENDER),
            Tab(text="Paint-by-Example", content=page.PaintByExample, icon=icons.FORMAT_SHAPES),
            Tab(text="CLIP-Styler", content=page.CLIPstyler, icon=icons.STYLE),
            Tab(text="Semantic Guidance", content=page.SemanticGuidance, icon=icons.ROUTE),
            Tab(text="Material Diffusion", content=page.MaterialDiffusion, icon=icons.TEXTURE),
            Tab(text="OpenAI Dall-E 2", content=page.DallE2, icon=icons.BLUR_CIRCULAR),
            Tab(text="DiT", content=page.DiT, icon=icons.ANALYTICS),
            #Tab(text="DreamFusion 3D", content=page.DreamFusion, icon=icons.THREED_ROTATION),
            #Tab(text="Point-E 3D", content=page.Point_E, icon=icons.SWIPE_UP),
            #Tab(text="Shap-E 3D", content=page.Shap_E, icon=icons.PRECISION_MANUFACTURING),
            #Tab(text="Instant-NGP", content=page.InstantNGP, icon=icons.STADIUM),
        ],
    )
    return diffusersTabs

def build3DAIs(page):
    page.DreamFusion = buildDreamFusion(page)
    page.Point_E = buildPoint_E(page)
    page.Shap_E = buildShap_E(page)
    page.InstantNGP = buildInstantNGP(page)
    diffusersTabs = Tabs(selected_index=0, animation_duration=300, expand=1,
        tabs=[
            Tab(text="DreamFusion 3D", content=page.DreamFusion, icon=icons.THREED_ROTATION),
            Tab(text="Point-E 3D", content=page.Point_E, icon=icons.SWIPE_UP),
            Tab(text="Shap-E 3D", content=page.Shap_E, icon=icons.PRECISION_MANUFACTURING),
            Tab(text="Instant-NGP", content=page.InstantNGP, icon=icons.STADIUM),
        ],
    )
    return diffusersTabs

def buildTrainers(page):
    page.DreamBooth = buildDreamBooth(page)
    page.TexualInversion = buildTextualInversion(page)
    page.LoRA_Dreambooth = buildLoRA_Dreambooth(page)
    page.LoRA = buildLoRA(page)
    page.Converter = buildConverter(page)
    page.CheckpointMerger = buildCheckpointMerger(page)
    trainersTabs = Tabs(selected_index=0, animation_duration=300, expand=1,
        tabs=[
            Tab(text="LoRA DreamBooth", content=page.LoRA_Dreambooth, icon=icons.SETTINGS_BRIGHTNESS),
            Tab(text="LoRA", content=page.LoRA, icon=icons.SETTINGS_SYSTEM_DAYDREAM),
            Tab(text="DreamBooth", content=page.DreamBooth, icon=icons.PHOTO),
            Tab(text="Texual-Inversion", content=page.TexualInversion, icon=icons.PHOTO_ALBUM),
            Tab(text="Model Converter", content=page.Converter, icon=icons.PUBLISHED_WITH_CHANGES),
            Tab(text="Checkpoint Merger", content=page.CheckpointMerger, icon=icons.JOIN_FULL),
        ],
    )
    return trainersTabs

def buildVideoAIs(page):
    page.TextToVideo = buildTextToVideo(page)
    page.TextToVideoZero = buildTextToVideoZero(page)
    page.VideoToVideo = buildVideoToVideo(page)
    page.Potat1 = buildPotat1(page)
    page.StableAnimation = buildStableAnimation(page)
    page.ControlNet = buildControlNet(page)
    page.ControlNet_Video2Video = buildControlNet_Video2Video(page)
    page.Roop = buildROOP(page)
    page.AnimateDiff = buildAnimateDiff(page)

    videoAIsTabs = Tabs(selected_index=0, animation_duration=300, expand=1,
        tabs=[
            Tab(text="Stable Animation", content=page.StableAnimation, icon=icons.SHUTTER_SPEED),
            Tab(text="AnimateDiff", content=page.AnimateDiff, icon=icons.AUTO_MODE),
            Tab(text="Text-to-Video", content=page.TextToVideo, icon=icons.MISSED_VIDEO_CALL),
            Tab(text="Text-to-Video Zero", content=page.TextToVideoZero, icon=icons.ONDEMAND_VIDEO),
            Tab(text="Potat1", content=page.Potat1, icon=icons.FILTER_1),
            Tab(text="ROOP Face-Swap", content=page.Roop, icon=icons.FACE_RETOUCHING_NATURAL),
            Tab(text="ControlNet Video2Video", content=page.ControlNet_Video2Video, icon=icons.PSYCHOLOGY),
            Tab(text="Video-to-Video", content=page.VideoToVideo, icon=icons.CAMERA_ROLL),
            Tab(text="ControlNet Init-Video", content=page.ControlNet, icon=icons.HUB),
        ],
    )
    return videoAIsTabs

def buildAudioAIs(page):
    page.TortoiseTTS = buildTortoiseTTS(page)
    page.MusicGen = buildMusicGen(page)
    page.DanceDiffusion = buildDanceDiffusion(page)
    page.AudioDiffusion = buildAudioDiffusion(page)
    page.AudioLDM = buildAudioLDM(page)
    page.AudioLDM2 = buildAudioLDM2(page)
    page.Bark = buildBark(page)
    page.Riffusion = buildRiffusion(page)
    page.Mubert = buildMubert(page)
    page.Whisper = buildWhisper(page)
    page.VoiceFixer = buildVoiceFixer(page)
    audioAIsTabs = Tabs(selected_index=0, animation_duration=300, expand=1,
        tabs=[
            Tab(text="Tortoise-TTS", content=page.TortoiseTTS, icon=icons.RECORD_VOICE_OVER),
            Tab(text="MusicGen", content=page.MusicGen, icon=icons.MUSIC_NOTE),
            Tab(text="AudioLDM", content=page.AudioLDM, icon=icons.NOISE_AWARE),
            Tab(text="AudioLDM-2", content=page.AudioLDM2, icon=icons.NOISE_CONTROL_OFF),
            Tab(text="Bark", content=page.Bark, icon=icons.PETS),
            Tab(text="Riffusion", content=page.Riffusion, icon=icons.SPATIAL_AUDIO),
            Tab(text="Audio Diffusion", content=page.AudioDiffusion, icon=icons.GRAPHIC_EQ),
            Tab(text="HarmonAI Dance Diffusion", content=page.DanceDiffusion, icon=icons.QUEUE_MUSIC),
            Tab(text="Whisper-STT", content=page.Whisper, icon=icons.HEARING),
            Tab(text="Voice Fixer", content=page.VoiceFixer, icon=icons.VOICE_CHAT),
            Tab(text="Mubert Music", content=page.Mubert, icon=icons.MUSIC_VIDEO),
        ],
    )
    return audioAIsTabs

def buildExtras(page):
    page.ESRGAN_upscaler = buildESRGANupscaler(page)
    page.CachedModelManager = buildCachedModelManager(page)
    page.CustomModelManager = buildCustomModelManager(page)
    page.MaskMaker = buildDreamMask(page)
    page.BackgroundRemover = buildBackgroundRemover(page)
    page.BLIP2Image2Text = buildBLIP2Image2Text(page)
    page.Kandinsky2 = buildKandinsky2(page)
    page.Kandinsky2Fuse = buildKandinsky2Fuse(page)
    page.DeepDaze = buildDeepDaze(page)
    extrasTabs = Tabs(selected_index=0, animation_duration=300, expand=1,
        tabs=[
            Tab(text="Real-ESRGAN Batch Upscaler", content=page.ESRGAN_upscaler, icon=icons.PHOTO_SIZE_SELECT_LARGE),
            Tab(text="Cache Manager", content=page.CachedModelManager, icon=icons.CACHED),
            Tab(text="Model Manager", content=page.CustomModelManager, icon=icons.DIFFERENCE),
            Tab(text="Background Remover", content=page.BackgroundRemover, icon=icons.WALLPAPER),
            #Tab(text="Dream Mask Maker", content=page.MaskMaker, icon=icons.GRADIENT),
            Tab(text="BLIP2 Image2Text", content=page.BLIP2Image2Text, icon=icons.BATHTUB),
            Tab(text="Kandinsky 2.1", content=page.Kandinsky2, icon=icons.AC_UNIT),
            Tab(text="Kandinsky Fuse", content=page.Kandinsky2Fuse, icon=icons.FIREPLACE),
            Tab(text="DeepDaze", content=page.DeepDaze, icon=icons.FACE),
        ],
    )
    return extrasTabs


def b_style():
    return ButtonStyle(elevation=8)
def dict_diff(dict1, dict2):
    return {k: v for k, v in dict1.items() if k in dict2 and v != dict2[k]}
def arg_diffs(dict1, dict2):
    diff = dict_diff(dict1, dict2)
    if len(diff) > 0:
      dif = []
      for k, v in diff.items():
        dif.append(f'{to_title(k)}: {v}')
      return ', '.join(dif)
    else: return None
def get_color(color):
    if color == "green": return colors.GREEN
    elif color == "blue": return colors.BLUE
    elif color == "indigo": return colors.INDIGO
    elif color == "red": return colors.RED
    elif color == "purple": return colors.DEEP_PURPLE
    elif color == "orange": return colors.ORANGE
    elif color == "amber": return colors.AMBER
    elif color == "brown": return colors.BROWN
    elif color == "teal": return colors.TEAL
    elif color == "yellow": return colors.YELLOW


# Delete these after everyone's updated
if 'file_datetime' not in prefs: prefs['file_datetime'] = False
if 'install_conceptualizer' not in prefs: prefs['install_conceptualizer'] = False
if 'use_conceptualizer' not in prefs: prefs['use_conceptualizer'] = False
if 'concepts_model' not in prefs: prefs['concepts_model'] = 'cat-toy'
if 'memory_optimization' not in prefs: prefs['memory_optimization'] = 'None'
if 'sequential_cpu_offload' not in prefs: prefs['sequential_cpu_offload'] = False
if 'vae_slicing' not in prefs: prefs['vae_slicing'] = True
if 'vae_tiling' not in prefs: prefs['vae_tiling'] = False
if 'use_inpaint_model' not in prefs: prefs['use_inpaint_model'] = False
if 'cache_dir' not in prefs: prefs['cache_dir'] = ''
if 'Replicate_api_key' not in prefs: prefs['Replicate_api_key'] = ''
if 'install_dreamfusion' not in prefs: prefs['install_dreamfusion'] = False
if 'install_repaint' not in prefs: prefs['install_repaint'] = False
if 'finetuned_model' not in prefs: prefs['finetuned_model'] = 'Midjourney v4 style'
if 'dreambooth_model' not in prefs: prefs['dreambooth_model'] = 'disco-diffusion-style'
if 'custom_model' not in prefs: prefs['custom_model'] = ''
if 'custom_models' not in prefs: prefs['custom_models'] = []
if 'start_in_installation' not in prefs: prefs['start_in_installation'] = False
if 'install_imagic' not in prefs: prefs['install_imagic'] = False
if 'use_imagic' not in prefs: prefs['use_imagic'] = False
if 'install_composable' not in prefs: prefs['install_composable'] = False
if 'use_composable' not in prefs: prefs['use_composable'] = False
if 'install_safe' not in prefs: prefs['install_safe'] = False
if 'use_safe' not in prefs: prefs['use_safe'] = False
if 'safety_config' not in prefs: prefs['safety_config'] = "Strong"
if 'install_versatile' not in prefs: prefs['install_versatile'] = False
if 'use_versatile' not in prefs: prefs['use_versatile'] = False
if 'install_depth2img' not in prefs: prefs['install_depth2img'] = False
if 'use_depth2img' not in prefs: prefs['use_depth2img'] = False
if 'install_alt_diffusion' not in prefs: prefs['install_alt_diffusion'] = False
if 'use_alt_diffusion' not in prefs: prefs['use_alt_diffusion'] = False
if 'install_upscale' not in prefs: prefs['install_upscale'] = False
if 'use_upscale' not in prefs: prefs['use_upscale'] = False
if 'upscale_noise_level' not in prefs: prefs['upscale_noise_level'] = 20
if 'alpha_mask' not in prefs: prefs['alpha_mask'] = False
if 'invert_mask' not in prefs: prefs['invert_mask'] = False
if 'clip_guidance_preset' not in prefs: prefs['clip_guidance_preset'] = "FAST_BLUE"
if 'tortoise_custom_voices' not in prefs: prefs['tortoise_custom_voices'] = []
if 'use_LoRA_model' not in prefs: prefs['use_LoRA_model'] = False
if 'LoRA_model' not in prefs: prefs['LoRA_model'] = "Von Platen LoRA"
if 'custom_LoRA_models' not in prefs: prefs['custom_LoRA_models'] = []
if 'custom_LoRA_model' not in prefs: prefs['custom_LoRA_model'] = ''
if 'custom_dance_diffusion_models' not in prefs: prefs['custom_dance_diffusion_models'] = []
if 'negative_prompt' not in prefs['prompt_writer']: prefs['prompt_writer']['negative_prompt'] = ''
if 'install_attend_and_excite' not in prefs: prefs['install_attend_and_excite'] = False
if 'use_attend_and_excite' not in prefs: prefs['use_attend_and_excite'] = False
if 'max_iter_to_alter' not in prefs: prefs['max_iter_to_alter'] = 25
if 'install_SAG' not in prefs: prefs['install_SAG'] = False
if 'use_SAG' not in prefs: prefs['use_SAG'] = False
if 'sag_scale' not in prefs: prefs['sag_scale'] = 0.75
if 'install_SDXL' not in prefs: prefs['install_SDXL'] = False
if 'use_SDXL' not in prefs: prefs['use_SDXL'] = False
if 'SDXL_high_noise_frac' not in prefs: prefs['SDXL_high_noise_frac'] = 0.7
if 'SDXL_compel' not in prefs: prefs['SDXL_compel'] = False
if 'install_panorama' not in prefs: prefs['install_panorama'] = False
if 'use_panorama' not in prefs: prefs['use_panorama'] = False
if 'panorama_circular_padding' not in prefs: prefs['panorama_circular_padding'] = False
if 'panorama_width' not in prefs: prefs['panorama_width'] = 2048
if 'AI_engine' not in prefs['prompt_generator']: prefs['prompt_generator']['AI_engine'] = 'OpenAI GPT-3'
if 'AI_engine' not in prefs['prompt_remixer']: prefs['prompt_remixer']['AI_engine'] = 'OpenAI GPT-3'
if 'AIHorde_api_key' not in prefs: prefs['AIHorde_api_key'] = '0000000000'
if 'install_AIHorde_api' not in prefs: prefs['install_AIHorde_api'] = False
if 'use_AIHorde_api' not in prefs: prefs['use_AIHorde_api'] = False
if 'AIHorde_model' not in prefs: prefs['AIHorde_model'] = 'stable_diffusion'
if 'AIHorde_sampler' not in prefs: prefs['AIHorde_sampler'] = 'k_euler_a'
if 'AIHorde_post_processing' not in prefs: prefs['AIHorde_post_processing'] = "None"
if 'enable_torch_compile' not in prefs: prefs['enable_torch_compile'] = False
if 'enable_tome' not in prefs: prefs['enable_tome'] = False
if 'tome_ratio' not in prefs: prefs['tome_ratio'] = 0.5
if 'negatives' not in prefs: prefs['negatives'] = ['Blurry']
if 'custom_negatives' not in prefs: prefs['custom_negatives'] = ""
if 'prompt_style' not in prefs: prefs['prompt_style'] = "cinematic-default"
if 'prompt_styler' not in prefs: prefs['prompt_styler'] = ""
if bool(prefs['init_image']):
    if not os.path.isfile(prefs['init_image']): prefs['init_image'] = ""
if bool(prefs['mask_image']):
    if not os.path.isfile(prefs['mask_image']): prefs['mask_image'] = ""
    
def initState(page):
    global status, current_tab
    if os.path.isdir(os.path.join(root_dir, 'Real-ESRGAN')):
      status['installed_ESRGAN'] = True
    page.load_prompts()

    # TODO: Try to load from assets folder
    page.snd_alert = Audio(src=os.path.join(assets, "snd-alert.mp3"), autoplay=False)
    page.snd_delete = Audio(src=os.path.join(assets, "snd-delete.mp3"), autoplay=False)
    page.snd_error = Audio(src=os.path.join(assets, "snd-error.mp3"), autoplay=False)
    page.snd_done = Audio(src=os.path.join(assets, "snd-done.mp3"), autoplay=False)
    page.snd_drop = Audio(src=os.path.join(assets, "snd-drop.mp3"), autoplay=False)
    #page.snd_notification = Audio(src="https://github.com/Skquark/AI-Friends/blob/main/assets/snd-notification.mp3?raw=true", autoplay=False)
    page.overlay.append(page.snd_alert)
    page.overlay.append(page.snd_delete)
    page.overlay.append(page.snd_error)
    page.overlay.append(page.snd_done)
    page.overlay.append(page.snd_drop)
    #page.overlay.append(page.snd_notification)
    #print("Running Init State")
    import importlib
    if '_PYIBoot_SPLASH' in os.environ and importlib.util.find_spec("pyi_splash"):
      import pyi_splash
      pyi_splash.update_text('Ready to get creative...')
      pyi_splash.close()
      #log.info('Splash screen closed.')
    if prefs['scheduler_mode'] != "DDIM":
      for eta in page.etas:
        if isinstance(eta, SliderRow):
          eta.show = False
        else:
          eta.visible = False
          eta.update()
    if prefs['start_in_installation'] and current_tab == 0:
      page.tabs.selected_index = 1
      page.tabs.update()
      page.show_install_fab(True)
      page.update()
      current_tab = 1

def buildSettings(page):
  global prefs, status
  def open_url(e):
    page.launch_url(e.data)
  def save_settings(e):
    save_settings_file(e.page)
    page.snack_bar = SnackBar(content=Text(f"Saving all settings to {saved_settings_json.rpartition(slash)[2]}"))
    page.snack_bar.open = True
    page.tabs.selected_index = 1
    page.tabs.update()
    page.update()
  def changed(e, pref=None):
      if pref is not None:
        prefs[pref] = e.control.value
      has_changed = True
      page.update()
      status['changed_settings'] = True
  def change_theme_mode(e):
    prefs['theme_mode'] = e.control.value
    if prefs['theme_mode'].lower() == "dark":
      page.dark_theme = Theme(color_scheme_seed=get_color(prefs['theme_color'].lower()))
    else:
      page.theme = theme.Theme(color_scheme_seed=get_color(prefs['theme_color'].lower()))
    page.theme_mode = prefs['theme_mode'].lower()
    page.update()
    status['changed_settings'] = True
  def change_theme_color(e):
    prefs['theme_color'] = e.control.value
    if prefs['theme_mode'].lower() == "dark":
      page.dark_theme = Theme(color_scheme_seed=get_color(prefs['theme_color'].lower()))
    else:
      page.theme = theme.Theme(color_scheme_seed=get_color(prefs['theme_color'].lower()))
    page.update()
    status['changed_settings'] = True
  def toggle_nsfw(e):
    #TODO: Add Popup alert with disclaimer, age verification and I Accept the Terms
    global safety
    retry_attempts.width = 0 if e.control.value else None
    retry_attempts.update()
    changed(e, 'disable_nsfw_filter')
    safety = {'safety_checker':None, 'requires_safety_checker':False, 'feature_extractor':None} if prefs['disable_nsfw_filter'] else {}
  def default_cache_dir(e):
    default_dir = prefs['image_output'].strip()
    if default_dir.endswith(slash):
      default_dir = default_dir[:-1]
    default_dir = default_dir.rpartition(slash)[0]
    default_dir = os.path.join(default_dir, 'models')
    prefs['cache_dir'] = default_dir
    optional_cache_dir.value = default_dir
    optional_cache_dir.update()
  def folder_picker_result(e):
    folder = folder_picker.result
    if folder != None and folder.path != None:
      image_output.value = folder.path
      image_output.update()
  folder_picker = FilePicker(on_result=folder_picker_result)
  page.overlay.append(folder_picker)
  def pick_output_dir(e):
    if not is_Colab:
      folder_picker.get_directory_path(dialog_title="Pick Directory to Save Outputs")
  image_output = TextField(label="Image Output Path", value=prefs['image_output'], on_change=lambda e:changed(e, 'image_output'), col={"md":12, "lg":6}, suffix=IconButton(icon=icons.FOLDER_OUTLINED, on_click=pick_output_dir))
  optional_cache_dir = TextField(label="Optional Cache Directory (saves large models to drive)", hint_text="(button on right inserts recommended folder)", value=prefs['cache_dir'], on_change=lambda e:changed(e, 'cache_dir'), suffix=IconButton(icon=icons.ARCHIVE, tooltip="Insert recommended models cache path", on_click=default_cache_dir), col={"md":12, "lg":6})
  file_prefix = TextField(label="Filename Prefix",  value=prefs['file_prefix'], width=150, height=60, on_change=lambda e:changed(e, 'file_prefix'))
  file_suffix_seed = Checkbox(label="Filename Suffix Seed   ", tooltip="Appends -seed# to the end of the image name", value=prefs['file_suffix_seed'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e, 'file_suffix_seed'))
  file_allowSpace = Checkbox(label="Filename Allow Space", tooltip="Otherwise will replace spaces with _ underscores", value=prefs['file_allowSpace'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e, 'file_allowSpace'))
  file_max_length = TextField(label="Filename Max Length", tooltip="How long can the name taken from prompt text be? Max 250", value=prefs['file_max_length'], keyboard_type=KeyboardType.NUMBER, width=150, height=60, on_change=lambda e:changed(e, 'file_max_length'))
  file_datetime = Checkbox(label="Filename DateTime instead of Prompt Text", tooltip="Save File with Date-Time Stamp to protect your prompt.", value=prefs['file_datetime'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e, 'file_datetime'))
  save_image_metadata = Checkbox(label="Save Image Metadata in png", tooltip="Embeds your Artist Name & Copyright in the file's EXIF", value=prefs['save_image_metadata'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e, 'save_image_metadata'))
  meta_ArtistName = TextField(label="Artist Name Metadata", value=prefs['meta_ArtistName'], keyboard_type=KeyboardType.NAME, on_change=lambda e:changed(e, 'meta_ArtistName'))
  meta_Copyright = TextField(label="Copyright Metadata", value=prefs['meta_Copyright'], keyboard_type=KeyboardType.NAME, on_change=lambda e:changed(e, 'meta_Copyright'))
  save_config_in_metadata = Checkbox(label="Save Config in Metadata    ", tooltip="Embeds all prompt parameters in the file's EXIF to recreate", value=prefs['save_config_in_metadata'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e, 'save_config_in_metadata'))
  save_config_json = Checkbox(label="Save Config JSON files", tooltip="Creates a json text file with all prompt parameters with each image", value=prefs['save_config_json'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e, 'save_config_json'))
  theme_mode = Dropdown(label="Theme Mode", width=200, options=[dropdown.Option("Dark"), dropdown.Option("Light")], value=prefs['theme_mode'], on_change=change_theme_mode)
  theme_color = Dropdown(label="Accent Color", width=200, options=[dropdown.Option("Green"), dropdown.Option("Blue"), dropdown.Option("Red"), dropdown.Option("Indigo"), dropdown.Option("Purple"), dropdown.Option("Orange"), dropdown.Option("Amber"), dropdown.Option("Brown"), dropdown.Option("Teal"), dropdown.Option("Yellow")], value=prefs['theme_color'], on_change=change_theme_color)
  enable_sounds = Checkbox(label="Enable UI Sound Effects    ", tooltip="Turn on for audible errors, deletes and generation done notifications", value=prefs['enable_sounds'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e, 'enable_sounds'))
  start_in_installation = Checkbox(label="Start in Installation Page", tooltip="When launching app, switch to Installer tab. Saves time..", value=prefs['start_in_installation'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e, 'start_in_installation'))
  disable_nsfw_filter = Checkbox(label="Disable NSFW Filters for Uncensored Images", value=prefs['disable_nsfw_filter'], tooltip="If you're over 18 & promise not to abuse, allow Not Safe For Work. Otherwise, will filter mature content...", fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=toggle_nsfw)
  retry_attempts = Container(NumberPicker(label="Retry Attempts if Not Safe", min=0, max=8, value=prefs['retry_attempts'], on_change=lambda e:changed(e, 'retry_attempts')), padding=padding.only(left=20), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
  retry_attempts.width = 0 if prefs['disable_nsfw_filter'] else None
  api_instructions = Container(height=170, content=Markdown("Get **HuggingFace API key** from https://huggingface.co/settings/tokens, preferably the WRITE access key.\n\nGet **Stability-API key** from https://beta.dreamstudio.ai/membership?tab=apiKeys then API key\n\nGet **OpenAI GPT-3 API key** from https://beta.openai.com, user menu, View API Keys\n\nGet **TextSynth GPT-J key** from https://TextSynth.com, login, Setup\n\nGet **Replicate API Token** from https://replicate.com/account, for Material Diffusion\n\nGet **AIHorde API Token** from https://aihorde.net/register, for Stable Horde cloud", extension_set="gitHubWeb", on_tap_link=open_url))
  HuggingFace_api = TextField(label="HuggingFace API Key", value=prefs['HuggingFace_api_key'], password=True, can_reveal_password=True, on_change=lambda e:changed(e, 'HuggingFace_api_key'))
  Stability_api = TextField(label="Stability.ai API Key (optional)", value=prefs['Stability_api_key'], password=True, can_reveal_password=True, on_change=lambda e:changed(e, 'Stability_api_key'))
  OpenAI_api = TextField(label="OpenAI API Key (optional)", value=prefs['OpenAI_api_key'], password=True, can_reveal_password=True, on_change=lambda e:changed(e, 'OpenAI_api_key'))
  TextSynth_api = TextField(label="TextSynth API Key (optional)", value=prefs['TextSynth_api_key'], password=True, can_reveal_password=True, on_change=lambda e:changed(e, 'TextSynth_api_key'))
  Replicate_api = TextField(label="Replicate API Key (optional)", value=prefs['Replicate_api_key'], password=True, can_reveal_password=True, on_change=lambda e:changed(e, 'Replicate_api_key'))
  AIHorde_api = TextField(label="AIHorde API Key (optional)", value=prefs['AIHorde_api_key'], password=True, can_reveal_password=True, on_change=lambda e:changed(e, 'AIHorde_api_key'))
  #save_button = ElevatedButton(content=Text(value="💾  Save Settings", size=20), on_click=save_settings, style=b_style())

  c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("⚙️   Stable Diffusion Deluxe Settings & Preferences"),
        ResponsiveRow([image_output, optional_cache_dir], run_spacing=2),
        #VerticalDivider(thickness=2),
        Row([file_prefix, file_suffix_seed]) if (page.width if page.web else page.window_width) > 500 else Column([file_prefix, file_suffix_seed]),
        Row([file_max_length, file_allowSpace]),
        file_datetime,
        Row([disable_nsfw_filter, retry_attempts]),
        #VerticalDivider(thickness=2, width=1),
        save_image_metadata,
        Row([meta_ArtistName, meta_Copyright]) if (page.width if page.web else page.window_width) > 712 else Column([meta_ArtistName, meta_Copyright]),
        Row([save_config_in_metadata, save_config_json]),
        Row([theme_mode, theme_color]),
        Row([enable_sounds, start_in_installation]),
        HuggingFace_api,
        Stability_api,
        OpenAI_api,
        TextSynth_api,
        Replicate_api,
        AIHorde_api,
        api_instructions,
        #save_button,
        Container(content=None, height=32),
      ],
  ))], scroll=ScrollMode.AUTO,)
  return c

def run_process(cmd_str, cwd=None, realtime=True, page=None, close_at_end=False, show=False, print=False): # show when debugging
  cmd_list = cmd_str if type(cmd_str) is list else cmd_str.split()
  if realtime:
    if cwd is None:
      process = subprocess.Popen(cmd_str, shell = True, env=env, bufsize = 1, stdout=subprocess.PIPE, stderr = subprocess.STDOUT, encoding='utf-8', errors = 'replace' )
    else:
      process = subprocess.Popen(cmd_str, shell = True, cwd=cwd, env=env, bufsize = 1, stdout=subprocess.PIPE, stderr = subprocess.STDOUT, encoding='utf-8', errors = 'replace' )
    while True:
      realtime_output = process.stdout.readline()
      if realtime_output == '' and process.poll() is not None:
        break
      if realtime_output and show and page != None:
        #print(realtime_output.strip(), flush=False)
        page.banner.content.controls.append(Text(realtime_output.strip()))
        page.update()
        sys.stdout.flush()
      if print:
        print(realtime_output.strip())
    if close_at_end:
      page.banner.open = False
      page.update()
  else:
    if cwd is None:
      output = subprocess.run(cmd_list, stdout=subprocess.PIPE, env=env).stdout.decode('utf-8')
    else:
      output = subprocess.run(cmd_list, stdout=subprocess.PIPE, env=env, cwd=cwd).stdout.decode('utf-8')
    if print:
      print(output)
    return output

def close_alert_dlg(e):
      e.page.alert_dlg.open = False
      e.page.update()
def alert_msg(page, msg, content=None, okay="", sound=True, width=None, wide=False):
      try:
        if page.alert_dlg.open == True: return
      except Exception: pass
      try:
        if prefs['enable_sounds'] and sound: page.snd_error.play()
      except Exception:
        msg += " May have to restart runtime."
        pass
      okay_button = ElevatedButton(content=Text("👌  OKAY " if okay == "" else okay, size=18), on_click=close_alert_dlg)
      if content == None: content = Container(content=None)
      page.alert_dlg = AlertDialog(title=Text(msg), content=Column([content], scroll=ScrollMode.AUTO), actions=[okay_button], actions_alignment=MainAxisAlignment.END)#, width=None if not wide else (page.width if page.web else page.window_width) - 200)
      page.dialog = page.alert_dlg
      page.alert_dlg.open = True
      try:
        page.update()
      except Exception: pass

def save_installers(controls):
  for c in controls:
    if isinstance(c, Switch):
      #print(f"elif c.value == '{c.label}': prefs[''] = c.value")
      if c.value == 'Install HuggingFace Diffusers Pipeline': prefs['install_diffusers'] = c.value
      elif c.value == 'Install Stability-API DreamStudio Pipeline': prefs['install_Stability_api'] = c.value
      elif c.value == 'Install Real-ESRGAN AI Upscaler': prefs['install_ESRGAN'] = c.value
      elif c.value == 'Install OpenAI GPT-3 Text Engine': prefs['install_OpenAI'] = c.value
      elif c.value == 'Install TextSynth GPT-J Text Engine': prefs['install_TextSynth'] = c.value
    elif isinstance(c, Container):
      '''try:
        for i in c.content.controls:
          if isinstance(i, Switch):
            print(f"elif i.value == '{c.label}': prefs[''] = i.value")
      except: continue'''
def refresh_installers(controls):
  for c in controls:
    if isinstance(c, Switch):
      c.update()

def buildInstallers(page):
  global prefs, status, model_path
  def changed(e, pref=None):
      if pref is not None:
        prefs[pref] = e.control.value
      #page.update()
      status['changed_installers'] = True
  def changed_status(e, stat=None):
      if stat is not None:
        status[stat] = e.control.value
  def toggle_diffusers(e):
      prefs['install_diffusers'] = e.control.value
      diffusers_settings.height=None if prefs['install_diffusers'] else 0
      diffusers_settings.update()
      status['changed_installers'] = True
  install_diffusers = Switcher(label="Install HuggingFace Diffusers Pipeline", value=prefs['install_diffusers'], disabled=status['installed_diffusers'], on_change=toggle_diffusers, tooltip="Required Libraries for most Image Generation functionality")
  def change_scheduler(e):
      show = e.control.value == "DDIM"
      update = prefs['scheduler_mode'] == "DDIM" or show
      changed(e, 'scheduler_mode')
      if update:
          for eta in page.etas:
            if isinstance(eta, SliderRow):
              eta.show = show
            else:
              eta.visible = show
              eta.update()
  scheduler_mode = Dropdown(label="Scheduler/Sampler Mode", hint_text="They're very similar, with minor differences in the generated noise", width=220,
            options=[
                dropdown.Option("DDIM"),
                dropdown.Option("LMS Discrete"),
                dropdown.Option("PNDM"),
                dropdown.Option("IPMDM"),
                dropdown.Option("DPM Solver"),
                dropdown.Option("DPM Solver++"),
                dropdown.Option("DPM Solver Inverse"),
                dropdown.Option("SDE-DPM Solver++"),
                #dropdown.Option("DPM Stochastic"),
                dropdown.Option("K-Euler Discrete"),
                dropdown.Option("K-Euler Ancestral"),
                dropdown.Option("DEIS Multistep"),
                dropdown.Option("UniPC Multistep"),
                dropdown.Option("Heun Discrete"),
                dropdown.Option("Karras Heun Discrete"),
                dropdown.Option("K-DPM2 Ancestral"),
                dropdown.Option("K-DPM2 Discrete"),
                dropdown.Option("Karras-LMS"),
            ], value=prefs['scheduler_mode'], autofocus=False, on_change=change_scheduler,
        )
  def changed_model_ckpt(e):
      changed(e, 'model_ckpt')
      model = get_model(e.control.value)
      model_card.value = f"  [**Model Card**](https://huggingface.co/{model['path']})"
      model_card.update()
      if e.control.value.startswith("Stable"):
        custom_area.content = model_card
      elif e.control.value == "Community Finetuned Model":
        custom_area.content = Row([finetuned_model, model_card])
      elif e.control.value == "DreamBooth Library Model":
        custom_area.content = Row([dreambooth_library, model_card])
      elif e.control.value == "Custom Model Path":
        custom_area.content = Row([custom_model, model_card])
      custom_area.update()
  def changed_finetuned_model(e):
      changed(e, 'finetuned_model')
      model = get_finetuned_model(e.control.value)
      model_card.value = f"  [**Model Card**](https://huggingface.co/{model['path']})"
      model_card.update()
  def changed_dreambooth_library(e):
      changed(e, 'dreambooth_model')
      model = get_dreambooth_model(e.control.value)
      model_card.value = f"  [**Model Card**](https://huggingface.co/{model['path']})"
      model_card.update()
  def changed_custom_model(e):
      changed(e, 'custom_model')
      model = {'name': 'Custom Model', 'path': e.control.value, 'prefix': ''}
      model_card.value = f"  [**Model Card**](https://huggingface.co/{model['path']})"
      model_card.update()
  def toggle_safe(e):
      changed(e, 'install_safe')
      safety_config.visible = e.control.value
      safety_config.update()
  def toggle_SDXL(e):
      changed(e, 'install_SDXL')
      #TODO: Reenable when Compel works right
      SDXL_params.height = None if e.control.value else 0
      SDXL_params.update()
  model = get_model(prefs['model_ckpt'])
  model_path = model['path']
  model_ckpt = Container(Dropdown(label="Model Checkpoint", width=262, options=[
      dropdown.Option("Stable Diffusion v2.1 x768"), dropdown.Option("Stable Diffusion v2.1 x512"),
      dropdown.Option("Stable Diffusion v2.0 x768"), dropdown.Option("Stable Diffusion v2.0 x512"), dropdown.Option("Stable Diffusion v1.5"), dropdown.Option("Stable Diffusion v1.4"),
      dropdown.Option("Community Finetuned Model"), dropdown.Option("DreamBooth Library Model"), dropdown.Option("Custom Model Path")], value=prefs['model_ckpt'], tooltip="Make sure you accepted the HuggingFace Model Cards first", autofocus=False, on_change=changed_model_ckpt), col={'xs':9, 'lg':4}, width=262)
  finetuned_model = Dropdown(label="Finetuned Model", tooltip="Make sure you accepted the HuggingFace Model Cards first", width=370, options=[], value=prefs['finetuned_model'], autofocus=False, on_change=changed_finetuned_model, col={'xs':11, 'lg':6})
  model_card = Markdown(f"  [**Model Card**](https://huggingface.co/{model['path']})", on_tap_link=lambda e: e.page.launch_url(e.data))
  for mod in finetuned_models:
      finetuned_model.options.append(dropdown.Option(mod["name"]))
  page.finetuned_model = finetuned_model
  dreambooth_library = Dropdown(label="DreamBooth Library", hint_text="", width=370, options=[], value=prefs['dreambooth_model'], autofocus=False, on_change=changed_dreambooth_library, col={'xs':10, 'md':4})
  for db in dreambooth_models:
      dreambooth_library.options.append(dropdown.Option(db["name"]))
  custom_model = TextField(label="Custom Model Path", value=prefs['custom_model'], width=370, on_change=changed_custom_model)
  page.custom_model = custom_model
  #custom_area = AnimatedSwitcher(model_card, transition="scale", duration=500, reverse_duration=200, switch_in_curve=AnimationCurve.EASE_OUT, switch_out_curve="easeIn")
  custom_area = Container(model_card, col={'xs':12, 'lg':6})
  if prefs['model_ckpt'].startswith("Stable"):
      custom_area.content = model_card
  elif prefs['model_ckpt'] == "Community Finetuned Model":
      custom_area.content = Row([finetuned_model, model_card], col={'xs':9, 'lg':4})
  elif prefs['model_ckpt'] == "DreamBooth Library Model":
      custom_area.content = Row([dreambooth_library, model_card], col={'xs':9, 'lg':4})
  elif prefs['model_ckpt'] == "Custom Model Path":
      custom_area.content = Row([custom_model, model_card], col={'xs':9, 'lg':4})
  model_row = ResponsiveRow([model_ckpt, custom_area], run_spacing=8)
  memory_optimization = Dropdown(label="Enable Memory Optimization", width=290, options=[dropdown.Option("None"), dropdown.Option("Attention Slicing")], value=prefs['memory_optimization'], on_change=lambda e:changed(e, 'memory_optimization'))
  if version.parse(torch.__version__) < version.parse("2.0.0"):
      memory_optimization.options.append(dropdown.Option("Xformers Mem Efficient Attention"))
  higher_vram_mode = Checkbox(label="Higher VRAM Mode", tooltip="Adds a bit more precision & uses much more GPU memory. Not recommended unless you have >16GB VRAM.", value=prefs['higher_vram_mode'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e, 'higher_vram_mode'))
  sequential_cpu_offload = Checkbox(label="Enable Sequential CPU Offload", tooltip="Offloads all models to CPU using accelerate, significantly reducing memory usage.", value=prefs['sequential_cpu_offload'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e, 'sequential_cpu_offload'))
  enable_attention_slicing = Checkbox(label="Enable Attention Slicing", tooltip="Saves VRAM while creating images so you can go bigger without running out of mem.", value=prefs['enable_attention_slicing'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e, 'enable_attention_slicing'))
  enable_vae_tiling = Checkbox(label="Enable VAE Tiling", tooltip="The VAE will split the input tensor into tiles to compute decoding and encoding in several steps. This is useful to save a large amount of memory and to allow the processing of larger images.", value=prefs['vae_tiling'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e, 'vae_tiling'))
  enable_vae_slicing = Checkbox(label="Enable VAE Slicing", tooltip="Sliced VAE decode latents for larger batches of images with limited VRAM. Splits the input tensor in slices to compute decoding in several steps", value=prefs['vae_slicing'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e, 'vae_slicing'))
  enable_tome = Checkbox(label="Enable Token Merging", tooltip="ToMe optimizes the Pipelines to create images faster, at the expense of some quality. Works by merging the redundant tokens / patches progressively in the forward pass of a Transformer-based network.", value=prefs['enable_tome'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e, 'enable_tome'))
  enable_torch_compile = Checkbox(label="Enable Torch Compiling", tooltip="Speeds up Torch 2.0 Processing, but takes a bit longer to initialize.", value=prefs['enable_torch_compile'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e, 'enable_torch_compile'))
  #install_megapipe = Switcher(label="Install Stable Diffusion txt2image, img2img & Inpaint Mega Pipeline", value=prefs['install_megapipe'], disabled=status['installed_megapipe'], on_change=lambda e:changed(e, 'install_megapipe'))
  install_text2img = Switcher(label="Install Stable Diffusion text2image, image2image & Inpaint Pipeline (/w Long Prompt Weighting)", value=prefs['install_text2img'], disabled=status['installed_txt2img'], on_change=lambda e:changed(e, 'install_text2img'), tooltip="The best general purpose component. Create images with long prompts, weights & models")
  SDXL_model_card = Markdown(f"  [**Accept Model Card**](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)", on_tap_link=lambda e: e.page.launch_url(e.data))
  install_SDXL = Switcher(label="Install Stable Diffusion XL 1.0 text2image, image2image & Inpaint Pipeline", value=prefs['install_SDXL'], disabled=status['installed_SDXL'], on_change=toggle_SDXL, tooltip="Latest SDXL v1.0 trained on 1080p images.")
  SDXL_compel = Switcher(label="Use Compel Long Prompt Weighting Embeds with SDXL", tooltip="Re-weight different parts of a prompt string like positive+++ AND (bad negative)-- or (subject)1.3 syntax.", value=prefs['SDXL_compel'], on_change=lambda e:changed(e,'SDXL_compel'))
  SDXL_params = Container(Column([SDXL_compel]), padding=padding.only(top=5, left=20), height=None if prefs['install_SDXL'] else 0, animate_size=animation.Animation(1000, AnimationCurve.EASE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
  #SDXL_params.visible = False
  install_img2img = Switcher(label="Install Stable Diffusion Specialized Inpainting Model for image2image & Inpaint Pipeline", value=prefs['install_img2img'], disabled=status['installed_img2img'], on_change=lambda e:changed(e, 'install_img2img'), tooltip="Gets more coherant results modifying Inpaint init & mask images")
  #install_repaint = Tooltip(message="Without using prompts, redraw masked areas to remove and repaint.", content=Switcher(label="Install Stable Diffusion RePaint Pipeline", value=prefs['install_repaint'], disabled=status['installed_repaint'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e, 'install_repaint')))
  install_interpolation = Switcher(label="Install Stable Diffusion Prompt Walk Interpolation Pipeline", value=prefs['install_interpolation'], disabled=status['installed_interpolation'], on_change=lambda e:changed(e, 'install_interpolation'), tooltip="Create multiple tween images between prompts latent space. Almost animation.")
  #install_dreamfusion = Tooltip(message="Generate interesting mesh .obj, texture and preview video from a prompt.", content=Switcher(label="Install Stable Diffusion DreamFusion 3D Pipeline", value=prefs['install_dreamfusion'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, disabled=status['installed_dreamfusion'], on_change=lambda e:changed(e, 'install_dreamfusion')))
  install_alt_diffusion = Switcher(label="Install AltDiffusion text2image & image2image Multilingual Pipeline", value=prefs['install_alt_diffusion'], disabled=status['installed_alt_diffusion'], on_change=lambda e:changed(e, 'install_alt_diffusion'), tooltip="Multilingual Stable Diffusion supporting English, Chinese, Spanish, French, Russian, Japanese, Korean, Arabic and Italian.")
  install_attend_and_excite = Switcher(label="Install Attend and Excite text2image Pipeline", value=prefs['install_attend_and_excite'], disabled=status['installed_attend_and_excite'], on_change=lambda e:changed(e, 'install_attend_and_excite'), tooltip="Provides textual Attention-Based Semantic Guidance control over the image generation.")
  install_SAG = Switcher(label="Install Self-Attention Guidance (SAG) text2image Pipeline", value=prefs['install_SAG'], disabled=status['installed_SAG'], on_change=lambda e:changed(e, 'install_SAG'), tooltip="Intelligent guidance that can plugged into any diffusion model using their self-attention map, improving sample quality.")
  install_panorama = Switcher(label="Install MultiDiffusion Panorama text2image Pipeline", value=prefs['install_panorama'], disabled=status['installed_panorama'], on_change=lambda e:changed(e, 'install_panorama'), tooltip="Generate panorama-like wide images, Fusing Diffusion Paths for Controlled Image Generation")
  install_imagic = Switcher(label="Install Stable Diffusion iMagic image2image Pipeline", value=prefs['install_imagic'], disabled=status['installed_imagic'], on_change=lambda e:changed(e, 'install_imagic'), tooltip="Edit your image according to the prompted instructions like magic.")
  install_depth2img = Switcher(label="Install Stable Diffusion Depth2Image Pipeline", value=prefs['install_depth2img'], disabled=status['installed_depth2img'], on_change=lambda e:changed(e, 'install_depth2img'), tooltip="Uses Depth-map of init image for text-guided image to image generation.")
  install_composable = Switcher(label="Install Stable Diffusion Composable text2image Pipeline", value=prefs['install_composable'], disabled=status['installed_composable'], on_change=lambda e:changed(e, 'install_composable'), tooltip="Craft your prompts with | precise | weights AND composed together components | with AND NOT negatives.")
  install_safe = Switcher(label="Install Stable Diffusion Safe text2image Pipeline", value=prefs['install_safe'], disabled=status['installed_safe'], on_change=toggle_safe, tooltip="Use a content quality tuned safety model, providing levels of NSFW protection.")
  safety_config = Container(Dropdown(label="Model Safety Level", width=350, options=[dropdown.Option("Weak"), dropdown.Option("Medium"), dropdown.Option("Strong"), dropdown.Option("Max")], value=prefs['safety_config'], on_change=lambda e:changed(e, 'safety_config')), padding=padding.only(left=32))
  safety_config.visible = prefs['install_safe']
  install_versatile = Switcher(label="Install Versatile Diffusion text2image, Dual Guided & Image Variation Pipeline", value=prefs['install_versatile'], disabled=status['installed_versatile'], on_change=lambda e:changed(e, 'install_versatile'), tooltip="Multi-flow model that provides both image and text data streams and conditioned on both text and image.")

  def toggle_clip(e):
      prefs['install_CLIP_guided'] = e.control.value
      status['changed_installers'] = True
      clip_settings.height=None if prefs['install_CLIP_guided'] else 0
      clip_settings.update()
  install_CLIP_guided = Switcher(label="Install Stable Diffusion CLIP-Guided Pipeline", value=prefs['install_CLIP_guided'], disabled=status['installed_clip'], on_change=toggle_clip, tooltip="Uses alternative LAION & OpenAI ViT diffusion. Takes more VRAM, so may need to make images smaller")
  clip_model_id = Dropdown(label="CLIP Model ID", width=350,
            options=[
                dropdown.Option("laion/CLIP-ViT-B-32-laion2B-s34B-b79K"),
                dropdown.Option("laion/CLIP-ViT-L-14-laion2B-s32B-b82K"),
                dropdown.Option("laion/CLIP-ViT-H-14-laion2B-s32B-b79K"),
                dropdown.Option("laion/CLIP-ViT-g-14-laion2B-s12B-b42K"),
                dropdown.Option("openai/clip-vit-base-patch32"),
                dropdown.Option("openai/clip-vit-base-patch16"),
                dropdown.Option("openai/clip-vit-large-patch14"),
            ], value=prefs['clip_model_id'], autofocus=False, on_change=lambda e:changed(e, 'clip_model_id'),
        )
  clip_settings = Container(animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE, padding=padding.only(left=32, top=4), content=Column([clip_model_id]))

  def toggle_conceptualizer(e):
      changed(e, 'install_conceptualizer')
      conceptualizer_settings.height = None if e.control.value else 0
      conceptualizer_settings.update()
  def change_concepts_model(e):
      nonlocal concept
      changed(e, 'concepts_model')
      concept = get_concept(e.control.value)
      concepts_info.value = f"To use the concept, include keyword token **<{concept['token']}>** in your Prompts. Info at [https://huggingface.co/sd-concepts-library/{concept['name']}](https://huggingface.co/sd-concepts-library/{concept['name']})"
      concepts_info.update()
  def open_url(e):
      page.launch_url(e.data)
  def copy_token(e):
      nonlocal concept
      page.set_clipboard(f"<{concept['token']}>")
      page.snack_bar = SnackBar(content=Text(f"📋  Token <{concept['token']}> copied to clipboard... Paste as word in your Prompt Text."))
      page.snack_bar.open = True
      page.update()
  install_conceptualizer = Switcher(label="Install Stable Diffusion Textual-Inversion Conceptualizer Pipeline", value=prefs['install_conceptualizer'], on_change=toggle_conceptualizer, tooltip="Loads specially trained concept models to include in prompt with token")
  concept = get_concept(prefs['concepts_model'])
  concepts_model = Dropdown(label="SD-Concepts Library Model", hint_text="Specially trained community models made with Textual-Inversion", width=451, options=[], value=prefs['concepts_model'], on_change=change_concepts_model)
  copy_token_btn = IconButton(icon=icons.CONTENT_COPY, tooltip="Copy Token to Clipboard", on_click=copy_token)
  concepts_row = Row([concepts_model, copy_token_btn])
  concepts_info = Markdown(f"To use the concept, include keyword token **<{concept['token']}>** in your Prompts. Info at [https://huggingface.co/sd-concepts-library/{concept['name']}](https://huggingface.co/sd-concepts-library/{concept['name']})", selectable=True, on_tap_link=open_url)
  conceptualizer_settings = Container(animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE, padding=padding.only(left=32, top=5), content=Column([concepts_row, concepts_info]))
  conceptualizer_settings.height = None if prefs['install_conceptualizer'] else 0
  for c in concepts: concepts_model.options.append(dropdown.Option(c['name']))
  install_upscale = Switcher(label="Install Stable Diffusion v2 Upscale 4X Pipeline", value=prefs['install_upscale'], disabled=status['installed_upscale'], on_change=lambda e:changed(e, 'install_upscale'), tooltip="Allows you to enlarge images with prompts. Note: Will run out of mem for images larger than 512px, start small.")

  diffusers_settings = Container(animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE, content=
                                 Column([Container(Column([Container(None, height=3), model_row, Container(content=None, height=4), scheduler_mode,
                                 Row([memory_optimization,
                                 higher_vram_mode]),
                                 #  enable_vae_slicing
                                 #enable_attention_slicing,
                                 #Row([sequential_cpu_offload, enable_vae_tiling]),
                                 Row([enable_tome, enable_torch_compile]),
                                 ]), padding=padding.only(left=32, top=4)),
                                         install_text2img, Row([install_SDXL, SDXL_model_card]), SDXL_params, install_img2img, #install_repaint, #install_megapipe, install_alt_diffusion,
                                         install_interpolation, install_CLIP_guided, clip_settings, install_conceptualizer, conceptualizer_settings, install_safe, safety_config,
                                         install_versatile, install_SAG, install_attend_and_excite, install_panorama, install_imagic, install_depth2img, install_composable, install_upscale]))
  def toggle_stability(e):
      prefs['install_Stability_api'] = e.control.value
      has_changed=True
      stability_settings.height=None if prefs['install_Stability_api'] else 0
      stability_settings.update()
      page.update()
  install_Stability_api = Switcher(label="Install Stability-API DreamStudio Pipeline", value=prefs['install_Stability_api'], disabled=status['installed_stability'], on_change=toggle_stability, tooltip="Use DreamStudio.com servers without your GPU to create images on CPU.")
  use_Stability_api = Checkbox(label="Use Stability-ai API by default", tooltip="Instead of using Diffusers, generate images in their cloud. Can toggle to compare batches..", value=prefs['use_Stability_api'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e, 'use_Stability_api'))
  model_checkpoint = Dropdown(label="Model Checkpoint", hint_text="", width=350, options=[dropdown.Option("stable-diffusion-xl-1024-v1-0"), dropdown.Option("stable-diffusion-xl-1024-v0-9"), dropdown.Option("stable-diffusion-xl-beta-v2-2-2"), dropdown.Option("stable-diffusion-768-v2-1"), dropdown.Option("stable-diffusion-512-v2-1"), dropdown.Option("stable-diffusion-768-v2-0"), dropdown.Option("stable-diffusion-512-v2-0"), dropdown.Option("stable-diffusion-v1-5"), dropdown.Option("stable-diffusion-v1"), dropdown.Option("stable-inpainting-512-v2-0"), dropdown.Option("stable-inpainting-v1-0")], value=prefs['model_checkpoint'], autofocus=False, on_change=lambda e:changed(e, 'model_checkpoint'))
  clip_guidance_preset = Dropdown(label="Clip Guidance Preset", width=350, options=[dropdown.Option("SIMPLE"), dropdown.Option("FAST_BLUE"), dropdown.Option("FAST_GREEN"), dropdown.Option("SLOW"), dropdown.Option("SLOWER"), dropdown.Option("SLOWEST"), dropdown.Option("NONE")], value=prefs['clip_guidance_preset'], autofocus=False, on_change=lambda e:changed(e, 'clip_guidance_preset'))
  #generation_sampler = Dropdown(label="Generation Sampler", hint_text="", width=350, options=[dropdown.Option("ddim"), dropdown.Option("plms"), dropdown.Option("k_euler"), dropdown.Option("k_euler_ancestral"), dropdown.Option("k_heun"), dropdown.Option("k_dpm_2"), dropdown.Option("k_dpm_2_ancestral"), dropdown.Option("k_lms")], value=prefs['generation_sampler'], autofocus=False, on_change=lambda e:changed(e, 'generation_sampler'))
  generation_sampler = Dropdown(label="Generation Sampler", hint_text="", width=350, options=[dropdown.Option("DDIM"), dropdown.Option("DDPM"), dropdown.Option("K_EULER"), dropdown.Option("K_EULER_ANCESTRAL"), dropdown.Option("K_HEUN"), dropdown.Option("K_DPMPP_2M"), dropdown.Option("K_DPM_2_ANCESTRAL"), dropdown.Option("K_LMS"), dropdown.Option("K_DPMPP_2S_ANCESTRAL"), dropdown.Option("K_DPM_2")], value=prefs['generation_sampler'], autofocus=False, on_change=lambda e:changed(e, 'generation_sampler'))
  #"K_EULER" "K_DPM_2" "K_LMS" "K_DPMPP_2S_ANCESTRAL" "K_DPMPP_2M" "DDIM" "DDPM" "K_EULER_ANCESTRAL" "K_HEUN" "K_DPM_2_ANCESTRAL"
  stability_settings = Container(animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE, padding=padding.only(left=32), content=Column([use_Stability_api, model_checkpoint, generation_sampler, clip_guidance_preset]))

  def toggle_AIHorde(e):
      prefs['install_AIHorde_api'] = e.control.value
      AIHorde_settings.height=None if prefs['install_AIHorde_api'] else 0
      AIHorde_settings.update()
      page.update()
  install_AIHorde = Switcher(label="Install AIHorde Crowdsorced Pipeline", value=prefs['install_AIHorde_api'], on_change=toggle_AIHorde, tooltip="Use AIHorde.net Crowdsourced cloud without your GPU to create images on CPU.")
  use_AIHorde = Checkbox(label="Use Stable Horde API by default", tooltip="Instead of using Diffusers, generate images in their cloud. Can toggle to compare batches..", value=prefs['use_AIHorde_api'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e, 'use_AIHorde_api'))
  AIHorde_model = Dropdown(label="Model Checkpoint", hint_text="", width=350, options=[dropdown.Option("3DKX"), dropdown.Option("Abyss OrangeMix"), dropdown.Option("AbyssOrangeMix-AfterDark"), dropdown.Option("ACertainThing"), dropdown.Option("AIO Pixel Art"), dropdown.Option("Analog Diffusion"), dropdown.Option("Anime Pencil Diffusion"), dropdown.Option("Anygen"), dropdown.Option("Anything Diffusion"), dropdown.Option("Anything v3"), dropdown.Option("App Icon Diffusion"), dropdown.Option("Arcane Diffusion"), dropdown.Option("Archer Diffusion"), dropdown.Option("Asim Simpsons"), dropdown.Option("A to Zovya RPG"), dropdown.Option("Balloon Art"), dropdown.Option("Borderlands"), dropdown.Option("BPModel"), dropdown.Option("BubblyDubbly"), dropdown.Option("Char"), dropdown.Option("CharHelper"), dropdown.Option("Cheese Daddys Landscape Mix"), dropdown.Option("ChilloutMix"), dropdown.Option("ChromaV5"), dropdown.Option("Classic Animation Diffusion"), dropdown.Option("Clazy"), dropdown.Option("Colorful"), dropdown.Option("Coloring Book"), dropdown.Option("Comic-Diffusion"), dropdown.Option("Concept Sheet"), dropdown.Option("Counterfeit"), dropdown.Option("Cyberpunk Anime Diffusion"), dropdown.Option("CyriousMix"), dropdown.Option("Dan Mumford Style"), dropdown.Option("Darkest Diffusion"), dropdown.Option("Dark Victorian Diffusion"), dropdown.Option("Deliberate"), dropdown.Option("DGSpitzer Art Diffusion"), dropdown.Option("Disco Elysium"), dropdown.Option("DnD Item"), dropdown.Option("Double Exposure Diffusion"), dropdown.Option("Dreamlike Diffusion"), dropdown.Option("Dreamlike Photoreal"), dropdown.Option("DreamLikeSamKuvshinov"), dropdown.Option("Dreamshaper"), dropdown.Option("DucHaiten"), dropdown.Option("DucHaiten Classic Anime"), dropdown.Option("Dungeons and Diffusion"), dropdown.Option("Dungeons n Waifus"), dropdown.Option("Eimis Anime Diffusion"), dropdown.Option("Elden Ring Diffusion"), dropdown.Option("Elldreth's Lucid Mix"), dropdown.Option("Elldreths Retro Mix"), dropdown.Option("Epic Diffusion"), dropdown.Option("Eternos"), dropdown.Option("Experience"), dropdown.Option("ExpMix Line"), dropdown.Option("FaeTastic"), dropdown.Option("Fantasy Card Diffusion"), dropdown.Option("FKing SciFi"), dropdown.Option("Funko Diffusion"), dropdown.Option("Furry Epoch"), dropdown.Option("Future Diffusion"), dropdown.Option("Ghibli Diffusion"), dropdown.Option("GorynichMix"), dropdown.Option("Grapefruit Hentai"), dropdown.Option("Graphic-Art"), dropdown.Option("GTA5 Artwork Diffusion"), dropdown.Option("GuoFeng"), dropdown.Option("Guohua Diffusion"), dropdown.Option("HASDX"), dropdown.Option("Hassanblend"), dropdown.Option("Healy's Anime Blend"), dropdown.Option("Hentai Diffusion"), dropdown.Option("HRL"), dropdown.Option("iCoMix"), dropdown.Option("Illuminati Diffusion"), dropdown.Option("Inkpunk Diffusion"), dropdown.Option("Jim Eidomode"), dropdown.Option("JWST Deep Space Diffusion"), dropdown.Option("Kenshi"), dropdown.Option("Knollingcase"), dropdown.Option("Korestyle"), dropdown.Option("kurzgesagt"), dropdown.Option("Laolei New Berry Protogen Mix"), dropdown.Option("Lawlas's yiff mix"), dropdown.Option("Liberty"), dropdown.Option("Marvel Diffusion"), dropdown.Option("Mega Merge Diffusion"), dropdown.Option("Microcasing"), dropdown.Option("Microchars"), dropdown.Option("Microcritters"), dropdown.Option("Microscopic"), dropdown.Option("Microworlds"), dropdown.Option("Midjourney Diffusion"), dropdown.Option("Midjourney PaintArt"), dropdown.Option("Min Illust Background"), dropdown.Option("ModernArt Diffusion"), dropdown.Option("mo-di-diffusion"), dropdown.Option("Moedel"), dropdown.Option("MoistMix"), dropdown.Option("Movie Diffusion"), dropdown.Option("NeverEnding Dream"), dropdown.Option("Nitro Diffusion"), dropdown.Option("Openniji"), dropdown.Option("OrbAI"), dropdown.Option("Papercutcraft"), dropdown.Option("Papercut Diffusion"), dropdown.Option("Pastel Mix"), dropdown.Option("Perfect World"), dropdown.Option("PFG"), dropdown.Option("pix2pix"), dropdown.Option("PIXHELL"), dropdown.Option("Poison"), dropdown.Option("Pokemon3D"), dropdown.Option("PortraitPlus"), dropdown.Option("PPP"), dropdown.Option("Pretty 2.5D"), dropdown.Option("PRMJ"), dropdown.Option("Project Unreal Engine 5"), dropdown.Option("ProtoGen"), dropdown.Option("Protogen Anime"), dropdown.Option("Protogen Infinity"), dropdown.Option("Pulp Vector Art"), dropdown.Option("PVC"), dropdown.Option("Rachel Walker Watercolors"), dropdown.Option("Rainbowpatch"), dropdown.Option("Ranma Diffusion"), dropdown.Option("RCNZ Dumb Monkey"), dropdown.Option("RCNZ Gorilla With A Brick"), dropdown.Option("RealBiter"), dropdown.Option("Realism Engine"), dropdown.Option("Realistic Vision"), dropdown.Option("Redshift Diffusion"), dropdown.Option("Rev Animated"), dropdown.Option("Robo-Diffusion"), dropdown.Option("Rodent Diffusion"), dropdown.Option("RPG"), dropdown.Option("Samdoesarts Ultmerge"), dropdown.Option("Sci-Fi Diffusion"), dropdown.Option("SD-Silicon"), dropdown.Option("Seek.art MEGA"), dropdown.Option("Smoke Diffusion"), dropdown.Option("Something"), dropdown.Option("Sonic Diffusion"), dropdown.Option("Spider-Verse Diffusion"), dropdown.Option("Squishmallow Diffusion"), dropdown.Option("SDXL_beta"), dropdown.Option("stable_diffusion"), dropdown.Option("stable_diffusion_2.1"), dropdown.Option("stable_diffusion_inpainting"), dropdown.Option("Supermarionation"), dropdown.Option("Sygil-Dev Diffusion"), dropdown.Option("Synthwave"), dropdown.Option("SynthwavePunk"), dropdown.Option("TrexMix"), dropdown.Option("trinart"), dropdown.Option("Trinart Characters"), dropdown.Option("Tron Legacy Diffusion"), dropdown.Option("T-Shirt Diffusion"), dropdown.Option("T-Shirt Print Designs"), dropdown.Option("Uhmami"), dropdown.Option("Ultraskin"), dropdown.Option("UMI Olympus"), dropdown.Option("Unstable Ink Dream"), dropdown.Option("URPM"), dropdown.Option("Valorant Diffusion"), dropdown.Option("Van Gogh Diffusion"), dropdown.Option("Vector Art"), dropdown.Option("vectorartz"), dropdown.Option("Vintedois Diffusion"), dropdown.Option("VinteProtogenMix"), dropdown.Option("Vivid Watercolors"), dropdown.Option("Voxel Art Diffusion"), dropdown.Option("waifu_diffusion"), dropdown.Option("Wavyfusion"), dropdown.Option("Woop-Woop Photo"), dropdown.Option("Xynthii-Diffusion"), dropdown.Option("Yiffy"), dropdown.Option("Zack3D"), dropdown.Option("Zeipher Female Model"), dropdown.Option("Zelda BOTW")], value=prefs['AIHorde_model'], autofocus=False, on_change=lambda e:changed(e, 'AIHorde_model'))
  AIHorde_sampler = Dropdown(label="Generation Sampler", hint_text="", width=350, options=[dropdown.Option("k_lms"), dropdown.Option("k_heun"), dropdown.Option("k_euler"), dropdown.Option("k_euler_a"), dropdown.Option("k_dpm_2"), dropdown.Option("k_dpm_2_a"), dropdown.Option("k_dpm_fast"), dropdown.Option("k_dpm_adaptive"), dropdown.Option("k_dpmpp_2s_a"), dropdown.Option("k_dpmpp_2m"), dropdown.Option("dpmsolver"), dropdown.Option("k_dpmpp_sde"), dropdown.Option("DDIM")], value=prefs['AIHorde_sampler'], autofocus=False, on_change=lambda e:changed(e, 'AIHorde_sampler'))
  AIHorde_post_processing = Dropdown(label="Post-Processing", hint_text="", width=350, options=[dropdown.Option("None"), dropdown.Option("GFPGAN"), dropdown.Option("RealESRGAN_x4plus"), dropdown.Option("RealESRGAN_x2plus"), dropdown.Option("RealESRGAN_x4plus_anime_6B"), dropdown.Option("NMKD_Siax"), dropdown.Option("4x_AnimeSharp"), dropdown.Option("CodeFormers"), dropdown.Option("strip_background")], value=prefs['AIHorde_post_processing'], autofocus=False, on_change=lambda e:changed(e, 'AIHorde_post_processing'))
  AIHorde_settings = Container(animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE, padding=padding.only(left=32), content=Column([use_AIHorde, AIHorde_model, AIHorde_sampler, AIHorde_post_processing]))
  AIHorde_settings.height = None if prefs['install_AIHorde_api'] else 0

  install_ESRGAN = Switcher(label="Install Real-ESRGAN AI Upscaler", value=prefs['install_ESRGAN'], disabled=status['installed_ESRGAN'], on_change=lambda e:changed(e, 'install_ESRGAN'), tooltip="Recommended to enlarge & sharpen all images as they're made.")
  install_OpenAI = Switcher(label="Install OpenAI GPT-3 Text Engine", value=prefs['install_OpenAI'], disabled=status['installed_OpenAI'], on_change=lambda e:changed(e, 'install_OpenAI'), tooltip="Use advanced AI to help make creative prompts. Also enables DALL-E 2 generation.")
  install_TextSynth = Switcher(label="Install TextSynth GPT-J Text Engine", value=prefs['install_TextSynth'], disabled=status['installed_TextSynth'], on_change=lambda e:changed(e, 'install_TextSynth'), tooltip="Alternative Text AI for brainstorming & rewriting your prompts. Pretty smart..")
  diffusers_settings.height = None if prefs['install_diffusers'] else 0
  stability_settings.height = None if prefs['install_Stability_api'] else 0
  clip_settings.height = None if prefs['install_CLIP_guided'] else 0

  def run_installers(e):
      global force_updates
      def console_clear():
        page.banner.content.controls = []
        page.update()
      def console_msg(msg, clear=True, show_progress=True):
        if not page.banner.open:
          page.banner.open = True
        if clear:
          page.banner.content.controls = []
        if show_progress:
          page.banner.content.controls.append(Row([Stack([Icon(icons.DOWNLOADING, color=colors.AMBER, size=48), Container(content=ProgressRing(), padding=padding.only(top=6, left=6), alignment=alignment.center)]), Container(content=Text("  " + msg.strip() , weight=FontWeight.BOLD, color=colors.ON_SECONDARY_CONTAINER, size=18), alignment=alignment.bottom_left, padding=padding.only(top=6)) ]))
          #page.banner.content.controls.append(Stack([Container(content=Text(msg.strip() + "  ", weight=FontWeight.BOLD, color=colors.ON_SECONDARY_CONTAINER, size=18), alignment=alignment.bottom_left, padding=padding.only(top=6)), Container(content=ProgressRing(), alignment=alignment.center if (page.width if page.web else page.window_width) > 768 else alignment.center_right)]))
          #page.banner.content.controls.append(Stack([Container(content=Text(msg.strip() + "  ", weight=FontWeight.BOLD, color=colors.ON_SECONDARY_CONTAINER, size=18), alignment=alignment.bottom_left, padding=padding.only(top=6)), Container(content=ProgressRing(), alignment=alignment.center)]))
          #page.banner.content.controls.append(Row([Text(msg.strip() + "  ", weight=FontWeight.BOLD, color=colors.GREEN_600), ProgressRing()]))
        else:
          page.banner.content.controls.append(Text(msg.strip(), weight=FontWeight.BOLD, color=colors.GREEN_600))
        page.update()
      page.console_msg = console_msg
      if status['changed_installers']:
        save_settings_file(page, change_icon=False)
        status['changed_installers'] = False
      # Temporary until I get Xformers to work
      #prefs['memory_optimization'] = 'Attention Slicing' if prefs['enable_attention_slicing'] else 'None'
      if prefs['install_diffusers'] and not bool(prefs['HuggingFace_api_key']):
        alert_msg(e.page, "You must provide your HuggingFace API Key to use Diffusers.")
        return
      if prefs['install_Stability_api'] and not bool(prefs['Stability_api_key']):
        alert_msg(e.page, "You must have your DreamStudio.ai Stability-API Key to use Stability.  Note that it will use your tokens.")
        return
      if prefs['install_OpenAI'] and not bool(prefs['OpenAI_api_key']):
        alert_msg(e.page, "You must have your OpenAI API Key to use GPT-3 Text AI.")
        return
      if prefs['install_TextSynth'] and not bool(prefs['TextSynth_api_key']):
        alert_msg(e.page, "You must have your TextSynth API Key to use GPT-J Text AI.")
        return
      if prefs['install_AIHorde_api'] and not bool(prefs['AIHorde_api_key']):
        alert_msg(e.page, "You must have your AIHorde.net API Key to use Stable Horde.  Note that it will use your Kudos.")
        return
      page.banner.content = Column([], scroll=ScrollMode.AUTO, auto_scroll=True, tight=True, spacing=0, alignment=MainAxisAlignment.END)
      page.banner.open = True
      page.update()
      if prefs['install_diffusers']:
        console_msg("Installing Hugging Face Diffusers Pipeline...")
        get_diffusers(page)
        status['installed_diffusers'] = True
      if prefs['install_text2img'] and prefs['install_diffusers']:
        console_msg("Downloading Stable Diffusion Text2Image, Image2Image & Inpaint Pipeline...")
        #with io.StringIO() as buf, redirect_stdout(buf):
        #print('redirected')
        get_text2image(page)
        #output = buf.getvalue()
        #page.banner.content.controls.append(Text(output.strip()))
        status['installed_txt2img'] = True
        page.img_block.height = None
        page.img_block.update()
        page.update()
      if prefs['install_img2img'] and prefs['install_diffusers']:
        console_msg("Downloading Stable Diffusion Inpaint Model & Image2Image Pipeline...")
        get_image2image(page)
        status['installed_img2img'] = True
        page.img_block.height = None
        page.img_block.update()
        page.use_inpaint_model.visible = True
        page.use_inpaint_model.update()
        if not status['installed_txt2img']:
          prefs['use_inpaint_model'] = True
      '''if prefs['install_megapipe'] and prefs['install_diffusers']:
        console_msg("Downloading Stable Diffusion Unified Mega Pipeline...")
        get_text2image(page)
        status['installed_megapipe'] = True
        page.img_block.height = None
        page.img_block.update()'''
      if prefs['install_SDXL'] and prefs['install_diffusers']:
        console_msg("Installing Stable Diffusion XL Text2Image, Image2Image & Inpaint Pipeline...")
        if get_SDXL(page):
          status['installed_SDXL'] = True
          page.img_block.height = None
          page.img_block.update()
          page.use_SDXL.visible = True
          page.use_SDXL.update()
          page.SDXL_params.visible = True
          page.SDXL_params.update()
      if prefs['install_alt_diffusion'] and prefs['install_diffusers']:
        console_msg("Installing AltDiffusion Text2Image & Image2Image Pipeline...")
        get_alt_diffusion(page)
        status['installed_alt_diffusion'] = True
        page.use_alt_diffusion.visible = True
        page.use_alt_diffusion.update()
      if prefs['install_interpolation'] and prefs['install_diffusers']:
        console_msg("Downloading Stable Diffusion Walk Interpolation Pipeline...")
        get_interpolation(page)
        status['installed_interpolation'] = True
        page.interpolation_block.visible = True
        page.interpolation_block.update()
      if prefs['install_CLIP_guided'] and prefs['install_diffusers']:
        console_msg("Downloading Stable Diffusion CLIP-Guided Pipeline...")
        get_clip(page)
        status['installed_clip'] = True
        page.use_clip_guided_model.visible = True
        page.use_clip_guided_model.update()
        page.clip_block.height = None if prefs['use_clip_guided_model'] else 0
        page.clip_block.update()
        if prefs['use_clip_guided_model']:
          page.img_block.height = 0
          page.img_block.update()
      if prefs['install_conceptualizer'] and prefs['install_diffusers']:
        console_msg("Installing SD Concepts Library Textual Inversion Pipeline...")
        get_conceptualizer(page)
        page.use_conceptualizer_model.visible = True
        page.use_conceptualizer_model.update()
        if prefs['use_conceptualizer']:
          page.img_block.height = 0
          page.img_block.update()
        status['installed_conceptualizer'] = True
      if prefs['install_repaint'] and not status['installed_repaint'] and prefs['install_diffusers']:
        console_msg("Installing Stable Diffusion RePaint Pipeline...")
        get_repaint(page)
        status['installed_repaint'] = True
      if prefs['install_depth2img'] and prefs['install_diffusers']:
        console_msg("Installing Stable Diffusion 2 Depth2Image Pipeline...")
        get_depth2img(page)
        status['installed_depth2img'] = True
        if not status['installed_txt2img']:
          page.img_block.height = None
          page.img_block.update()
        page.use_depth2img.visible = True
        page.use_depth2img.update()
      if prefs['install_SAG'] and prefs['install_diffusers']:
        console_msg("Installing Stable Diffusion Self-Attention Guidance Text2Image Pipeline...")
        get_SAG(page)
        status['installed_SAG'] = True
        page.use_SAG.visible = True
        page.use_SAG.update()
      if prefs['install_attend_and_excite'] and prefs['install_diffusers']:
        console_msg("Installing Stable Diffusion Attend and Excite Text2Image Pipeline...")
        get_attend_and_excite(page)
        status['installed_attend_and_excite'] = True
        page.use_attend_and_excite.visible = True
        page.use_attend_and_excite.update()
      if prefs['install_imagic'] and prefs['install_diffusers']:
        console_msg("Installing Stable Diffusion iMagic Image2Image Pipeline...")
        get_imagic(page)
        status['installed_imagic'] = True
        if not status['installed_txt2img']:
          page.img_block.height = None
          page.img_block.update()
        page.use_imagic.visible = True
        page.use_imagic.update()
      if prefs['install_composable'] and prefs['install_diffusers']:
        console_msg("Installing Stable Diffusion Composable Text2Image Pipeline...")
        get_composable(page)
        status['installed_composable'] = True
        page.use_composable.visible = True
        page.use_composable.update()
      if prefs['install_versatile'] and prefs['install_diffusers']:
        console_msg("Installing Stable Diffusion Versatile Text2Image, Variation & Inpaint Pipeline...")
        get_versatile(page)
        status['installed_versatile'] = True
        if not status['installed_txt2img']:
          page.img_block.height = None
          page.img_block.update()
        page.use_versatile.visible = True
        page.use_versatile.update()
      if prefs['install_safe'] and prefs['install_diffusers']:
        console_msg("Installing Stable Diffusion Safe Text2Image Pipeline...")
        get_safe(page)
        status['installed_safe'] = True
        page.use_safe.visible = True
        page.use_safe.update()
      if prefs['install_panorama'] and prefs['install_diffusers']:
        console_msg("Installing MultiDiffusion Panorama Text2Image Pipeline...")
        get_panorama(page)
        status['installed_panorama'] = True
        page.use_panorama.visible = True
        page.use_panorama.update()
      if prefs['install_upscale'] and prefs['install_diffusers']:
        console_msg("Installing Stable Diffusion 4X Upscale Pipeline...")
        get_upscale(page)
        status['installed_upscale'] = True
        page.use_upscale.visible = True
        page.use_upscale.update()
      if prefs['install_dreamfusion'] and not status['installed_dreamfusion'] and prefs['install_diffusers']:
        console_msg("Installing Stable Diffusion DreamFusion 3D Pipeline...")
        get_dreamfusion(page) # No longer installing from here
        status['installed_dreamfusion'] = True
      if prefs['install_Stability_api']:
        console_msg("Installing Stability-API DreamStudio.ai Pipeline...")
        get_stability(page)
        status['installed_stability'] = True
      if prefs['install_AIHorde_api']:
        console_msg("Installing Stable Horde AIHorde.net Pipeline...")
        get_AIHorde(page)
      if prefs['install_ESRGAN'] and not status['installed_ESRGAN']:
        if not os.path.isdir(os.path.join(dist_dir, 'Real-ESRGAN')):
          get_ESRGAN(page)
          console_msg("Installing Real-ESRGAN Upscaler...")
        status['installed_ESRGAN'] = True
      if prefs['install_ESRGAN']:
        page.ESRGAN_block.height = None
        page.ESRGAN_block_material.height = None
        page.ESRGAN_block_dalle.height = None
        page.ESRGAN_block_kandinsky.height = None
        page.ESRGAN_block_kandinsky_fuse.height = None
        page.ESRGAN_block_kandinsky2.height = None
        page.ESRGAN_block_kandinsky2_fuse.height = None
        page.ESRGAN_block_kandinsky_controlnet.height = None
        page.ESRGAN_block_deepfloyd.height = None
        page.ESRGAN_block_reference.height = None
        page.ESRGAN_block_unCLIP.height = None
        page.ESRGAN_block_unCLIP_image_variation.height = None
        page.ESRGAN_block_unCLIP_interpolation.height = None
        page.ESRGAN_block_unCLIP_image_interpolation.height = None
        page.ESRGAN_block_semantic.height = None
        page.ESRGAN_block_EDICT.height = None
        page.ESRGAN_block_DiffEdit.height = None
        page.ESRGAN_block_magic_mix.height = None
        page.ESRGAN_block_paint_by_example.height = None
        page.ESRGAN_block_instruct_pix2pix.height = None
        page.ESRGAN_block_controlnet.height = None
        page.ESRGAN_block_controlnet_qr.height = None
        page.ESRGAN_block_controlnet_segment.height = None
        page.ESRGAN_block_styler.height = None
        page.ESRGAN_block_deep_daze.height = None
        page.ESRGAN_block_DiT.height = None
        page.ESRGAN_block_text_to_video.height = None
        page.ESRGAN_block_text_to_video_zero.height = None
        page.ESRGAN_block_stable_animation.height = None
        page.ESRGAN_block.update()
        page.ESRGAN_block_material.update()
        page.ESRGAN_block_dalle.update()
        page.ESRGAN_block_kandinsky.update()
        page.ESRGAN_block_kandinsky_fuse.update()
        page.ESRGAN_block_kandinsky_controlnet.update()
        page.ESRGAN_block_kandinsky2.update()
        page.ESRGAN_block_kandinsky2_fuse.update()
        page.ESRGAN_block_deepfloyd.update()
        page.ESRGAN_block_reference.update()
        page.ESRGAN_block_unCLIP.update()
        page.ESRGAN_block_unCLIP_image_variation.update()
        page.ESRGAN_block_unCLIP_interpolation.update()
        page.ESRGAN_block_unCLIP_image_interpolation.update()
        page.ESRGAN_block_semantic.update()
        page.ESRGAN_block_EDICT.update()
        page.ESRGAN_block_DiffEdit.update()
        page.ESRGAN_block_magic_mix.update()
        page.ESRGAN_block_paint_by_example.update()
        page.ESRGAN_block_instruct_pix2pix.update()
        page.ESRGAN_block_controlnet.update()
        page.ESRGAN_block_controlnet_qr.update()
        page.ESRGAN_block_controlnet_segment.update()
        page.ESRGAN_block_styler.update()
        page.ESRGAN_block_deep_daze.update()
        page.ESRGAN_block_DiT.update()
        page.ESRGAN_block_text_to_video.update()
        page.ESRGAN_block_text_to_video_zero.update()
        page.ESRGAN_block_stable_animation.update()
      if prefs['install_OpenAI'] and not status['installed_OpenAI']:
        try:
          import openai
        except ModuleNotFoundError as e:
          console_msg("Installing OpenAI GPT-3 Libraries...")
          run_process("pip install openai -qq", page=page)
          pass
        status['installed_OpenAI'] = True
      if prefs['install_TextSynth'] and not status['installed_TextSynth']:
        try:
          from textsynthpy import TextSynth, Complete
        except ModuleNotFoundError as e:
          console_msg("Installing TextSynth GPT-J Libraries...")
          run_process("pip install textsynthpy -qq", page=page)
          pass
        status['installed_TextSynth'] = True
      #print('Done Installing...')
      if prefs['enable_sounds']: page.snd_done.play()
      console_clear()
      page.banner.open = False
      page.banner.update()
      page.update()
      install_diffusers.update()
      #install_text2img.update()
      #install_img2img.update()
      install_Stability_api.update()
      install_CLIP_guided.update()
      install_ESRGAN.update()
      install_OpenAI.update()
      install_TextSynth.update()
      update_parameters(page)
      page.Parameters.controls[0].content.update()
      #page.Parameters.updater()
      force_updates = False
      if current_tab==1:
        page.Installers.controls[0].content.update()
        page.Installers.update()
        page.show_install_fab(False)
        page.tabs.selected_index = 2
        page.tabs.update()
        page.update()
  def show_install_fab(show = True):
    if show:
      page.floating_action_button = FloatingActionButton(content=Row([Icon(icons.FILE_DOWNLOAD), Text("Run Installations", size=18)], alignment="center", spacing=5), width=205, shape=ft.RoundedRectangleBorder(radius=22), on_click=run_installers)
      #page.floating_action_button = FloatingActionButton(icon=icons.FILE_DOWNLOAD, text="Run Installations", on_click=run_installers)
      page.update()
    else:
      if page.floating_action_button is not None:
        page.floating_action_button = None
        page.update()
  page.show_install_fab = show_install_fab
  install_button = ElevatedButton(content=Text(value="⏬   Run Installations ", size=20), on_click=run_installers)

  #image_output = TextField(label="Image Output Path", value=prefs['image_output'], on_change=changed)
  c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
                content=Column([
        Header("📥  Stable Diffusion Required & Optional Installers"),
        install_diffusers,
        diffusers_settings,
        #install_text2img,
        #install_img2img,
        install_Stability_api,
        stability_settings,
        install_AIHorde,
        AIHorde_settings,
        #install_CLIP_guided,
        #clip_settings,
        install_ESRGAN,
        install_OpenAI,
        install_TextSynth,
        #install_button,
        Container(content=None, height=32),
      ],
  ))], scroll=ScrollMode.AUTO)
  def init_boxes():
    diffusers_settings.height = None if prefs['install_diffusers'] else 0
    stability_settings.height = None if prefs['install_Stability_api'] else 0
    clip_settings.height = None if prefs['install_CLIP_guided'] else 0
    diffusers_settings.update()
    stability_settings.update()
    clip_settings.update()
    page.update()
  #init_boxes()
  return c

def update_parameters(page):
  #page.img_block.height = None if status['installed_img2img'] or status['installed_megapipe'] or status['installed_stability'] else 0
  page.img_block.height = None if (status['installed_txt2img'] or status['installed_stability'] or status['installed_AIHorde'] or status['installed_SDXL']) and not (status['installed_clip'] and prefs['use_clip_guided_model']) else 0
  page.clip_block.height = None if status['installed_clip']  and prefs['use_clip_guided_model'] else 0
  page.ESRGAN_block.height = None if status['installed_ESRGAN'] else 0
  page.img_block.update()
  page.clip_block.update()
  page.ESRGAN_block.update()
  page.Parameters.update()
  #print("Updated Parameters")

if is_Colab:
    from google.colab import files

#LoRA_models = [{'name': 'Von Platen LoRA', 'path': 'patrickvonplaten/lora'}, {'name': 'Dog Example', 'path':'patrickvonplaten/lora_dreambooth_dog_example'}, {'name': 'Trauter LoRAs', 'path': 'YoungMasterFromSect/Trauter_LoRAs'}, {'name': 'Capitalize T5', 'path': 'ShengdingHu/Capitalize_T5-LoRA'}, {'name': 'SayakPaul LoRA-T4', 'path': 'sayakpaul/sd-model-finetuned-lora-t4'}]
#[{'name': 'sample-dog', 'path': 'lora-library/lora-dreambooth-sample-dog', 'prefix': 'sksdog'}, {'name': 'kdekuni', 'path': 'lora-library/kdekuni', 'prefix': 'a kdekuni golden funkopop'}, {'name': 'yarosnnv', 'path': 'lora-library/yarosnnv', 'prefix': 'yarosnnv'}, {'name': '', 'path': '', 'prefix': ''}, {'name': '', 'path': '', 'prefix': ''}, {'name': '', 'path': '', 'prefix': ''}, {'name': '', 'path': '', 'prefix': ''}, {'name': '', 'path': '', 'prefix': ''}, {'name': '', 'path': '', 'prefix': ''}, {'name': '', 'path': '', 'prefix': ''}, ]
LoRA_models = [{'name': 'Dog Example', 'path':'patrickvonplaten/lora_dreambooth_dog_example'}, {'name': 'SayakPaul LoRA-T4', 'path': 'sayakpaul/sd-model-finetuned-lora-t4'}, {'name':'Openjourney LoRA', 'path':'prompthero/openjourney-lora', 'prefix': ''}, {'name':'Analog Diffusion', 'path':'https://replicate.delivery/pbxt/IzbeguwVsW3PcC1gbiLy5SeALwk4sGgWroHagcYIn9I960bQA/tmpjlodd7vazekezip.safetensors', 'prefix':'<1> '}, {'name': 'Analog.Redmond', 'path': 'artificialguybr/analogredmond', 'prefix':'AnalogRedmAF'}, {'name': 'LogoLoraForSDXL', 'path': 'artificialguybr/LogoRedmond-LogoLoraForSDXL', 'prefix':'LogoRedAF'}]

def buildParameters(page):
  global prefs, status, args
  def changed(e, pref=None, asInt=False, apply=True):
      if pref is not None:
        prefs[pref] = e.control.value if not asInt else int(e.control.value)
      if page.floating_action_button is None and apply:
        show_apply_fab(len(prompts) > 0)
      #if apply_changes_button.visible != (len(prompts) > 0): #status['changed_parameters']:
      #  apply_changes_button.visible = len(prompts) > 0
      #  apply_changes_button.update()
      status['changed_parameters'] = True
      #page.update()
  def change(e):
      if page.floating_action_button is None:
        show_apply_fab(len(prompts) > 0)
      status['changed_parameters'] = True
  def run_parameters(e):
      save_parameters()
      #page.tabs.current_tab = 3
      page.show_apply_fab(False)
      page.tabs.selected_index = 3
      page.tabs.update()
      page.update()
  def save_parameters():
      update_args()
      page.update_prompts()
      save_settings_file(page)
      status['changed_parameters'] = False
  def apply_to_prompts(e):
      update_args()
      page.apply_changes(e)
      save_settings_file(page)
      show_apply_fab(False)
      #apply_changes_button.visible = False
      #apply_changes_button.update()
  def pick_files_result(e: FilePickerResultEvent):
      # TODO: This is not working on Colab, maybe it can get_upload_url on other platform?
      if e.files:
        img = e.files
        uf = []
        fname = img[0]
        print(", ".join(map(lambda f: f.name, e.files)))
        #print(os.path.join(fname.path, fname.name))
        #src_path = os.path.join(fname.path, fname.name)
        #for f in pick_files_dialog.result.files:
        src_path = page.get_upload_url(fname.name, 600)
        uf.append(FilePickerUploadFile(fname.name, upload_url=src_path))
        pick_files_dialog.upload(uf)
        print(str(src_path))
        #src_path = ''.join(src_path)
        print(str(uf[0]))
        dst_path = os.path.join(root_dir, fname.name)
        print(f'Copy {src_path} to {dst_path}')
        #shutil.copy(src_path, dst_path)
        # TODO: is init or mask?
        init_image.value = dst_path
      #selected_files.value = (", ".join(map(lambda f: f.name, e.files)) if e.files else "Cancelled!")
      #selected_files.update()

  pick_files_dialog = FilePicker(on_result=pick_files_result)
  page.overlay.append(pick_files_dialog)
  #selected_files = Text()

  def file_picker_result(e: FilePickerResultEvent):
      if e.files != None:
        upload_files(e)
  def on_upload_progress(e: FilePickerUploadEvent):
    nonlocal pick_type
    if e.progress == 1:
      if not slash in e.file_name:
        fname = os.path.join(root_dir, e.file_name)
      else:
        fname = e.file_name
      if pick_type == "init":
        init_image.value = fname
        init_image.update()
        prefs['init_image'] = fname
      elif pick_type == "mask":
        mask_image.value = fname
        mask_image.update()
        prefs['mask_image'] = fname
      page.update()
  file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)
  def upload_files(e):
      uf = []
      if file_picker.result != None and file_picker.result.files != None:
          for f in file_picker.result.files:
              if page.web:
                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))
              else:
                on_upload_progress(FilePickerUploadEvent(f.path, 1, ""))
          file_picker.upload(uf)
  page.overlay.append(file_picker)
  pick_type = ""
  #page.overlay.append(pick_files_dialog)
  def pick_init(e):
      nonlocal pick_type
      pick_type = "init"
      file_picker.pick_files(allow_multiple=False, allowed_extensions=["png", "PNG"], dialog_title="Pick Init Image File")
  def pick_mask(e):
      nonlocal pick_type
      pick_type = "mask"
      file_picker.pick_files(allow_multiple=False, allowed_extensions=["png", "PNG"], dialog_title="Pick Black & White Mask Image")
  def toggle_ESRGAN(e):
      ESRGAN_settings.height = None if e.control.value else 0
      prefs['apply_ESRGAN_upscale'] = e.control.value
      ESRGAN_settings.update()
      has_changed = True
  def toggle_clip(e):
      if e.control.value:
        page.img_block.height = 0
        page.clip_block.height = None if status['installed_clip'] else 0
      else:
        page.img_block.height = None if status['installed_txt2img'] or status['installed_stability'] else 0
        page.clip_block.height = 0
      page.img_block.update()
      page.clip_block.update()
      changed(e, 'use_clip_guided_model')
  def change_use_cutouts(e):
      num_cutouts.visible = e.control.value
      num_cutouts.update()
      changed(e, 'use_cutouts')
  def toggle_interpolation(e):
      interpolation_steps_slider.height = None if e.control.value else 0
      interpolation_steps_slider.update()
      if e.control.value: page.img_block.height = 0
      else: page.img_block.height = None if status['installed_txt2img'] or status['installed_stability'] else 0
      page.img_block.update()
      changed(e, 'use_interpolation', apply=False)
  def change_interpolation_steps(e):
      interpolation_steps_value.value = f" {int(e.control.value)} steps"
      interpolation_steps_value.update()
      changed(e, 'num_interpolation_steps', asInt=True, apply=False)
  def toggle_SAG(e):
      sag_scale_slider.height = None if e.control.value else 0
      sag_scale_slider.update()
      changed(e, 'use_SAG', apply=False)
  def change_sag_scale(e):
      sag_scale_value.value = f" {float(e.control.value)}"
      sag_scale_value.update()
      changed(e, 'sag_scale', apply=False)
  def toggle_attend_and_excite(e):
      max_iter_to_alter_slider.height = None if e.control.value else 0
      max_iter_to_alter_slider.update()
      changed(e, 'use_attend_and_excite', apply=False)
  def change_max_iter_to_alter(e):
      max_iter_to_alter_value.value = f" {int(e.control.value)} Iterations"
      max_iter_to_alter_value.update()
      changed(e, 'max_iter_to_alter', asInt=True, apply=False)
  def change_enlarge_scale(e):
      enlarge_scale_slider.controls[1].value = f" {float(e.control.value)}x"
      enlarge_scale_slider.update()
      changed(e, 'enlarge_scale', apply=False)
  def change_strength(e):
      strength_value.value = f" {int(e.control.value * 100)}"
      strength_value.update()
      guidance.update()
      changed(e, 'init_image_strength')
  def toggle_SDXL(e):
      changed(e,'use_SDXL', apply=False)
      page.SDXL_params.height = None if e.control.value else 0
      page.SDXL_params.update()
  def toggle_conceptualizer(e):
      if e.control.value:
        page.img_block.height = 0
      else:
        page.img_block.height = None if status['installed_txt2img'] or status['installed_stability'] else 0
      page.img_block.update()
      changed(e, 'use_conceptualizer')
  def toggle_centipede(e):
      changed(e,'centipede_prompts_as_init_images', apply=False)
      image_pickers.height = None if not e.control.value else 0
      image_pickers.update()
  def toggle_LoRA(e):
      changed(e,'use_LoRA_model', apply=False)
      LoRA_block.width = None if e.control.value else 0
      LoRA_block.update()
  def changed_LoRA(e):
      changed(e, 'LoRA_model', apply=False)
      custom_LoRA_model.visible = True if prefs['LoRA_model'] == "Custom LoRA Path" else False
      custom_LoRA_model.update()

  batch_folder_name = TextField(label="Batch Folder Name", value=prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name', apply=False))
  #batch_size = TextField(label="Batch Size", value=prefs['batch_size'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'batch_size'))
  #n_iterations = TextField(label="Number of Iterations", value=prefs['n_iterations'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'n_iterations'))
  batch_size = NumberPicker(label="Batch Size: ", min=1, max=10, value=prefs['batch_size'], tooltip="Generates multiple images at the same time. Uses more memory...", on_change=lambda e: changed(e, 'batch_size'))
  n_iterations = NumberPicker(label="Number of Iterations: ", min=1, max=30, value=prefs['n_iterations'], tooltip="Creates multiple images in batch seperately", on_change=lambda e: changed(e, 'n_iterations'))
  #steps = TextField(label="Steps", value=prefs['steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'steps', asInt=True))
  steps = SliderRow(label="Steps", min=0, max=200, divisions=200, pref=prefs, key='steps', on_change=change)
  #eta = TextField(label="DDIM ETA", value=prefs['eta'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'eta'))
  eta = SliderRow(label="DDIM ETA", min=0, max=1, divisions=20, round=1, pref=prefs, key='eta', tooltip="", visible=False, on_change=change)
  page.etas.append(eta)
  seed = TextField(label="Seed", value=prefs['seed'], keyboard_type=KeyboardType.NUMBER, width = 160, on_change=lambda e:changed(e,'seed'))
  param_rows = Row([Column([batch_folder_name, seed, batch_size]), Column([steps, eta, n_iterations])])
  batch_row = Row([batch_folder_name, seed])
  number_row = Row([batch_size, n_iterations])
  guidance = SliderRow(label="Guidance Scale", min=0, max=50, divisions=100, round=1, pref=prefs, key='guidance_scale', on_change=change)
  width_slider = SliderRow(label="Width", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=prefs, key='width', on_change=change)
  height_slider = SliderRow(label="Height", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=prefs, key='height', on_change=change)

  init_image = TextField(label="Init Image", value=prefs['init_image'], on_change=lambda e:changed(e,'init_image'), expand=True, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_init))
  mask_image = TextField(label="Mask Image", value=prefs['mask_image'], on_change=lambda e:changed(e,'mask_image'), expand=True, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD_OUTLINED, on_click=pick_mask))
  alpha_mask = Checkbox(label="Alpha Mask", value=prefs['alpha_mask'], tooltip="Use Transparent Alpha Channel of Init as Mask", fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'alpha_mask'))
  invert_mask = Checkbox(label="Invert Mask", value=prefs['invert_mask'], tooltip="Reverse Black & White of Image Mask", fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'invert_mask'))
  image_pickers = Container(content=ResponsiveRow([Row([init_image, alpha_mask], col={"lg":6}), Row([mask_image, invert_mask], col={"lg":6})]), padding=padding.only(top=5), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
  image_pickers.height = None if not prefs['centipede_prompts_as_init_images'] else 0
  init_image_strength = Slider(min=0.1, max=0.9, divisions=16, label="{value}%", value=prefs['init_image_strength'], on_change=change_strength, expand=True)
  strength_value = Text(f" {int(prefs['init_image_strength'] * 100)}%", weight=FontWeight.BOLD)
  strength_slider = Row([Text("Init Image Strength: "), strength_value, init_image_strength])
  page.use_SDXL = Switcher(label="Use Stable Diffusion XL Model/Pipeline Instead", tooltip="The latest SDXL base model, with img2img Refiner. It's tasty..", value=prefs['use_SDXL'], on_change=toggle_SDXL)
  page.use_SDXL.visible = status['installed_SDXL']
  #SDXL_compel = Switcher(label="Use Compel Long Prompt Weighting Embeds", tooltip="Re-weight different parts of a prompt string like positive+++ AND (bad negative)-- or (subject)1.3 syntax.", value=prefs['SDXL_compel'], on_change=lambda e:changed(e,'SDXL_compel'))
  SDXL_high_noise_frac = SliderRow(label="SDXL High Noise Fraction", min=0, max=1, divisions=20, round=2, pref=prefs, key='SDXL_high_noise_frac', tooltip="Percentage of Steps to use Base model, then Refiner model. Known as an Ensemble of Expert Denoisers. Value of 1 skips Refine steps.", on_change=lambda e:changed(e,'SDXL_high_noise_frac', apply=False))
  page.SDXL_params = Container(Column([SDXL_high_noise_frac]), padding=padding.only(top=5, left=20), height=None if prefs['use_SDXL'] else 0, visible=status['installed_SDXL'], animate_size=animation.Animation(1000, AnimationCurve.EASE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
  page.use_inpaint_model = Switcher(label="Use Specialized Inpaint Model Instead", tooltip="When using init_image and/or mask, use the newer pipeline for potentially better results", value=prefs['use_inpaint_model'], on_change=lambda e:changed(e,'use_inpaint_model', apply=False))
  page.use_inpaint_model.visible = status['installed_img2img']
  page.use_alt_diffusion = Switcher(label="Use AltDiffusion Pipeline Model Instead", value=prefs['use_versatile'], on_change=lambda e:changed(e,'use_versatile', apply=False), tooltip="Supports 9 different languages for text2image & image2image")
  page.use_alt_diffusion.visible = status['installed_alt_diffusion']
  page.use_versatile = Switcher(label="Use Versatile Pipeline Model Instead", value=prefs['use_versatile'], on_change=lambda e:changed(e,'use_versatile', apply=False), tooltip="Dual Guided between prompt & image, or create Image Variation.")
  page.use_versatile.visible = status['installed_versatile']
  use_LoRA_model = Switcher(label="Use LoRA Model Adapter Layer ", value=prefs['use_LoRA_model'], on_change=toggle_LoRA, tooltip="Applies custom trained weighted attention model on top of loaded model")
  page.LoRA_model = Dropdown(label="LoRA Model Weights", width=200, options=[], value=prefs['LoRA_model'], on_change=changed_LoRA)
  if len(prefs['custom_LoRA_models']) > 0:
    for l in prefs['custom_LoRA_models']:
      page.LoRA_model.options.append(dropdown.Option(l['name']))
  for m in LoRA_models:
      page.LoRA_model.options.append(dropdown.Option(m['name']))
  page.LoRA_model.options.append(dropdown.Option("Custom LoRA Path"))

  custom_LoRA_model = TextField(label="Custom LoRA Model Path", value=prefs['custom_LoRA_model'], width=370, on_change=lambda e:changed(e, 'custom_LoRA_model', apply=False))
  custom_LoRA_model.visible = True if prefs['LoRA_model'] == "Custom LoRA Path" else False
  LoRA_block = Container(Row([page.LoRA_model, custom_LoRA_model]), padding=padding.only(top=3), animate_size=animation.Animation(800, AnimationCurve.EASE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
  LoRA_block.width = None if prefs['use_LoRA_model'] else 0
  centipede_prompts_as_init_images = Switcher(label="Centipede Prompts as Init Images", tooltip="Feeds each image to the next prompt sequentially down the line", value=prefs['centipede_prompts_as_init_images'], on_change=toggle_centipede)
  use_interpolation = Switcher(label="Use Interpolation to Walk Latent Space between Prompts", tooltip="Creates animation frames transitioning, but it's not always perfect.", value=prefs['use_interpolation'], on_change=toggle_interpolation)
  interpolation_steps = Slider(min=1, max=100, divisions=99, label="{value}", value=prefs['num_interpolation_steps'], on_change=change_interpolation_steps, expand=True)
  interpolation_steps_value = Text(f" {int(prefs['num_interpolation_steps'])} steps", weight=FontWeight.BOLD)
  interpolation_steps_slider = Container(Row([Text(f"Number of Interpolation Steps between Prompts: "), interpolation_steps_value, interpolation_steps]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
  Row([Text(f"Number of Interpolation Steps between Prompts: "), interpolation_steps_value, interpolation_steps])
  if not bool(prefs['use_interpolation']):
    interpolation_steps_slider.height = 0
  page.interpolation_block = Column([use_interpolation, interpolation_steps_slider])
  page.img_block = Container(Column([image_pickers, strength_slider, page.use_inpaint_model, centipede_prompts_as_init_images, Divider(height=9, thickness=2)]), padding=padding.only(top=5), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
  if not status['installed_interpolation']:
    page.interpolation_block.visible = False
  elif bool(prefs['use_interpolation']):
    page.img_block.height = 0
  use_SAG = Switcher(label="Use Self-Attention Guidance (SAG) Text-to-Image", value=prefs['use_SAG'], on_change=toggle_SAG, tooltip="Can drastically boost the performance and quality. Extracts the intermediate attention map from a diffusion model at every iteration and selects tokens above a certain attention score for masking and blurring to obtain a partially blurred input.")
  sag_scale = Slider(min=0, max=1, divisions=20, label="{value}", value=prefs['sag_scale'], tooltip="How much Self-Attention Guidance to apply.", on_change=change_sag_scale, expand=True)
  sag_scale_value = Text(f" {float(prefs['sag_scale'])}", weight=FontWeight.BOLD)
  sag_scale_slider = Container(Row([Text(f"SAG Scale: "),sag_scale_value, sag_scale]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
  page.use_SAG = Column([use_SAG, sag_scale_slider])
  if not bool(prefs['use_SAG']):
    sag_scale_slider.height = 0
  if not status['installed_SAG']:
    page.use_SAG.visible = False
  use_attend_and_excite = Switcher(label="Use Attend and Excite", value=prefs['use_attend_and_excite'], on_change=toggle_attend_and_excite, tooltip="To use, include plus signs before subject words in prompt to indicate token indices, like 'a +cat and a +frog'.")
  max_iter_to_alter = Slider(min=1, max=100, divisions=99, label="{value}", value=prefs['max_iter_to_alter'], tooltip="The first denoising steps are where the attend-and-excite is applied. I.e. if `max_iter_to_alter` is 25 and there are a total of `30` denoising steps, the first 25 denoising steps will apply attend-and-excite and the last 5 will not apply attend-and-excite.", on_change=change_max_iter_to_alter, expand=True)
  max_iter_to_alter_value = Text(f" {int(prefs['max_iter_to_alter'])} iterations", weight=FontWeight.BOLD)
  max_iter_to_alter_slider = Container(Row([Text(f"Max Iterations to Alter: "), max_iter_to_alter_value, max_iter_to_alter]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
  page.use_attend_and_excite = Column([use_attend_and_excite, max_iter_to_alter_slider])
  if not bool(prefs['use_attend_and_excite']):
    max_iter_to_alter_slider.height = 0
  if not status['installed_attend_and_excite']:
    page.use_attend_and_excite.visible = False
  page.use_clip_guided_model = Switcher(label="Use CLIP-Guided Model", tooltip="Uses more VRAM, so you'll probably need to make image size smaller", value=prefs['use_clip_guided_model'], on_change=toggle_clip)
  clip_guidance_scale = Slider(min=1, max=5000, divisions=4999, label="{value}", value=prefs['clip_guidance_scale'], on_change=lambda e:changed(e,'clip_guidance_scale'), expand=True)
  clip_guidance_scale_slider = Row([Text("CLIP Guidance Scale: "), clip_guidance_scale])
  use_cutouts = Checkbox(label="Use Cutouts", value=bool(prefs['use_cutouts']), fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=change_use_cutouts)
  num_cutouts = NumberPicker(label="    Number of Cutouts: ", min=1, max=10, value=prefs['num_cutouts'], on_change=lambda e: changed(e, 'num_cutouts', asInt=True))
  num_cutouts.visible = bool(prefs['use_cutouts'])
  #num_cutouts = TextField(label="Number of Cutouts", value=prefs['num_cutouts'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'num_cutouts', asInt=True))
  unfreeze_unet = Checkbox(label="Unfreeze UNET", value=prefs['unfreeze_unet'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'unfreeze_unet', apply=False))
  unfreeze_vae = Checkbox(label="Unfreeze VAE", value=prefs['unfreeze_vae'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'unfreeze_vae', apply=False))
  page.clip_block = Container(Column([clip_guidance_scale_slider, Row([use_cutouts, num_cutouts], expand=False), unfreeze_unet, unfreeze_vae, Divider(height=9, thickness=2)]), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
  page.use_conceptualizer_model = Switcher(label="Use Custom Conceptualizer Model", tooltip="Use Textual-Inversion Community Model Concepts", value=prefs['use_conceptualizer'], on_change=toggle_conceptualizer)
  page.use_conceptualizer_model.visible = bool(status['installed_conceptualizer'])
  page.use_depth2img = Switcher(label="Use Depth2Image Pipeline for img2img init image generation", value=prefs['use_depth2img'], on_change=lambda e:changed(e,'use_depth2img', apply=False), tooltip="To use, provide init_image with a good composition and prompts to approximate same depth.")
  page.use_depth2img.visible = bool(status['installed_depth2img'])
  page.use_imagic = Switcher(label="Use iMagic for img2img init image editing", value=prefs['use_imagic'], on_change=lambda e:changed(e,'use_imagic', apply=False), tooltip="Allows you to edit an image with prompt text.")
  page.use_imagic.visible = bool(status['installed_imagic'])
  page.use_composable = Switcher(label="Use Composable Prompts for txt2img Weight | Segments", value=prefs['use_composable'], on_change=lambda e:changed(e,'use_composable', apply=False), tooltip="Allows conjunction and negation operators for compositional generation with conditional diffusion models")
  page.use_composable.visible = bool(status['installed_composable'])
  page.use_panorama = Column([Switcher(label="Use Panorama text2image Pipeline Instead", value=prefs['use_panorama'], on_change=lambda e:changed(e,'use_panorama', apply=False), tooltip="Fuses together images to make extra-wide 2048x512"),
                              Checkbox(label="Use Circular Padding to remove stitching artifacts", value=prefs['panorama_circular_padding'], tooltip="To seamlessly generate a transition from the right to left, maintaining consistency in a 360-degree sense.", fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'panorama_circular_padding', apply=False)),
                              Row([Text("Panoramic Width x 512:"), Slider(min=1024, max=4608, divisions=28, label="{value}px", expand=True, value=prefs['panorama_width'], on_change=lambda e:changed(e, 'panorama_width', asInt=True, apply=False))])])
  page.use_panorama.visible = status['installed_panorama']
  page.use_safe = Switcher(label="Use Safe Diffusion Pipeline instead", value=prefs['use_safe'], on_change=lambda e:changed(e,'use_safe', apply=False), tooltip="Models trained only on Safe images")
  page.use_safe.visible = bool(status['installed_safe'])
  page.use_upscale = Switcher(label="Upscale 4X with Stable Diffusion 2", value=prefs['use_upscale'], on_change=lambda e:changed(e,'use_upscale', apply=False), tooltip="Enlarges your Image Generations guided by the same Prompt.")
  page.use_upscale.visible = bool(status['installed_upscale'])
  apply_ESRGAN_upscale = Switcher(label="Apply ESRGAN Upscale", value=prefs['apply_ESRGAN_upscale'], on_change=toggle_ESRGAN)
  enlarge_scale_value = Text(f" {float(prefs['enlarge_scale'])}x", weight=FontWeight.BOLD)
  enlarge_scale = Slider(min=1, max=4, divisions=6, label="{value}x", value=prefs['enlarge_scale'], on_change=change_enlarge_scale, expand=True)
  enlarge_scale_slider = Row([Text("Enlarge Scale: "), enlarge_scale_value, enlarge_scale])
  face_enhance = Checkbox(label="Use Face Enhance GPFGAN", value=prefs['face_enhance'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'face_enhance', apply=False))
  display_upscaled_image = Checkbox(label="Display Upscaled Image", value=prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image', apply=False))
  ESRGAN_settings = Container(Column([enlarge_scale_slider, face_enhance, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
  page.ESRGAN_block = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
  #page.img_block.height = None if status['installed_txt2img'] or status['installed_stability'] else 0
  page.use_clip_guided_model.visible = status['installed_clip']
  page.clip_block.height = None if status['installed_clip'] and prefs['use_clip_guided_model'] else 0
  page.ESRGAN_block.height = None if status['installed_ESRGAN'] else 0
  if not prefs['apply_ESRGAN_upscale']:
    ESRGAN_settings.height = 0
  parameters_button = ElevatedButton(content=Text(value="📜   Continue to Image Prompts", size=20), on_click=run_parameters)
  parameters_row = Row([parameters_button], alignment=MainAxisAlignment.SPACE_BETWEEN)
  #apply_changes_button = ElevatedButton(content=Text(value="🔀   Apply Changes to Current Prompts", size=20), on_click=apply_to_prompts)
  #apply_changes_button.visible = len(prompts) > 0 and status['changed_parameters']
  def show_apply_fab(show = True):
    if show:
      page.floating_action_button = FloatingActionButton(content=Row([Icon(icons.TRANSFORM), Text("Apply Changes to Current Prompts", size=18)], alignment="center", spacing=5), width=333, shape=ft.RoundedRectangleBorder(radius=22), on_click=apply_to_prompts)
      #page.floating_action_button = FloatingActionButton(icon=icons.TRANSFORM, text="Apply Changes to Current Prompts", on_click=apply_to_prompts)
      page.update()
    else:
      if page.floating_action_button is not None:
        page.floating_action_button = None
        page.update()
  show_apply_fab(len(prompts) > 0 and status['changed_parameters'])
  page.show_apply_fab = show_apply_fab
  def updater():
      #parameters.update()
      c.update()
      page.update()
      #print("Updated Parameters Page")

  c = Column([Container(
      padding=padding.only(18, 14, 20, 10), content=Column([
        Header("📝  Stable Diffusion Image Parameters"),
        #param_rows,
        batch_row,
        number_row,
        steps,
        guidance,
        eta,
        width_slider, height_slider, #Divider(height=9, thickness=2),
        page.interpolation_block, page.img_block, page.use_safe, page.use_SDXL, page.SDXL_params, page.use_alt_diffusion, page.use_clip_guided_model, page.clip_block, page.use_versatile, page.use_SAG, page.use_attend_and_excite, page.use_panorama, page.use_conceptualizer_model,
        Row([use_LoRA_model, LoRA_block]), page.use_imagic, page.use_depth2img, page.use_composable, page.use_upscale, page.ESRGAN_block,
        #(img_block if status['installed_img2img'] or status['installed_stability'] else Container(content=None)), (clip_block if prefs['install_CLIP_guided'] else Container(content=None)), (ESRGAN_block if prefs['install_ESRGAN'] else Container(content=None)),
        #parameters_row,
      ],
  ))], scroll=ScrollMode.AUTO)#batch_folder_name, batch_size, n_iterations, steps, eta, seed,
  return c

prompts = []
args = {}

def update_args():
    global args
    args = {
        "batch_size":int(prefs['batch_size']),
        "n_iterations":int(prefs['n_iterations']),
        "steps":int(prefs['steps']),
        "eta":float(prefs['eta']),
        "width":int(prefs['width']),
        "height":int(prefs['height']),
        "guidance_scale":float(prefs['guidance_scale']),
        "seed":int(prefs['seed']),
        "precision":prefs['precision'],
        "init_image": prefs['init_image'],
        "init_image_strength": prefs['init_image_strength'],
        "mask_image": prefs['mask_image'],
        "alpha_mask": prefs['alpha_mask'],
        "invert_mask": prefs['invert_mask'],
        "prompt2": None, "tweens": 10,
        "negative_prompt": None,
        "use_clip_guided_model": prefs['use_clip_guided_model'],
        "clip_prompt": "",
        "clip_guidance_scale": float(prefs['clip_guidance_scale']),
        "use_cutouts": bool(prefs['use_cutouts']),
        "num_cutouts": int(prefs['num_cutouts']),
        "unfreeze_unet": prefs['unfreeze_unet'],
        "unfreeze_vae": prefs['unfreeze_vae'],
        "use_Stability": False,
        "use_conceptualizer": False}

update_args()

class Dream:
    def __init__(self, prompt, **kwargs):
        self.prompt = prompt
        self.arg = args.copy()
        for key, value in kwargs.items():
          if key=='arg': self.arg = value
          elif key=="batch_size": self.arg[key] = int(value)
          elif key=="n_iterations": self.arg[key] = int(value)
          elif key=="steps": self.arg[key] = int(value)
          elif key=="eta": self.arg[key] = float(value)
          elif key=="width": self.arg[key] = int(value)
          elif key=="height": self.arg[key] = int(value)
          elif key=="guidance_scale": self.arg[key] = float(value)
          elif key=="seed": self.arg[key] = int(value)
          elif key=="precision": self.arg[key] = value
          elif key=="init_image": self.arg[key] = value
          elif key=="init_image_strength": self.arg[key] = value
          elif key=="mask_image": self.arg[key] = value
          elif key=="alpha_mask": self.arg[key] = value
          elif key=="invert_mask": self.arg[key] = value
          elif key=="prompt2": self.arg[key] = value
          elif key=="tweens": self.arg[key] = int(value)
          elif key=="negative_prompt": self.arg[key] = value
          elif key=="clip_prompt": self.arg[key] = value
          elif key=="use_clip_guided_model": self.arg[key] = value
          elif key=="clip_guidance_scale": self.arg[key] = float(value)
          elif key=="use_cutouts": self.arg[key] = value
          elif key=="num_cutouts": self.arg[key] = int(value)
          elif key=="unfreeze_unet": self.arg[key] = value
          elif key=="unfreeze_vae": self.arg[key] = value
          elif key=="use_Stability": self.arg[key] = value
          elif key=="use_conceptualizer": self.arg[key] = value
          elif key=="prompt": self.prompt = value
          else: print(f"Unknown argument: {key} = {value}")
    def __getitem__(self, key):
        if key in self.arg:
          return self.arg[key]
        else:
          print(f"Error getting arg {key} from Dream")
          return ""
    def __setitem__(self, key, value):
        self.arg[key] = value


def format_filename(s, force_underscore=False, use_dash=False, max_length=None):
    if prefs['file_datetime'] and not use_dash and not force_underscore:
        return datetime.now().strftime("%Y%m%d-%H%M%S")
    file_max_length = int(prefs['file_max_length']) if max_length == None else int(max_length)
    valid_chars = "-_.() %s%s" % (string.ascii_letters, string.digits)
    filename = ''.join(c for c in s if c in valid_chars)
    if use_dash:
        filename = filename.replace(' ','-')
    else:
        if not prefs['file_allowSpace'] or force_underscore: filename = filename.replace(' ','_')
    return filename[:file_max_length]

def to_title(s, sentence=False, clean=True):
    if clean:
        s = s.replace('_',' ')
        s = s.replace('-',' ')
    if sentence:
        sentences = s.split(". ")
        sentences2 = [sentence[0].capitalize() + sentence[1:] for sentence in sentences]
        s2 = '. '.join(sentences2)
        return s2
    else:
        return string.capwords(s)

'''from collections import ChainMap
def merge_dict(*dicts):
    all_keys  = set(k for d in dicts for k in d.keys())
    chain_map = ChainMap(*reversed(dicts))
    return {k: chain_map[k] for k in all_keys}
def merge_dict(dict1, dict2):
    merged_dict = {**dict1, **dict2}
    return merged_dict
'''
def merge_dict(dict1, dict2):
    new_dict = {}
    for key in dict1:
        new_dict[key] = dict1[key]
    for key in dict2:
        new_dict[key] = dict2[key]
    return new_dict

import copy

def editPrompt(e):
    global prompts, prefs, status
    open_dream = e.control.data
    idx = prompts.index(open_dream)
    def changed_tweening(e):
        status['changed_prompts'] = True
        tweening_params.height = None if e.control.value else 0
        tweening_params.update()
        #prompt2.visible = e.control.value
        #tweens.visible = e.control.value
        prompt_tweening = e.control.value
        e.page.update()
    def changed_tweens(e):
        prefs['tweens'] = int(e.control.value)
    def close_dlg(e):
        nonlocal edit_dlg
        edit_dlg.open = False
        #e.page.dialog.open = False
        e.page.update()
        #del edit_dlg
        #e.page.dialog = None
    def open_dlg(e):
        nonlocal edit_dlg
        e.page.dialog = edit_dlg
        edit_dlg.open = True
        e.page.update()
    def save_dlg(e):
        nonlocal arg, open_dream, edit_dlg
        dream = open_dream #e.control.data
        dream.prompt = edit_text.value
        arg['batch_size'] = int(batch_size.value)
        arg['n_iterations'] = int(n_iterations.value)
        arg['steps'] = int(steps.value)
        arg['eta'] = float(eta.value)
        arg['seed'] = int(seed.value)
        arg['guidance_scale'] = float(guidance_scale.value)
        arg['width'] = int(width_slider.value)
        arg['height'] = int(height_slider.value)
        arg['init_image'] = init_image.value
        arg['mask_image'] = mask_image.value
        arg['init_image_strength'] = float(init_image_strength.value)
        arg['alpha_mask'] = alpha_mask.value
        arg['invert_mask'] = invert_mask.value
        arg['prompt2'] = prompt2.value if bool(use_prompt_tweening.value) else None
        arg['tweens'] = int(tweens.value)
        arg['negative_prompt'] = negative_prompt.value if bool(negative_prompt.value) else None
        arg['use_clip_guided_model'] = use_clip_guided_model.value
        arg['clip_guidance_scale'] = float(clip_guidance_scale.value)
        arg['use_cutouts'] = use_cutouts.value
        arg['num_cutouts'] = int(num_cutouts.value)
        arg['unfreeze_unet'] = unfreeze_unet.value
        arg['unfreeze_vae'] = unfreeze_vae.value
        dream.arg = arg
        diffs = arg_diffs(arg, args)
        if bool(diffs):
          e.page.prompts_list.controls[idx].subtitle = Text("    " + diffs)
        else:
          e.page.prompts_list.controls[idx].subtitle = None
        e.page.prompts_list.controls[idx].title.value = dream.prompt # = Text(edit_text.value)
        status['changed_prompts'] = True
        edit_dlg.open = False
        e.page.update()
    def file_picker_result(e: FilePickerResultEvent):
        if e.files != None:
          upload_files(e)
    def on_upload_progress(e: FilePickerUploadEvent):
      nonlocal pick_type
      if e.progress == 1:
        if not slash in e.file_name:
          fname = os.path.join(root_dir, e.file_name)
        else:
          fname = e.file_name
        if pick_type == "init":
          init_image.value = fname
          init_image.update()
          prefs['init_image'] = fname
        elif pick_type == "mask":
          mask_image.value = fname
          mask_image.update()
          prefs['mask_image'] = fname
        e.page.update()
    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)
    def upload_files(e):
        uf = []
        if file_picker.result != None and file_picker.result.files != None:
            for f in file_picker.result.files:
              if e.page.web:
                uf.append(FilePickerUploadFile(f.name, upload_url=e.page.get_upload_url(f.name, 600)))
              else:
                on_upload_progress(FilePickerUploadEvent(f.path, 1, ""))
            file_picker.upload(uf)
    e.page.overlay.append(file_picker)
    pick_type = ""
    #page.overlay.append(pick_files_dialog)
    def pick_init(e):
        nonlocal pick_type
        pick_type = "init"
        file_picker.pick_files(allow_multiple=False, allowed_extensions=["png", "PNG"], dialog_title="Pick Init Image File")
    def pick_mask(e):
        nonlocal pick_type
        pick_type = "mask"
        file_picker.pick_files(allow_multiple=False, allowed_extensions=["png", "PNG"], dialog_title="Pick Black & White Mask Image")
    def toggle_clip(e):
        if e.control.value:
          img_block.height = 0
          clip_block.height = None if status['installed_clip'] else 0
        else:
          img_block.height = None if status['installed_txt2img'] or status['installed_stability'] else 0
          clip_block.height = 0
        img_block.update()
        clip_block.update()
        #changed(e)
    arg = open_dream.arg #e.control.data.arg
    edit_text = TextField(label="Composable | Prompt | Text" if prefs['use_composable'] and status['installed_composable'] else "Prompt Text", col={'lg':9}, value=open_dream.prompt, multiline=True)
    negative_prompt = TextField(label="Segmented Weights 1 | -0.7 | 1.2" if prefs['use_composable'] and status['installed_composable'] else "Negative Prompt Text", col={'lg':3}, value=str((arg['negative_prompt'] or '') if 'negative_prompt' in arg else ''))
    #batch_folder_name = TextField(label="Batch Folder Name", value=arg['batch_folder_name'], on_change=changed)
    #print(str(arg))
    prompt_tweening = bool(arg['prompt2']) if 'prompt2' in arg else False
    use_prompt_tweening = Switcher(label="Prompt Tweening", value=prompt_tweening, on_change=changed_tweening)
    use_prompt_tweening.visible = True if status['installed_txt2img'] and prefs['higher_vram_mode'] else False
#TODO: Fix tweening code for float16 lpw pipeline to reactivate tweening
    prompt2 = TextField(label="Prompt 2 Transition Text", expand=True, value=arg['prompt2'] if 'prompt2' in arg else '')
    tweens = TextField(label="# of Tweens", value=str(arg['tweens'] if 'tweens' in arg else 8), keyboard_type=KeyboardType.NUMBER, width = 90)
    #tweens =  NumberPicker(label="# of Tweens: ", min=2, max=300, value=int(arg['tweens'] if 'tweens' in arg else 8), on_change=changed_tweens),
    #prompt2.visible = prompt_tweening
    #tweens.visible = prompt_tweening
    tweening_params = Container(Row([Container(content=None, width=8), prompt2, tweens]), padding=padding.only(top=4, bottom=3), animate_size=animation.Animation(1000, AnimationCurve.EASE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    tweening_params.height = None if prompt_tweening else 0
    #tweening_row = Row([use_prompt_tweening, ])#tweening_params
    batch_size = NumberPicker(label="Batch Size: ", min=1, max=10, value=arg['batch_size'])
    n_iterations = NumberPicker(label="Number of Iterations: ", min=1, max=30, value=arg['n_iterations'])
    #batch_size = TextField(label="Batch Size", value=str(arg['batch_size']), keyboard_type=KeyboardType.NUMBER)
    #n_iterations = TextField(label="Number of Iterations", value=str(arg['n_iterations']), keyboard_type=KeyboardType.NUMBER)
    steps = TextField(label="Steps", value=str(arg['steps']), keyboard_type=KeyboardType.NUMBER)
    eta = TextField(label="DDIM ETA", value=str(arg['eta']), keyboard_type=KeyboardType.NUMBER, hint_text="Amount of Noise (only with DDIM sampler)")
    seed = TextField(label="Seed", value=str(arg['seed']), keyboard_type=KeyboardType.NUMBER, hint_text="0 or -1 picks a Random seed")
    guidance_scale = TextField(label="Guidance Scale", value=str(arg['guidance_scale']), keyboard_type=KeyboardType.NUMBER)
    param_columns = Row([Column([steps, seed, batch_size]), Column([guidance_scale, eta, n_iterations])])
    #guidance_scale = Slider(min=0, max=50, divisions=100, label="{value}", value=arg['guidance_scale'], expand=True)
    #guidance = Row([Text("Guidance Scale: "), guidance_scale])
    width_slider = SliderRow(label="Width", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=arg, key='width')
    height_slider = SliderRow(label="Height", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=arg, key='height')
    init_image = TextField(label="Init Image", value=arg['init_image'], expand=1, height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_init))
    mask_image = TextField(label="Mask Image", value=arg['mask_image'], expand=1, height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD_OUTLINED, on_click=pick_mask))
    alpha_mask = Checkbox(label="Alpha Mask", value=arg['alpha_mask'], tooltip="Use Transparent Alpha Channel of Init as Mask", fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER)
    invert_mask = Checkbox(label="Invert Mask", value=arg['invert_mask'], tooltip="Reverse Black & White of Image Mask", fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER)
    image_row = ResponsiveRow([Row([init_image, alpha_mask], col={"lg":6}), Row([mask_image, invert_mask], col={"lg":6})])
    init_image_strength = Slider(min=0.1, max=0.9, divisions=16, label="{value}%", value=float(arg['init_image_strength']), expand=True)
    strength_slider = Row([Text("Init Image Strength: "), init_image_strength])
    img_block = Container(content=Column([image_row, strength_slider]), padding=padding.only(top=4, bottom=3), animate_size=animation.Animation(1000, AnimationCurve.EASE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    img_block.height = None if (status['installed_txt2img'] or status['installed_stability'] or status['installed_SDXL']) else 0
    use_clip_guided_model = Switcher(label="Use CLIP-Guided Model", tooltip="Uses more VRAM, so you'll probably need to make image size smaller", value=arg['use_clip_guided_model'], on_change=toggle_clip)
    clip_guidance_scale = Slider(min=1, max=5000, divisions=4999, label="{value}", value=arg['clip_guidance_scale'], expand=True)
    clip_guidance_scale_slider = Row([Text("CLIP Guidance Scale: "), clip_guidance_scale])
    use_cutouts = Checkbox(label="Use Cutouts", value=bool(arg['use_cutouts']), fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER)
    num_cutouts = NumberPicker(label="    Number of Cutouts: ", min=1, max=10, value=arg['num_cutouts'])
    #num_cutouts.visible = bool(prefs['use_cutouts'])
    #num_cutouts = TextField(label="Number of Cutouts", value=prefs['num_cutouts'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'num_cutouts', asInt=True))
    unfreeze_unet = Checkbox(label="Unfreeze UNET", value=arg['unfreeze_unet'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER)
    unfreeze_vae = Checkbox(label="Unfreeze VAE", value=arg['unfreeze_vae'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER)
    clip_block = Container(Column([clip_guidance_scale_slider, Row([use_cutouts, num_cutouts], expand=False), unfreeze_unet, unfreeze_vae, Divider(height=9, thickness=2)]), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    if not status['installed_clip']:
      use_clip_guided_model.visible = False
      clip_block.height = 0
    elif not arg['use_clip_guided_model']:
      clip_block.height = 0
    edit_dlg = AlertDialog(title=Text("📝  Edit Prompt Dream Parameters"), content=Container(Column([
          Container(content=None, height=7),
          ResponsiveRow([
            edit_text,
            negative_prompt,
          ]),
          #Text("Override any Default Parameters"),
          #use_prompt_tweening,
          #tweening_params,
          #batch_size, n_iterations, steps, eta, seed, guidance,
          param_columns,
          width_slider, height_slider, img_block,
          use_clip_guided_model, clip_block,
          #Row([Column([batch_size, n_iterations, steps, eta, seed,]), Column([guidance, width_slider, height_slider, Divider(height=9, thickness=2), (img_block if prefs['install_img2img'] else Container(content=None))])],),
        ], alignment=MainAxisAlignment.START, width=(e.page.width if e.page.web else e.page.window_width) - 200, height=(e.page.height if e.page.web else e.page.window_height) - 100, scroll=ScrollMode.AUTO), width=(e.page.width if e.page.web else e.page.window_width) - 200, height=(e.page.height if e.page.web else e.page.window_height) - 100),
        actions=[TextButton(content=Text("Cancel", size=18), on_click=close_dlg), ElevatedButton(content=Text(value=emojize(":floppy_disk:") + "  Save Prompt ", size=19, weight=FontWeight.BOLD), on_click=save_dlg)], actions_alignment=MainAxisAlignment.END)
    #open_dlg(e)
    e.page.dialog = edit_dlg
    edit_dlg.open = True
    e.page.update()

def buildPromptsList(page):
  parameter = Ref[ListTile]()
  global prompts, args, prefs
  def changed(e):
      status['changed_prompts'] = True
      page.update()
  def prompt_help(e):
      def close_help_dlg(e):
        nonlocal prompt_help_dlg
        prompt_help_dlg.open = False
        page.update()
      prompt_help_dlg = AlertDialog(title=Text("💁   Help with Prompt Creations"), content=Column([
          Text("You can keep your text prompts simple, or get really complex with it. Just describe the image you want it to dream up with as many details as you can think of. Add artists, styles, colors, adjectives and get creative..."),
          Text('Now you can add prompt weighting, so you can emphasize the strength of certain words between parentheses, and de-emphasize words between brackets. For example: "A (hyper realistic) painting of (magical:1.8) owl with the (((face of a cat))), without [[tail]], in a [twisted:0.6] tree, by Thomas Kinkade"'),
          Text('After adding your prompts, click on a prompt line to edit all the parameters of it. There you can add negative prompts like "lowres, bad_anatomy, error_body, bad_fingers, missing_fingers, error_lighting, jpeg_artifacts, signature, watermark, username, blurry" or anything else you don\'t want.'),
          Text('Then you can override all the parameters for each individual prompt, playing with variations of sizes, steps, guidance scale, init & mask image, seeds, etc.  In the prompts list, you can press the ... options button to duplicate, delete and move prompts in the batch queue.  When ready, Run Diffusion on Prompts...')
        ], scroll=ScrollMode.AUTO), actions=[TextButton("😀  Very nice... ", on_click=close_help_dlg)], actions_alignment=MainAxisAlignment.END)
      page.dialog = prompt_help_dlg
      prompt_help_dlg.open = True
      page.update()
  def paste_prompts(e):
      def save_prompts_list(e):
        plist = enter_text.value.strip()
        prompts_list = plist.split('\n')
        negative_prompt = negative_prompt_text.value
        negative = None
        if bool(negative_prompt):
          if '_' in negative_prompt:
            negative_prompt = nsp_parse(negative_prompt)
          negative = {'negative_prompt': negative_prompt}
        for pr in prompts_list:
          if bool(pr.strip()):
            if '_' in pr:
              pr = nsp_parse(pr)
            add_to_prompts(pr.strip(), negative)
        status['changed_prompts'] = True
        close_dlg(e)
      def close_dlg(e):
          dlg_paste.open = False
          page.update()
      enter_text = TextField(label="Enter Prompts List with multiple lines", expand=True, filled=True, min_lines=30, multiline=True, autofocus=True)
      dlg_paste = AlertDialog(modal=False, title=Text("📝  Paste or Write Prompts List from Simple Text"), content=Container(Column([enter_text], alignment=MainAxisAlignment.START, tight=True, width=(page.width if page.web else page.window_width) - 180, height=(page.height if page.web else page.window_height) - 100, scroll="none"), width=(page.width if page.web else page.window_width) - 180, height=(page.height if page.web else page.window_height) - 100), actions=[TextButton(content=Text("Cancel", size=18), on_click=close_dlg), ElevatedButton(content=Text(value=emojize(":floppy_disk:") + "  Add to Prompts List ", size=19, weight=FontWeight.BOLD), on_click=save_prompts_list)], actions_alignment=MainAxisAlignment.END)
      page.dialog = dlg_paste
      dlg_paste.open = True
      page.update()
  def copy_prompts(e):
      def copy_prompts_list(pl):
        nonlocal text_list, enter_text
        page.set_clipboard(enter_text.value)
        page.snack_bar = SnackBar(content=Text(f"📋   Prompt Text copied to clipboard..."))
        page.snack_bar.open = True
        close_dlg(e)
      def close_dlg(e):
          dlg_copy.open = False
          page.update()
      text_list = ""
      for d in prompts:
          p = d.prompt[0] if type(d.prompt) == list else d.prompt if bool(d.prompt) else d['prompt'] if 'prompt' in d else d.arg['prompt'] if 'prompt' in d.arg else ''
          text_list += f"{p}\n"
      enter_text = TextField(label="Prompts on multiple lines", value=text_list.strip(), expand=True, filled=True, multiline=True, autofocus=True)
      dlg_copy = AlertDialog(modal=False, title=Text("📝  Prompts List as Plain Text"), content=Container(Column([enter_text], alignment=MainAxisAlignment.START, tight=True, width=(page.width if page.web else page.window_width) - 180, height=(page.height if page.web else page.window_height) - 100, scroll="none"), width=(page.width if page.web else page.window_width) - 180, height=(page.height if page.web else page.window_height) - 100), actions=[TextButton(content=Text("Cancel", size=18), on_click=close_dlg), ElevatedButton(content=Text(value=emojize(":floppy_disk:") + "  Copy Prompts List to Clipboard", size=19, weight=FontWeight.BOLD), data=text_list, on_click=lambda ev: copy_prompts_list(text_list))], actions_alignment=MainAxisAlignment.END)
      page.dialog = dlg_copy
      dlg_copy.open = True
      page.update()
  def delete_prompt(e):
      idx = prompts.index(e.control.data)
      prompts.pop(idx)
      prompts_list.controls.pop(idx)
      prompts_list.update()
      status['changed_prompts'] = True
      if prefs['enable_sounds']: page.snd_delete.play()
  def copy_prompt(e):
      open_dream = e.control.data
      page.set_clipboard(open_dream.prompt)
      page.snack_bar = SnackBar(content=Text(f"📋   Prompt Text copied to clipboard..."))
      page.snack_bar.open = True
      page.update()
  def duplicate_prompt(e):
      open_dream = e.control.data
      add_to_prompts(open_dream.prompt, open_dream.arg)
  def duplicate_multiple(e):
      open_dream = e.control.data
      num_times = 2
      def close_dlg(e):
          duplicate_modal.open = False
          page.update()
      def save_dlg(e):
          for i in range(num_times):
            add_to_prompts(open_dream.prompt, open_dream.arg)
          duplicate_modal.open = False
          page.update()
      def change_num(e):
          nonlocal num_times
          num_times = int(e.control.value)
      duplicate_modal = AlertDialog(modal=False, title=Text("🌀  Duplicate Prompt Multiple Times"), content=Container(Column([
            Container(content=None, height=7),
            NumberPicker(label="Number of Copies: ", min=1, max=99, value=num_times, on_change=change_num),
          ], alignment=MainAxisAlignment.START, tight=True, scroll=ScrollMode.AUTO)), actions=[TextButton(content=Text("Cancel", size=18), on_click=close_dlg), ElevatedButton(content=Text(value=emojize(":bowling:") + "  Duplicate Prompt ", size=19, weight=FontWeight.BOLD), on_click=save_dlg)], actions_alignment=MainAxisAlignment.END)
      e.page.dialog = duplicate_modal
      duplicate_modal.open = True
      e.page.update()
  def move_down(e):
      idx = prompts.index(e.control.data)
      if idx < (len(prompts) - 1):
        d = prompts.pop(idx)
        prompts.insert(idx+1, d)
        dr = prompts_list.controls.pop(idx)
        prompts_list.controls.insert(idx+1, dr)
        prompts_list.update()
  def move_up(e):
      idx = prompts.index(e.control.data)
      if idx > 0:
        d = prompts.pop(idx)
        prompts.insert(idx-1, d)
        dr = prompts_list.controls.pop(idx)
        prompts_list.controls.insert(idx-1, dr)
        prompts_list.update()
  def add_prompt(e):
      positive_prompt = prompt_text.value
      negative_prompt = negative_prompt_text.value
      if '_' in positive_prompt:
        positive_prompt = nsp_parse(positive_prompt)
      if bool(negative_prompt):
        if '_' in negative_prompt:
          negative_prompt = nsp_parse(negative_prompt)
        add_to_prompts(positive_prompt, {'negative_prompt': negative_prompt})
      else:
        add_to_prompts(positive_prompt)
  def add_to_prompts(p, arg=None):
      global prompts
      update_args()
      dream = Dream(p)
      if arg is not None:
        if 'prompt' in arg: del arg['prompt']
        arg = merge_dict(args, arg)
        dream.arg = arg
      #if prefs['']
      prompts.append(dream)
      prompts_list.controls.append(ListTile(title=Text(p, max_lines=6, style=TextThemeStyle.BODY_LARGE), dense=True, data=dream, on_click=editPrompt, trailing=PopupMenuButton(icon=icons.MORE_VERT,
          items=[
              PopupMenuItem(icon=icons.EDIT, text="Edit Prompt", on_click=editPrompt, data=dream),
              PopupMenuItem(icon=icons.DELETE, text="Delete Prompt", on_click=delete_prompt, data=dream),
              PopupMenuItem(icon=icons.CONTROL_POINT_DUPLICATE, text="Duplicate Prompt", on_click=duplicate_prompt, data=dream),
              PopupMenuItem(icon=icons.CONTROL_POINT_DUPLICATE_SHARP, text="Duplicate Multiple", on_click=duplicate_multiple, data=dream),
              PopupMenuItem(icon=icons.CONTENT_COPY, text="Copy to Clipboard", on_click=copy_prompt, data=dream),
              PopupMenuItem(icon=icons.ARROW_UPWARD, text="Move Up", on_click=move_up, data=dream),
              PopupMenuItem(icon=icons.ARROW_DOWNWARD, text="Move Down", on_click=move_down, data=dream),
          ],
      )))
      #prompts_list.controls.append(Text("Prompt 1 added to the list of prompts"))
      prompts_list.update()
      if prompts_buttons.visible==False:
          prompts_buttons.visible=True
          prompts_buttons.update()
          if current_tab == 3:
            show_run_diffusion_fab(True)
      if arg is not None:
        update_prompts()
      else:
        prompt_text.focus()
      page.update()
      status['changed_prompts'] = True
  page.add_to_prompts = add_to_prompts

  def save_prompts():
      if len(prompts) > 0:
          #print("Saving your Prompts List")
          prompts_prefs = []
          for d in prompts:
            a = d.arg.copy()
            #a['prompt'] = d.prompt if bool(d.prompt) else d['prompt']
            a['prompt'] = d.prompt[0] if type(d.prompt) == list else d.prompt if bool(d.prompt) else d.arg['prompt'] if 'prompt' in d.arg else ''
            if 'batch_size' in a: del a['batch_size']
            if 'n_iterations' in a: del a['n_iterations']
            if 'precision' in a: del a['precision']
            #a['sampler'] = prefs['generation_sampler'] if prefs['use_Stability_api'] else prefs['scheduler_mode']
            if prefs['use_Stability_api']: del a['eta']
            if 'use_Stability' in a: del a['use_Stability']
            if 'negative_prompt' in a:
              if not bool(a['negative_prompt']): del a['negative_prompt']
            if 'prompt2' in a:
              if not bool(a['prompt2']):
                del a['prompt2']
                del a['tweens']
            if 'init_image' in a:
              if not bool(a['init_image']):
                del a['init_image']
                del a['init_image_strength']
                del a['invert_mask']
                if 'alpha_image' in a:
                  del a['alpha_mask']
              elif bool(a['mask_image']) and 'alpha_image' in a:
                del a['alpha_mask']
            if 'mask_image' in a:
              if not bool(a['mask_image']):
                del a['mask_image']
                if 'alpha_image' in a:
                  del a['alpha_mask']
            if 'use_clip_guided_model' in a:
              if not bool(a['use_clip_guided_model']):
                del a["use_clip_guided_model"]
                del a["clip_prompt"]
                del a["clip_guidance_scale"]
                del a["num_cutouts"]
                del a["use_cutouts"]
                del a["unfreeze_unet"]
                del a["unfreeze_vae"]
              else:
                a["clip_model_id"] = prefs['clip_model_id']
            if 'use_conceptualizer' in a:
              if not bool(a['use_conceptualizer']):
                del a['use_conceptualizer']
            prompts_prefs.append(a)
            #j = json.dumps(a)
          prefs['prompt_list'] = prompts_prefs
  page.save_prompts = save_prompts
  def load_prompts():
      saved_prompts = prefs['prompt_list']
      if len(saved_prompts) > 0:
          for d in saved_prompts:
            #print(f'Loading {d}')
            if 'prompt' not in d: continue
            p = d['prompt']
            page.add_to_prompts(p, d)
          page.update()

  page.load_prompts = load_prompts

  def update_prompts():
      if len(prompts_list.controls) > 0:
        for p in prompts_list.controls:
          diffs = arg_diffs(p.data.arg, args)
          if bool(diffs):
            subtitle = Text("    " + diffs)
          else: subtitle = None
          p.subtitle = subtitle
          p.update()
        prompts_list.update()
  page.update_prompts = update_prompts

  def apply_changes(e):
      global prompts
      if len(prompts_list.controls) > 0:
        i = 0
        for p in prompts_list.controls:
          negative = prompts[i].arg['negative_prompt']
          init = prompts[i].arg['init_image']
          mask = prompts[i].arg['mask_image']
          prompts[i].arg = merge_dict(prompts[i].arg, args)
          prompts[i].arg['negative_prompt'] = negative
          if not bool(args['init_image']):
            prompts[i].arg['init_image'] = init
          if not bool(args['mask_image']):
            prompts[i].arg['mask_image'] = mask
          p.data = prompts[i]
          i += 1
        update_prompts()

  page.apply_changes = apply_changes
  def clear_prompt(e):
      prompt_text.value = ""
      prompt_text.update()
  def clear_negative_prompt(e):
      negative_prompt_text.value = ""
      negative_prompt_text.update()
  def clear_list(e):
      global prompts
      prompts_list.controls = []
      prompts_list.update()
      prompts = []
      prefs['prompt_list'] = []
      prompts_buttons.visible=False
      prompts_buttons.update()
      show_run_diffusion_fab(False)
      e.page.save_prompts()
      save_settings_file(e.page)
      #status['changed_prompts'] = True
      if prefs['enable_sounds']: page.snd_delete.play()
  page.clear_prompts_list = clear_list
  def on_keyboard (e: KeyboardEvent):
      if e.key == "Escape":
        if current_tab == 3:
          clear_prompt(None)
  page.on_keyboard_event = on_keyboard
  def run_diffusion(e):
      if not status['installed_diffusers'] and not status['installed_stability'] and not status['installed_AIHorde']:
        alert_msg(e.page, "You must Install the required Diffusers or Stability api first...")
        return
      if prefs['use_interpolation'] and prefs['install_interpolation'] and not status['installed_interpolation']:
        alert_msg(e.page, "You must Install Walk Interpolation Pipeline first...")
        return
      if len(prompts) < 1:
        if not bool(prompt_text.value):
          alert_msg(p, "Add some Prompts to the Batch List before running, or at least fill in Prompt Text first.")
          return
        add_prompt(e)
      page.tabs.selected_index = 4
      page.tabs.update()
      show_run_diffusion_fab(False)
      if status['changed_prompts']:
        page.save_prompts()
        save_settings_file(page)
        status['changed_prompts'] = False
      page.update()
      start_diffusion(page)
  has_changed = False
  prompts_list = Column([],spacing=1)
  page.prompts_list = prompts_list
  prompt_text = TextField(label="Prompt Text", suffix=IconButton(icons.CLEAR, on_click=clear_prompt), autofocus=True, filled=True, multiline=True, max_lines=6, on_submit=add_prompt, col={'lg':9})
  negative_prompt_text = TextField(label="Segmented Weights 1 | -0.7 | 1.2" if prefs['use_composable'] and status['installed_composable'] else "Negative Prompt Text", filled=True, multiline=True, max_lines=4, suffix=IconButton(icons.CLEAR, on_click=clear_negative_prompt), col={'lg':3})
  add_prompt_button = ElevatedButton(content=Text(value="➕  Add" + (" Prompt" if (page.width if page.web else page.window_width) > 720 else ""), size=17, weight=FontWeight.BOLD), height=52, on_click=add_prompt)
  prompt_help_button = IconButton(icons.HELP_OUTLINE, tooltip="Help with Prompt Creation", on_click=prompt_help)
  copy_prompts_button = IconButton(icons.COPY_ALL, tooltip="Save Prompts as Plain-Text List", on_click=copy_prompts)
  paste_prompts_button = IconButton(icons.CONTENT_PASTE, tooltip="Load Prompts from Plain-Text List", on_click=paste_prompts)
  prompt_row = Row([ResponsiveRow([prompt_text, negative_prompt_text], expand=True, vertical_alignment=CrossAxisAlignment.START), add_prompt_button])
  #diffuse_prompts_button = ElevatedButton(content=Text(value="▶️    Run Diffusion on Prompts ", size=20), on_click=run_diffusion)
  clear_prompts_button = ElevatedButton(content=Text("❌   Clear Prompts List", size=18), on_click=clear_list)
  prompts_buttons = Row([clear_prompts_button], alignment=MainAxisAlignment.SPACE_BETWEEN)
  def show_run_diffusion_fab(show = True, p = None):
    if p is None:
      p = page
    if show:
      p.floating_action_button = FloatingActionButton(content=Row([Icon(icons.PLAY_ARROW), Text("Run Diffusion on Prompts", size=18)], alignment="center", spacing=5), width=270, shape=ft.RoundedRectangleBorder(radius=22), on_click=run_diffusion)
      #page.floating_action_button = FloatingActionButton(icon=icons.PLAY_ARROW, text="Run Diffusion on Prompts", on_click=run_diffusion)
      p.update()
    else:
      if p.floating_action_button is not None:
        p.floating_action_button = None
        try:
          p.update()
        except Exception:
          print("Problem updating page while showing Run Diffusion FAB")
          pass

  page.show_run_diffusion_fab = show_run_diffusion_fab
  show_run_diffusion_fab(False)#(len(prompts_list.controls) > 0)
  #page.load_prompts()
  if len(prompts_list.controls) < 1:
    prompts_buttons.visible=False
  c = Column([Container(
      padding=padding.only(18, 14, 20, 10), content=Column([
        Header("🗒️   List of Prompts to Diffuse", actions=[prompt_help_button, copy_prompts_button, paste_prompts_button]),
        #add_prompt_button,
        prompt_row,
        prompts_list,
        prompts_buttons,
      ],
  ))], scroll=ScrollMode.AUTO)
  return c

def buildImages(page):
    auto_scroll = True
    def auto_scrolling(auto):
      page.imageColumn.auto_scroll = auto
      page.imageColumn.update()
      c.update()
    page.auto_scrolling = auto_scrolling
    page.imageColumn = Column([Text("▶️   Start Run from Prompts List.  Get ready...", style=TextThemeStyle.TITLE_LARGE, color=colors.SECONDARY, weight=FontWeight.BOLD), Divider(thickness=3, height=5, color=colors.SURFACE_VARIANT)], scroll=ScrollMode.AUTO, auto_scroll=True)
    c = Container(padding=padding.only(18, 12, 0, 0), content=page.imageColumn)
    return c

def buildPromptGenerator(page):
    def changed(e, pref=None):
      if pref is not None:
        prefs['prompt_generator'][pref] = e.control.value
      status['changed_prompt_generator'] = True
    page.prompt_generator_list = Column([], spacing=0)
    def add_to_prompt_list(p):
      page.add_to_prompts(p)
      if prefs['enable_sounds']: page.snd_drop.play()
    def add_to_prompt_generator(p):
      page.prompt_generator_list.controls.append(ListTile(title=Text(p, max_lines=3, style=TextThemeStyle.BODY_LARGE), dense=True, on_click=lambda _: add_to_prompt_list(p)))
      page.prompt_generator_list.update()
      generator_list_buttons.visible = True
      generator_list_buttons.update()
    page.add_to_prompt_generator = add_to_prompt_generator
    def click_prompt_generator(e):
      if status['installed_OpenAI']:
        run_prompt_generator(page)
      else:
        alert_msg(page, "You must Install OpenAI GPT-3 Library first before using...")
    def add_to_list(e):
      for p in page.prompt_generator_list.controls:
        page.add_to_prompts(p.title.value)
      if prefs['enable_sounds']: page.snd_drop.play()
    def clear_prompts(e):
      page.prompt_generator_list.controls = []
      page.prompt_generator_list.update()
      #prompts = []
      generator_list_buttons.visible = False
      generator_list_buttons.update()
      if prefs['enable_sounds']: page.snd_delete.play()
    def changed_request(e):
      request_slider.label = generator_request_modes[int(request_slider.value)]
      request_slider.update()
      changed(e, 'request_mode')
    request_slider = Slider(label="{value}", min=0, max=7, divisions=7, expand=True, value=prefs['prompt_generator']['request_mode'], on_change=changed_request)
    AI_engine = Dropdown(label="AI Engine", width=250, options=[dropdown.Option("OpenAI GPT-3"), dropdown.Option("ChatGPT-3.5 Turbo")], value=prefs['prompt_generator']['AI_engine'], on_change=lambda e: changed(e, 'AI_engine'))
    generator_list_buttons = Row([
        ElevatedButton(content=Text("❌   Clear Prompts", size=18), on_click=clear_prompts),
        FilledButton(content=Text("➕  Add All Prompts to List", size=20), on_click=add_to_list)
    ], alignment=MainAxisAlignment.SPACE_BETWEEN)
    if len(page.prompt_generator_list.controls) < 1:
      generator_list_buttons.visible = False
      #generator_list_buttons.update()
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🧠  OpenAI GPT-3 Prompt Genenerator", "Enter a phrase each prompt should start with and the amount of prompts to generate. 'Subject Details' is optional to influence the output. 'Phase as subject' makes it about phrase and subject detail. 'Request mode' is the way it asks for the visual description. Just experiment, AI will continue to surprise."),
        Row([TextField(label="Subject Phrase", expand=True, value=prefs['prompt_generator']['phrase'], multiline=True, on_change=lambda e: changed(e, 'phrase')), TextField(label="Subject Detail", expand=True, hint_text="Optional about detail", value=prefs['prompt_generator']['subject_detail'], multiline=True, on_change=lambda e: changed(e, 'subject_detail')), Checkbox(label="Phrase as Subject", value=prefs['prompt_generator']['phrase_as_subject'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e: changed(e, 'phrase_as_subject'))]),
        ResponsiveRow([
          Row([NumberPicker(label="Amount: ", min=1, max=20, value=prefs['prompt_generator']['amount'], on_change=lambda e: changed(e, 'amount')),
              NumberPicker(label="Random Artists: ", min=0, max=10, value=prefs['prompt_generator']['random_artists'], on_change=lambda e: changed(e, 'random_artists')),], col={'lg':6}, alignment=MainAxisAlignment.SPACE_BETWEEN),
          Row([NumberPicker(label="Random Styles: ", min=0, max=10, value=prefs['prompt_generator']['random_styles'], on_change=lambda e: changed(e, 'random_styles')),
              Checkbox(label="Permutate Artists", value=prefs['prompt_generator']['permutate_artists'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e: changed(e, 'permutate_artists'))], col={'lg':6}, alignment=MainAxisAlignment.SPACE_BETWEEN),
        ]),
        AI_engine,
        ResponsiveRow([
          Row([Text("Request Mode:"), request_slider,], col={'lg':6}),
          Row([Text(" AI Temperature:"), Slider(label="{value}", min=0, max=1, divisions=10, expand=True, value=prefs['prompt_generator']['AI_temperature'], on_change=lambda e: changed(e, 'AI_temperature'))], col={'lg':6}),
        ]),
        ElevatedButton(content=Text("💭   Generate Prompts", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda e: run_prompt_generator(page)),
        page.prompt_generator_list,
        generator_list_buttons,
      ],
    ))], scroll=ScrollMode.AUTO)
    return c

def buildPromptRemixer(page):
    def changed(e, pref=None):
      if pref is not None:
        prefs['prompt_remixer'][pref] = e.control.value
      status['changed_prompt_remixer'] = True
    page.prompt_remixer_list = Column([], spacing=0)
    def click_prompt_remixer(e):
      if status['installed_OpenAI']:
        run_prompt_remixer(page)
      else:
        alert_msg(page, "You must Install OpenAI GPT-3 Library first before using...")
    def add_to_prompt_list(p):
      page.add_to_prompts(p)
      if prefs['enable_sounds']: page.snd_drop.play()
    def add_to_prompt_remixer(p):
      page.prompt_remixer_list.controls.append(ListTile(title=Text(p, max_lines=4, style=TextThemeStyle.BODY_LARGE), dense=True, data=p, on_click=lambda _: add_to_prompt_list(p)))
      page.prompt_remixer_list.update()
      remixer_list_buttons.visible = True
      remixer_list_buttons.update()
    page.add_to_prompt_remixer = add_to_prompt_remixer
    def add_to_list(e):
      if prefs['enable_sounds']: page.snd_drop.play()
      for p in page.prompt_remixer_list.controls:
        page.add_to_prompts(p.data)#(p.title.value)
    def clear_prompts(e):
      if prefs['enable_sounds']: page.snd_delete.play()
      page.prompt_remixer_list.controls = []
      page.prompt_remixer_list.update()
      remixer_list_buttons.visible = False
      remixer_list_buttons.update()
    def changed_request(e):
      request_slider.label = remixer_request_modes[int(request_slider.value)]
      request_slider.update()
      changed(e, 'request_mode')
    request_slider = Slider(label="{value}", min=0, max=8, divisions=8, expand=True, value=prefs['prompt_remixer']['request_mode'], on_change=changed_request)
    AI_engine = Dropdown(label="AI Engine", width=250, options=[dropdown.Option("OpenAI GPT-3"), dropdown.Option("ChatGPT-3.5 Turbo")], value=prefs['prompt_remixer']['AI_engine'], on_change=lambda e: changed(e, 'AI_engine'))
    remixer_list_buttons = Row([
        ElevatedButton(content=Text("❌   Clear Prompts", size=18), on_click=clear_prompts),
        FilledButton(content=Text("Add All Prompts to List", size=20), height=45, on_click=add_to_list),
    ], alignment=MainAxisAlignment.SPACE_BETWEEN)
    if len(page.prompt_remixer_list.controls) < 1:
      remixer_list_buttons.visible = False

    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🔄  Prompt Remixer - GPT-3 AI Helper", "Enter a complete prompt you've written that is well worded and descriptive, and get variations of it with our AI Friend. Experiment.", actions=[ElevatedButton(content=Text("🍜  NSP Instructions", size=18), on_click=lambda _: NSP_instructions(page))]),
        Row([TextField(label="Seed Prompt", expand=True, value=prefs['prompt_remixer']['seed_prompt'], multiline=True, on_change=lambda e: changed(e, 'seed_prompt')), TextField(label="Optional About Detail", expand=True, hint_text="Optional about detail", value=prefs['prompt_remixer']['optional_about_influencer'], multiline=True, on_change=lambda e: changed(e, 'optional_about_influencer'))]),
        ResponsiveRow([
          Row([NumberPicker(label="Amount: ", min=1, max=20, value=prefs['prompt_remixer']['amount'], on_change=lambda e: changed(e, 'amount')),
              NumberPicker(label="Random Artists: ", min=0, max=10, value=prefs['prompt_remixer']['random_artists'], on_change=lambda e: changed(e, 'random_artists')),], col={'lg':6}, alignment=MainAxisAlignment.SPACE_BETWEEN),
          Row([NumberPicker(label="Random Styles: ", min=0, max=10, value=prefs['prompt_remixer']['random_styles'], on_change=lambda e: changed(e, 'random_styles')),
              Checkbox(label="Permutate Artists", value=prefs['prompt_remixer']['permutate_artists'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e: changed(e, 'permutate_artists'))], col={'lg':6}, alignment=MainAxisAlignment.SPACE_BETWEEN),
        ]),
        AI_engine,
        ResponsiveRow([
          Row([Text("Request Mode:"), request_slider,], col={'lg':6}),
          Row([Text(" AI Temperature:"), Slider(label="{value}", min=0, max=1, divisions=10, expand=True, value=prefs['prompt_remixer']['AI_temperature'], on_change=lambda e: changed(e, 'AI_temperature'))], col={'lg':6}),
        ]),
        ElevatedButton(content=Text("🍹   Remix Prompts", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda e: run_prompt_remixer(page)),
        page.prompt_remixer_list,
        remixer_list_buttons,
      ],
    ))], scroll=ScrollMode.AUTO)
    return c

def buildPromptBrainstormer(page):
    def changed(e, pref=None):
      if pref is not None:
        prefs['prompt_brainstormer'][pref] = e.control.value
      status['changed_prompt_brainstormer'] = True
    def click_prompt_brainstormer(e):
      if prefs['prompt_brainstormer']['AI_engine'] == "OpenAI GPT-3":
        if status['installed_OpenAI']:
          run_prompt_brainstormer(page)
        else: alert_msg(page, "You must Install OpenAI GPT-3 Library first before using this Request Mode...")
      elif prefs['prompt_brainstormer']['AI_engine'] == "TextSynth GPT-J":
        if status['installed_TextSynth']:
          run_prompt_brainstormer(page)
        else: alert_msg(page, "You must Install TextSynth GPT-J Library first before using this Request Mode...")
      elif prefs['prompt_brainstormer']['AI_engine'] == "HuggingFace Bloom 176B":
        if bool(prefs['HuggingFace_api_key']):
          run_prompt_brainstormer(page)
        else: alert_msg(page, "You must provide your HuggingFace API Key in settings first before using this Request Mode...")
      elif prefs['prompt_brainstormer']['AI_engine'] == "HuggingFace Flan-T5 XXL":
        if bool(prefs['HuggingFace_api_key']):
          run_prompt_brainstormer(page)
        else: alert_msg(page, "You must provide your HuggingFace API Key in settings first before using this Request Mode...")
    page.prompt_brainstormer_list = Column([], spacing=0)
    def add_to_prompt_brainstormer(p):
      page.prompt_brainstormer_list.controls.append(Text(p, max_lines=3, style=TextThemeStyle.BODY_LARGE, selectable=True))
      page.prompt_brainstormer_list.update()
      brainstormer_list_buttons.visible = True
      brainstormer_list_buttons.update()
    page.add_to_prompt_brainstormer = add_to_prompt_brainstormer
    def add_to_prompts(e):
      page.add_to_prompts(new_prompt_text.value)
    def clear_prompts(e):
      page.prompt_brainstormer_list.controls = []
      page.prompt_brainstormer_list.update()
      brainstormer_list_buttons.visible = False
      brainstormer_list_buttons.update()
    def clear_prompt_text(e):
      new_prompt_text.value = ""
      new_prompt_text.update()

    new_prompt_text = TextField(label="New Prompt Text", expand=True, suffix=IconButton(icons.CLEAR, on_click=clear_prompt_text), autofocus=True, on_submit=add_to_prompts)
    add_to_prompts_button = ElevatedButton("➕  Add to Prompts", on_click=add_to_prompts)#, icon=icons.ADD_ROUNDED
    brainstormer_list_buttons = Row([
        new_prompt_text, add_to_prompts_button,
        ElevatedButton(content=Text("❌   Clear Brainstorms"), on_click=clear_prompts),
    ], alignment=MainAxisAlignment.END)

    if len(page.prompt_brainstormer_list.controls) < 1:
      brainstormer_list_buttons.visible = False
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🤔  Prompt Brainstormer - TextSynth GPT-J-6B, OpenAI GPT-3 & HuggingFace Bloom AI",
               "Enter a complete prompt you've written that is well worded and descriptive, and get variations of it with our AI Friends. Experiment, each has different personalities.", actions=[ElevatedButton(content=Text("🍜  NSP Instructions", size=18), on_click=lambda _: NSP_instructions(page))]),
        Row([Dropdown(label="AI Engine", width=250, options=[dropdown.Option("TextSynth GPT-J"), dropdown.Option("OpenAI GPT-3"), dropdown.Option("ChatGPT-3.5 Turbo"), dropdown.Option("HuggingFace Bloom 176B"), dropdown.Option("HuggingFace Flan-T5 XXL"), dropdown.Option("StableLM 7b"), dropdown.Option("StableLM 3b")], value=prefs['prompt_brainstormer']['AI_engine'], on_change=lambda e: changed(e, 'AI_engine')),
          Dropdown(label="Request Mode", width=250, options=[dropdown.Option("Brainstorm"), dropdown.Option("Write"), dropdown.Option("Rewrite"), dropdown.Option("Edit"), dropdown.Option("Story"), dropdown.Option("Description"), dropdown.Option("Picture"), dropdown.Option("Raw Request")], value=prefs['prompt_brainstormer']['request_mode'], on_change=lambda e: changed(e, 'request_mode')),
        ], alignment=MainAxisAlignment.START),
        Row([TextField(label="About Prompt", expand=True, value=prefs['prompt_brainstormer']['about_prompt'], multiline=True, on_change=lambda e: changed(e, 'about_prompt')),]),
        ElevatedButton(content=Text("⛈️    Brainstorm Prompt", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_prompt_brainstormer(page)),
        page.prompt_brainstormer_list,
        brainstormer_list_buttons,
      ],
    ))], scroll=ScrollMode.AUTO)
    return c

def buildPromptWriter(page):
    def changed(e, pref=None):
      if pref is not None:
        prefs['prompt_writer'][pref] = e.control.value
      status['changed_prompt_writer'] = True
    page.prompt_writer_list = Column([], spacing=0)
    def add_to_prompt_list(p):
      negative_prompt = prefs['prompt_writer']['negative_prompt']
      if bool(negative_prompt):
        if '_' in negative_prompt:
          negative_prompt = nsp_parse(negative_prompt)
        page.add_to_prompts(p, {'negative_prompt':negative_prompt})
      else:
        page.add_to_prompts(p)
      if prefs['enable_sounds']: page.snd_drop.play()
    def add_to_prompt_writer(p):
      page.prompt_writer_list.controls.append(ListTile(title=Text(p, max_lines=3, style=TextThemeStyle.BODY_LARGE), dense=True, on_click=lambda _: add_to_prompt_list(p)))
      page.prompt_writer_list.update()
      writer_list_buttons.visible = True
      writer_list_buttons.update()
    page.add_to_prompt_writer = add_to_prompt_writer

    def add_to_list(e):
      if prefs['enable_sounds']: page.snd_drop.play()
      negative_prompt = prefs['prompt_writer']['negative_prompt']
      if bool(negative_prompt):
        if '_' in negative_prompt:
          negative_prompt = nsp_parse(negative_prompt)
      for p in page.prompt_writer_list.controls:
        if bool(negative_prompt):
          page.add_to_prompts(p.title.value, {'negative_prompt':negative_prompt})
        else:
          page.add_to_prompts(p.title.value)
    def clear_prompts(e):
      if prefs['enable_sounds']: page.snd_delete.play()
      page.prompt_writer_list.controls = []
      page.prompt_writer_list.update()
      writer_list_buttons.visible = False
      writer_list_buttons.update()
    writer_list_buttons = Row([
        ElevatedButton(content=Text("❌   Clear Prompts", size=18), on_click=clear_prompts),
        FilledButton(content=Text("➕  Add All Prompts to List", size=20), on_click=add_to_list),
    ], alignment=MainAxisAlignment.SPACE_BETWEEN)
    if len(page.prompt_writer_list.controls) < 1:
      writer_list_buttons.visible = False

    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("📜 Advanced Prompt Writer with Noodle Soup Prompt random variables ", "Construct your Art descriptions easier, with all the extras you need to engineer perfect prompts faster. Note, you don't have to use any randoms if you rather do all custom.", actions=[ElevatedButton(content=Text("🍜  NSP Instructions", size=18), on_click=lambda _: NSP_instructions(page))]),
        ResponsiveRow([
          TextField(label="Prompt Art Subjects", value=prefs['prompt_writer']['art_Subjects'], on_change=lambda e: changed(e, 'art_Subjects'), multiline=True, max_lines=4, col={'lg':9}),
          TextField(label="Negative Prompt (optional)", value=prefs['prompt_writer']['negative_prompt'], on_change=lambda e: changed(e, 'negative_prompt'), multiline=True, max_lines=4, col={'lg':3}),
        ]),
        Row([TextField(label="by Artists", value=prefs['prompt_writer']['by_Artists'], on_change=lambda e: changed(e, 'by_Artists')),
             TextField(label="Art Styles", value=prefs['prompt_writer']['art_Styles'], on_change=lambda e: changed(e, 'art_Styles')),]),
        ResponsiveRow([
          Row([NumberPicker(label="Amount: ", min=1, max=20, value=prefs['prompt_writer']['amount'], on_change=lambda e: changed(e, 'amount')),
              NumberPicker(label="Random Artists: ", min=0, max=10, value=prefs['prompt_writer']['random_artists'], on_change=lambda e: changed(e, 'random_artists')),], col={'lg':6}, alignment=MainAxisAlignment.SPACE_BETWEEN),
          Row([NumberPicker(label="Random Styles: ", min=0, max=10, value=prefs['prompt_writer']['random_styles'], on_change=lambda e: changed(e, 'random_styles')),
              Checkbox(label="Permutate Artists", value=prefs['prompt_writer']['permutate_artists'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e: changed(e, 'permutate_artists'))], col={'lg':6}, alignment=MainAxisAlignment.SPACE_BETWEEN),
        ]),
        ElevatedButton(content=Text("✍️   Write Prompts", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_prompt_writer(page)),
        page.prompt_writer_list,
        writer_list_buttons,
      ],
    ))], scroll=ScrollMode.AUTO)
    return c

magic_prompt_prefs = {
    'seed_prompt': '',
    'seed': 0,
    'amount': 8,
    'random_artists': 0,
    'random_styles': 0,
    'permutate_artists': True,
}

def buildMagicPrompt(page):
    global magic_prompt_prefs, prefs, status
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          if ptype == "int":
            magic_prompt_prefs[pref] = int(e.control.value)
          elif ptype == "float":
            magic_prompt_prefs[pref] = float(e.control.value)
          else:
            magic_prompt_prefs[pref] = e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    page.magic_prompt_list = Column([], spacing=0)
    page.magic_prompt_output = Column([])
    def add_to_prompt_list(p):
      page.add_to_prompts(p)
      if prefs['enable_sounds']: page.snd_drop.play()
    def add_to_magic_prompt(p):
      page.magic_prompt_list.controls.append(ListTile(title=Text(p, max_lines=3, style=TextThemeStyle.BODY_LARGE), dense=True, on_click=lambda _: add_to_prompt_list(p)))
      page.magic_prompt_list.update()
      magic_list_buttons.visible = True
      magic_list_buttons.update()
    page.add_to_magic_prompt = add_to_magic_prompt
    def add_to_list(e):
      if prefs['enable_sounds']: page.snd_drop.play()
      for p in page.magic_prompt_list.controls:
        page.add_to_prompts(p.title.value)
    def clear_prompts(e):
      if prefs['enable_sounds']: page.snd_delete.play()
      page.magic_prompt_list.controls = []
      page.magic_prompt_list.update()
      magic_list_buttons.visible = False
      magic_list_buttons.update()
    seed = TextField(label="Seed", value=magic_prompt_prefs['seed'], keyboard_type=KeyboardType.NUMBER, width = 90, on_change=lambda e:changed(e,'seed', ptype="int"))
    magic_list_buttons = Row([
        ElevatedButton(content=Text("❌   Clear Prompts", size=18), on_click=clear_prompts),
        FilledButton(content=Text("Add All Prompts to List", size=20), height=45, on_click=add_to_list),
    ], alignment=MainAxisAlignment.SPACE_BETWEEN)
    if len(page.magic_prompt_list.controls) < 1:
      magic_list_buttons.visible = False

    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🎩  Magic Prompt Generator - GPT-2 AI Helper", "Generates new Image Prompts made for Stable Diffusion with a specially trained GPT-2 Text AI by Gustavosta...", actions=[ElevatedButton(content=Text("🍜  NSP Instructions", size=18), on_click=lambda _: NSP_instructions(page))]),
        Row([TextField(label="Starter Prompt Text", expand=True, value=magic_prompt_prefs['seed_prompt'], multiline=True, on_change=lambda e: changed(e, 'seed_prompt'))]),
        ResponsiveRow([
          Row([NumberPicker(label="Amount: ", min=1, max=40, value=magic_prompt_prefs['amount'], on_change=lambda e: changed(e, 'amount')), seed,
              NumberPicker(label="Random Artists: ", min=0, max=10, value=magic_prompt_prefs['random_artists'], on_change=lambda e: changed(e, 'random_artists')),], col={'xl':6}, alignment=MainAxisAlignment.SPACE_BETWEEN),
          Row([NumberPicker(label="Random Styles: ", min=0, max=10, value=magic_prompt_prefs['random_styles'], on_change=lambda e: changed(e, 'random_styles')),
              Checkbox(label="Permutate Artists", value=magic_prompt_prefs['permutate_artists'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e: changed(e, 'permutate_artists'))], col={'xl':6}, alignment=MainAxisAlignment.SPACE_BETWEEN),
        ]),
        ElevatedButton(content=Text("🧙   Make Magic Prompts", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_magic_prompt(page)),
        page.magic_prompt_output,
        page.magic_prompt_list,
        magic_list_buttons,
      ],
    ))], scroll=ScrollMode.AUTO)
    return c

distil_gpt2_prefs = {
    'seed_prompt': '',
    'seed': 0,
    'amount': 8,
    'AI_temperature': 0.9,
    'top_k': 8,
    'max_length': 80,
    'repetition_penalty': 1.2,
    'penalty_alpha': 0.6,
    'no_repeat_ngram_size': 1,
    'random_artists': 0,
    'random_styles': 0,
    'permutate_artists': True,
}

def buildDistilGPT2(page):
    global distil_gpt2_prefs, prefs, status
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          if ptype == "int":
            distil_gpt2_prefs[pref] = int(e.control.value)
          elif ptype == "float":
            distil_gpt2_prefs[pref] = float(e.control.value)
          else:
            distil_gpt2_prefs[pref] = e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    page.distil_gpt2_list = Column([], spacing=0)
    page.distil_gpt2_output = Column([])
    def add_to_prompt_list(p):
      page.add_to_prompts(p)
      if prefs['enable_sounds']: page.snd_drop.play()
    def add_to_distil_gpt2(p):
      page.distil_gpt2_list.controls.append(ListTile(title=Text(p, max_lines=3, style=TextThemeStyle.BODY_LARGE), dense=True, on_click=lambda _: add_to_prompt_list(p)))
      page.distil_gpt2_list.update()
      distil_list_buttons.visible = True
      distil_list_buttons.update()
    page.add_to_distil_gpt2 = add_to_distil_gpt2
    def add_to_list(e):
      if prefs['enable_sounds']: page.snd_drop.play()
      for p in page.distil_gpt2_list.controls:
        page.add_to_prompts(p.title.value)
    def clear_prompts(e):
      if prefs['enable_sounds']: page.snd_delete.play()
      page.distil_gpt2_list.controls = []
      page.distil_gpt2_list.update()
      distil_list_buttons.visible = False
      distil_list_buttons.update()
    AI_temperature = Row([Text("AI Temperature:"), Slider(label="{value}", min=0, max=1, divisions=10, expand=True, tooltip="The value used to module the next token probabilities", value=distil_gpt2_prefs['AI_temperature'], on_change=lambda e: changed(e, 'AI_temperature'))], col={'lg':6})
    top_k = Row([Text("Top-K Samples:"), Slider(label="{value}", min=0, max=50, divisions=50, expand=True, tooltip="Number of highest probability vocabulary tokens to keep for top-k-filtering", value=distil_gpt2_prefs['top_k'], on_change=lambda e: changed(e, 'top_k'))], col={'lg':6})
    #max_length = Row([Text("Max Length:"), Slider(label="{value}", min=0, max=1024, divisions=1024, expand=True, tooltip="The maximum length the generated tokens can have. Corresponds to the length of the input prompt + max_new_tokens.", value=distil_gpt2_prefs['max_length'], on_change=lambda e: changed(e, 'max_length', ptype="int"))], col={'lg':6})
    max_length = SliderRow(label="Max Length", min=0, max=1024, divisions=1024, pref=distil_gpt2_prefs, key='max_length')
    repetition_penalty = Row([Text("Repetition Penalty:"), Slider(label="{value}", min=1.0, max=3.0, divisions=20, expand=True, tooltip="Penalizes repetition by discounting the scores of previously generated tokens", value=distil_gpt2_prefs['repetition_penalty'], on_change=lambda e: changed(e, 'repetition_penalty'))], col={'lg':6})
    penalty_alpha = Row([Text("Penalty Alpha:"), Slider(label="{value}", min=0, max=1, divisions=10, expand=True, tooltip="The degeneration penalty for contrastive search; activate when it is larger than 0", value=distil_gpt2_prefs['penalty_alpha'], on_change=lambda e: changed(e, 'penalty_alpha', ptype="float"))], col={'lg':6})
    no_repeat_ngram_size = Row([Text("No Repeat NGRAM Size:"), Slider(label="{value}", min=0, max=50, expand=True, divisions=50, tooltip="If set > 0, all ngrams of that size can only occur once. 0 adds more commas.", value=distil_gpt2_prefs['no_repeat_ngram_size'], on_change=lambda e: changed(e, 'no_repeat_ngram_size', ptype="int"))], col={'lg':6})
    seed = TextField(label="Seed", value=distil_gpt2_prefs['seed'], keyboard_type=KeyboardType.NUMBER, width = 90, on_change=lambda e:changed(e,'seed', ptype="int"))
    distil_list_buttons = Row([
        ElevatedButton(content=Text("❌   Clear Prompts", size=18), on_click=clear_prompts),
        FilledButton(content=Text("Add All Prompts to List", size=20), height=45, on_click=add_to_list),
    ], alignment=MainAxisAlignment.SPACE_BETWEEN)
    if len(page.distil_gpt2_list.controls) < 1:
      distil_list_buttons.visible = False

    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("⚗️  Distilled GPT-2 Generator - GPT-2 AI Helper", "Generates new Image Prompts with a model trained on 2,470,000 descriptive Stable Diffusion prompts...", actions=[ElevatedButton(content=Text("🍜  NSP Instructions", size=18), on_click=lambda _: NSP_instructions(page))]),
        Row([TextField(label="Starter Prompt Text", expand=True, value=distil_gpt2_prefs['seed_prompt'], multiline=True, on_change=lambda e: changed(e, 'seed_prompt'))]),
        AI_temperature,
        ResponsiveRow([top_k, no_repeat_ngram_size]),
        ResponsiveRow([repetition_penalty, penalty_alpha]),
        max_length,
        ResponsiveRow([
          Row([NumberPicker(label="Amount: ", min=1, max=40, value=distil_gpt2_prefs['amount'], on_change=lambda e: changed(e, 'amount')), seed,
              NumberPicker(label="Random Artists: ", min=0, max=10, value=distil_gpt2_prefs['random_artists'], on_change=lambda e: changed(e, 'random_artists')),], col={'xl':6}, alignment=MainAxisAlignment.SPACE_BETWEEN),
          Row([NumberPicker(label="Random Styles: ", min=0, max=10, value=distil_gpt2_prefs['random_styles'], on_change=lambda e: changed(e, 'random_styles')),
              Checkbox(label="Permutate Artists", value=distil_gpt2_prefs['permutate_artists'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e: changed(e, 'permutate_artists'))], col={'xl':6}, alignment=MainAxisAlignment.SPACE_BETWEEN),
        ]),
        ElevatedButton(content=Text("📝   Make Distil GPT-2 Prompts", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_distil_gpt2(page)),
        page.distil_gpt2_output,
        page.distil_gpt2_list,
        distil_list_buttons,
      ],
    ))], scroll=ScrollMode.AUTO)
    return c


def NSP_instructions(page):
    def open_url(e):
        if e.data.startswith('http'):
          page.launch_url(e.data)
        else:
          page.set_clipboard(e.data)
          page.snack_bar = SnackBar(content=Text(f"📋   NSP variable {e.data} copied to clipboard..."))
          page.snack_bar.open = True
          page.update()
    NSP_markdown = '''To use a term database, simply use any of the keys below in sentence. Copy to Clipboard with click.

For example if you wanted beauty adjective, you would write `_adj-beauty_` in your prompt.

## Terminology Keys (by [@WAS](https://rebrand.ly/easy-diffusion))

### Adjective Types
   - [\_adj-architecture\_](_adj-architecture_) - A list of architectural adjectives and styles
   - [\_adj-beauty\_](_adj-beauty_) - A list of beauty adjectives for people (maybe things?)
   - [\_adj-general\_](_adj-general_) - A list of general adjectives for people/things.
   - [\_adj-horror\_](_adj-horror_) - A list of horror adjectives
### Art Types
   - [\_artist\_](_artist_) - A comprehensive list of artists by [**MisterRuffian**](https://docs.google.com/spreadsheets/d/1_jgQ9SyvUaBNP1mHHEzZ6HhL_Es1KwBKQtnpnmWW82I/edit) (Discord _Misterruffian#2891_)
   - [\_color\_](_color_) - A comprehensive list of colors
   - [\_portrait-type\_](_portrait-type_) - A list of common portrait types/poses
   - [\_style\_](_style_) - A list of art styles and mediums
### Computer Graphics Types
   - [\_3d-terms\_](_3d-terms_) - A list of 3D graphics terminology
   - [\_color-palette\_](_color-palette_) - A list of computer and video game console color palettes
   - [\_hd\_](_hd_) - A list of high definition resolution terms
### Miscellaneous Types
   - [\_details\_](_details_) - A list of detail descriptors
   - [\_site\_](_site_) - A list of websites to query
   - [\_gen-modififer\_](_gen-modififer_) - A list of general modifiers adopted from [Weird Wonderful AI Art](https://weirdwonderfulai.art/)
   - [\_neg-weight\_](_neg-weight_) - A lsit of negative weight ideas
   - [\_punk\_](_punk_) - A list of punk modifier (eg. cyberpunk)
   - [ _pop-culture\_](_pop-culture_) - A list of popular culture movies, shows, etc
   - [\_pop-location\_](_pop-location_) - A list of popular tourist locations
   - [\_fantasy-setting\_](_fantasy-setting_) - A list of fantasy location settings
   - [\_fantasy-creature\_](_fantasy-creature_) - A list of fantasy creatures
   - [\_animals\_](_animals_) - A list of modern animals
### Noun Types
   - [\_noun-beauty\_](_noun-beauty_) - A list of beauty related nouns
   - [\_noun-emote\_](_noun-emote_) - A list of emotions and expressions
   - [\_noun-fantasy\_](_noun-fantasy_) - A list of fantasy nouns
   - [\_noun-general\_](_noun-general_) - A list of general nouns
   - [\_noun-horror\_](_noun-horror_) - A list of horror nouns
### People Types
   - [\_bodyshape\_](_bodyshape_) - A list of body shapes
   - [\_celeb\_](_celeb_) - A list of celebrities
   - [\_eyecolor\_](_eyecolor_) - A list of eye colors
   - [\_hair\_](_hair_) - A list of hair types
   - [\_nationality\_](_nationality_) - A list of nationalities
   - [\_occputation\_](_occputation_) A list of occupation types
   - [\_skin-color\_](_skin-color_) - A list of skin tones
   - [\_identity-young\_](_identity-young_) A list of young identifiers
   - [\_identity-adult\_](_identity-adult_) A list of adult identifiers
   - [\_identity\_](_identity_) A list of general identifiers
### Photography / Image / Film Types
   - [\_aspect-ratio\_](_aspect-ratio_) - A list of common aspect ratios
   - [\_cameras\_](_cameras_) - A list of camera models *(including manufactuerer)*
   - [\_camera-manu\_](_camera-manu_) - A list of camera manufacturers
   - [\_f-stop\_](_f-stop_) - A list of camera aperture f-stop
   - [\_focal-length\_](_focal-length_) - A list of focal length ranges
   - [\_photo-term\_](_photo-term_) - A list of photography terms relating to photos

So in Subject try something like: `A _color_ _noun-general_ that is _adj-beauty_ and _adj-general_ with a _noun-emote_ _noun-fantasy_`
'''
    def close_NSP_dlg(e):
      instruction_alert.open = False
      page.update()
    instruction_alert = AlertDialog(title=Text("🍜  Noodle Soup Prompt Variables Instructions"), content=Column([Markdown(NSP_markdown, extension_set="gitHubWeb", on_tap_link=open_url)], scroll=ScrollMode.AUTO), actions=[TextButton("🍲  Good Soup! ", on_click=close_NSP_dlg)], actions_alignment=MainAxisAlignment.END,)
    page.dialog = instruction_alert
    instruction_alert.open = True
    page.update()

negatives = {
    'Blurry': 'blurry, blur, out of focus, jpeg artifacts, pixelated',
    'Text': 'text, words, signature, watermark',
    'Bad Quality': 'bad quality, low-res, worst quality, grainy',
    'Ugly': 'ugly, deformed, distorted, poorly drawn',
    'Black & White': 'black and white, monochrome, monotone, colorless',
    'Bad Hands': 'bad hands, deformed fingers, fewer digits, fused fingers',
    'Bad Body': 'bad body, disfigured, amputation, wrong anatomy, missing limbs, extra limbs',
    'Bad Eyes': 'bad eyes, deformed iris, deformed pupils, unclear eyes, cross-eyed',
    'Bad Legs': 'bad legs, missing legs, extra legs, missing feet, imperfect feet, bad knee, fused calf',
    'Mutilated': 'mutilated, mutilation, mutated, morbid, bad anatomy',
    'Proportion': 'bad proportions, gross proportions, long neck, long body, malformed',
    'Saturation': 'over saturated, unsaturated, washed out, bad saturation',
    'Contrast': 'high contrast, low contrast, High pass filter',
    'Boring': 'boring, unappealing, tasteless, tacky, lackluster',
    'Simple': 'simple, simplistic, sketch, amateur, plain background',
    'Unrealistic': 'unrealistic, vector, cartoon',
    'Cropped': 'cropped, crop, out of frame, cut off',
    'NSFW': 'NSFW, nude, naked, censored, censor_bar, nipples',
}

def buildNegatives(page):
    global prefs, negatives, status
    def change_neg(e):
        if e.control.data in prefs['negatives']:
            prefs['negatives'].remove(e.control.data)
        else:
            prefs['negatives'].append(e.control.data)
        update_negs()
        status['changed_prompt_generator'] = True
    negs = ""
    def changed_custom(e):
        nonlocal negs
        prefs['custom_negatives'] = e.control.value
        cust = f", {prefs['custom_negatives']}"
        neg_text.value = negs + (cust if bool(prefs['custom_negatives']) else '')
        neg_text.update()
    def update_negs(update=True):
        nonlocal negs
        negs_list = []
        for n in prefs['negatives']:
            negs_list.append(negatives[n])
        negs = ', '.join(negs_list)
        cust = f", {prefs['custom_negatives']}"
        neg_text.value = negs + (cust if bool(prefs['custom_negatives']) else '')
        if update: neg_text.update()
    def copy_clip(e):
        page.set_clipboard(neg_text.value)
        page.snack_bar = SnackBar(content=Text(f"📋  Copied to clipboard... Paste into your Negative Prompt Text."))
        page.snack_bar.open = True
        page.update()
    neg_list = ResponsiveRow(controls=[])
    for k, v in negatives.items():
        neg_list.controls.append(Checkbox(label=k, tooltip=v, data=k, value=k in prefs['negatives'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=change_neg, col={'xs':12, 'sm':6, 'md':3, 'lg':3, 'xl': 2}))
    custom_negatives = TextField(label="Custom Negative Text", value=prefs['custom_negatives'], on_change=changed_custom)
    neg_text = Text(negs, size=18, color=colors.ON_SECONDARY_CONTAINER, selectable=True)
    update_negs(False)
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🚫   Negative Prompt Builder", "Generate your Negatives with ease to subtract what you don't want in your images."),
        neg_list,
        custom_negatives,
        ElevatedButton(content=Text("➖  Copy Negs to Clipboard", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=copy_clip),
        Container(neg_text, bgcolor=colors.SECONDARY_CONTAINER, padding=10, border_radius=border_radius.all(12), alignment = alignment.center, margin=margin.only(top=10)),#], alignment=CrossAxisAlignment.CENTER),
      ],
    ))], scroll=ScrollMode.AUTO)
    return c

def buildPromptStyler(page):
    global prefs, status
    def styler_help(e):
        def close_styler_dlg(e):
          nonlocal styler_help_dlg
          styler_help_dlg.open = False
          page.update()
        styler_help_dlg = AlertDialog(title=Text("💁   Help with Prompt Styler"), content=Column([
            Text("This allows you to take a simple base prompt and apply a preset style to the the positive and negative prompt variables.  You can then add that stylized prompt to your Prompts List, or copy/paste it for other Image Generators."),
            Markdown("Credit goes to [Prompt Styler](https://github.com/twri/sdxl_prompt_styler) by twri and [Fooocus UI](https://github.com/lllyasviel/Fooocus) by Illyasviel for the Styler presets, which is a pretty good GUI alternative for easy SDXL generation. Launch [Fooocus Colab](https://colab.research.google.com/github/camenduru/Fooocus-colab/blob/main/Fooocus_colab.ipynb) and read this [Fooocus Style Reference Doc](https://docs.google.com/spreadsheets/d/1AF5bd-fALxlu0lguZQiQVn1yZwxUiBJGyh2eyJJWl74/edit#gid=0)...", on_tap_link=lambda e: e.page.launch_url(e.data)),
          ], scroll=ScrollMode.AUTO), actions=[TextButton("💅  So Stylish... ", on_click=close_styler_dlg)], actions_alignment=MainAxisAlignment.END)
        page.dialog = styler_help_dlg
        styler_help_dlg.open = True
        page.update()
    def change_style(e):
        prefs['prompt_style'] = e.control.value
        update_style()
        status['changed_prompt_generator'] = True
    negative = ""
    prompt = ""
    def changed_custom(e):
        nonlocal prompt
        prefs['prompt_styler'] = e.control.value
        styler = sdd_utils.prompt_styles[prefs['prompt_style']]
        prompt = styler[0].replace("{prompt}", prefs['prompt_styler'])
        prompt = to_title(prompt, sentence=True, clean=False)
        prompt_text.value = prompt
        prompt_text.update()
        status['changed_prompt_generator'] = True
    def update_style(update=True):
        nonlocal negative, prompt
        styler = sdd_utils.prompt_styles[prefs['prompt_style']]
        prompt = styler[0].replace("{prompt}", prefs['prompt_styler'])
        prompt = to_title(prompt, sentence=True, clean=False)
        negative = styler[1]
        prompt_text.value = prompt
        neg_text.value = negative
        if update:
          prompt_text.update()
          neg_text.update()
    def add_to_prompts(e):
        nonlocal negative, prompt
        page.add_to_prompts(prompt, {'negative_prompt': negative})
        if prefs['enable_sounds']: page.snd_drop.play()
    def copy_clip(e):
        page.set_clipboard(neg_text.value)
        page.snack_bar = SnackBar(content=Text(f"📋  Copied to clipboard... Paste into your Negative Prompt Text."))
        page.snack_bar.open = True
        page.update()
    style_list = ResponsiveRow(controls=[], run_spacing=0)
    for k in sdd_utils.style_keys:
        style_list.controls.append(ft.Radio(label=k, value=k, fill_color=colors.PRIMARY_CONTAINER, col={'xs':12, 'sm':6, 'md':3, 'lg':3, 'xl': 2}))
    prompt_styler = TextField(label="Subject Prompt Text", value=prefs['prompt_styler'], filled=True, on_change=changed_custom)
    prompt_text = Text(prompt, size=18, color=colors.ON_SECONDARY_CONTAINER, selectable=True)
    neg_text = Text(negative, size=18, color=colors.ON_SECONDARY_CONTAINER, selectable=True)
    update_style(False)
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("👓   Prompt Styler", "Generate your Prompts with Premade Style Templates.", actions=[IconButton(icon=icons.HELP, tooltip="Help with Prompt Styler", on_click=styler_help)]),
        ft.RadioGroup(content=style_list, value=prefs['prompt_style'], on_change=change_style),
        prompt_styler,
        ElevatedButton(content=Text("➕  Add to Prompts List", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=add_to_prompts),
        ResponsiveRow([
          Container(prompt_text, bgcolor=colors.SECONDARY_CONTAINER, padding=10, border_radius=border_radius.all(12), margin=margin.only(top=10), col={'md': 8}),
          Container(neg_text, bgcolor=colors.SECONDARY_CONTAINER, padding=10, border_radius=border_radius.all(12), margin=margin.only(top=10), col={'md': 4}),
        ], vertical_alignment=CrossAxisAlignment.START),
      ],
    ))], scroll=ScrollMode.AUTO)
    return c


ESRGAN_prefs = {
    'enlarge_scale': 1.5,
    'face_enhance': False,
    'image_path': '',
    'save_to_GDrive': True,
    'upload_file': False,
    'download_locally': False,
    'display_image': False,
    'dst_image_path': '',
    'filename_suffix': '',
    'split_image_grid': False,
    'rows': 3,
    'cols': 3,
}
def buildESRGANupscaler(page):
    def changed(e, pref=None):
      if pref is not None:
        ESRGAN_prefs[pref] = e.control.value
    def add_to_ESRGAN_output(o):
      ESRGAN_output.controls.append(o)
      ESRGAN_output.update()
      if clear_button.visible == False:
        clear_button.visible = True
        clear_button.update()
      #generator_list_buttons.visible = True
      #generator_list_buttons.update()
    page.add_to_ESRGAN_output = add_to_ESRGAN_output
    def toggle_split(e):
      split_container.height = None if e.control.value else 0
      changed(e, 'split_image_grid')
      split_container.update()
    def clear_output(e):
      if prefs['enable_sounds']: page.snd_delete.play()
      ESRGAN_output.controls = []
      ESRGAN_output.update()
      clear_button.visible = False
      clear_button.update()
    page.clear_ESRGAN_output = clear_output
    def file_picker_result(e: FilePickerResultEvent):
        if e.files != None:
          upload_files(e)
    def on_upload_progress(e: FilePickerUploadEvent):
      if e.progress == 1:
        if not slash in e.file_name:
          fname = os.path.join(root_dir, e.file_name)
        else:
          fname = e.file_name
        image_path.value = fname
        image_path.update()
        ESRGAN_prefs['image_path'] = fname
        page.update()
    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)
    def pick_path(e):
        file_picker.pick_files(allow_multiple=False, allowed_extensions=["png", "PNG", "jpg", "jpeg"], dialog_title="Pick Image File to Enlarge")
    def upload_files(e):
        uf = []
        if file_picker.result != None and file_picker.result.files != None:
            for f in file_picker.result.files:
              if page.web:
                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))
              else:
                on_upload_progress(FilePickerUploadEvent(f.path, 1, ""))
            file_picker.upload(uf)
    def pick_destination(e):
        alert_msg(page, "Switch to Colab tab and press Files button on the Left & Find the Path you want to Save Images into, Right Click and Copy Path, then Paste here")
    page.overlay.append(file_picker)
    enlarge_scale_slider = SliderRow(label="Enlarge Scale", min=1, max=4, divisions=6, round=1, suffix="x", pref=ESRGAN_prefs, key='enlarge_scale')
    face_enhance = Checkbox(label="Use Face Enhance GPFGAN", value=ESRGAN_prefs['face_enhance'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'face_enhance'))
    image_path = TextField(label="Image File or Folder Path", value=ESRGAN_prefs['image_path'], col={'md':6}, on_change=lambda e:changed(e,'image_path'), suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_path))
    dst_image_path = TextField(label="Destination Image Path", value=ESRGAN_prefs['dst_image_path'], col={'md':6}, on_change=lambda e:changed(e,'dst_image_path'), suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD_OUTLINED, on_click=pick_destination))
    filename_suffix = TextField(label="Optional Filename Suffix", hint_text="-big", value=ESRGAN_prefs['filename_suffix'], on_change=lambda e:changed(e,'filename_suffix'), width=260)
    download_locally = Checkbox(label="Download Images Locally", value=ESRGAN_prefs['download_locally'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'download_locally'))
    display_image = Checkbox(label="Display Upscaled Image", value=ESRGAN_prefs['display_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_image'))
    split_image_grid = Switcher(label="Split Image Grid", value=ESRGAN_prefs['split_image_grid'], on_change=toggle_split)
    rows = NumberPicker(label="Rows: ", min=1, max=8, value=ESRGAN_prefs['rows'], on_change=lambda e: changed(e, 'rows'))
    cols = NumberPicker(label="Columns: ", min=1, max=8, value=ESRGAN_prefs['cols'], on_change=lambda e: changed(e, 'cols'))
    split_container = Container(Row([rows, Container(content=None, width=25), cols]), animate_size=animation.Animation(800, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE, padding=padding.only(left=28), height=0)
    ESRGAN_output = Column([])
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(ESRGAN_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("↕️   Real-ESRGAN AI Upscale Enlarging", "Select one or more files, or give path to image or folder. Save to your Google Drive and/or Download."),
        enlarge_scale_slider,
        face_enhance,
        ResponsiveRow([image_path, dst_image_path]),
        filename_suffix,
        download_locally,
        display_image,
        #Divider(thickness=2, height=4),
        split_image_grid,
        split_container,
        ElevatedButton(content=Text("🐘  Run AI Upscaling", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_upscaling(page)),
        ESRGAN_output,
        clear_button,
      ],
    ))], scroll=ScrollMode.AUTO)
    return c

retrieve_prefs = {
    'image_path': '',
    'add_to_prompts': True,
    'display_full_metadata': False,
    'display_image': False,
    'upload_file': False,
}
def buildRetrievePrompts(page):
    def changed(e, pref=None):
        if pref is not None:
          retrieve_prefs[pref] = e.control.value
    def add_to_retrieve_output(o):
      retrieve_output.controls.append(o)
      retrieve_output.update()
    def clear_output(e):
      if prefs['enable_sounds']: page.snd_delete.play()
      retrieve_output.controls = []
      retrieve_output.update()
      clear_button.visible = False
      clear_button.update()
    def pick_image(e):
        alert_msg(page, "Switch to Colab tab and press Files button on the Left & Find the Path you want to Retrieve, Right Click and Copy Path, then Paste here")
    page.add_to_retrieve_output = add_to_retrieve_output
    image_path = TextField(label="Image File or Folder Path", value=retrieve_prefs['image_path'], on_change=lambda e:changed(e,'image_path'), suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_image))
    add_to_prompts = Checkbox(label="Add to Prompts", value=retrieve_prefs['add_to_prompts'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'add_to_prompts'))
    display_full_metadata = Checkbox(label="Display Full Metadata", value=retrieve_prefs['display_full_metadata'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_full_metadata'))
    display_image = Checkbox(label="Display Image", value=retrieve_prefs['display_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_image'))
    retrieve_output = Column([])
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(retrieve_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("📰  Retrieve Dream Prompts from Image Metadata", "Give it images made here and gives you all parameters used to recreate it. Either upload png file(s) or paste path to image or folder or config.json to revive your dreams.."),
        image_path,
        add_to_prompts,
        display_full_metadata,
        display_image,
        ElevatedButton(content=Text("😴  Retrieve Dream", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_retrieve(page)),
        retrieve_output,
        clear_button,
      ],
    ))], scroll=ScrollMode.AUTO)
    return c

initfolder_prefs = {
    'prompt_string': '',
    'negative_prompt': '',
    'init_folder': '',
    'include_strength': True,
    'image_strength': 0.5,
}
def buildInitFolder(page):
    def changed(e, pref=None):
        if pref is not None:
          initfolder_prefs[pref] = e.control.value
    def add_to_initfolder_output(o):
      initfolder_output.controls.append(o)
      initfolder_output.update()
    def clear_output(e):
      if prefs['enable_sounds']: page.snd_delete.play()
      initfolder_output.controls = []
      initfolder_output.update()
      clear_button.visible = False
      clear_button.update()
    def pick_init(e):
        alert_msg(page, "Switch to Colab tab and press Files button on the Left & Find the Path you want to use as Init Folder, Right Click and Copy Path, then Paste here")
    def toggle_strength(e):
      changed(e,'include_strength')
      strength_row.visible = e.control.value
      strength_row.update()
    page.add_to_initfolder_output = add_to_initfolder_output
    prompt_string = TextField(label="Prompt Text", value=initfolder_prefs['prompt_string'], col={'md': 9}, on_change=lambda e:changed(e,'prompt_string'))
    negative_prompt  = TextField(label="Negative Prompt Text", value=initfolder_prefs['negative_prompt'], col={'md':3}, on_change=lambda e:changed(e,'negative_prompt'))

    init_folder = TextField(label="Init Image Folder Path", value=initfolder_prefs['init_folder'], on_change=lambda e:changed(e,'init_folder'), suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_init))
    include_strength = Checkbox(label="Include Strength", value=initfolder_prefs['include_strength'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=toggle_strength)
    image_strength = Slider(min=0.1, max=0.9, divisions=16, label="{value}%", value=float(initfolder_prefs['image_strength']), expand=True)
    strength_row = Row([Text("Image Strength:"), image_strength])
    strength_row.visible = initfolder_prefs['include_strength']
    initfolder_output = Column([])
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(initfolder_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("📂 Generate Prompts from Folder as Init Images", "Provide a Folder with a collection of images that you want to automatically add to prompts list with init_image overides..."),
        init_folder,
        ResponsiveRow([prompt_string, negative_prompt]),
        include_strength,
        strength_row,
        ElevatedButton(content=Text("➕  Add to Prompts", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_initfolder(page)),
        initfolder_output,
        clear_button,
      ]
    ))], scroll=ScrollMode.AUTO)
    return c

init_video_prefs = {
    'prompt': '',
    'negative_prompt': '',
    'init_folder': '',
    'include_strength': False,
    'image_strength': 0.5,
    'max_size': 960,
    'file_prefix': 'frame-',
    'video_file': '',
    'fps': 15,
    'start_time': 0.0,
    'end_time': 0.0,
    'batch_folder_name': '',
    'show_images': False,
}
def buildInitVideo(page):
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          if ptype == "int":
            init_video_prefs[pref] = int(e.control.value)
          elif ptype == "float":
            init_video_prefs[pref] = float(e.control.value)
          else:
            init_video_prefs[pref] = e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def add_to_init_video_output(o):
      init_video_output.controls.append(o)
      init_video_output.update()
      if clear_button.visible == False:
          clear_button.visible = True
          clear_button.update()
    def clear_output(e):
      if prefs['enable_sounds']: page.snd_delete.play()
      init_video_output.controls = []
      init_video_output.update()
      clear_button.visible = False
      clear_button.update()
    def file_picker_result(e: FilePickerResultEvent):
        if e.files != None:
          upload_files(e)
    def on_upload_progress(e: FilePickerUploadEvent):
      if e.progress == 1:
        if not slash in e.file_name:
          fname = os.path.join(root_dir, e.file_name)
          init_video_prefs['file_name'] = e.file_name.rpartition('.')[0]
        else:
          fname = e.file_name
          init_video_prefs['file_name'] = e.file_name.rpartition(slash)[2].rpartition('.')[0]
        video_file.value = fname
        video_file.update()
        init_video_prefs['video_file'] = fname
        page.update()
    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)
    def upload_files(e):
        uf = []
        if file_picker.result != None and file_picker.result.files != None:
            for f in file_picker.result.files:
              if page.web:
                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))
              else:
                on_upload_progress(FilePickerUploadEvent(f.path, 1, ""))
            file_picker.upload(uf)
    page.overlay.append(file_picker)
    #page.overlay.append(pick_files_dialog)
    def pick_video(e):
        file_picker.pick_files(allow_multiple=False, allowed_extensions=["avi", "mp4", "mov"], dialog_title="Pick Video File")
    def toggle_strength(e):
      changed(e,'include_strength')
      strength_row.visible = e.control.value
      strength_row.update()
    page.add_to_init_video_output = add_to_init_video_output
    prompt = TextField(label="Prompt Text", value=init_video_prefs['prompt'], col={'md': 9}, on_change=lambda e:changed(e,'prompt'))
    negative_prompt  = TextField(label="Negative Prompt Text", value=init_video_prefs['negative_prompt'], col={'md':3}, on_change=lambda e:changed(e,'negative_prompt'))
    batch_folder_name = TextField(label="Batch Folder Name", value=init_video_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    file_prefix = TextField(label="Image Filename Prefix", value=init_video_prefs['file_prefix'], width=150, on_change=lambda e:changed(e,'file_prefix'))
    video_file = TextField(label="Video File", value=init_video_prefs['video_file'], on_change=lambda e:changed(e,'video_file'), height=60, suffix=IconButton(icon=icons.VIDEO_CALL, on_click=pick_video))
    #init_folder = TextField(label="Init Image Folder Path", value=init_video_prefs['init_folder'], on_change=lambda e:changed(e,'init_folder'), suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_init))
    fps = TextField(label="Frames Per Seccond", value=init_video_prefs['fps'], width=145, keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'fps', ptype="int"))
    start_time = TextField(label="Start Time (s)", value=init_video_prefs['start_time'], width=145, keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'start_time', ptype="float"))
    end_time = TextField(label="End Time (0 for all)", value=init_video_prefs['end_time'], width=145, keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'end_time', ptype="float"))
    max_row = SliderRow(label="Max Resolution Size", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=init_video_prefs, key='max_size')
    show_images = Checkbox(label="Show Extracted Images", value=init_video_prefs['show_images'], tooltip="Fills up screen with all frames, you probably don't need to.", fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'show_images'))
    include_strength = Checkbox(label="Include Strength   ", value=init_video_prefs['include_strength'], tooltip="Otherwise defaults to setting in Image Parameters", fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=toggle_strength)
    image_strength = Slider(min=0.1, max=0.9, divisions=16, label="{value}%", value=float(init_video_prefs['image_strength']), expand=True)
    strength_row = Row([Text("Image Strength:"), image_strength])
    strength_row.visible = init_video_prefs['include_strength']
    init_video_output = Column([])
    page.init_video_output = init_video_output
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Prompts List", size=18), on_click=page.clear_prompts_list), ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(init_video_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🎥 Generate Prompts from Video File Frames", "Provide a short video clip to automatically add sequence to prompts list with init_image overides..."),
        video_file, #init_folder,
        Row([fps, start_time, end_time]),
        Row([batch_folder_name, file_prefix]),
        ResponsiveRow([prompt, negative_prompt]),
        Row([include_strength, show_images]),
        strength_row,
        max_row,
        ElevatedButton(content=Text("⏩  Frames to Prompts", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_init_video(page)),
        init_video_output,
        clear_button,
      ]
    ))], scroll=ScrollMode.AUTO)
    return c

image2text_prefs = {
    'mode': 'best',
    'request_mode': 'Caption',
    'slow_workers': True,
    'trusted_workers': False,
    'folder_path': '',
    'image_path': '',
    'max_size': 768,
    'save_csv': False,
    'images': [],
    'use_AIHorde': False,
}

def buildImage2Text(page):
    global prefs, image2text_prefs
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          if ptype == "int":
            image2text_prefs[pref] = int(e.control.value)
          elif ptype == "float":
            image2text_prefs[pref] = float(e.control.value)
          else:
            image2text_prefs[pref] = e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def add_to_image2text_output(o):
      page.image2text_output.controls.append(o)
      page.image2text_output.update()
    def clear_output(e):
      if prefs['enable_sounds']: page.snd_delete.play()
      page.image2text_output.controls = []
      page.image2text_output.update()
      save_dir = os.path.join(root_dir, 'image2text')
      if os.path.exists(save_dir):
        for f in os.listdir(save_dir):
            os.remove(os.path.join(save_dir, f))
        os.rmdir(save_dir)
      page.image2text_file_list.controls = []
      page.image2text_file_list.update()
      image2text_list_buttons.visible = False
      image2text_list_buttons.update()
    def i2t_help(e):
      def close_i2t_dlg(e):
        nonlocal i2t_help_dlg
        i2t_help_dlg.open = False
        page.update()
      i2t_help_dlg = AlertDialog(title=Text("💁   Help with Image2Text CLIP Interrogator"), content=Column([
          Text(""),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("😪  Okay then... ", on_click=close_i2t_dlg)], actions_alignment=MainAxisAlignment.END)
      page.dialog = i2t_help_dlg
      i2t_help_dlg.open = True
      page.update()
    def file_picker_result(e: FilePickerResultEvent):
        if e.files != None:
          upload_files(e)
    def on_upload_progress(e: FilePickerUploadEvent):
      if e.progress == 1:
        save_dir = os.path.join(root_dir, 'image2text')
        if not os.path.exists(save_dir):
          os.mkdir(save_dir)
        image2text_prefs['folder_path'] = save_dir
        if not slash in e.file_name:
          fname = os.path.join(root_dir, e.file_name)
          fpath = os.path.join(save_dir, e.file_name)
        else:
          fname = e.file_name
          fpath = os.path.join(save_dir, e.file_name.rpartition(slash)[2])
        original_img = PILImage.open(fname)
        width, height = original_img.size
        width, height = scale_dimensions(width, height, image2text_prefs['max_size'])
        original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert("RGB")
        original_img.save(fpath)
        #shutil.move(fname, fpath)
        page.image2text_file_list.controls.append(ListTile(title=Text(fpath), dense=True))
        page.image2text_file_list.update()
        image2text_prefs['images'].append(fpath)
    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)
    def pick_path(e):
        file_picker.pick_files(allow_multiple=True, allowed_extensions=["png", "PNG", "jpg", "jpeg"], dialog_title="Pick Image File to Enlarge")
    def upload_files(e):
        uf = []
        if file_picker.result != None and file_picker.result.files != None:
            for f in file_picker.result.files:
              if page.web:
                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))
              else:
                on_upload_progress(FilePickerUploadEvent(f.path, 1, ""))
                #uf.append(FilePickerUploadFile(f.name, upload_url=f.path))
            file_picker.upload(uf)
    page.overlay.append(file_picker)
    def add_image(e):
        save_dir = os.path.join(root_dir, 'image2text')
        if not os.path.exists(save_dir):
          os.mkdir(save_dir)
        image2text_prefs['folder_path'] = save_dir
        if image_path.value.startswith('http'):
          import requests
          from io import BytesIO
          response = requests.get(image_path.value)
          fpath = os.path.join(save_dir, image_path.value.rpartition(slash)[2])
          original_img = PILImage.open(BytesIO(response.content)).convert("RGB")
          width, height = original_img.size
          width, height = scale_dimensions(width, height, image2text_prefs['max_size'])
          original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert("RGB")
          original_img.save(fpath)
          page.image2text_file_list.controls.append(ListTile(title=Text(fpath), dense=True))
          page.image2text_file_list.update()
          image2text_prefs['images'].append(fpath)
        elif os.path.isfile(image_path.value):
          fpath = os.path.join(save_dir, image_path.value.rpartition(slash)[2])
          original_img = PILImage.open(image_path.value)
          width, height = original_img.size
          width, height = scale_dimensions(width, height, image2text_prefs['max_size'])
          original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert("RGB")
          original_img.save(fpath)
          #shutil.copy(image_path.value, fpath)
          page.image2text_file_list.controls.append(ListTile(title=Text(fpath), dense=True))
          page.image2text_file_list.update()
          image2text_prefs['images'].append(fpath)
        elif os.path.isdir(image_path.value):
          for f in os.listdir(image_path.value):
            file_path = os.path.join(image_path.value, f)
            if os.path.isdir(file_path): continue
            if f.lower().endswith(('.png', '.jpg', '.jpeg')):
              fpath = os.path.join(save_dir, f)
              original_img = PILImage.open(file_path)
              width, height = original_img.size
              width, height = scale_dimensions(width, height, image2text_prefs['max_size'])
              original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert("RGB")
              original_img.save(fpath)
              #shutil.copy(file_path, fpath)
              page.image2text_file_list.controls.append(ListTile(title=Text(fpath), dense=True))
              page.image2text_file_list.update()
              image2text_prefs['images'].append(fpath)
        else:
          if bool(image_path.value):
            alert_msg(page, "Couldn't find a valid File, Path or URL...")
          else:
            pick_path(e)
          return
        image_path.value = ""
        image_path.update()
    page.image2text_list = Column([], spacing=0)
    def add_to_prompt_list(p):
      page.add_to_prompts(p)
      if prefs['enable_sounds']: page.snd_drop.play()
    def add_to_image2text(p):
      page.image2text_list.controls.append(ListTile(title=Text(p, max_lines=3, style=TextThemeStyle.BODY_LARGE), dense=True, on_click=lambda _: add_to_prompt_list(p)))
      page.image2text_list.update()
      image2text_list_buttons.visible = True
      image2text_list_buttons.update()
    page.add_to_image2text = add_to_image2text
    def add_to_list(e):
      if prefs['enable_sounds']: page.snd_drop.play()
      for p in page.image2text_list.controls:
        page.add_to_prompts(p.title.value)
    def toggle_AIHorde(e):
      use = e.control.value
      changed(e,'use_AIHorde')
      AIHorde_row.height=None if use else 0
      AIHorde_row.update()
      mode.visible = not use
      mode.update()
      request_mode.visible = use
      request_mode.update()
    def clear_prompts(e):
      if prefs['enable_sounds']: page.snd_delete.play()
      page.image2text_list.controls = []
      page.image2text_list.update()
      prompts = []
      image2text_list_buttons.visible = False
      image2text_list_buttons.update()
    image2text_list_buttons = Row([
        ElevatedButton(content=Text("❌   Clear Prompts"), on_click=clear_prompts),
        FilledButton(content=Text("➕  Add All Prompts to List", size=20), height=45, on_click=add_to_list),
    ], alignment=MainAxisAlignment.SPACE_BETWEEN)
    if len(page.image2text_list.controls) < 1:
      image2text_list_buttons.visible = False

    mode = Dropdown(label="Interrogation Mode", width=250, options=[dropdown.Option("best"), dropdown.Option("classic"), dropdown.Option("fast")], value=image2text_prefs['mode'], on_change=lambda e: changed(e, 'mode'))
    use_AIHorde = Switcher(label="Use AIHorde Crowdsourced Interrogator", value=image2text_prefs['use_AIHorde'], on_change=toggle_AIHorde)
    request_mode = Dropdown(label="Request Mode", width=250, options=[dropdown.Option("Caption"), dropdown.Option("Interrogation"), dropdown.Option("Full Prompt")], value=image2text_prefs['request_mode'], visible=False, on_change=lambda e: changed(e, 'request_mode'))
    slow_workers = Checkbox(label="Allow Slow Workers", tooltip="", value=image2text_prefs['slow_workers'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'slow_workers'))
    trusted_workers = Checkbox(label="Only Trusted Workers", tooltip="", value=image2text_prefs['trusted_workers'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'trusted_workers'))
    AIHorde_row = Container(content=Row([slow_workers, trusted_workers]), height=None if image2text_prefs['use_AIHorde'] else 0, animate_size=animation.Animation(800, AnimationCurve.EASE_OUT_CIRC), clip_behavior=ClipBehavior.HARD_EDGE)
    save_csv = Checkbox(label="Save CSV file of Prompts", tooltip="", value=image2text_prefs['save_csv'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'save_csv'))
    max_row = SliderRow(label="Max Resolution Size", min=256, max=1024, divisions=32, multiple=16, suffix="px", pref=image2text_prefs, key='max_size')
    image_path = TextField(label="Image File or Folder Path or URL to Train", value=image2text_prefs['image_path'], on_change=lambda e:changed(e,'image_path'), suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_path), expand=1)
    add_image_button = ElevatedButton(content=Text("Add File or Folder"), on_click=add_image)
    page.image2text_file_list = Column([], tight=True, spacing=0)
    page.image2text_output = Column([])
    #clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    #clear_button.visible = len(page.image2text_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("😶‍🌫️  Image2Text CLIP-Interrogator", subtitle="Create text prompts by describing input images...", actions=[IconButton(icon=icons.HELP, tooltip="Help with Image2Text Interrogator", on_click=i2t_help)]),
        #mode,
        Row([mode, request_mode, use_AIHorde]),
        AIHorde_row,
        max_row,
        Row([image_path, add_image_button]),
        page.image2text_file_list,
        ElevatedButton(content=Text("👨‍🎨️  Get Prompts from Images", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_image2text(page)),
        page.image2text_list,
        image2text_list_buttons,
        page.image2text_output,
      ],
    ))], scroll=ScrollMode.AUTO)
    return c

BLIP2_image2text_prefs = {
    'folder_path': '',
    'image_path': '',
    'max_size': 768,
    'num_captions': 1,
    'question_prompt': '',
    #'model_name': 'blip2_t5',
    'model_type': 'pretrain_flant5xxl', #pretrain_opt2.7b, pretrain_opt6.7b, caption_coco_opt2.7b, caption_coco_opt6.7b, pretrain_flant5xl, caption_coco_flant5xl
    'images': [],
}

def buildBLIP2Image2Text(page):
    global prefs, BLIP2_image2text_prefs
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          if ptype == "int":
            BLIP2_image2text_prefs[pref] = int(e.control.value)
          elif ptype == "float":
            BLIP2_image2text_prefs[pref] = float(e.control.value)
          else:
            BLIP2_image2text_prefs[pref] = e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def add_to_BLIP2_image2text_output(o):
      page.BLIP2_image2text_output.controls.append(o)
      page.BLIP2_image2text_output.update()
    def clear_output(e):
      if prefs['enable_sounds']: page.snd_delete.play()
      page.BLIP2_image2text_output.controls = []
      page.BLIP2_image2text_output.update()
      save_dir = os.path.join(root_dir, 'BLIP2_image2text')
      if os.path.exists(save_dir):
        for f in os.listdir(save_dir):
            os.remove(os.path.join(save_dir, f))
        os.rmdir(save_dir)
      page.BLIP2_image2text_file_list.controls = []
      page.BLIP2_image2text_file_list.update()
      BLIP2_image2text_list_buttons.visible = False
      BLIP2_image2text_list_buttons.update()
    def BLIP2_i2t_help(e):
      def close_BLIP2_i2t_dlg(e):
        nonlocal BLIP2_i2t_help_dlg
        BLIP2_i2t_help_dlg.open = False
        page.update()
      BLIP2_i2t_help_dlg = AlertDialog(title=Text("💁   Help with BLIP2 Image2Text Interrogator"), content=Column([
          Text("Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Model", weight=FontWeight.BOLD),
          Text("BLIP-2 is a generic and efficient pre-training strategy that bootstraps vision-language pre-training from off-the-shelf frozen pre-trained image encoders and frozen large language models. BLIP-2 bridges the modality gap with a lightweight Querying Transformer, which is pre-trained in two stages. The first stage bootstraps vision-language representation learning from a frozen image encoder. The second stage bootstraps vision-to-language generative learning from a frozen language model. BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods. For example, our model outperforms Flamingo80B by 8.7% on zero-shot VQAv2 with 54x fewer trainable parameters. We also demonstrate the model's emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions."),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("😑  Alright, got it... ", on_click=close_BLIP2_i2t_dlg)], actions_alignment=MainAxisAlignment.END)
      page.dialog = BLIP2_i2t_help_dlg
      BLIP2_i2t_help_dlg.open = True
      page.update()
    def file_picker_result(e: FilePickerResultEvent):
        if e.files != None:
          upload_files(e)
    def on_upload_progress(e: FilePickerUploadEvent):
      if e.progress == 1:
        save_dir = os.path.join(root_dir, 'BLIP2_image2text')
        if not os.path.exists(save_dir):
          os.mkdir(save_dir)
        BLIP2_image2text_prefs['folder_path'] = save_dir
        if not slash in e.file_name:
          fname = os.path.join(root_dir, e.file_name)
          fpath = os.path.join(save_dir, e.file_name)
        else:
          fname = e.file_name
          fpath = os.path.join(save_dir, e.file_name.rpartition(slash)[2])
        original_img = PILImage.open(fname)
        width, height = original_img.size
        width, height = scale_dimensions(width, height, BLIP2_image2text_prefs['max_size'])
        original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert("RGB")
        original_img.save(fpath)
        #shutil.move(fname, fpath)
        page.BLIP2_image2text_file_list.controls.append(ListTile(title=Text(fpath), dense=True))
        page.BLIP2_image2text_file_list.update()
    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)
    def pick_path(e):
        file_picker.pick_files(allow_multiple=True, allowed_extensions=["png", "PNG", "jpg", "jpeg"], dialog_title="Pick Image File to Enlarge")
    def upload_files(e):
        uf = []
        if file_picker.result != None and file_picker.result.files != None:
            for f in file_picker.result.files:
              if page.web:
                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))
              else:
                on_upload_progress(FilePickerUploadEvent(f.path, 1, ""))
            file_picker.upload(uf)
    page.overlay.append(file_picker)
    def add_image(e):
        save_dir = os.path.join(root_dir, 'BLIP2_image2text')
        if not os.path.exists(save_dir):
          os.mkdir(save_dir)
        BLIP2_image2text_prefs['folder_path'] = save_dir
        if image_path.value.startswith('http'):
          import requests
          from io import BytesIO
          response = requests.get(image_path.value)
          fpath = os.path.join(save_dir, image_path.value.rpartition(slash)[2])
          original_img = PILImage.open(BytesIO(response.content)).convert("RGB")
          width, height = original_img.size
          width, height = scale_dimensions(width, height, BLIP2_image2text_prefs['max_size'])
          original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert("RGB")
          original_img.save(fpath)
          page.BLIP2_image2text_file_list.controls.append(ListTile(title=Text(fpath), dense=True))
          page.BLIP2_image2text_file_list.update()
        elif os.path.isfile(image_path.value):
          fpath = os.path.join(save_dir, image_path.value.rpartition(slash)[2])
          original_img = PILImage.open(image_path.value)
          width, height = original_img.size
          width, height = scale_dimensions(width, height, BLIP2_image2text_prefs['max_size'])
          original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert("RGB")
          original_img.save(fpath)
          #shutil.copy(image_path.value, fpath)
          page.BLIP2_image2text_file_list.controls.append(ListTile(title=Text(fpath), dense=True))
          page.BLIP2_image2text_file_list.update()
        elif os.path.isdir(image_path.value):
          for f in os.listdir(image_path.value):
            file_path = os.path.join(image_path.value, f)
            if os.path.isdir(file_path): continue
            if f.lower().endswith(('.png', '.jpg', '.jpeg')):
              fpath = os.path.join(save_dir, f)
              original_img = PILImage.open(file_path)
              width, height = original_img.size
              width, height = scale_dimensions(width, height, BLIP2_image2text_prefs['max_size'])
              original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert("RGB")
              original_img.save(fpath)
              #shutil.copy(file_path, fpath)
              page.BLIP2_image2text_file_list.controls.append(ListTile(title=Text(fpath), dense=True))
              page.BLIP2_image2text_file_list.update()
        else:
          if bool(image_path.value):
            alert_msg(page, "Couldn't find a valid File, Path or URL...")
          else:
            pick_path(e)
          return
        image_path.value = ""
        image_path.update()
    page.BLIP2_image2text_list = Column([], spacing=0)
    def add_to_prompt_list(p):
      page.add_to_prompts(p)
      if prefs['enable_sounds']: page.snd_drop.play()
    def add_to_BLIP2_image2text(p):
      page.BLIP2_image2text_list.controls.append(ListTile(title=Text(p, max_lines=3, style=TextThemeStyle.BODY_LARGE), dense=True, on_click=lambda _: add_to_prompt_list(p)))
      page.BLIP2_image2text_list.update()
      BLIP2_image2text_list_buttons.visible = True
      BLIP2_image2text_list_buttons.update()
    page.add_to_BLIP2_image2text = add_to_BLIP2_image2text
    def add_to_list(e):
      if prefs['enable_sounds']: page.snd_drop.play()
      for p in page.BLIP2_image2text_list.controls:
        page.add_to_prompts(p.title.value)
    def clear_prompts(e):
      if prefs['enable_sounds']: page.snd_delete.play()
      page.BLIP2_image2text_list.controls = []
      page.BLIP2_image2text_list.update()
      prompts = []
      BLIP2_image2text_list_buttons.visible = False
      BLIP2_image2text_list_buttons.update()
    BLIP2_image2text_list_buttons = Row([
        ElevatedButton(content=Text("❌   Clear Prompts"), on_click=clear_prompts),
        FilledButton(content=Text("➕  Add All Prompts to List", size=20), height=45, on_click=add_to_list),
    ], alignment=MainAxisAlignment.SPACE_BETWEEN)
    if len(page.BLIP2_image2text_list.controls) < 1:
      BLIP2_image2text_list_buttons.visible = False
    #'pretrain_flant5xxl', pretrain_opt2.7b, pretrain_opt6.7b, caption_coco_opt2.7b, caption_coco_opt6.7b, pretrain_flant5xl, caption_coco_flant5xl
    #model_name = Dropdown(label="Model Nane", width=250, options=[dropdown.Option("blip2_t5"), dropdown.Option("blip2_opt"), dropdown.Option("img2prompt_vqa")], value=BLIP2_image2text_prefs['model_name'], on_change=lambda e: changed(e, 'model_name'))
    model_type = Dropdown(label="Model Type", width=250, options=[dropdown.Option("pretrain_flant5xxl"), dropdown.Option("pretrain_opt2.7b"), dropdown.Option("pretrain_opt6.7b"), dropdown.Option("caption_coco_opt6.7b"), dropdown.Option("caption_coco_flant5xl"), dropdown.Option("base")], value=BLIP2_image2text_prefs['model_type'], on_change=lambda e: changed(e, 'model_type'))
    num_captions = Container(NumberPicker(label="Number of Captions: ", min=1, max=10, value=BLIP2_image2text_prefs['num_captions'], on_change=lambda e: changed(e, 'num_captions')))
    max_row = SliderRow(label="Max Resolution Size", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=BLIP2_image2text_prefs, key='max_size')
    question_prompt = TextField(label="Ask Question about Image (optional)", value=BLIP2_image2text_prefs['question_prompt'], on_change=lambda e:changed(e,'question_prompt'))
    image_path = TextField(label="Image File or Folder Path or URL to Examine", value=BLIP2_image2text_prefs['image_path'], on_change=lambda e:changed(e,'image_path'), suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_path), expand=True)
    add_image_button = ElevatedButton(content=Text("Add File or Folder"), on_click=add_image)
    page.BLIP2_image2text_file_list = Column([], tight=True, spacing=0)
    page.BLIP2_image2text_output = Column([])
    #clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    #clear_button.visible = len(page.BLIP2_image2text_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🤳  BLIP2 Image2Text Examiner", subtitle="Create prompts by describing input images... Warning: Uses A LOT of VRAM, may crash session.", actions=[IconButton(icon=icons.HELP, tooltip="Help with Image2Text Interrogator", on_click=BLIP2_i2t_help)]),
        Row([model_type, num_captions]),
        max_row,
        question_prompt,
        Row([image_path, add_image_button]),
        page.BLIP2_image2text_file_list,
        page.BLIP2_image2text_list,
        BLIP2_image2text_list_buttons,
        ElevatedButton(content=Text("📸  Get Prompts from Images", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_BLIP2_image2text(page)),
        page.BLIP2_image2text_output,
      ],
    ))], scroll=ScrollMode.AUTO)
    return c


dance_prefs = {
    'dance_model': 'maestro-150k',
    'installed_model': None,
    'inference_steps': 50,
    'batch_size': 1,
    'seed': 0,
    'audio_length_in_s': 4.5,
    'community_model': 'Daft Punk',
    'custom_model': '',
    'train_custom': False,#https://colab.research.google.com/github/Harmonai-org/sample-generator/blob/main/Finetune_Dance_Diffusion.ipynb
    'custom_name': '',
    'wav_path': '',
    'demo_every': 250,#Number of training steps between demos
    'checkpoint_every': 500,#Number of training steps between saving model checkpoints
    'sample_rate': 48000,#Sample rate to train at
    'sample_size': 65536,#Number of audio samples per training sample
    'random_crop': True,#If true, the audio samples provided will be randomly cropped to SAMPLE_SIZE samples. Turn off if you want to ensure the training data always starts at the beginning of the audio files (good for things like drum one-shots)
    'finetune_batch_size': 2,#Batch size to fine-tune (make it as high as it can go for your GPU)
    'accumulate_batches': 4,#Accumulate gradients over n batches, useful for training on one GPU. Effective batch size is BATCH_SIZE * ACCUM_BATCHES. Also increases the time between demos and saved checkpoints
    'save_model': False,
    'where_to_save_model': "Public Library",
    'readme_description': '',
}
community_dance_diffusion_models = [
    #{'name': 'LCD Soundsystem', 'download': 'https://drive.google.com/uc?id=1WX8nL4_x49h0OJE5iGrjXJnIJ0yvsTxI', 'ckpt':'lcd-soundsystem-200k.ckpt'}, #https://huggingface.co/Gecktendo/lcd-soundsystem/
    {'name': 'Daft Punk', 'download': 'https://drive.google.com/uc?id=1CZjWIcL528zbZa6GrS_triob0hUy6KEs', 'ckpt':'daft-punk-241.5k.ckpt'}, #https://huggingface.co/Gecktendo/daft-punk
    {'name': 'Vague Phrases', 'download': 'https://drive.google.com/uc?id=1nUn2qydqU7hlDUT-Skq_Ionte_8-Vdjr', 'ckpt': 'SingingInFepoch=1028-step=195500-pruned.ckpt'},
    {'name': 'Gesaffelstein', 'download': 'https://drive.google.com/uc?id=1-BuDzz4ajX-ufVByEX_fCkOtB00DVygB', 'ckpt':'Gesaffelstein_epoch=2537-step=445000.ckpt'},
    {'name': 'Smash Mouth Vocals', 'download': 'https://drive.google.com/uc?id=1h3fkJnByw3mKpXUiNPWKoYtzmpeg1QEt', 'ckpt':'epoch=773-step=191500.ckpt'},
    {'name': 'Dubstep Bass Growls', 'download': 'https://drive.google.com/file/d/104Ni-suQ0-tt2Xe9SbWjTnWFOSkT6O47', 'ckpt':'epoch=1266-step=195000.ckpt'},
    {'name': 'Jumango Ambient', 'download': 'https://drive.google.com/file/d/1-gpOee-v7ZGFJtzr76sYTKuIKTQKTCmN', 'ckpt':'jumango-ambient-v1.ckpt'},
    {'name': 'Serum Wavetables', 'download': 'https://drive.google.com/file/d/1l0JhA2qTaXtt5pdyv7rW1Ls4wmAFWVD1', 'ckpt':'serumwavetables-49k.ckpt'},
    {'name': 'Textured Grooves Club', 'download': 'https://drive.google.com/file/d/13VAGMSPaIGo7FGDAtedaykhcasrFk2Z_', 'ckpt':'TexturedGroovesLargeClub_step_592200.ckpt'},
    {'name': 'Abstract Vocal', 'download': 'https://drive.google.com/file/d/1izVPIYgPhpIT8lZtaWIO8gbTnfEL9g_J', 'ckpt':'Singing-step277000-pruned.ckpt'},
    {'name': 'Gesaffelstein', 'download': 'https://drive.google.com/file/d/1-BuDzz4ajX-ufVByEX_fCkOtB00DVygB', 'ckpt':'Gesaffelstein_epoch=2537-step=445000.ckpt'},
    {'name': 'Paul McCartney Vocals', 'download': 'https://drive.google.com/file/d/1-_FtUwLMnMUGLMpvtnE0EDAcUjDlXSeS', 'ckpt':'epoch=1148-step=193000.ckpt'},
    {'name': 'Techno Kicks', 'download': 'https://drive.google.com/file/d/1-gR9QFq7ZYHn2ep0gw5IyQ6WzRZJW_tG', 'ckpt':'epoch=1296-step=441500.ckpt'},
    {'name': 'Electronic Snare Drums', 'download': 'https://drive.google.com/file/d/1-T-PFtfyc_JUan71Px_FWsxiiuBuNN_1', 'ckpt':'epoch=1078-step=195000.ckpt'},
    {'name': 'Electronic Snare Drums+', 'download': 'https://drive.google.com/file/d/1-50R5wwyhNrQSlvaxEqQ8CiRJPYgzGsr', 'ckpt':'epoch=1110-step=195500.ckpt'},
    {'name': 'Electronic Kick Drums', 'download': 'https://drive.google.com/file/d/1-46jYgYfz_Jbnu-dNvepWqa7rcSP__zo', 'ckpt':'epoch=1234-step=197500.ckpt'},
]
dance_pipe = None
def buildDanceDiffusion(page):
    global dance_pipe, dance_prefs
    def changed(e, pref=None, isInt=False):
        if pref is not None:
          if isInt:
            dance_prefs[pref] = int(e.control.value)
          else:
            dance_prefs[pref] = e.control.value
    def dance_help(e):
        def close_dance_dlg(e):
          nonlocal dance_help_dlg
          dance_help_dlg.open = False
          page.update()
        dance_help_dlg = AlertDialog(title=Text("💁   Help with Dance Diffusion"), content=Column([
            Text("HarmonAI Dance Diffusion"),
          ], scroll=ScrollMode.AUTO), actions=[TextButton("👄  Sounds Good...I hope ", on_click=close_dance_dlg)], actions_alignment=MainAxisAlignment.END)
        page.dialog = dance_help_dlg
        dance_help_dlg.open = True
        page.update()
    def delete_audio(e):
        f = e.control.data
        if os.path.isfile(f):
          os.remove(f)
          for i, fl in enumerate(page.dance_file_list.controls):
            if fl.title.value == f:
              del page.dance_file_list.controls[i]
              page.dance_file_list.update()
              if f in dance_prefs['custom_wavs']:
                dance_prefs['custom_wavs'].remove(f)
              continue
    def delete_all_audios(e):
        for fl in page.dance_file_list.controls:
          f = fl.title.value
          if os.path.isfile(f):
            os.remove(f)
        page.dance_file_list.controls.clear()
        page.dance_file_list.update()
        dance_prefs['custom_wavs'].clear()
    def add_file(fpath, update=True):
        page.dance_file_list.controls.append(ListTile(title=Text(fpath), dense=True, trailing=PopupMenuButton(icon=icons.MORE_VERT,
          items=[#TODO: View Image
              PopupMenuItem(icon=icons.DELETE, text="Delete Audio", on_click=delete_audio, data=fpath),
              PopupMenuItem(icon=icons.DELETE_SWEEP, text="Delete All", on_click=delete_all_audios, data=fpath),
          ])))
        dance_prefs['custom_wavs'].append(fpath)
        if update: page.dance_file_list.update()
    def file_picker_result(e: FilePickerResultEvent):
        if e.files != None:
          upload_files(e)
    save_dir = os.path.join(root_dir, 'dance-audio')
    def on_upload_progress(e: FilePickerUploadEvent):
        if e.progress == 1:
          if not os.path.exists(save_dir):
            os.mkdir(save_dir)
          if not slash in e.file_name:
            fname = os.path.join(root_dir, e.file_name)
            fpath = os.path.join(save_dir, e.file_name)
            shutil.move(fname, fpath)
          else:
            fname = e.file_name
            fpath = os.path.join(save_dir, e.file_name.rpartition(slash)[2])
          add_file(fpath)
    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)
    def pick_path(e):
        file_picker.pick_files(allow_multiple=True, allowed_extensions=["wav", "WAV", "mp3", "MP3"], dialog_title="Pick Audio WAV or MP3 Files to Train")
    def upload_files(e):
        uf = []
        if file_picker.result != None and file_picker.result.files != None:
            for f in file_picker.result.files:
              if page.web:
                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))
              else:
                on_upload_progress(FilePickerUploadEvent(f.path, 1, ""))
            file_picker.upload(uf)
    page.overlay.append(file_picker)
    def add_wav(e):
        if not os.path.exists(save_dir):
            os.mkdir(save_dir)
        if wav_path.value.startswith('http'):
            #import requests
            from io import BytesIO
            #response = requests.get(wav_path.value)
            fpath = download_file(wav_path.value)
            #fpath = os.path.join(save_dir, wav_path.value.rpartition(slash)[2])
            add_file(fpath)
        elif os.path.isfile(wav_path.value):
          fpath = os.path.join(save_dir, wav_path.value.rpartition(slash)[2])
          shutil.copy(wav_path.value, fpath)
          add_file(fpath)
        elif os.path.isdir(wav_path.value):
          for f in os.listdir(wav_path.value):
            file_path = os.path.join(wav_path.value, f)
            if os.path.isdir(file_path): continue
            if f.lower().endswith(('.wav', '.WAV', '.mp3', '.MP3')):
              fpath = os.path.join(save_dir, f)
              shutil.copy(file_path, fpath)
              add_file(fpath)
        else:
          if bool(wav_path.value):
            alert_msg(page, "Couldn't find a valid File, Path or URL...")
          else:
            pick_path(e)
          return
        wav_path.value = ""
        wav_path.update()
    def load_wavs():
        if os.path.exists(save_dir):
          for f in os.listdir(save_dir):
            existing = os.path.join(save_dir, f)
            if os.path.isdir(existing): continue
            if f.lower().endswith(('.wav', '.WAV', '.mp3', '.MP3')):
              add_file(existing, update=False)
    def toggle_custom(e):
        changed(e, 'train_custom')
        custom_box.height = None if dance_prefs['train_custom'] else 0
        custom_box.update()
        custom_audio_name.visible = dance_prefs['train_custom']
        custom_audio_name.update()
    def toggle_save(e):
        changed(e, 'save_model')
        where_to_save_model.visible = dance_prefs['save_model']
        where_to_save_model.update()
        readme_description.visible = dance_prefs['save_model']
        readme_description.update()
    def changed_model(e):
      dance_prefs['dance_model'] = e.control.value
      if e.control.value == 'Community':
        community_dance_diffusion_model.visible = True
        community_dance_diffusion_model.update()
      else:
        if community_dance_diffusion_model.visible:
          community_dance_diffusion_model.visible = False
          community_dance_diffusion_model.update()
      if e.control.value == 'Custom':
        custom_model.visible = True
        custom_model.update()
      else:
        if custom_model.visible:
          custom_model.visible = False
          custom_model.update()
    dance_model = Dropdown(label="Dance Diffusion Model", width=250, options=[dropdown.Option("maestro-150k"), dropdown.Option("glitch-440k"), dropdown.Option("jmann-small-190k"), dropdown.Option("jmann-large-580k"), dropdown.Option("unlocked-250k"), dropdown.Option("honk-140k"), dropdown.Option("gwf-440k"), dropdown.Option("Community"), dropdown.Option("Custom")], value=dance_prefs['dance_model'], on_change=changed_model)
    community_dance_diffusion_model = Dropdown(label="Community Model", width=250, options=[], value=dance_prefs['community_model'], on_change=lambda e: changed(e, 'community_model'))
    page.community_dance_diffusion_model = community_dance_diffusion_model
    custom_model = TextField(label="Custom Model Path", value=dance_prefs['custom_model'], on_change=lambda e:changed(e,'custom_model'))
    for c in prefs['custom_dance_diffusion_models']:
      community_dance_diffusion_model.options.append(dropdown.Option(c['name']))
    for c in community_dance_diffusion_models:
      community_dance_diffusion_model.options.append(dropdown.Option(c['name']))
    if not dance_prefs['dance_model'] == 'Community':
      community_dance_diffusion_model.visible = False
    if not dance_prefs['dance_model'] == 'Custom':
      custom_model.visible = False
    inference_row = SliderRow(label="Number of Inference Steps", min=10, max=200, divisions=190, pref=dance_prefs, key='inference_steps')
    batch_size = TextField(label="Batch Size", value=dance_prefs['batch_size'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'batch_size', isInt=True), width = 90)
    seed = TextField(label="Random Seed", value=dance_prefs['seed'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'seed', isInt=True), width = 110)
    audio_length_in_s = TextField(label="Audio Length in Seconds", value=dance_prefs['audio_length_in_s'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'audio_length_in_s'), width = 190)
    number_row = Row([batch_size, seed, audio_length_in_s])

    train_custom = Switcher(label="Train Custom Audio ", value=dance_prefs['train_custom'], on_change=toggle_custom)
    custom_audio_name = TextField(label="Custom Audio Name", value=dance_prefs['custom_name'], on_change=lambda e:changed(e,'custom_name'))
    wav_path = TextField(label="Audio Files or Folder Path or URL to Train", value=dance_prefs['wav_path'], on_change=lambda e:changed(e,'wav_path'), suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_path), expand=1)
    add_wav_button = ElevatedButton(content=Text("Add Audio Files"), on_click=add_wav)
    page.dance_file_list = Column([], tight=True, spacing=0)
    sample_rate = Tooltip(message="Sample rate to train at", content=TextField(label="Sample Rate", value=dance_prefs['sample_rate'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'sample_rate', ptype='int'), width = 160))
    sample_size = Tooltip(message="Number of audio samples per training sample", content=TextField(label="Sample Size", value=dance_prefs['sample_size'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'sample_size', ptype='int'), width = 160))
    finetune_batch_size = Tooltip(message="Batch size to fine-tune (make it as high as it can go for your GPU)", content=TextField(label="Finetune Batch Size", value=dance_prefs['finetune_batch_size'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'finetune_batch_size', ptype='int'), width = 160))
    accumulate_batches = Tooltip(message="Accumulate gradients over n batches, useful for training on one GPU. Effective batch size is BATCH_SIZE * ACCUM_BATCHES. Also increases the time between demos and saved checkpoints", content=TextField(label="Accumulate Batches", value=dance_prefs['accumulate_batches'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'accumulate_batches', ptype='int'), width = 160))
    demo_every = Tooltip(message="Number of training steps between demos", content=TextField(label="Demo Every", value=dance_prefs['demo_every'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'demo_every', ptype='int'), width = 160))
    checkpoint_every = Tooltip(message="Number of training steps between saving model checkpoints", content=TextField(label="Checkpoint Every", value=dance_prefs['checkpoint_every'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'checkpoint_every', ptype='int'), width = 160))
    save_model = Switcher(label="Save Model to HuggingFace ", value=dance_prefs['save_model'], on_change=toggle_save, tooltip="Requires WRITE access on API Key to Upload Checkpoint")
    where_to_save_model = Dropdown(label="Where to Save Model", width=250, options=[dropdown.Option("Public Library"), dropdown.Option("Privately to my Profile")], value=dance_prefs['where_to_save_model'], on_change=lambda e: changed(e, 'where_to_save_model'))
    readme_description = TextField(label="Extra README Description", value=dance_prefs['readme_description'], on_change=lambda e:changed(e,'readme_description'))
    where_to_save_model.visible = dance_prefs['save_model']
    readme_description.visible = dance_prefs['save_model']
    custom_box = Container(Column([
        Container(content=None, height=3),
        Row([sample_rate, sample_size]),
        Row([finetune_batch_size, accumulate_batches]),
        Row([demo_every, checkpoint_every]),
        Row([save_model, where_to_save_model]),
        readme_description,
        Text("Provide 3 or more ~10 second clips of Music as mp3 or wav files:", weight=FontWeight.BOLD),
        Row([wav_path, add_wav_button]),
        page.dance_file_list,]), padding=padding.only(left=11), animate_size=animation.Animation(800, AnimationCurve.EASE_OUT_CIRC), clip_behavior=ClipBehavior.HARD_EDGE)
    custom_box.height = None if dance_prefs['train_custom'] else 0
    custom_audio_name.visible = dance_prefs['train_custom']
    load_wavs()
    #seed = TextField(label="Seed", value=dance_prefs['seed'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'seed', ptype='int'), width = 160)
    #lambda_entropy = TextField(label="Lambda Entropy", value=dreamfusdance_prefsion_prefs['lambda_entropy'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'lambda_entropy', ptype='float'), width = 160)
    #max_steps = TextField(label="Max Steps", value=dance_prefs['max_steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'max_steps', ptype='int'), width = 160)
    page.dance_output = Column([])
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.dance_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("👯 Create experimental music or sounds with HarmonAI trained audio models", "Tools to train a generative model on arbitrary audio samples...", actions=[IconButton(icon=icons.HELP, tooltip="Help with DanceDiffusion Settings", on_click=dance_help)]),
        Row([dance_model, community_dance_diffusion_model, custom_model]),
        inference_row,
        number_row,
        Row([train_custom, custom_audio_name], vertical_alignment=CrossAxisAlignment.START),
        custom_box,
        ElevatedButton(content=Text("🎵  Run Dance Diffusion", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_dance_diffusion(page)),
        page.dance_output,
      ]
    ))], scroll=ScrollMode.AUTO)
    return c

audio_diffusion_prefs = {
    'audio_file': '',
    'file_name': '',
    'audio_model': 'teticio/audio-diffusion-ddim-256',
    'scheduler': 'DDIM', #DDPM
    'steps': 50, #number of de-noising steps (defaults to 50 for DDIM, 1000 for DDPM)
    'start_step': 0, #step to start from
    'slice': 0, #slice number of audio to convert
    'eta': 0.2, #parameter between 0 and 1 used with DDIM scheduler
    'mask_start_secs': 0.0, #number of seconds of audio to mask (not generate) at start
    'mask_end_secs': 0.0, #number of seconds of audio to mask (not generate) at end
    'seed': 0,
    'audio_name': '',
    'wav_path': '',
    'batch_size': 1,
    'batch_folder_name': '',
    'file_prefix': 'ad-',
    'loaded_model': '',
}

def buildAudioDiffusion(page):
    global prefs, audio_diffusion_prefs
    def changed(e, pref=None, ptype="str"):
        if pref is not None:
          try:
            if ptype == "int":
              audio_diffusion_prefs[pref] = int(e.control.value)
            elif ptype == "float":
              audio_diffusion_prefs[pref] = float(e.control.value)
            else:
              audio_diffusion_prefs[pref] = e.control.value
          except Exception:
            alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
            pass
    def add_to_audio_diffusion_output(o):
        page.audio_diffusion_output.controls.append(o)
        page.audio_diffusion_output.update()
    def clear_output(e):
        if prefs['enable_sounds']: page.snd_delete.play()
        page.audio_diffusion_output.controls = []
        page.audio_diffusion_output.update()
        clear_button.visible = False
        clear_button.update()
    def audio_diffusion_help(e):
        def close_audio_diffusion_dlg(e):
          nonlocal audio_diffusion_help_dlg
          audio_diffusion_help_dlg.open = False
          page.update()
        audio_diffusion_help_dlg = AlertDialog(title=Text("💁   Help with Audio-Diffusion"), content=Column([
            Text("Audio Diffusion leverages the recent advances in image generation using diffusion models by converting audio samples to and from mel spectrogram images."),
            Markdown("The original codebase of this implementation can be found [here](https://github.com/teticio/audio-diffusion), including training scripts and example notebooks.\n[Audio Diffusion](https://github.com/teticio/audio-diffusion) by Robert Dargavel Smith.", on_tap_link=lambda e: e.page.launch_url(e.data)),
          ], scroll=ScrollMode.AUTO), actions=[TextButton("🔊  Sounds Groovie... ", on_click=close_audio_diffusion_dlg)], actions_alignment=MainAxisAlignment.END)
        page.dialog = audio_diffusion_help_dlg
        audio_diffusion_help_dlg.open = True
        page.update()
    def file_picker_result(e: FilePickerResultEvent):
        if e.files != None:
          upload_files(e)
    def on_upload_progress(e: FilePickerUploadEvent):
        if e.progress == 1:
            if not slash in e.file_name:
              fname = os.path.join(root_dir, e.file_name)
              audio_diffusion_prefs['file_name'] = e.file_name.rpartition('.')[0]
            else:
              fname = e.file_name
              fpath = os.path.join(root_dir, e.file_name.rpartition(slash)[2])
              audio_diffusion_prefs['file_name'] = e.file_name.rparition(slash)[2].rpartition('.')[0]
            audio_file.value = fname
            audio_file.update()
            audio_diffusion_prefs['audio_file'] = fname
            page.update()
    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)
    def upload_files(e):
        uf = []
        if file_picker.result != None and file_picker.result.files != None:
            for f in file_picker.result.files:
              if page.web:
                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))
              else:
                on_upload_progress(FilePickerUploadEvent(f.path, 1, ""))
            file_picker.upload(uf)
    page.overlay.append(file_picker)
    def pick_audio(e):
        file_picker.pick_files(allow_multiple=False, allowed_extensions=["mp3", "wav"], dialog_title="Pick Init Audio File")
    audio_file = TextField(label="Input Audio File (optional)", value=audio_diffusion_prefs['audio_file'], on_change=lambda e:changed(e,'audio_file'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_audio))
    def change_eta(e):
        changed(e, 'eta', ptype="float")
        eta_value.value = f" {audio_diffusion_prefs['eta']}"
        eta_value.update()
        eta_row.update()
    steps_row = SliderRow(label="Number of Inference Steps", min=1, max=100, divisions=99, pref=audio_diffusion_prefs, key='steps', tooltip="The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.")
    eta = Slider(min=0.0, max=1.0, divisions=20, label="{value}", value=float(audio_diffusion_prefs['eta']), tooltip="The weight of noise for added noise in a diffusion step. Its value is between 0.0 and 1.0 - 0.0 is DDIM and 1.0 is DDPM scheduler respectively.", expand=True, on_change=change_eta)
    eta_value = Text(f" {audio_diffusion_prefs['eta']}", weight=FontWeight.BOLD)
    eta_row = Row([Text("DDIM ETA:"), eta_value, eta,])
    page.etas.append(eta_row)
    audio_model = Dropdown(label="Audio Model", width=400, options=[dropdown.Option("teticio/audio-diffusion-ddim-256"), dropdown.Option("teticio/audio-diffusion-breaks-256"), dropdown.Option("teticio/audio-diffusion-instrumental-hiphop-256"), dropdown.Option("teticio/latent-audio-diffusion-256"), dropdown.Option("teticio/latent-audio-diffusion-ddim-256"), dropdown.Option("teticio/conditional-latent-audio-diffusion-512")], value=audio_diffusion_prefs['audio_model'], on_change=lambda e: changed(e, 'audio_model'))
    scheduler = Dropdown(label="De-noise Scheduler", width=250, options=[dropdown.Option("DDIM"), dropdown.Option("DDPM")], value=audio_diffusion_prefs['scheduler'], on_change=lambda e: changed(e, 'scheduler'))
    slice_audio = TextField(label="Slice of Audio", value=audio_diffusion_prefs['slice'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'slice', ptype='int'), width = 130)
    start_step = TextField(label="Starting Step", value=audio_diffusion_prefs['start_step'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'start_step', ptype='int'), width = 130)
    mask_start_secs = TextField(label="Mask Start (s)", value=audio_diffusion_prefs['mask_start_secs'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'mask_start_secs', ptype='float'), width = 130)
    mask_end_secs = TextField(label="Mask End (s)", value=audio_diffusion_prefs['mask_end_secs'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'mask_end_secs', ptype='float'), width = 130)
    audio_name = TextField(label="Audio File Name", value=audio_diffusion_prefs['audio_name'], on_change=lambda e:changed(e,'audio_name'))
    batch_folder_name = TextField(label="Batch Folder Name", value=audio_diffusion_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    file_prefix = TextField(label="Filename Prefix", value=audio_diffusion_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))
    batch_size = NumberPicker(label="Batch Size:  ", min=1, max=5, value=audio_diffusion_prefs['batch_size'], on_change=lambda e: changed(e, 'batch_size'))
    seed = TextField(label="Seed", value=audio_diffusion_prefs['seed'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'seed', ptype='int'), width = 120)
    page.audio_diffusion_output = Column([])
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.audio_diffusion_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🎶  Audio Diffusion Modeling", "Converts Audio Samples to and from Mel Spectrogram Images...", actions=[IconButton(icon=icons.HELP, tooltip="Help with Audio Diffusion-TTS Settings", on_click=audio_diffusion_help)]),
        audio_file,
        audio_model,
        #scheduler,
        steps_row,
        eta_row,
        Row([slice_audio, start_step, mask_start_secs, mask_end_secs]),
        Row([batch_size, seed, file_prefix]),
        Row([audio_name, batch_folder_name]),
        ElevatedButton(content=Text("🪗  Run Audio Diffusion", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_audio_diffusion(page)),
        page.audio_diffusion_output,
        clear_button,
      ]
    ))], scroll=ScrollMode.AUTO)
    return c

music_gen_prefs = {
    'prompt': '',
    'audio_file': '',
    'file_name': '',
    'audio_model': 'medium',
    'duration': 30,
    'top_k': 250,
    'top_p': 0,
    'temperature': 1,
    'guidance': 5,
    'overlap': 5,
    'dimension': 1,
    #'recondition': False,
    'harmony_only': False,
    'use_sampling': True,
    'two_step_cfg': False, #If True, performs 2 forward for Classifier Free Guidance, instead of batching together the two. This has some impact on how things are padded but seems to have little impact in practice.
    'seed': 0,
    'audio_name': '',
    'wav_path': '',
    'num_samples': 1,
    'batch_size': 1,
    'batch_folder_name': '',
    'file_prefix': 'musicgen-',
    'loaded_model': '',
}

def buildMusicGen(page):
    global prefs, music_gen_prefs
    def changed(e, pref=None, ptype="str"):
        if pref is not None:
          try:
            if ptype == "int":
              music_gen_prefs[pref] = int(e.control.value)
            elif ptype == "float":
              music_gen_prefs[pref] = float(e.control.value)
            else:
              music_gen_prefs[pref] = e.control.value
          except Exception:
            alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
            pass
    def add_to_music_gen_output(o):
        page.music_gen_output.controls.append(o)
        page.music_gen_output.update()
    def clear_output(e):
        if prefs['enable_sounds']: page.snd_delete.play()
        page.music_gen_output.controls = []
        page.music_gen_output.update()
        clear_button.visible = False
        clear_button.update()
    def music_gen_help(e):
        def close_music_gen_dlg(e):
          nonlocal music_gen_help_dlg
          music_gen_help_dlg.open = False
          page.update()
        music_gen_help_dlg = AlertDialog(title=Text("💁   Help with Audiocraft MusicGen"), content=Column([
            Text("Meta releases new SOTA text to music model MusicGen. Demonstrated samples are better than existing models including Google's MusicLM. We tackle the task of conditional music generation. We introduce MusicGen, a single Language Model (LM) that operates over several streams of compressed discrete music representation, i.e., tokens. Unlike prior work, MusicGen is comprised of a single-stage transformer LM together with efficient token interleaving patterns, which eliminates the need for cascading several models, e.g., hierarchically or upsampling. Following this approach, we demonstrate how MusicGen can generate high-quality samples, while being conditioned on textual description or melodic features, allowing better controls over the generated output. We conduct extensive empirical evaluation, considering both automatic and human studies, showing the proposed approach is superior to the evaluated baselines on a standard text-to-music benchmark. Through ablation studies, we shed light over the importance of each of the components comprising MusicGen."),
            Text("Small: This model has 300M parameters and can only generate music from text. It is the quickest model, however it may not yield the best results.\nMedium: This model has 1.5B parameters and can generate music from text as well. It is slower than the little model, but it produces better results.\nMelody: This 1.5B parameter model can generate music from both text and melody. It is the slowest model, but it produces the best results.\nLarge: This model has 3.3B parameters and can only generate music from text. It is the slowest model, but it produces the best results."),
            Markdown("[GitHub Code](https://github.com/facebookresearch/audiocraft) | [Paper](https://arxiv.org/abs/2306.05284)", on_tap_link=lambda e: e.page.launch_url(e.data)),
          ], scroll=ScrollMode.AUTO), actions=[TextButton("🥁  Let's hear it... ", on_click=close_music_gen_dlg)], actions_alignment=MainAxisAlignment.END)
        page.dialog = music_gen_help_dlg
        music_gen_help_dlg.open = True
        page.update()
    def file_picker_result(e: FilePickerResultEvent):
        if e.files != None:
          upload_files(e)
    def on_upload_progress(e: FilePickerUploadEvent):
        if e.progress == 1:
            if not slash in e.file_name:
              fname = os.path.join(root_dir, e.file_name)
              music_gen_prefs['file_name'] = e.file_name.rpartition('.')[0]
            else:
              fname = e.file_name
              fpath = os.path.join(root_dir, e.file_name.rpartition(slash)[2])
              music_gen_prefs['file_name'] = e.file_name.rparition(slash)[2].rpartition('.')[0]
            audio_file.value = fname
            audio_file.update()
            music_gen_prefs['audio_file'] = fname
            page.update()
    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)
    def upload_files(e):
        uf = []
        if file_picker.result != None and file_picker.result.files != None:
            for f in file_picker.result.files:
              if page.web:
                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))
              else:
                on_upload_progress(FilePickerUploadEvent(f.path, 1, ""))
            file_picker.upload(uf)
    page.overlay.append(file_picker)
    def pick_audio(e):
        file_picker.pick_files(allow_multiple=False, allowed_extensions=["mp3", "wav"], dialog_title="Pick Init Audio File")
    audio_file = TextField(label="Melody Conditioning Audio File (optional)", value=music_gen_prefs['audio_file'], on_change=lambda e:changed(e,'audio_file'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_audio))
    prompt = TextField(label="Prompt to generate a track (genre, theme, etc.)", value=music_gen_prefs['prompt'], multiline=True, min_lines=1, max_lines=8, on_change=lambda e:changed(e,'prompt'))
    duration_row = SliderRow(label="Duration", min=1, max=720, divisions=718, suffix="s", expand=True, pref=music_gen_prefs, key='duration')
    #steps_row = SliderRow(label="Number of Inference Steps", min=1, max=100, divisions=99, pref=music_gen_prefs, key='steps', tooltip="The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.")
    audio_model = Dropdown(label="Audio Model", width=150, options=[dropdown.Option("small"), dropdown.Option("medium"), dropdown.Option("large")], value=music_gen_prefs['audio_model'], on_change=lambda e: changed(e, 'audio_model'))
    guidance = SliderRow(label="Classifier Free Guidance", min=0, max=10, divisions=20, round=1, pref=music_gen_prefs, key='guidance', tooltip="Large => better quality and relavancy to text; Small => better diversity", col={'lg':6})
    temperature = SliderRow(label="AI Temperature", min=0, max=1, divisions=10, round=1, pref=music_gen_prefs, key='temperature', tooltip="Softmax value used to module the next token probabilities", col={'lg':6})
    top_k = SliderRow(label="Top-K Samples", min=0, max=300, divisions=299, round=0, pref=music_gen_prefs, key='top_k', tooltip="Number of highest probability vocabulary tokens to keep for top-k-filtering", col={'lg':6})
    top_p = SliderRow(label="Top-P Samples", min=0, max=1, divisions=11, round=1, pref=music_gen_prefs, key='top_p', tooltip="Percent of highest probability vocabulary tokens to keep, , when set to 0 top_k is used", col={'lg':6})
    #recondition = Checkbox(label="Recondition Chunks over 30s", tooltip="Condition next chunks with the first chunk.", value=music_gen_prefs['recondition'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'recondition'), col={'lg':6})
    harmony_only = Checkbox(label="Harmony Only", tooltip="Remove Drums?", value=music_gen_prefs['harmony_only'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'harmony_only'), col={'lg':6})
    #harmony_only = gr.Radio(label="Harmony Only",choices=["No", "Yes"], value="No", interactive=True, info="Remove Drums?")
    overlap = SliderRow(label="Overlap", min=0, max=29, divisions=29, round=0, pref=music_gen_prefs, key='overlap', tooltip="Time to resample chunks longer than 30s.", col={'lg':6})
    dimension = SliderRow(label="Dimension", min=-2, max=2, divisions=3, round=0, pref=music_gen_prefs, key='dimension', tooltip="Which direction to add new segements of audio. (1 = stack tracks, 2 = lengthen, -2..0 = ?)", col={'lg':6})
    #dimension = gr.Slider(minimum=-2, maximum=2, value=2, step=1, label="Dimension", info="determines which direction to add new segements of audio. (1 = stack tracks, 2 = lengthen, -2..0 = ?)", interactive=True)
    audio_name = TextField(label="Audio File Name", value=music_gen_prefs['audio_name'], on_change=lambda e:changed(e,'audio_name'))
    batch_folder_name = TextField(label="Batch Folder Name", value=music_gen_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    file_prefix = TextField(label="Filename Prefix", value=music_gen_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))
    num_samples = NumberPicker(label="Number of Samples:  ", min=1, max=10, value=music_gen_prefs['num_samples'], on_change=lambda e: changed(e, 'num_samples'))
    batch_size = NumberPicker(label="Batch Size:  ", min=1, max=4, value=music_gen_prefs['batch_size'], on_change=lambda e: changed(e, 'batch_size'))
    seed = TextField(label="Seed", value=music_gen_prefs['seed'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'seed', ptype='int'), width = 120)
    page.music_gen_output = Column([])
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.music_gen_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🪗  Meta Audiocraft MusicGen", "Simple and Controllable Music Generation with Audio tokenization model...", actions=[IconButton(icon=icons.HELP, tooltip="Help with MusicGen Settings", on_click=music_gen_help)]),
        prompt,
        audio_file,
        Row([audio_model, duration_row]),
        ResponsiveRow([guidance, temperature]),
        ResponsiveRow([top_k, top_p]),
        ResponsiveRow([dimension, overlap]),
        Row([num_samples, seed, file_prefix]),
        Row([audio_name, batch_folder_name]),
        ElevatedButton(content=Text("🎷  Run MusicGen", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_music_gen(page)),
        page.music_gen_output,
        clear_button,
      ]
    ))], scroll=ScrollMode.AUTO)
    return c

dreamfusion_prefs = {
    'prompt_text': '',
    'training_iters': 5000,
    'learning_rate': 0.001,
    'training_nerf_resolution': 64,
    'seed': 0,
    'lambda_entropy': 0.0001,
    'max_steps': 512,
    'checkpoint': 'latest',
    'workspace': 'trial',
}

def buildDreamFusion(page):
    global prefs, dreamfusion_prefs
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          if ptype == "int":
            dreamfusion_prefs[pref] = int(e.control.value)
          elif ptype == "float":
            dreamfusion_prefs[pref] = float(e.control.value)
          else:
            dreamfusion_prefs[pref] = e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def add_to_dreamfusion_output(o):
      page.dreamfusion_output.controls.append(o)
      page.dreamfusion_output.update()
    def clear_output(e):
      if prefs['enable_sounds']: page.snd_delete.play()
      page.dreamfusion_output.controls = []
      page.dreamfusion_output.update()
      clear_button.visible = False
      clear_button.update()
    def df_help(e):
      def close_df_dlg(e):
        nonlocal df_help_dlg
        df_help_dlg.open = False
        page.update()
      df_help_dlg = AlertDialog(title=Text("💁   Help with DreamFusion"), content=Column([
          Text("It's difficult to explain exactly what all these parameters do, but keep it close to defaults, keep prompt simple, or experiment to see what's what, we don't know."),
          Text('It takes about 0.7s per training step, so the default 5000 training steps take around 1 hour to finish. A larger Training_iters usually leads to better results.'),
          Text('If CUDA OOM, try to decrease Max_steps and Training_nerf_resolution.'),
          Text('If the NeRF fails to learn anything (empty scene, only background), try to decrease Lambda_entropy which regularizes the learned opacity.')
        ], scroll=ScrollMode.AUTO), actions=[TextButton("😊  So Exciting... ", on_click=close_df_dlg)], actions_alignment=MainAxisAlignment.END)
      page.dialog = df_help_dlg
      df_help_dlg.open = True
      page.update()
    prompt_text = TextField(label="Prompt Text", value=dreamfusion_prefs['prompt_text'], on_change=lambda e:changed(e,'prompt_text'))
    training_iters = TextField(label="Training Iterations", value=dreamfusion_prefs['training_iters'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'training_iters', ptype='int'), width = 160)
    learning_rate = TextField(label="Learning Rate", value=dreamfusion_prefs['learning_rate'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'learning_rate', ptype='float'), width = 160)
    training_nerf_resolution = TextField(label="Training NERF Res", value=dreamfusion_prefs['training_nerf_resolution'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'training_nerf_resolution', ptype='int'), width = 160)
    seed = TextField(label="Seed", value=dreamfusion_prefs['seed'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'seed', ptype='int'), width = 160)
    lambda_entropy = TextField(label="Lambda Entropy", value=dreamfusion_prefs['lambda_entropy'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'lambda_entropy', ptype='float'), width = 160)
    max_steps = TextField(label="Max Steps", value=dreamfusion_prefs['max_steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'max_steps', ptype='int'), width = 160)
    workspace = TextField(label="Workspace Folder", value=dreamfusion_prefs['workspace'], on_change=lambda e:changed(e,'workspace'))
    page.dreamfusion_output = Column([])
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.dreamfusion_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🗿  Create experimental DreamFusion 3D Model and Video", "Provide a prompt to render a model. Warning: May take over an hour to run the training...", actions=[IconButton(icon=icons.HELP, tooltip="Help with DreamFusion Settings", on_click=df_help)]),
        prompt_text,
        Row([training_iters,learning_rate, lambda_entropy]),
        Row([seed, training_nerf_resolution, max_steps]),
        Row([workspace]),
        ElevatedButton(content=Text("🔨  Run DreamFusion", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_dreamfusion(page)),
        page.dreamfusion_output,
        clear_button,
      ]
    ))], scroll=ScrollMode.AUTO)
    return c

point_e_prefs = {
    'prompt_text': '',
    'init_image': '',
    'guidance_scale': 3.0,
    'base_model': 'base40M-textvec', #'base40M', 'base300M' or 'base1B'
    'upsample': False,
    'batch_size': 1,
    'batch_folder_name': '',
    'seed': 0,
    'max_steps': 512,
}

def buildPoint_E(page):
    global prefs, point_e_prefs
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          if ptype == "int":
            point_e_prefs[pref] = int(e.control.value)
          elif ptype == "float":
            point_e_prefs[pref] = float(e.control.value)
          else:
            point_e_prefs[pref] = e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def add_to_point_e_output(o):
      page.point_e_output.controls.append(o)
      page.point_e_output.update()
    def clear_output(e):
      if prefs['enable_sounds']: page.snd_delete.play()
      page.point_e_output.controls = []
      page.point_e_output.update()
      clear_button.visible = False
      clear_button.update()
    def df_help(e):
      def close_df_dlg(e):
        nonlocal df_help_dlg
        df_help_dlg.open = False
        page.update()
      df_help_dlg = AlertDialog(title=Text("💁   Help with OpenAI Point-E"), content=Column([
          Markdown("This is an interface for running the [official codebase](https://github.com/openai/point-e) for point cloud diffusion models and SDF regression models described in [Point-E: A System for Generating 3D Point Clouds from Complex Prompts](https://arxiv.org/abs/2212.08751). These models were trained and released by OpenAI. Following [Model Cards for Model Reporting (Mitchell et al.)](https://arxiv.org/abs/1810.03993), we're providing some information about how the models were trained and evaluated.", on_tap_link=lambda e: e.page.launch_url(e.data)),
          Text("The Point-E models are trained for use as point cloud diffusion models and SDF regression models. Our image-conditional models are often capable of producing coherent 3D point clouds, given a single rendering of a 3D object. However, the models sometimes fail to do so, either producing incorrect geometry where the rendering is occluded, or producing geometry that is inconsistent with visible parts of the rendering. The resulting point clouds are relatively low-resolution, and are often noisy and contain defects such as outliers or cracks. Our text-conditional model is sometimes capable of producing 3D point clouds which can be recognized as the provided text description, especially when the text description is simple. However, we find that this model fails to generalize to complex prompts or unusual objects."),
          Text("While recent work on text-conditional 3D object generation has shown promising results, the state-of-the-art methods typically require multiple GPU-hours to produce a single sample. This is in stark contrast to state-of-the-art generative image models, which produce samples in a number of seconds or minutes. In this paper, we explore an alternative method for 3D object generation which produces 3D models in only 1-2 minutes on a single GPU. Our method first generates a single synthetic view using a text-to-image diffusion model, and then produces a 3D point cloud using a second diffusion model which conditions on the generated image. While our method still falls short of the state-of-the-art in terms of sample quality, it is one to two orders of magnitude faster to sample from, offering a practical trade-off for some use cases. We release our pre-trained point cloud diffusion models, as well as evaluation code and models for you to use."),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("☝️  Good Points... ", on_click=close_df_dlg)], actions_alignment=MainAxisAlignment.END)
      page.dialog = df_help_dlg
      df_help_dlg.open = True
      page.update()
    def file_picker_result(e: FilePickerResultEvent):
        if e.files != None:
          upload_files(e)
    def on_upload_progress(e: FilePickerUploadEvent):
      if e.progress == 1:
        if not slash in e.file_name:
          fname = os.path.join(root_dir, e.file_name)
          point_e_prefs['file_name'] = e.file_name.rpartition('.')[0]
        else:
          fname = e.file_name
          point_e_prefs['file_name'] = e.file_name.rparition(slash)[2].rpartition('.')[0]
        init_image.value = fname
        init_image.update()
        point_e_prefs['init_image'] = fname
        page.update()
    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)
    def upload_files(e):
        uf = []
        if file_picker.result != None and file_picker.result.files != None:
            for f in file_picker.result.files:
              if page.web:
                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))
              else:
                on_upload_progress(FilePickerUploadEvent(f.path, 1, ""))
            file_picker.upload(uf)
    page.overlay.append(file_picker)
    #page.overlay.append(pick_files_dialog)
    def pick_original(e):
        file_picker.pick_files(allow_multiple=False, allowed_extensions=["png", "PNG", "jpg", "jpeg"], dialog_title="Pick Original Image File")
    prompt_text = TextField(label="Prompt Text", value=point_e_prefs['prompt_text'], on_change=lambda e:changed(e,'prompt_text'))
    init_image = TextField(label="Sample Image (instead of prompt)", value=point_e_prefs['init_image'], on_change=lambda e:changed(e,'init_image'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_original))
    base_model = Dropdown(label="Base Model", width=250, options=[dropdown.Option("base40M-imagevec"), dropdown.Option("base40M-textvec"), dropdown.Option("base40M"), dropdown.Option("base300M"), dropdown.Option("base1B")], value=point_e_prefs['base_model'], on_change=lambda e: changed(e, 'base_model'))
    batch_folder_name = TextField(label="3D Model Folder Name", value=point_e_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    #batch_size = TextField(label="Batch Size", value=point_e_prefs['batch_size'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'batch_size', isInt=True), width = 90)
    batch_size = NumberPicker(label="Batch Size: ", min=1, max=5, value=point_e_prefs['batch_size'], on_change=lambda e: changed(e, 'batch_size'))
    guidance = SliderRow(label="Guidance Scale", min=0, max=10, divisions=20, round=1, pref=point_e_prefs, key='guidance_scale')
    #seed = TextField(label="Seed", value=point_e_prefs['seed'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'seed', ptype='int'), width = 160)
    page.point_e_output = Column([])
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.point_e_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("👆  Point-E 3D Point Clouds", "Provide a Prompt or Image to render from a CLIP ViT-L/14 diffusion model...", actions=[IconButton(icon=icons.HELP, tooltip="Help with Point-E Settings", on_click=df_help)]),
        prompt_text,
        init_image,
        base_model,
        Row([batch_folder_name, batch_size]),
        guidance,
        ElevatedButton(content=Text("🐞  Run Point-E", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_point_e(page)),
        page.point_e_output,
        clear_button,
      ]
    ))], scroll=ScrollMode.AUTO)
    return c

shap_e_prefs = {
    'prompt_text': '',
    'init_image': '',
    'guidance_scale': 15.0,
    'base_model': 'base40M-textvec', #'base40M', 'base300M' or 'base1B'
    'render_mode': 'NeRF', #STF
    'use_karras': True,
    'karras_steps': 64,
    'size': 64,
    'save_frames': False,
    'batch_size': 1,
    'batch_folder_name': '',
    #'seed': 0,
    #'max_steps': 512,
}

def buildShap_E(page):
    global prefs, shap_e_prefs
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          if ptype == "int":
            shap_e_prefs[pref] = int(e.control.value)
          elif ptype == "float":
            shap_e_prefs[pref] = float(e.control.value)
          else:
            shap_e_prefs[pref] = e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def clear_output(e):
      if prefs['enable_sounds']: page.snd_delete.play()
      page.shap_e_output.controls = []
      page.shap_e_output.update()
      clear_button.visible = False
      clear_button.update()
    def df_help(e):
      def close_df_dlg(e):
        nonlocal df_help_dlg
        df_help_dlg.open = False
        page.update()
      df_help_dlg = AlertDialog(title=Text("💁   Help with OpenAI Shap-E"), content=Column([
          Text("We present Shap-E, a conditional generative model for 3D assets. Unlike recent work on 3D generative models which produce a single output representation, Shap-E directly generates the parameters of implicit functions that can be rendered as both textured meshes and neural radiance fields. We train Shap-E in two stages: first, we train an encoder that deterministically maps 3D assets into the parameters of an implicit function; second, we train a conditional diffusion model on outputs of the encoder. When trained on a large dataset of paired 3D and text data, our resulting models are capable of generating complex and diverse 3D assets in a matter of seconds. When compared to Point-E, an explicit generative model over point clouds, Shap-E converges faster and reaches comparable or better sample quality despite modeling a higher-dimensional, multi-representation output space."),
          Markdown("[GitHub Page](https://github.com/openai/shap-e) - [Read the Paper](https://arxiv.org/pdf/2305.02463.pdf)", on_tap_link=lambda e: e.page.launch_url(e.data)),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("🚴  Shaping up... ", on_click=close_df_dlg)], actions_alignment=MainAxisAlignment.END)
      page.dialog = df_help_dlg
      df_help_dlg.open = True
      page.update()
    def file_picker_result(e: FilePickerResultEvent):
        if e.files != None:
          upload_files(e)
    def on_upload_progress(e: FilePickerUploadEvent):
      if e.progress == 1:
        if not slash in e.file_name:
          fname = os.path.join(root_dir, e.file_name)
          shap_e_prefs['file_name'] = e.file_name.rpartition('.')[0]
        else:
          fname = e.file_name
          shap_e_prefs['file_name'] = e.file_name.rparition(slash)[2].rpartition('.')[0]
        init_image.value = fname
        init_image.update()
        shap_e_prefs['init_image'] = fname
        page.update()
    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)
    def upload_files(e):
        uf = []
        if file_picker.result != None and file_picker.result.files != None:
            for f in file_picker.result.files:
              if page.web:
                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))
              else:
                on_upload_progress(FilePickerUploadEvent(f.path, 1, ""))
            file_picker.upload(uf)
    page.overlay.append(file_picker)
    #page.overlay.append(pick_files_dialog)
    def pick_original(e):
        file_picker.pick_files(allow_multiple=False, allowed_extensions=["png", "PNG", "jpg", "jpeg"], dialog_title="Pick Original Image File")
    prompt_text = TextField(label="Prompt Text", value=shap_e_prefs['prompt_text'], on_change=lambda e:changed(e,'prompt_text'))
    init_image = TextField(label="Sample Image (optional, instead of prompt)", value=shap_e_prefs['init_image'], on_change=lambda e:changed(e,'init_image'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_original))
    #base_model = Dropdown(label="Base Model", width=250, options=[dropdown.Option("base40M-imagevec"), dropdown.Option("base40M-textvec"), dropdown.Option("base40M"), dropdown.Option("base300M"), dropdown.Option("base1B")], value=shap_e_prefs['base_model'], on_change=lambda e: changed(e, 'base_model'))
    render_mode = Dropdown(label="Render Mode", width=250, options=[dropdown.Option("NeRF"), dropdown.Option("STF")], value=shap_e_prefs['render_mode'], on_change=lambda e: changed(e, 'render_mode'))
    size = SliderRow(label="Size of Render", min=32, max=512, divisions=15, multiple=32, tooltip="Higher values take longer to render.", suffix="px", pref=shap_e_prefs, key='size')
    karras_steps = SliderRow(label="Karras Steps", min=1, max=100, divisions=99, round=0, pref=shap_e_prefs, key='karras_steps')
    batch_folder_name = TextField(label="3D Model Folder Name", value=shap_e_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    #batch_size = TextField(label="Batch Size", value=shap_e_prefs['batch_size'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'batch_size', isInt=True), width = 90)
    batch_size = NumberPicker(label="Batch Size: ", min=1, max=5, value=shap_e_prefs['batch_size'], on_change=lambda e: changed(e, 'batch_size'))
    guidance = SliderRow(label="Guidance Scale", min=0, max=10, divisions=20, round=1, pref=shap_e_prefs, key='guidance_scale')
    save_frames = Checkbox(label="Save Preview Frames", tooltip="Saves PNG Sequence of camera rotation, same as animated gif preview.", value=shap_e_prefs['save_frames'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'save_frames'))
    #seed = TextField(label="Seed", value=shap_e_prefs['seed'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'seed', ptype='int'), width = 160)
    page.shap_e_output = Column([])
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.shap_e_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🧊  Shap-E 3D Mesh", "Provide a Prompt or Image to Generate Conditional 3D PLY Models...", actions=[IconButton(icon=icons.HELP, tooltip="Help with Shap-E Settings", on_click=df_help)]),
        prompt_text,
        init_image,
        Row([render_mode, save_frames]),
        guidance,
        karras_steps,
        size,
        Row([batch_folder_name, batch_size]),
        ElevatedButton(content=Text("🪀  Run Shap-E", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_shap_e(page)),
        page.shap_e_output,
        clear_button,
      ]
    ))], scroll=ScrollMode.AUTO)
    return c


instant_ngp_prefs = {
    'train_steps': 2000, #Total number of training steps to perform.  If provided, overrides num_train_epochs.
    'sharpen': 0.0,
    'exposure': 0.0,
    'vr_mode': False,
    'name_of_your_model': '',
    'save_model': True,
    'where_to_save_model': 'Public HuggingFace',
    'resolution': 768,
    'image_path': '',
    'readme_description': '',
    'urls': [],
}

def buildInstantNGP(page):
    global prefs, instant_ngp_prefs
    def changed(e, pref=None, ptype="str"):
        if pref is not None:
          try:
            if ptype == "int":
              instant_ngp_prefs[pref] = int(e.control.value)
            elif ptype == "float":
              instant_ngp_prefs[pref] = float(e.control.value)
            else:
              instant_ngp_prefs[pref] = e.control.value
          except Exception:
            alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
            pass
    def add_to_instant_ngp_output(o):
        page.instant_ngp_output.controls.append(o)
        page.instant_ngp_output.update()
    def clear_output(e):
        if prefs['enable_sounds']: page.snd_delete.play()
        page.instant_ngp_output.controls = []
        page.instant_ngp_output.update()
        clear_button.visible = False
        clear_button.update()
    def instant_ngp_help(e):
        def close_instant_ngp_dlg(e):
          nonlocal instant_ngp_help_dlg
          instant_ngp_help_dlg.open = False
          page.update()
        instant_ngp_help_dlg = AlertDialog(title=Text("💁   Help with Instant-NGP"), content=Column([
            Text("Ever wanted to train a NeRF model of a fox in under 5 seconds? Or fly around a scene captured from photos of a factory robot? Of course you have! Here you will find an implementation of four neural graphics primitives, being neural radiance fields (NeRF), signed distance functions (SDFs), neural images, and neural volumes. In each case, we train and render a MLP with multiresolution hash input encoding using the tiny-cuda-nn framework."),
            Text("We demonstrate near-instant training of neural graphics primitives on a single GPU for multiple tasks. In gigapixel image we represent an image by a neural network. SDF learns a signed distance function in 3D space whose zero level-set represents a 2D surface. NeRF [Mildenhall et al. 2020] uses 2D images and their camera poses to reconstruct a volumetric radiance-and-density field that is visualized using ray marching. Lastly, neural volume learns a denoised radiance and density field directly from a volumetric path tracer. In all tasks, our encoding and its efficient implementation provide clear benefits: instant training, high quality, and simplicity. Our encoding is task-agnostic: we use the same implementation and hyperparameters across all tasks and only vary the hash table size which trades off quality and performance."),
            Text("Neural graphics primitives, parameterized by fully connected neural networks, can be costly to train and evaluate. We reduce this cost with a versatile new input encoding that permits the use of a smaller network without sacrificing quality, thus significantly reducing the number of floating point and memory access operations. A small neural network is augmented by a multiresolution hash table of trainable feature vectors whose values are optimized through stochastic gradient descent. The multiresolution structure allows the network to disambiguate hash collisions, making for a simple architecture that is trivial to parallelize on modern GPUs. We leverage this parallelism by implementing the whole system using fully-fused CUDA kernels with a focus on minimizing wasted bandwidth and compute operations. We achieve a combined speedup of several orders of magnitude, enabling training of high-quality neural graphics primitives in a matter of seconds, and rendering in tens of milliseconds at a resolution of 1920x1080."),
            Markdown("[Project page](https://nvlabs.github.io/instant-ngp) / [Paper](https://nvlabs.github.io/instant-ngp/assets/mueller2022instant.pdf) / [Video](https://nvlabs.github.io/instant-ngp/assets/mueller2022instant.mp4) / [Presentation](https://tom94.net/data/publications/mueller22instant/mueller22instant-gtc.mp4) / [Real-Time Live](https://tom94.net/data/publications/mueller22instant/mueller22instant-rtl.mp4) / [BibTeX](https://nvlabs.github.io/instant-ngp/assets/mueller2022instant.bib)", on_tap_link=lambda e: e.page.launch_url(e.data)),
          ], scroll=ScrollMode.AUTO), actions=[TextButton("🌠  Like Magic... ", on_click=close_instant_ngp_dlg)], actions_alignment=MainAxisAlignment.END)
        page.dialog = instant_ngp_help_dlg
        instant_ngp_help_dlg.open = True
        page.update()
    def delete_image(e):
        f = e.control.data
        if os.path.isfile(f):
          os.remove(f)
          for i, fl in enumerate(page.instant_ngp_file_list.controls):
            if fl.title.value == f:
              del page.instant_ngp_file_list.controls[i]
              page.instant_ngp_file_list.update()
              continue
    def delete_all_images(e):
        for fl in page.instant_ngp_file_list.controls:
          f = fl.title.value
          if os.path.isfile(f):
            os.remove(f)
        page.instant_ngp_file_list.controls.clear()
        page.instant_ngp_file_list.update()
    def image_details(e):
        img = e.control.data
        alert_msg(e.page, "Image Details", content=Image(src=img), sound=False)
    def add_file(fpath, update=True):
        page.instant_ngp_file_list.controls.append(ListTile(title=Text(fpath), dense=False, trailing=PopupMenuButton(icon=icons.MORE_VERT,
          items=[
              PopupMenuItem(icon=icons.INFO, text="Image Details", on_click=image_details, data=fpath),
              PopupMenuItem(icon=icons.DELETE, text="Delete Image", on_click=delete_image, data=fpath),
              PopupMenuItem(icon=icons.DELETE_SWEEP, text="Delete All", on_click=delete_all_images, data=fpath),
          ]), data=fpath, on_click=image_details))
        if update: page.instant_ngp_file_list.update()
    def file_picker_result(e: FilePickerResultEvent):
        if e.files != None:
          upload_files(e)
    save_dir = os.path.join(root_dir, 'my_ngp')
    def on_upload_progress(e: FilePickerUploadEvent):
        if e.progress == 1:
          if not os.path.exists(save_dir):
            os.mkdir(save_dir)
          if not slash in e.file_name:
            fname = os.path.join(root_dir, e.file_name)
            fpath = os.path.join(save_dir, e.file_name)
          else:
            fname = e.file_name
            fpath = os.path.join(save_dir, e.file_name.rpartition(slash)[2])
          original_img = PILImage.open(fname)
          width, height = original_img.size
          width, height = scale_dimensions(width, height, instant_ngp_prefs['resolution'])
          original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert("RGB")
          original_img.save(fpath)
          if page.web:
            os.remove(fname)
          #shutil.move(fname, fpath)
          add_file(fpath)
    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)
    def pick_path(e):
        file_picker.pick_files(allow_multiple=True, allowed_extensions=["png", "PNG", "jpg", "jpeg", 'obj', 'stl', 'nvdb'], dialog_title="Pick Image File to Enlarge")
    def upload_files(e):
        uf = []
        if file_picker.result != None and file_picker.result.files != None:
            for f in file_picker.result.files:
              if page.web:
                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))
              else:
                on_upload_progress(FilePickerUploadEvent(f.path, 1, ""))
            file_picker.upload(uf)
    page.overlay.append(file_picker)
    def add_image(e):
        save_dir = os.path.join(root_dir, 'my_ngp')
        if not os.path.exists(save_dir):
          os.mkdir(save_dir)
        if image_path.value.startswith('http'):
          import requests
          from io import BytesIO
          response = requests.get(image_path.value)
          fpath = os.path.join(save_dir, image_path.value.rpartition(slash)[2])
          model_image = PILImage.open(BytesIO(response.content)).convert("RGB")
          width, height = model_image.size
          width, height = scale_dimensions(width, height, instant_ngp_prefs['resolution'])
          model_image = model_image.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert("RGB")
          model_image.save(fpath)
          add_file(fpath)
        elif os.path.isfile(image_path.value):
          fpath = os.path.join(save_dir, image_path.value.rpartition(slash)[2])
          original_img = PILImage.open(image_path.value)
          width, height = original_img.size
          width, height = scale_dimensions(width, height, instant_ngp_prefs['resolution'])
          original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert("RGB")
          original_img.save(fpath)
          #shutil.copy(image_path.value, fpath)
          add_file(fpath)
        elif os.path.isdir(image_path.value):
          for f in os.listdir(image_path.value):
            file_path = os.path.join(image_path.value, f)
            if os.path.isdir(file_path): continue
            if f.lower().endswith(('.png', '.jpg', '.jpeg', 'obj', 'stl', 'nvdb')):
              fpath = os.path.join(save_dir, f)
              original_img = PILImage.open(file_path)
              width, height = original_img.size
              width, height = scale_dimensions(width, height, instant_ngp_prefs['resolution'])
              original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert("RGB")
              original_img.save(fpath)
              #shutil.copy(file_path, fpath)
              add_file(fpath)
        else:
          if bool(image_path.value):
            alert_msg(page, "Couldn't find a valid File, Path or URL...")
          else:
            pick_path(e)
          return
        image_path.value = ""
        image_path.update()
    def load_images():
        if os.path.exists(save_dir):
          for f in os.listdir(save_dir):
            existing = os.path.join(save_dir, f)
            if os.path.isdir(existing): continue
            if f.lower().endswith(('.png', '.jpg', '.jpeg', 'obj', 'stl', 'nvdb')):
              add_file(existing, update=False)
    #instance_prompt = Container(content=Tooltip(message="The prompt with identifier specifying the instance", content=TextField(label="Instance Prompt Token Text", value=instant_ngp_prefs['instance_prompt'], on_change=lambda e:changed(e,'instance_prompt'))), col={'md':9})
    name_of_your_model = TextField(label="Name of your Model", value=instant_ngp_prefs['name_of_your_model'], on_change=lambda e:changed(e,'name_of_your_model'))
    train_steps = Tooltip(message="Total number of training steps to perform.  More the better, even around 35000..", content=TextField(label="Max Training Steps", value=instant_ngp_prefs['train_steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'train_steps', ptype='int'), width = 160))
    #save_model = Checkbox(label="Save Model to HuggingFace   ", tooltip="", value=instant_ngp_prefs['save_model'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'save_model'))
    #save_model = Tooltip(message="Requires WRITE access on API Key to Upload Checkpoint", content=Switcher(label="Save Model to HuggingFace    ", value=instant_ngp_prefs['save_model'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_save))
    #where_to_save_model = Dropdown(label="Where to Save Model", width=250, options=[dropdown.Option("Public HuggingFace"), dropdown.Option("Private HuggingFace")], value=instant_ngp_prefs['where_to_save_model'], on_change=lambda e: changed(e, 'where_to_save_model'))
    #class_data_dir = TextField(label="Prior Preservation Class Folder", value=instant_ngp_prefs['class_data_dir'], on_change=lambda e:changed(e,'class_data_dir'))
    #readme_description = TextField(label="Extra README Description", value=instant_ngp_prefs['readme_description'], on_change=lambda e:changed(e,'readme_description'))
    max_row = SliderRow(label="Max Resolution Size", min=256, max=1024, divisions=6, multiple=64, suffix="px", pref=instant_ngp_prefs, key='resolution')

    sharpen = Row([Text(" Shapen Images:"), Slider(label="{value}", min=0, max=1, divisions=10, expand=True, value=instant_ngp_prefs['sharpen'], on_change=lambda e: changed(e, 'sharpen'))], col={'lg':6})
    exposure = Row([Text(" Image Exposure:"), Slider(label="{value}", min=0, max=1, divisions=10, expand=True, value=instant_ngp_prefs['exposure'], on_change=lambda e: changed(e, 'exposure'))], col={'lg':6})
    vr_mode = Checkbox(label="Output VR Mode", tooltip="Render to a VR headset", value=instant_ngp_prefs['vr_mode'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'vr_mode'))
    image_path = TextField(label="Image Files or Folder Path or URL to Train", value=instant_ngp_prefs['image_path'], on_change=lambda e:changed(e,'image_path'), suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_path), expand=1)
    add_image_button = ElevatedButton(content=Text("Add File or Folder"), on_click=add_image)
    page.instant_ngp_file_list = Column([], tight=True, spacing=0)
    load_images()
    #where_to_save_model.visible = instant_ngp_prefs['save_model']
    #readme_description.visible = instant_ngp_prefs['save_model']
    #lambda_entropy = TextField(label="Lambda Entropy", value=dreamfusinstant_ngp_prefsion_prefs['lambda_entropy'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'lambda_entropy', ptype='float'), width = 160)
    #max_steps = TextField(label="Max Steps", value=instant_ngp_prefs['max_steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'max_steps', ptype='int'), width = 160)
    page.instant_ngp_output = Column([])
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.instant_ngp_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🎑  Instant Neural Graphics Primitives by NVidia", "Convert series of images into 3D Models with Multiresolution Hash Encoding...", actions=[IconButton(icon=icons.HELP, tooltip="Help with Instant NGP Settings", on_click=instant_ngp_help)]),
        Row([name_of_your_model]),
        Row([train_steps, vr_mode]),
        ResponsiveRow([sharpen, exposure]),
        #Row([save_model, where_to_save_model]),
        #readme_description,
        #Row([class_data_dir]),
        max_row,
        Text("The scene to load. Can be full path to the training data, multiple image angles, NeRF dataset, a *.obj/*.stl mesh for training a SDF, images, or a *.nvdb volume."),
        Row([image_path, add_image_button]),
        page.instant_ngp_file_list,
        Row([ElevatedButton(content=Text("🎇  Run Instant-NGP", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_instant_ngp(page))]),
        page.instant_ngp_output,
        clear_button,
      ]
    ))], scroll=ScrollMode.AUTO)
    return c


repaint_prefs = {
    'original_image': '',
    'mask_image': '',
    'num_inference_steps': 500,
    'eta': 0.0,
    'jump_length': 10,
    'jump_n_sample': 10,
    'seed': 0,
    'file_name': '',
    'max_size': 1024,
    'invert_mask': False,
}
def buildRepainter(page):
    global repaint_prefs, prefs, pipe_repaint
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          if ptype == "int":
            repaint_prefs[pref] = int(e.control.value)
          elif ptype == "float":
            repaint_prefs[pref] = float(e.control.value)
          else:
            repaint_prefs[pref] = e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def add_to_repaint_output(o):
      page.repaint_output.controls.append(o)
      page.repaint_output.update()
      if not clear_button.visible:
        clear_button.visible = True
        clear_button.update()
    def clear_output(e):
      if prefs['enable_sounds']: page.snd_delete.play()
      page.repaint_output.controls = []
      page.repaint_output.update()
      clear_button.visible = False
      clear_button.update()
    def repaint_help(e):
      def close_repaint_dlg(e):
        nonlocal repaint_help_dlg
        repaint_help_dlg.open = False
        page.update()
      repaint_help_dlg = AlertDialog(title=Text("💁   Help with Repainter"), content=Column([
          Text("It's difficult to explain exactly what all these parameters do, but keep it close to defaults, keep prompt simple, or experiment to see what's what, we don't know."),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("😪  Okay then... ", on_click=close_repaint_dlg)], actions_alignment=MainAxisAlignment.END)
      page.dialog = repaint_help_dlg
      repaint_help_dlg.open = True
      page.update()
    def file_picker_result(e: FilePickerResultEvent):
        if e.files != None:
          upload_files(e)
    def on_upload_progress(e: FilePickerUploadEvent):
      nonlocal pick_type
      if e.progress == 1:
        if not slash in e.file_name:
          fname = os.path.join(root_dir, e.file_name)
          repaint_prefs['file_name'] = e.file_name.rpartition('.')[0]
        else:
          fname = e.file_name
          repaint_prefs['file_name'] = e.file_name.rpartition(slash)[2].rpartition('.')[0]
        if pick_type == "original":
          original_image.value = fname
          original_image.update()
          repaint_prefs['original_image'] = fname
        elif pick_type == "mask":
          mask_image.value = fname
          mask_image.update()
          repaint_prefs['mask_image'] = fname
        page.update()
    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)
    def upload_files(e):
        uf = []
        if file_picker.result != None and file_picker.result.files != None:
            for f in file_picker.result.files:
              if page.web:
                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))
              else:
                on_upload_progress(FilePickerUploadEvent(f.path, 1, ""))
            file_picker.upload(uf)
    page.overlay.append(file_picker)
    pick_type = ""
    #page.overlay.append(pick_files_dialog)
    def pick_original(e):
        nonlocal pick_type
        pick_type = "original"
        file_picker.pick_files(allow_multiple=False, allowed_extensions=["png", "PNG", "jpg", "jpeg"], dialog_title="Pick Original Image File")
    def pick_mask(e):
        nonlocal pick_type
        pick_type = "mask"
        file_picker.pick_files(allow_multiple=False, allowed_extensions=["png", "PNG", "jpg", "jpeg"], dialog_title="Pick Black & White Mask Image")
    def change_eta(e):
        changed(e, 'eta', ptype="float")
        eta_value.value = f" {repaint_prefs['eta']}"
        eta_value.update()
        eta_row.update()
    original_image = TextField(label="Original Image", value=repaint_prefs['original_image'], expand=1, on_change=lambda e:changed(e,'original_image'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_original))
    mask_image = TextField(label="Mask Image", value=repaint_prefs['mask_image'], expand=1, on_change=lambda e:changed(e,'mask_image'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD_OUTLINED, on_click=pick_mask))
    invert_mask = Checkbox(label="Invert", tooltip="Swaps the Black & White of your Mask Image", value=repaint_prefs['invert_mask'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'invert_mask'))
    jump_length = TextField(label="Jump Length", width=130, tooltip="The number of steps taken forward in time before going backward in time for a single jump", value=repaint_prefs['jump_length'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'jump_length', ptype='int'))
    jump_n_sample = TextField(label="Jump # of Sample", width=130, tooltip="The number of times we will make forward time jump for a given chosen time sample.", value=repaint_prefs['jump_n_sample'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'jump_n_sample', ptype='int'))
    seed = TextField(label="Seed", width=90, value=str(repaint_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    #num_inference_steps = TextField(label="Inference Steps", value=str(repaint_prefs['num_inference_steps']), keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'num_inference_steps', ptype='int'))
    num_inference_row = SliderRow(label="Number of Inference Steps", min=10, max=3000, divisions=2990, pref=repaint_prefs, key='num_inference_steps', tooltip="The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.")
    #eta = TextField(label="ETA", value=str(repaint_prefs['eta']), keyboard_type=KeyboardType.NUMBER, hint_text="Amount of Noise", on_change=lambda e:changed(e,'eta', ptype='float'))
    eta = Slider(min=0.0, max=1.0, divisions=20, label="{value}", value=float(repaint_prefs['eta']), tooltip="The weight of noise for added noise in a diffusion step. Its value is between 0.0 and 1.0 - 0.0 is DDIM and 1.0 is DDPM scheduler respectively.", expand=True, on_change=change_eta)
    eta_value = Text(f" {repaint_prefs['eta']}", weight=FontWeight.BOLD)
    eta_row = Row([Text("ETA:"), eta_value, Text("  DDIM"), eta, Text("DDPM")])
    page.etas.append(eta_row)
    max_row = SliderRow(label="Max Resolution Size", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=repaint_prefs, key='max_size')
    page.repaint_output = Column([])
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.repaint_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("💅  Repaint masked areas of an image", "Fills in areas of picture with what it thinks it should be, without a prompt...", actions=[IconButton(icon=icons.HELP, tooltip="Help with Repainter Settings", on_click=repaint_help)]),
        Row([original_image, mask_image, invert_mask]),
        num_inference_row,
        eta_row,
        max_row,
        Row([jump_length, jump_n_sample, seed]),
        ElevatedButton(content=Text("🖌️  Run Repainter", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_repainter(page)),
        page.repaint_output,
        clear_button,
      ]
    ))], scroll=ScrollMode.AUTO)
    return c

image_variation_prefs = {
    'init_image': '',
    'guidance_scale': 7.5,
    'num_inference_steps': 50,
    'eta': 0.4,
    'seed': 0,
    'num_images': 1,
    'file_name': '',
    'max_size': 1024,
    'width': 960,
    'height': 512,
}
def buildImageVariation(page):
    global image_variation_prefs, prefs, pipe_image_variation
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          if ptype == "int":
            image_variation_prefs[pref] = int(e.control.value)
          elif ptype == "float":
            image_variation_prefs[pref] = float(e.control.value)
          else:
            image_variation_prefs[pref] = e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def add_to_image_variation_output(o):
      page.image_variation_output.controls.append(o)
      page.image_variation_output.update()
      if not clear_button.visible:
        clear_button.visible = True
        clear_button.update()
    page.add_to_image_variation_output = add_to_image_variation_output
    def clear_output(e):
      if prefs['enable_sounds']: page.snd_delete.play()
      page.image_variation_output.controls = []
      page.image_variation_output.update()
      clear_button.visible = False
      clear_button.update()
    def image_variation_help(e):
      def close_image_variation_dlg(e):
        nonlocal image_variation_help_dlg
        image_variation_help_dlg.open = False
        page.update()
      image_variation_help_dlg = AlertDialog(title=Text("🙅   Help with Image Variations"), content=Column([
          Text("Give it any of your favorite images and create variations of it.... Simple as that, no prompt needed."),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("🤗  Sounds Fun... ", on_click=close_image_variation_dlg)], actions_alignment=MainAxisAlignment.END)
      page.dialog = image_variation_help_dlg
      image_variation_help_dlg.open = True
      page.update()
    def file_picker_result(e: FilePickerResultEvent):
        if e.files != None:
          upload_files(e)
    def on_upload_progress(e: FilePickerUploadEvent):
      if e.progress == 1:
        if not slash in e.file_name:
          fname = os.path.join(root_dir, e.file_name)
          image_variation_prefs['file_name'] = e.file_name.rpartition('.')[0]
        else:
          fname = e.file_name
          image_variation_prefs['file_name'] = e.file_name.rpartition(slash)[2].rpartition('.')[0]
        init_image.value = fname
        init_image.update()
        image_variation_prefs['init_image'] = fname
        page.update()
    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)
    def upload_files(e):
        uf = []
        if file_picker.result != None and file_picker.result.files != None:
            for f in file_picker.result.files:
              if page.web:
                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))
              else:
                on_upload_progress(FilePickerUploadEvent(f.path, 1, ""))
            file_picker.upload(uf)
    page.overlay.append(file_picker)
    def pick_init(e):
        file_picker.pick_files(allow_multiple=False, allowed_extensions=["png", "PNG", "jpg", "jpeg"], dialog_title="Pick init Image File")
    init_image = TextField(label="Initial Image", value=image_variation_prefs['init_image'], on_change=lambda e:changed(e,'init_image'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_init))
    seed = TextField(label="Seed", width=90, value=str(image_variation_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    guidance = SliderRow(label="Guidance Scale", min=0, max=50, divisions=100, round=1, pref=image_variation_prefs, key='guidance_scale')
    num_inference_row = SliderRow(label="Number of Inference Steps", min=1, max=100, divisions=99, pref=image_variation_prefs, key='num_inference_steps', tooltip="The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.")
    #eta = TextField(label="ETA", value=str(image_variation_prefs['eta']), keyboard_type=KeyboardType.NUMBER, hint_text="Amount of Noise", on_change=lambda e:changed(e,'eta', ptype='float'))
    eta = Slider(min=0.0, max=1.0, divisions=20, label="{value}", value=float(image_variation_prefs['eta']), tooltip="The weight of noise for added noise in a diffusion step. Its value is between 0.0 and 1.0 - 0.0 is DDIM and 1.0 is DDPM scheduler respectively.", expand=True, on_change=lambda e:changed(e,'eta', ptype='float'))
    eta_row = Row([Text("DDIM ETA: "), eta])
    page.etas.append(eta_row)
    max_row = SliderRow(label="Max Resolution Size", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=image_variation_prefs, key='max_size')
    page.image_variation_output = Column([])
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.image_variation_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🪩  Image Variations of any Init Image", "Creates a new version of your picture, without a prompt...", actions=[IconButton(icon=icons.HELP, tooltip="Help with Image Variation Settings", on_click=image_variation_help)]),
        init_image,
        #Row([init_image, mask_image, invert_mask]),
        num_inference_row,
        guidance,
        eta_row,
        max_row,
        Row([NumberPicker(label="Number of Images: ", min=1, max=8, value=image_variation_prefs['num_images'], on_change=lambda e: changed(e, 'num_images')), seed]),
        ElevatedButton(content=Text("🖍️  Get Image Variation", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_image_variation(page)),
        page.image_variation_output,
        clear_button,
      ]
    ))], scroll=ScrollMode.AUTO, auto_scroll=False)
    return c

background_remover_prefs = {
    'init_image': '',
    'threshold': 100,
    'file_name': '',
    'max_size': 1024,
    'transparent_png': False,
    'save_mask': False,
    'output_name': '',
    'batch_folder_name': '',
}
def buildBackgroundRemover(page):
    global background_remover_prefs, prefs, pipe_background_remover
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          if ptype == "int":
            background_remover_prefs[pref] = int(e.control.value)
          elif ptype == "float":
            background_remover_prefs[pref] = float(e.control.value)
          else:
            background_remover_prefs[pref] = e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def add_to_background_remover_output(o):
      page.background_remover_output.controls.append(o)
      page.background_remover_output.update()
      if not clear_button.visible:
        clear_button.visible = True
        clear_button.update()
    page.add_to_background_remover_output = add_to_background_remover_output
    def clear_output(e):
      if prefs['enable_sounds']: page.snd_delete.play()
      page.background_remover_output.controls = []
      page.background_remover_output.update()
      clear_button.visible = False
      clear_button.update()
    def background_remover_help(e):
      def close_background_remover_dlg(e):
        nonlocal background_remover_help_dlg
        background_remover_help_dlg.open = False
        page.update()
      background_remover_help_dlg = AlertDialog(title=Text("🙅   Help with Background Remover"), content=Column([
          Text("Give it any of your favorite images and it finds the main subject within the threshold value and gives you a cleaned up version back.... Simple as that, very useful to reuse as init image in another pipeline without needing to edit in Photoshop first.."),
          Markdown("[MODNet GitHub](https://github.com/Mazhar004/MODNet-BGRemover) | [HuggingFace Space](https://huggingface.co/spaces/nateraw/background-remover)", on_tap_link=lambda e: e.page.launch_url(e.data)),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("😸  Quite Convenient... ", on_click=close_background_remover_dlg)], actions_alignment=MainAxisAlignment.END)
      page.dialog = background_remover_help_dlg
      background_remover_help_dlg.open = True
      page.update()
    def file_picker_result(e: FilePickerResultEvent):
        if e.files != None:
          upload_files(e)
    def on_upload_progress(e: FilePickerUploadEvent):
      if e.progress == 1:
        if not slash in e.file_name:
          fname = os.path.join(root_dir, e.file_name)
          background_remover_prefs['file_name'] = e.file_name.rpartition('.')[0]
        else:
          fname = e.file_name
          background_remover_prefs['file_name'] = e.file_name.rpartition(slash)[2].rpartition('.')[0]
        init_image.value = fname
        init_image.update()
        background_remover_prefs['init_image'] = fname
        page.update()
    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)
    def upload_files(e):
        uf = []
        if file_picker.result != None and file_picker.result.files != None:
            for f in file_picker.result.files:
              if page.web:
                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))
              else:
                on_upload_progress(FilePickerUploadEvent(f.path, 1, ""))
            file_picker.upload(uf)
    page.overlay.append(file_picker)
    def pick_init(e):
        file_picker.pick_files(allow_multiple=False, allowed_extensions=["png", "PNG", "jpg", "jpeg"], dialog_title="Pick init Image File")
    init_image = TextField(label="Initial Image", value=background_remover_prefs['init_image'], on_change=lambda e:changed(e,'init_image'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_init))
    threshold = SliderRow(label="Mask Cutoff Threshold", min=0, max=250, divisions=250, round=0, pref=background_remover_prefs, key='threshold')
    max_row = SliderRow(label="Max Resolution Size", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=background_remover_prefs, key='max_size')
    save_mask = Switcher(label="Save B&W Mask", value=background_remover_prefs['save_mask'], tooltip="Gives you a Mask File you can reuse for Inpainting.", on_change=lambda e:changed(e,'save_mask'))
    output_name = TextField(label="Output File Name", value=background_remover_prefs['output_name'], on_change=lambda e:changed(e,'output_name'))
    batch_folder_name = TextField(label="Batch Folder Name", value=background_remover_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    page.background_remover_output = Column([])
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.background_remover_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🖼  MODNet Background Remover", "A deep learning approach to clear the background of most images to isolate subject...", actions=[IconButton(icon=icons.HELP, tooltip="Help with Background Remover Settings", on_click=background_remover_help)]),
        init_image,
        threshold,
        max_row,
        save_mask,
        Row([output_name, batch_folder_name]),
        ElevatedButton(content=Text("🏘  Get Background Remover", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_background_remover(page)),
        page.background_remover_output,
        clear_button,
      ]
    ))], scroll=ScrollMode.AUTO, auto_scroll=False)
    return c


reference_prefs = {
    'ref_image': '',
    'prompt': '',
    'negative_prompt': '',
    'guidance_scale': 7.5,
    'num_inference_steps': 50,
    'seed': 0,
    'eta': 0.0,
    'attention_auto_machine_weight': 1.0,
    'gn_auto_machine_weight': 1.0,
    'style_fidelity': 0.5,
    'reference_attn': True,
    'reference_adain': True,
    'batch_size': 1,
    'num_images': 1,
    'max_size': 1024,
    'width': 960,
    'height': 768,
    'last_model': '',
    'batch_folder_name': '',
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": 4.0,
    "display_upscaled_image": False,
}
def buildReference(page):
    global reference_prefs, prefs, pipe_reference
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          if ptype == "int":
            reference_prefs[pref] = int(e.control.value)
          elif ptype == "float":
            reference_prefs[pref] = float(e.control.value)
          else:
            reference_prefs[pref] = e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def add_to_reference_output(o):
      page.Reference.controls.append(o)
      page.Reference.update()
      if not clear_button.visible:
        clear_button.visible = True
        clear_button.update()
    page.add_to_reference_output = add_to_reference_output
    def clear_output(e):
      if prefs['enable_sounds']: page.snd_delete.play()
      for i, c in enumerate(page.Reference.controls):
        if i == 0: continue
        else: del page.Reference.controls[i]
      page.Reference.update()
      clear_button.visible = False
      clear_button.update()
    def reference_help(e):
      def close_reference_dlg(e):
        nonlocal reference_help_dlg
        reference_help_dlg.open = False
        page.update()
      reference_help_dlg = AlertDialog(title=Text("🙅   Help with Reference-Only"), content=Column([
          Text("This reference-only ControlNet can directly link the attention layers of your SD to any independent images, so that your SD will read arbitary images for reference. This preprocessor does not require any control models. It can guide the diffusion directly using images as references. This method is similar to inpaint-based reference but it does not make your image disordered."),
          Text("Note that this method is as 'non-opinioned' as possible. It only contains very basic connection codes, without any personal preferences, to connect the attention layers with your reference images. However, even if we tried best to not include any opinioned codes, we still need to write some subjective implementations to deal with weighting, cfg-scale, etc - tech report is on the way."),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("🧙  Powerful... ", on_click=close_reference_dlg)], actions_alignment=MainAxisAlignment.END)
      page.dialog = reference_help_dlg
      reference_help_dlg.open = True
      page.update()
    def file_picker_result(e: FilePickerResultEvent):
        if e.files != None:
          upload_files(e)
    def on_upload_progress(e: FilePickerUploadEvent):
      if e.progress == 1:
        if not slash in e.file_name:
          fname = os.path.join(root_dir, e.file_name)
          reference_prefs['file_name'] = e.file_name.rpartition('.')[0]
        else:
          fname = e.file_name
          reference_prefs['file_name'] = e.file_name.rpartition(slash)[2].rpartition('.')[0]
        ref_image.value = fname
        ref_image.update()
        reference_prefs['ref_image'] = fname
        page.update()
    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)
    def upload_files(e):
        uf = []
        if file_picker.result != None and file_picker.result.files != None:
            for f in file_picker.result.files:
              if page.web:
                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))
              else:
                on_upload_progress(FilePickerUploadEvent(f.path, 1, ""))
            file_picker.upload(uf)
    page.overlay.append(file_picker)
    def pick_init(e):
        file_picker.pick_files(allow_multiple=False, allowed_extensions=["png", "PNG", "jpg", "jpeg"], dialog_title="Pick Init Image File")
    prompt = TextField(label="Prompt Text", value=reference_prefs['prompt'], col={'md':9}, on_change=lambda e:changed(e,'prompt'))
    negative_prompt  = TextField(label="Negative Prompt Text", value=reference_prefs['negative_prompt'], col={'md':3}, on_change=lambda e:changed(e,'negative_prompt'))
    def toggle_ESRGAN(e):
        ESRGAN_settings.height = None if e.control.value else 0
        reference_prefs['apply_ESRGAN_upscale'] = e.control.value
        ESRGAN_settings.update()
    ref_image = TextField(label="Reference Image", value=reference_prefs['ref_image'], on_change=lambda e:changed(e,'ref_image'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_init))
    seed = TextField(label="Seed", width=90, value=str(reference_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    num_inference_row = SliderRow(label="Number of Inference Steps", min=1, max=100, divisions=99, pref=reference_prefs, key='num_inference_steps', tooltip="The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.")
    guidance = SliderRow(label="Guidance Scale", min=0, max=50, divisions=100, round=1, pref=reference_prefs, key='guidance_scale')
    eta = SliderRow(label="DDIM ETA", min=0, max=1, divisions=20, round=1, pref=reference_prefs, key='eta', tooltip="", visible=False)
    page.etas.append(eta)
    style_fidelity = SliderRow(label="Style Fidelity", min=0.0, max=1.0, divisions=20, round=2, pref=reference_prefs, key='style_fidelity', tooltip="Style fidelity of ref_uncond_xt. If style_fidelity=1.0, control more important, elif style_fidelity=0.0, prompt more important, else balanced.")
    attention_auto_machine_weight = SliderRow(label="Attention Auto Machine Weight", min=0.0, max=1.0, divisions=20, round=2, pref=reference_prefs, key='attention_auto_machine_weight', tooltip="Weight of using reference query for self attention's context. If attention_auto_machine_weight=1.0, use reference query for all self attention's context.")
    gn_auto_machine_weight = SliderRow(label="Gn Auto Machine Weight", min=0.0, max=2.0, divisions=20, round=1, pref=reference_prefs, key='gn_auto_machine_weight', tooltip="Weight of using reference adain. If gn_auto_machine_weight=2.0, use all reference adain plugins.")
    #reference_attn = Checkbox(label="Reference Attention", value=reference_prefs['reference_attn'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'reference_attn'), tooltip="Whether to use reference query for self attention's context.")
    #reference_adain = Checkbox(label="Reference Adain", value=reference_prefs['reference_adain'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'reference_adain'), tooltip="Whether to use reference Adain (Adaptive Instance Normalization).")
    reference_attn = Switcher(label="Reference Attention ", value=reference_prefs['reference_attn'], on_change=lambda e:changed(e,'reference_attn'), tooltip="Whether to use reference query for self attention's context.")
    reference_adain = Switcher(label="Reference Adain", value=reference_prefs['reference_adain'], on_change=lambda e:changed(e,'reference_adain'), tooltip="Whether to use reference Adain (Adaptive Instance Normalization).")
    max_row = SliderRow(label="Max Resolution Size", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=reference_prefs, key='max_size')
    width_slider = SliderRow(label="Width", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=reference_prefs, key='width')
    height_slider = SliderRow(label="Height", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=reference_prefs, key='height')
    batch_size = NumberPicker(label="Batch Size: ", min=1, max=8, value=reference_prefs['batch_size'], on_change=lambda e: changed(e, 'batch_size'))
    num_images = NumberPicker(label="Number of Iterations: ", min=1, max=12, value=reference_prefs['num_images'], on_change=lambda e: changed(e, 'num_images'))
    batch_folder_name = TextField(label="Batch Folder Name", value=reference_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    apply_ESRGAN_upscale = Switcher(label="Apply ESRGAN Upscale", value=reference_prefs['apply_ESRGAN_upscale'], on_change=toggle_ESRGAN)
    enlarge_scale_slider = SliderRow(label="Enlarge Scale", min=1, max=4, divisions=6, round=1, suffix="x", pref=reference_prefs, key='enlarge_scale')
    display_upscaled_image = Checkbox(label="Display Upscaled Image", value=reference_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))
    ESRGAN_settings = Container(Column([enlarge_scale_slider, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    page.ESRGAN_block_reference = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    page.ESRGAN_block_reference.height = None if status['installed_ESRGAN'] else 0
    if not reference_prefs['apply_ESRGAN_upscale']:
        ESRGAN_settings.height = 0
    page.reference_output = Column([], auto_scroll=True)
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.reference_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🎩  Reference-Only Image with Prompt", "ControlNet Pipeline for Transfering Ref Subject to new images...", actions=[IconButton(icon=icons.HELP, tooltip="Help with Reference Settings", on_click=reference_help)]),
        ref_image,
        ResponsiveRow([prompt, negative_prompt]),
        num_inference_row,
        guidance,
        eta,
        style_fidelity,
        ResponsiveRow([attention_auto_machine_weight, gn_auto_machine_weight]),
        Row([reference_attn, reference_adain]),
        max_row,
        width_slider,
        height_slider,
        ResponsiveRow([Row([batch_size, num_images], col={'lg':6}), Row([seed, batch_folder_name], col={'lg':6})]),
        page.ESRGAN_block_reference,
        Row([ElevatedButton(content=Text("💗  Make Reference", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_reference(page)),
             ElevatedButton(content=Text(value="📜   Run from Prompts List", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_reference(page, from_list=True))]),
        page.reference_output,
        clear_button,
      ]
    ))], scroll=ScrollMode.AUTO, auto_scroll=False)
    return c

controlnet_qr_prefs = {
    'init_image': '',
    'ref_image': '',
    'qr_content': '',
    'border_thickness': 5,
    'use_image': False,
    'prompt': '',
    'negative_prompt': 'ugly, disfigured, low quality, blurry, nsfw',
    'guidance_scale': 8.0,
    'conditioning_scale': 1.8,
    'control_guidance_start': 0.0,
    'control_guidance_end': 1.0,
    'strength': 0.9,
    'num_inference_steps': 50,
    'controlnet_version': "Stable Diffusion 2.1",
    'seed': 0,
    'batch_size': 1,
    'num_images': 1,
    'max_size': 768,
    'last_model': '',
    'last_controlnet_model': '',
    'batch_folder_name': '',
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": 2.0,
    "display_upscaled_image": False,
}
def buildControlNetQR(page):
    global controlnet_qr_prefs, prefs, pipe_controlnet_qr
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          if ptype == "int":
            controlnet_qr_prefs[pref] = int(e.control.value)
          elif ptype == "float":
            controlnet_qr_prefs[pref] = float(e.control.value)
          else:
            controlnet_qr_prefs[pref] = e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def add_to_controlnet_qr_output(o):
      page.ControlNetQR.controls.append(o)
      page.ControlNetQR.update()
      if not clear_button.visible:
        clear_button.visible = True
        clear_button.update()
    page.add_to_controlnet_qr_output = add_to_controlnet_qr_output
    def clear_output(e):
      if prefs['enable_sounds']: page.snd_delete.play()
      for i, c in enumerate(page.ControlNetQR.controls):
        if i == 0: continue
        else: del page.ControlNetQR.controls[i]
      page.ControlNetQR.update()
      clear_button.visible = False
      clear_button.update()
    def controlnet_qr_help(e):
      def close_controlnet_qr_dlg(e):
        nonlocal controlnet_qr_help_dlg
        controlnet_qr_help_dlg.open = False
        page.update()
      controlnet_qr_help_dlg = AlertDialog(title=Text("🙅   Help with ControlNet-QRCode"), content=Column([
          Text("These ControlNet models have been trained on a large dataset of 150,000 QR code + QR code artwork couples. They provide a solid foundation for generating QR code-based artwork that is aesthetically pleasing, while still maintaining the integral QR code shape."),
          Text("The Stable Diffusion 2.1 version is marginally more effective, as it was developed to address my specific needs. However, a 1.5 version model was also trained on the same dataset for those who are using the older version."),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("🥃  Scantastic... ", on_click=close_controlnet_qr_dlg)], actions_alignment=MainAxisAlignment.END)
      page.dialog = controlnet_qr_help_dlg
      controlnet_qr_help_dlg.open = True
      page.update()
    def file_picker_result(e: FilePickerResultEvent):
        if e.files != None:
            upload_files(e)
    def on_upload_progress(e: FilePickerUploadEvent):
        nonlocal pick_type
        if e.progress == 1:
            if not slash in e.file_name:
                fname = os.path.join(root_dir, e.file_name)
                controlnet_qr_prefs['file_name'] = e.file_name.rpartition('.')[0]
            else:
                fname = e.file_name
                controlnet_qr_prefs['file_name'] = e.file_name.rpartition(slash)[2].rpartition('.')[0]
            if pick_type == "init":
                init_image.value = fname
                init_image.update()
                controlnet_qr_prefs['init_image'] = fname
            elif pick_type == "ref":
                ref_image.value = fname
                ref_image.update()
                controlnet_qr_prefs['ref_image'] = fname
            page.update()
    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)
    def upload_files(e):
        uf = []
        if file_picker.result != None and file_picker.result.files != None:
            for f in file_picker.result.files:
              if page.web:
                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))
              else:
                on_upload_progress(FilePickerUploadEvent(f.path, 1, ""))
            file_picker.upload(uf)
    page.overlay.append(file_picker)
    pick_type = ""
    def pick_ref(e):
        nonlocal pick_type
        pick_type = "ref"
        file_picker.pick_files(allow_multiple=False, allowed_extensions=["png", "PNG", "jpg", "jpeg"], dialog_title="Pick QR Code Image File")
    def pick_init(e):
        nonlocal pick_type
        pick_type = "init"
        file_picker.pick_files(allow_multiple=False, allowed_extensions=["png", "PNG", "jpg", "jpeg"], dialog_title="Pick Init Image File")
    prompt = TextField(label="Prompt Text", value=controlnet_qr_prefs['prompt'], col={'md':9}, on_change=lambda e:changed(e,'prompt'))
    negative_prompt  = TextField(label="Negative Prompt Text", value=controlnet_qr_prefs['negative_prompt'], col={'md':3}, on_change=lambda e:changed(e,'negative_prompt'))
    def toggle_ESRGAN(e):
        ESRGAN_settings.height = None if e.control.value else 0
        controlnet_qr_prefs['apply_ESRGAN_upscale'] = e.control.value
        ESRGAN_settings.update()
    def toggle_image(e):
        controlnet_qr_prefs['use_image'] = e.control.value
        qr_generator.height = None if not e.control.value else 0
        qr_generator.update()
        qr_content.visible = not e.control.value
        qr_content.update()
        ref_image.visible = e.control.value
        ref_image.update()
    qr_content = TextField(label="QR Code Content (URL or whatever text)", value=controlnet_qr_prefs['qr_content'], expand=True, visible=not controlnet_qr_prefs['use_image'], on_change=lambda e:changed(e,'qr_content'))
    init_image = TextField(label="Initial Image (optional)", value=controlnet_qr_prefs['init_image'], expand=True, on_change=lambda e:changed(e,'init_image'), height=64, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_init))
    ref_image = TextField(label="ControlNet QR Code Image", value=controlnet_qr_prefs['ref_image'], expand=True, visible=controlnet_qr_prefs['use_image'], on_change=lambda e:changed(e,'ref_image'), height=64, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_ref))
    use_image = Switcher(label="Use QR Image Instead", value=controlnet_qr_prefs['use_image'], tooltip="Provide your own QR Code made elsewhere, or generate here.", on_change=toggle_image)
    #use_image = Switcher(label="Use QR Image Instead", value=controlnet_qr_prefs['use_image'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_image)
    border_thickness = SliderRow(label="Border Thickness", min=0, max=20, divisions=20, round=0, pref=controlnet_qr_prefs, key='border_thickness', tooltip="The border is equal to the thickness of 5 tiny black boxes around QR.")
    qr_generator = Container(content=Column([border_thickness], alignment=MainAxisAlignment.START), animate_size=animation.Animation(1000, AnimationCurve.EASE_IN), clip_behavior=ClipBehavior.HARD_EDGE)
    qr_generator.height = None if not controlnet_qr_prefs['use_image'] else 0
    controlnet_version = Dropdown(label="ControlNet QRCode Version", width=250, options=[dropdown.Option("Stable Diffusion 2.1"), dropdown.Option("Stable Diffusion 1.5")], value=controlnet_qr_prefs['controlnet_version'], on_change=lambda e: changed(e, 'controlnet_version'))
    seed = TextField(label="Seed", width=90, value=str(controlnet_qr_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    num_inference_row = SliderRow(label="Number of Inference Steps", min=1, max=100, divisions=99, pref=controlnet_qr_prefs, key='num_inference_steps', tooltip="The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.")
    guidance = SliderRow(label="Guidance Scale", min=0, max=50, divisions=100, round=1, pref=controlnet_qr_prefs, key='guidance_scale')
    conditioning_scale = SliderRow(label="Conditioning Scale", min=0.0, max=5.0, divisions=50, round=1, pref=controlnet_qr_prefs, key='conditioning_scale', tooltip="Strength of the ControlNet Mask.")
    control_guidance_start = SliderRow(label="Control Guidance Start", min=0.0, max=1.0, divisions=10, round=1, expand=True, pref=controlnet_qr_prefs, key='control_guidance_start', tooltip="The percentage of total steps at which the controlnet starts applying.")
    control_guidance_end = SliderRow(label="Control Guidance End", min=0.0, max=1.0, divisions=10, round=1, expand=True, pref=controlnet_qr_prefs, key='control_guidance_end', tooltip="The percentage of total steps at which the controlnet stops applying.")
    strength = SliderRow(label="Init Image Strength", min=0.0, max=1.0, divisions=20, round=2, pref=controlnet_qr_prefs, key='strength', tooltip="How strong the Initial Image should be over the ControlNet. Higher value give less influence.")
    max_size = SliderRow(label="Max Resolution Size", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=controlnet_qr_prefs, key='max_size')
    batch_size = NumberPicker(label="Batch Size: ", min=1, max=8, value=controlnet_qr_prefs['batch_size'], on_change=lambda e: changed(e, 'batch_size'))
    num_images = NumberPicker(label="Number of Iterations: ", min=1, max=12, value=controlnet_qr_prefs['num_images'], on_change=lambda e: changed(e, 'num_images'))
    batch_folder_name = TextField(label="Batch Folder Name", value=controlnet_qr_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    apply_ESRGAN_upscale = Switcher(label="Apply ESRGAN Upscale", value=controlnet_qr_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)
    enlarge_scale_slider = SliderRow(label="Enlarge Scale", min=1, max=4, divisions=6, round=1, suffix="x", pref=controlnet_qr_prefs, key='enlarge_scale')
    display_upscaled_image = Checkbox(label="Display Upscaled Image", value=controlnet_qr_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))
    ESRGAN_settings = Container(Column([enlarge_scale_slider, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    page.ESRGAN_block_controlnet_qr = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    page.ESRGAN_block_controlnet_qr.height = None if status['installed_ESRGAN'] else 0
    if not controlnet_qr_prefs['apply_ESRGAN_upscale']:
        ESRGAN_settings.height = 0
    page.controlnet_qr_output = Column([], auto_scroll=True)
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.controlnet_qr_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🔗  ControlNet QRCode Art Generator", "ControlNet Img2Img for Inpainting QR Code with Prompt and/or Init Image...", actions=[IconButton(icon=icons.HELP, tooltip="Help with ControlNetQR Settings", on_click=controlnet_qr_help)]),
        Row([use_image, qr_content, ref_image]),
        qr_generator,
        ResponsiveRow([prompt, negative_prompt]),
        Row([controlnet_version, init_image]),
        num_inference_row,
        guidance,
        conditioning_scale,
        strength,
        Row([control_guidance_start, control_guidance_end]),
        max_size,
        ResponsiveRow([Row([batch_size, num_images], col={'lg':6}), Row([seed, batch_folder_name], col={'lg':6})]),
        page.ESRGAN_block_controlnet_qr,
        Row([ElevatedButton(content=Text("🧑‍💻️  Make ControlNet QR", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_controlnet_qr(page)),
             ElevatedButton(content=Text(value="📜   Run from Prompts List", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_controlnet_qr(page, from_list=True), tooltip="Runs from global Prompt Queue using Image Parameters for Prompt, Neg Prompt, Steps, Guidance, Init Image, Strength and Seed.")]),
        page.controlnet_qr_output,
        clear_button,
      ]
    ))], scroll=ScrollMode.AUTO, auto_scroll=False)
    return c

controlnet_segment_prefs = {
    'ref_image': '',
    'prompt': '',
    'negative_prompt': 'low quality, low resolution, render, oversaturated, low contrast',
    'guidance_scale': 7.5,
    'num_inference_steps': 50,
    'seed': 0,
    'batch_size': 1,
    'num_images': 1,
    'max_size': 768,
    'width': 960,
    'height': 768,
    'last_model': '',
    'batch_folder_name': '',
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": 2.0,
    "display_upscaled_image": False,
}
def buildControlNetSegmentAnything(page):
    global controlnet_segment_prefs, prefs, pipe_controlnet_segment
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          if ptype == "int":
            controlnet_segment_prefs[pref] = int(e.control.value)
          elif ptype == "float":
            controlnet_segment_prefs[pref] = float(e.control.value)
          else:
            controlnet_segment_prefs[pref] = e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def add_to_controlnet_segment_output(o):
      page.ControlNetSegmentAnything.controls.append(o)
      page.ControlNetSegmentAnything.update()
      if not clear_button.visible:
        clear_button.visible = True
        clear_button.update()
    page.add_to_controlnet_segment_output = add_to_controlnet_segment_output
    def clear_output(e):
      if prefs['enable_sounds']: page.snd_delete.play()
      for i, c in enumerate(page.ControlNetSegmentAnything.controls):
        if i == 0: continue
        else: del page.ControlNetSegmentAnything.controls[i]
      page.ControlNetSegmentAnything.update()
      clear_button.visible = False
      clear_button.update()
    def controlnet_segment_help(e):
      def close_controlnet_segment_dlg(e):
        nonlocal controlnet_segment_help_dlg
        controlnet_segment_help_dlg.open = False
        page.update()
      controlnet_segment_help_dlg = AlertDialog(title=Text("🙅   Help with ControlNet Segment-Anything"), content=Column([
          Text("This model is based on the ControlNet Model, which allow us to generate Images using some sort of condition image. For this model, we selected the segmentation maps produced by Meta’s new segmentation model called Segment Anything Model as the condition image. We then trained the model to generate images based on the structure of the segmentation maps and the text prompts given."),
          Text("For the training, we generated a segmented dataset based on the COYO-700M dataset. The dataset provided us with the images, and the text prompts. For the segmented images, we used Segment Anything Model. We then created 8k samples to train our model on, which isn’t a lot, but as a team, we have been very busy with many other responsibilities and time constraints, which made it challenging to dedicate a lot of time to generating a larger dataset. Despite the constraints we faced, we have still managed to achieve some nice results 🙌"),
          Markdown("[Project Page](https://segment-anything.com) | [Model Card](https://huggingface.co/mfidabel/controlnet-segment-anything) | [Segment-Anything Code](https://github.com/facebookresearch/segment-anything) | [HuggingFace Demo](https://huggingface.co/spaces/mfidabel/controlnet-segment-anything)", on_tap_link=lambda e: e.page.launch_url(e.data)),

        ], scroll=ScrollMode.AUTO), actions=[TextButton("🧩  Puzzle the Pieces... ", on_click=close_controlnet_segment_dlg)], actions_alignment=MainAxisAlignment.END)
      page.dialog = controlnet_segment_help_dlg
      controlnet_segment_help_dlg.open = True
      page.update()
    def file_picker_result(e: FilePickerResultEvent):
        if e.files != None:
          upload_files(e)
    def on_upload_progress(e: FilePickerUploadEvent):
      if e.progress == 1:
        if not slash in e.file_name:
          fname = os.path.join(root_dir, e.file_name)
          controlnet_segment_prefs['file_name'] = e.file_name.rpartition('.')[0]
        else:
          fname = e.file_name
          controlnet_segment_prefs['file_name'] = e.file_name.rpartition(slash)[2].rpartition('.')[0]
        ref_image.value = fname
        ref_image.update()
        controlnet_segment_prefs['ref_image'] = fname
        page.update()
    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)
    def upload_files(e):
        uf = []
        if file_picker.result != None and file_picker.result.files != None:
            for f in file_picker.result.files:
              if page.web:
                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))
              else:
                on_upload_progress(FilePickerUploadEvent(f.path, 1, ""))
            file_picker.upload(uf)
    page.overlay.append(file_picker)
    def pick_init(e):
        file_picker.pick_files(allow_multiple=False, allowed_extensions=["png", "PNG", "jpg", "jpeg"], dialog_title="Pick Init Image File")
    prompt = TextField(label="Prompt Text", value=controlnet_segment_prefs['prompt'], col={'md':9}, on_change=lambda e:changed(e,'prompt'))
    negative_prompt  = TextField(label="Negative Prompt Text", value=controlnet_segment_prefs['negative_prompt'], col={'md':3}, on_change=lambda e:changed(e,'negative_prompt'))
    def toggle_ESRGAN(e):
        ESRGAN_settings.height = None if e.control.value else 0
        controlnet_segment_prefs['apply_ESRGAN_upscale'] = e.control.value
        ESRGAN_settings.update()
    ref_image = TextField(label="Initial Image to Segment", value=controlnet_segment_prefs['ref_image'], on_change=lambda e:changed(e,'ref_image'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_init))
    seed = TextField(label="Seed", width=90, value=str(controlnet_segment_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    num_inference_row = SliderRow(label="Number of Inference Steps", min=1, max=100, divisions=99, pref=controlnet_segment_prefs, key='num_inference_steps', tooltip="The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.")
    guidance = SliderRow(label="Guidance Scale", min=0, max=50, divisions=100, round=1, pref=controlnet_segment_prefs, key='guidance_scale')
    max_row = SliderRow(label="Max Resolution Size", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=controlnet_segment_prefs, key='max_size')
    width_slider = SliderRow(label="Width", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=controlnet_segment_prefs, key='width')
    height_slider = SliderRow(label="Height", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=controlnet_segment_prefs, key='height')
    batch_size = NumberPicker(label="Batch Size: ", min=1, max=8, value=controlnet_segment_prefs['batch_size'], on_change=lambda e: changed(e, 'batch_size'))
    num_images = NumberPicker(label="Number of Iterations: ", min=1, max=12, value=controlnet_segment_prefs['num_images'], on_change=lambda e: changed(e, 'num_images'))
    batch_folder_name = TextField(label="Batch Folder Name", value=controlnet_segment_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    apply_ESRGAN_upscale = Switcher(label="Apply ESRGAN Upscale", value=controlnet_segment_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)
    enlarge_scale_slider = SliderRow(label="Enlarge Scale", min=1, max=4, divisions=6, round=1, suffix="x", pref=controlnet_segment_prefs, key='enlarge_scale')
    display_upscaled_image = Checkbox(label="Display Upscaled Image", value=controlnet_segment_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))
    ESRGAN_settings = Container(Column([enlarge_scale_slider, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    page.ESRGAN_block_controlnet_segment = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    page.ESRGAN_block_controlnet_segment.height = None if status['installed_ESRGAN'] else 0
    if not controlnet_segment_prefs['apply_ESRGAN_upscale']:
        ESRGAN_settings.height = 0
    page.controlnet_segment_output = Column([], auto_scroll=True)
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.controlnet_segment_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🥸  ControlNet on Meta's Segment-Anything", "Upload an Image, Segment it with Segment Anything, write a prompt, and generate images...", actions=[IconButton(icon=icons.HELP, tooltip="Help with ControlNet Segment Anything Settings", on_click=controlnet_segment_help)]),
        ref_image,
        ResponsiveRow([prompt, negative_prompt]),
        num_inference_row,
        guidance,
        max_row,
        #width_slider,
        #height_slider,
        ResponsiveRow([Row([batch_size, num_images], col={'lg':6}), Row([seed, batch_folder_name], col={'lg':6})]),
        page.ESRGAN_block_controlnet_segment,
        Row([ElevatedButton(content=Text("👹  Make ControlNet Segments", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_controlnet_segment(page)),
             ElevatedButton(content=Text(value="📜   Run from Prompts List", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_controlnet_segment(page, from_list=True))]),
        page.controlnet_segment_output,
        clear_button,
      ]
    ))], scroll=ScrollMode.AUTO, auto_scroll=False)
    return c

EDICT_prefs = {
    'target_prompt': '',
    'negative_prompt': '',
    'base_prompt': '',
    'init_image': '',
    'guidance_scale': 3.0,
    'num_inference_steps': 50,
    'strength': 0.8,
    'seed': 0,
    'num_images': 1,
    'batch_folder_name': '',
    'max_size': 512,
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": 4.0,
    "display_upscaled_image": False,
}

def buildEDICT(page):
    global EDICT_prefs, prefs, pipe_EDICT
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          if ptype == "int":
            EDICT_prefs[pref] = int(e.control.value)
          elif ptype == "float":
            EDICT_prefs[pref] = float(e.control.value)
          else:
            EDICT_prefs[pref] = e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def add_to_EDICT_output(o):
      page.EDICT_output.controls.append(o)
      page.EDICT_output.update()
      if not clear_button.visible:
        clear_button.visible = True
        clear_button.update()
    page.add_to_EDICT_output = add_to_EDICT_output
    def clear_output(e):
      if prefs['enable_sounds']: page.snd_delete.play()
      page.EDICT_output.controls = []
      page.EDICT_output.update()
      clear_button.visible = False
      clear_button.update()
    def EDICT_help(e):
      def close_EDICT_dlg(e):
        nonlocal EDICT_help_dlg
        EDICT_help_dlg.open = False
        page.update()
      EDICT_help_dlg = AlertDialog(title=Text("🙅   Help with EDICT Edit"), content=Column([
          Text("EDICT: Exact Diffusion Inversion via Coupled Transformations"),
          Text("Using the iterative denoising diffusion principle, denoising diffusion models (DDMs) trained with web-scale data can generate highly realistic images conditioned on input text, layouts, and scene graphs. After image generation, the next important application of DDMs being explored by the research community is that of image editing. Models such as DALL-E-2 and Stable Diffusion [24] can perform inpainting, allowing users to edit images through manual annotation. Methods such as SDEdit have demonstrated that both synthetic and real images can be edited using stroke or composite guidance via DDMs. However, the goal of a holistic image editing tool that can edit any real/artificial image using purely text has still not been achieved, until now."),
          Markdown("[Read Arxiv Paper](https://arxiv.org/pdf/2211.12446.pdf)", on_tap_link=lambda e: e.page.launch_url(e.data)),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("🥴  Not Complicated... ", on_click=close_EDICT_dlg)], actions_alignment=MainAxisAlignment.END)
      page.dialog = EDICT_help_dlg
      EDICT_help_dlg.open = True
      page.update()
    def toggle_ESRGAN(e):
        ESRGAN_settings.height = None if e.control.value else 0
        EDICT_prefs['apply_ESRGAN_upscale'] = e.control.value
        ESRGAN_settings.update()
    def file_picker_result(e: FilePickerResultEvent):
        if e.files != None:
          upload_files(e)
    def on_upload_progress(e: FilePickerUploadEvent):
      if e.progress == 1:
        if not slash in e.file_name:
          fname = os.path.join(root_dir, e.file_name)
          EDICT_prefs['file_name'] = e.file_name.rpartition('.')[0]
        else:
          fname = e.file_name
          EDICT_prefs['file_name'] = e.file_name.rpartition(slash)[2].rpartition('.')[0]
        init_image.value = fname
        init_image.update()
        EDICT_prefs['init_image'] = fname
        page.update()
    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)
    def upload_files(e):
        uf = []
        if file_picker.result != None and file_picker.result.files != None:
            for f in file_picker.result.files:
              if page.web:
                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))
              else:
                on_upload_progress(FilePickerUploadEvent(f.path, 1, ""))
            file_picker.upload(uf)
    page.overlay.append(file_picker)
    def pick_init(e):
        file_picker.pick_files(allow_multiple=False, allowed_extensions=["png", "PNG", "jpg", "jpeg"], dialog_title="Pick init Image File")
    base_prompt = TextField(label="Base Prompt Text (describe init image)", value=EDICT_prefs['base_prompt'], col={'md': 12}, multiline=True, on_change=lambda e:changed(e,'base_prompt'))
    target_prompt = TextField(label="Target Prompt Text", value=EDICT_prefs['target_prompt'], col={'md': 9}, multiline=True, on_change=lambda e:changed(e,'target_prompt'))
    negative_prompt  = TextField(label="Negative Prompt Text", value=EDICT_prefs['negative_prompt'], col={'md':3}, multiline=True, on_change=lambda e:changed(e,'negative_prompt'))
    init_image = TextField(label="Initial Image to Edit (crops square)", value=EDICT_prefs['init_image'], on_change=lambda e:changed(e,'init_image'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_init))
    seed = TextField(label="Seed", width=90, value=str(EDICT_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    guidance = SliderRow(label="Guidance Scale", min=0, max=50, divisions=100, round=1, pref=EDICT_prefs, key='guidance_scale')
    num_inference_row = SliderRow(label="Number of Inference Steps", min=1, max=100, divisions=99, pref=EDICT_prefs, key='num_inference_steps', tooltip="The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.")
    strength = SliderRow(label="Strength", min=0, max=1, divisions=20, round=2, pref=EDICT_prefs, key='strength')
    max_row = SliderRow(label="Max Resolution Size", min=256, max=1024, divisions=12, multiple=32, suffix="px", pref=EDICT_prefs, key='max_size')
    batch_folder_name = TextField(label="Batch Folder Name", value=EDICT_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    apply_ESRGAN_upscale = Switcher(label="Apply ESRGAN Upscale", value=EDICT_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)
    enlarge_scale_slider = SliderRow(label="Enlarge Scale", min=1, max=4, divisions=6, round=1, suffix="x", pref=EDICT_prefs, key='enlarge_scale')
    display_upscaled_image = Checkbox(label="Display Upscaled Image", value=EDICT_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))
    ESRGAN_settings = Container(Column([enlarge_scale_slider, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    page.ESRGAN_block_EDICT = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    page.ESRGAN_block_EDICT.height = None if status['installed_ESRGAN'] else 0
    if not unCLIP_interpolation_prefs['apply_ESRGAN_upscale']:
        ESRGAN_settings.height = 0
    page.EDICT_output = Column([])
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.EDICT_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🤹  EDICT Image Editing", "Diffusion pipeline for text-guided image editing... Exact Diffusion Inversion via Coupled Transformations.", actions=[IconButton(icon=icons.HELP, tooltip="Help with Image Variation Settings", on_click=EDICT_help)]),
        init_image,
        base_prompt,
        ResponsiveRow([target_prompt, negative_prompt]),
        #Row([init_image, mask_image, invert_mask]),
        num_inference_row,
        guidance,
        strength,
        max_row,
        Row([NumberPicker(label="Number of Images: ", min=1, max=8, value=EDICT_prefs['num_images'], on_change=lambda e: changed(e, 'num_images')), seed, batch_folder_name]),
        page.ESRGAN_block_EDICT,
        ElevatedButton(content=Text("🧝  Run EDICT Edit", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_EDICT(page)),
        page.EDICT_output,
        clear_button,
      ]
    ))], scroll=ScrollMode.AUTO, auto_scroll=False)
    return c

DiffEdit_prefs = {
    'target_prompt': '',
    'negative_prompt': '',
    'source_prompt': '',
    'init_image': '',
    'guidance_scale': 3.0,
    'num_inference_steps': 50,
    'strength': 0.8,
    'seed': 0,
    'num_images': 1,
    'batch_folder_name': '',
    'max_size': 768,
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": 4.0,
    "display_upscaled_image": False,
}

def buildDiffEdit(page):
    global DiffEdit_prefs, prefs, pipe_DiffEdit
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          if ptype == "int":
            DiffEdit_prefs[pref] = int(e.control.value)
          elif ptype == "float":
            DiffEdit_prefs[pref] = float(e.control.value)
          else:
            DiffEdit_prefs[pref] = e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def add_to_DiffEdit_output(o):
      page.DiffEdit_output.controls.append(o)
      page.DiffEdit_output.update()
      if not clear_button.visible:
        clear_button.visible = True
        clear_button.update()
    page.add_to_DiffEdit_output = add_to_DiffEdit_output
    def clear_output(e):
      if prefs['enable_sounds']: page.snd_delete.play()
      page.DiffEdit_output.controls = []
      page.DiffEdit_output.update()
      clear_button.visible = False
      clear_button.update()
    def DiffEdit_help(e):
      def close_DiffEdit_dlg(e):
        nonlocal DiffEdit_help_dlg
        DiffEdit_help_dlg.open = False
        page.update()
      DiffEdit_help_dlg = AlertDialog(title=Text("🙅   Help with DiffEdit"), content=Column([
          Text("Image generation has recently seen tremendous advances, with diffusion models allowing to synthesize convincing images for a large variety of text prompts. In this article, we propose DiffEdit, a method to take advantage of text-conditioned diffusion models for the task of semantic image editing, where the goal is to edit an image based on a text query. Semantic image editing is an extension of image generation, with the additional constraint that the generated image should be as similar as possible to a given input image. Current editing methods based on diffusion models usually require to provide a mask, making the task much easier by treating it as a conditional inpainting task. In contrast, our main contribution is able to automatically generate a mask highlighting regions of the input image that need to be edited, by contrasting predictions of a diffusion model conditioned on different text prompts. Moreover, we rely on latent inference to preserve content in those regions of interest and show excellent synergies with mask-based diffusion. DiffEdit achieves state-of-the-art editing performance on ImageNet. In addition, we evaluate semantic image editing in more challenging settings, using images from the COCO dataset as well as text-based generated images."),
          #Text(""),
          Markdown("[Read Arxiv Paper](https://arxiv.org/abs/2210.11427) - [Blog Post with Demo](https://blog.problemsolversguild.com/technical/research/2022/11/02/DiffEdit-Implementation.html) - [GitHub](https://github.com/Xiang-cd/DiffEdit-stable-diffusion/)", on_tap_link=lambda e: e.page.launch_url(e.data)),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("😇  Not Difficult... ", on_click=close_DiffEdit_dlg)], actions_alignment=MainAxisAlignment.END)
      page.dialog = DiffEdit_help_dlg
      DiffEdit_help_dlg.open = True
      page.update()
    def toggle_ESRGAN(e):
        ESRGAN_settings.height = None if e.control.value else 0
        DiffEdit_prefs['apply_ESRGAN_upscale'] = e.control.value
        ESRGAN_settings.update()
    def file_picker_result(e: FilePickerResultEvent):
        if e.files != None:
          upload_files(e)
    def on_upload_progress(e: FilePickerUploadEvent):
      if e.progress == 1:
        if not slash in e.file_name:
          fname = os.path.join(root_dir, e.file_name)
          DiffEdit_prefs['file_name'] = e.file_name.rpartition('.')[0]
        else:
          fname = e.file_name
          DiffEdit_prefs['file_name'] = e.file_name.rpartition(slash)[2].rpartition('.')[0]
        init_image.value = fname
        init_image.update()
        DiffEdit_prefs['init_image'] = fname
        page.update()
    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)
    def upload_files(e):
        uf = []
        if file_picker.result != None and file_picker.result.files != None:
            for f in file_picker.result.files:
              if page.web:
                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))
              else:
                on_upload_progress(FilePickerUploadEvent(f.path, 1, ""))
            file_picker.upload(uf)
    page.overlay.append(file_picker)
    def pick_init(e):
        file_picker.pick_files(allow_multiple=False, allowed_extensions=["png", "PNG", "jpg", "jpeg"], dialog_title="Pick init Image File")
    source_prompt = TextField(label="Source Prompt Text (blank to auto-caption)", value=DiffEdit_prefs['source_prompt'], col={'md': 12}, multiline=True, on_change=lambda e:changed(e,'source_prompt'))
    target_prompt = TextField(label="Target Prompt Text (describe edits)", value=DiffEdit_prefs['target_prompt'], col={'md': 9}, multiline=True, on_change=lambda e:changed(e,'target_prompt'))
    negative_prompt  = TextField(label="Negative Prompt Text", value=DiffEdit_prefs['negative_prompt'], col={'md':3}, multiline=True, on_change=lambda e:changed(e,'negative_prompt'))
    init_image = TextField(label="Initial Image to Edit (crops square)", value=DiffEdit_prefs['init_image'], on_change=lambda e:changed(e,'init_image'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_init))
    seed = TextField(label="Seed", width=90, value=str(DiffEdit_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    guidance = SliderRow(label="Guidance Scale", min=0, max=50, divisions=100, round=1, pref=DiffEdit_prefs, key='guidance_scale')
    num_inference_row = SliderRow(label="Number of Inference Steps", min=1, max=100, divisions=99, pref=DiffEdit_prefs, key='num_inference_steps', tooltip="The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.")
    strength = SliderRow(label="Strength", min=0, max=1, divisions=20, round=2, pref=DiffEdit_prefs, key='strength')
    max_row = SliderRow(label="Max Resolution Size", min=256, max=1024, divisions=12, multiple=32, suffix="px", pref=DiffEdit_prefs, key='max_size')
    batch_folder_name = TextField(label="Batch Folder Name", value=DiffEdit_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    apply_ESRGAN_upscale = Switcher(label="Apply ESRGAN Upscale", value=DiffEdit_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)
    enlarge_scale_slider = SliderRow(label="Enlarge Scale", min=1, max=4, divisions=6, round=1, suffix="x", pref=DiffEdit_prefs, key='enlarge_scale')
    display_upscaled_image = Checkbox(label="Display Upscaled Image", value=DiffEdit_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))
    ESRGAN_settings = Container(Column([enlarge_scale_slider, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    page.ESRGAN_block_DiffEdit = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    page.ESRGAN_block_DiffEdit.height = None if status['installed_ESRGAN'] else 0
    if not unCLIP_interpolation_prefs['apply_ESRGAN_upscale']:
        ESRGAN_settings.height = 0
    page.DiffEdit_output = Column([])
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.DiffEdit_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🍒  DiffEdit Image Editing", "Zero-shot Diffusion-based Semantic Image Editing with Mask Guidance...", actions=[IconButton(icon=icons.HELP, tooltip="Help with Image Variation Settings", on_click=DiffEdit_help)]),
        init_image,
        source_prompt,
        ResponsiveRow([target_prompt, negative_prompt]),
        #Row([init_image, mask_image, invert_mask]),
        num_inference_row,
        guidance,
        strength,
        max_row,
        Row([NumberPicker(label="Number of Images: ", min=1, max=8, value=DiffEdit_prefs['num_images'], on_change=lambda e: changed(e, 'num_images')), seed, batch_folder_name]),
        page.ESRGAN_block_DiffEdit,
        ElevatedButton(content=Text("😃  Run DiffEdit", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_DiffEdit(page)),
        page.DiffEdit_output,
        clear_button,
      ]
    ))], scroll=ScrollMode.AUTO, auto_scroll=False)
    return c


unCLIP_prefs = {
    'prompt': '',
    'batch_folder_name': '',
    'prior_guidance_scale': 4.0,
    'decoder_guidance_scale': 8.0,
    'prior_num_inference_steps': 25,
    'decoder_num_inference_steps': 25,
    'super_res_num_inference_steps': 7,
    'seed': 0,
    'num_images': 1,
    'use_StableUnCLIP_pipeline': False,
    #'variance_type': 'learned_range',#fixed_small_log
    #'num_train_timesteps': 1000,
    #'prediction_type': 'epsilon',#sample
    #'clip_sample': True,
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": 4.0,
    "display_upscaled_image": True,
}
def buildUnCLIP(page):
    global unCLIP_prefs, prefs, pipe_unCLIP
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          if ptype == "int":
            unCLIP_prefs[pref] = int(e.control.value)
          elif ptype == "float":
            unCLIP_prefs[pref] = float(e.control.value)
          else:
            unCLIP_prefs[pref] = e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def add_to_unCLIP_output(o):
      page.unCLIP_output.controls.append(o)
      page.unCLIP_output.update()
      if not clear_button.visible:
        clear_button.visible = True
        clear_button.update()
    page.add_to_unCLIP_output = add_to_unCLIP_output
    def clear_output(e):
      if prefs['enable_sounds']: page.snd_delete.play()
      page.unCLIP_output.controls = []
      page.unCLIP_output.update()
      clear_button.visible = False
      clear_button.update()
    def unCLIP_help(e):
      def close_unCLIP_dlg(e):
        nonlocal unCLIP_help_dlg
        unCLIP_help_dlg.open = False
        page.update()
      unCLIP_help_dlg = AlertDialog(title=Text("🙅   Help with unCLIP Pipeline"), content=Column([
          Text("Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we implemented a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples."),
          Text("The scheduler is a modified DDPM that has some minor variations in how it calculates the learned range variance and dynamically re-calculates betas based off the timesteps it is skipping. The scheduler also uses a slightly different step ratio when computing timesteps to use for inference."),
          Markdown("The unCLIP model in diffusers comes from kakaobrain's karlo and the original codebase can be found [here](https://github.com/kakaobrain/karlo). Additionally, lucidrains has a DALL-E 2 recreation [here](https://github.com/lucidrains/DALLE2-pytorch)."),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("😕  Tricky... ", on_click=close_unCLIP_dlg)], actions_alignment=MainAxisAlignment.END)
      page.dialog = unCLIP_help_dlg
      unCLIP_help_dlg.open = True
      page.update()
    def toggle_ESRGAN(e):
        ESRGAN_settings.height = None if e.control.value else 0
        unCLIP_prefs['apply_ESRGAN_upscale'] = e.control.value
        ESRGAN_settings.update()
    def toggle_unCLIP(e):
        unCLIP_prefs['use_StableUnCLIP_pipeline'] = e.control.value
        if unCLIP_prefs['use_StableUnCLIP_pipeline']:
            decoder_num_inference_row.visible = False
            super_res_num_inference_row.visible = False
            decoder_guidance.visible = False
            decoder_num_inference_row.update()
            super_res_num_inference_row.update()
            decoder_guidance.update()
        else:
            decoder_num_inference_row.visible = True
            super_res_num_inference_row.visible = True
            decoder_guidance.visible = True
            decoder_guidance.update()
            super_res_num_inference_row.update()
            decoder_num_inference_row.update()
    prompt = TextField(label="Prompt Text", value=unCLIP_prefs['prompt'], on_change=lambda e:changed(e,'prompt'))
    seed = TextField(label="Seed", width=90, value=str(unCLIP_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    prior_num_inference_row = SliderRow(label="Number of Prior Inference Steps", min=1, max=100, divisions=99, pref=unCLIP_prefs, key='prior_num_inference_steps', tooltip="The number of Prior denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.")
    decoder_num_inference_row = SliderRow(label="Number of Decoder Inference Steps", min=1, max=100, divisions=99, pref=unCLIP_prefs, key='decoder_num_inference_steps', tooltip="The number of Decoder denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.")
    super_res_num_inference_row = SliderRow(label="Number of Super-Res Inference Steps", min=1, max=100, divisions=99, pref=unCLIP_prefs, key='decoder_num_inference_steps', tooltip="The number of Super-Res denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.")
    prior_guidance = SliderRow(label="Prior Guidance Scale", min=0, max=50, divisions=100, round=1, pref=unCLIP_prefs, key='prior_guidance_scale')
    decoder_guidance = SliderRow(label="Decoder Guidance Scale", min=0, max=50, divisions=100, round=1, pref=unCLIP_prefs, key='decoder_guidance_scale')
    batch_folder_name = TextField(label="Batch Folder Name", value=unCLIP_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    use_StableUnCLIP_pipeline = Tooltip(message="Combines prior model (generate clip image embedding from text, UnCLIPPipeline) and decoder pipeline (decode clip image embedding to image, StableDiffusionImageVariationPipeline)", content=Switcher(label="Use Stable UnCLIP Pipeline Instead", value=unCLIP_prefs['use_StableUnCLIP_pipeline'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_unCLIP))
    #eta = TextField(label="ETA", value=str(unCLIP_prefs['eta']), keyboard_type=KeyboardType.NUMBER, hint_text="Amount of Noise", on_change=lambda e:changed(e,'eta', ptype='float'))
    #eta = Slider(min=0.0, max=1.0, divisions=20, label="{value}", value=float(unCLIP_prefs['eta']), tooltip="The weight of noise for added noise in a diffusion step. Its value is between 0.0 and 1.0 - 0.0 is DDIM and 1.0 is DDPM scheduler respectively.", expand=True, on_change=lambda e:changed(e,'eta', ptype='float'))
    #eta_row = Row([Text("DDIM ETA: "), eta])
    #max_size = Slider(min=256, max=1280, divisions=64, label="{value}px", value=int(unCLIP_prefs['max_size']), expand=True, on_change=lambda e:changed(e,'max_size', ptype='int'))
    #max_row = Row([Text("Max Resolution Size: "), max_size])
    apply_ESRGAN_upscale = Switcher(label="Apply ESRGAN Upscale", value=unCLIP_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)
    enlarge_scale_slider = SliderRow(label="Enlarge Scale", min=1, max=4, divisions=6, round=1, suffix="x", pref=unCLIP_prefs, key='enlarge_scale')
    display_upscaled_image = Checkbox(label="Display Upscaled Image", value=unCLIP_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))
    ESRGAN_settings = Container(Column([enlarge_scale_slider, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    page.ESRGAN_block_unCLIP = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    page.ESRGAN_block_unCLIP.height = None if status['installed_ESRGAN'] else 0
    if not unCLIP_prefs['apply_ESRGAN_upscale']:
        ESRGAN_settings.height = 0
    page.unCLIP_output = Column([], auto_scroll=True)
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.unCLIP_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🌐  unCLIP Text-to-Image Generator", "Hierarchical Text-Conditional Image Generation with CLIP Latents.  Similar results to DALL-E 2...", actions=[IconButton(icon=icons.HELP, tooltip="Help with unCLIP Settings", on_click=unCLIP_help)]),
        prompt,
        #Row([prompt, mask_image, invert_mask]),
        prior_num_inference_row, decoder_num_inference_row, super_res_num_inference_row,
        prior_guidance, decoder_guidance,
        #eta_row, max_row,
        use_StableUnCLIP_pipeline,
        Row([NumberPicker(label="Number of Images: ", min=1, max=20, value=unCLIP_prefs['num_images'], on_change=lambda e: changed(e, 'num_images')), seed, batch_folder_name]),
        page.ESRGAN_block_unCLIP,
        Row([ElevatedButton(content=Text("🖇️   Get unCLIP Generation", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_unCLIP(page)),
             ElevatedButton(content=Text(value="📜   Run from Prompts List", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_unCLIP(page, from_list=True))]),
        page.unCLIP_output,
        clear_button,
      ]
    )),
    ], scroll=ScrollMode.AUTO, auto_scroll=False)
    return c

unCLIP_image_variation_prefs = {
    'init_image': '',
    'file_name': '',
    'batch_folder_name': '',
    'decoder_guidance_scale': 8.0,
    'decoder_num_inference_steps': 25,
    'super_res_num_inference_steps': 7,
    'seed': 0,
    'num_images': 1,
    'max_size': 768,
    #'variance_type': 'learned_range',#fixed_small_log
    #'num_train_timesteps': 1000,
    #'prediction_type': 'epsilon',#sample
    #'clip_sample': True,
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": 4.0,
    "display_upscaled_image": True,
}
def buildUnCLIP_ImageVariation(page):
    global unCLIP_image_variation_prefs, prefs, pipe_unCLIP_image_variation
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          if ptype == "int":
            unCLIP_image_variation_prefs[pref] = int(e.control.value)
          elif ptype == "float":
            unCLIP_image_variation_prefs[pref] = float(e.control.value)
          else:
            unCLIP_image_variation_prefs[pref] = e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def add_to_unCLIP_image_variation_output(o):
      page.unCLIP_image_variation_output.controls.append(o)
      page.unCLIP_image_variation_output.update()
      if not clear_button.visible:
        clear_button.visible = True
        clear_button.update()
    page.add_to_unCLIP_image_variation_output = add_to_unCLIP_image_variation_output
    def clear_output(e):
      if prefs['enable_sounds']: page.snd_delete.play()
      page.unCLIP_image_variation_output.controls = []
      page.unCLIP_image_variation_output.update()
      clear_button.visible = False
      clear_button.update()
    def unCLIP_image_variation_help(e):
      def close_unCLIP_image_variation_dlg(e):
        nonlocal unCLIP_image_variation_help_dlg
        unCLIP_image_variation_help_dlg.open = False
        page.update()
      unCLIP_image_variation_help_dlg = AlertDialog(title=Text("🙅   Help with unCLIP Image Variation Pipeline"), content=Column([
          Text("Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we implemented a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples."),
          Text("The scheduler is a modified DDPM that has some minor variations in how it calculates the learned range variance and dynamically re-calculates betas based off the timesteps it is skipping. The scheduler also uses a slightly different step ratio when computing timesteps to use for inference."),
          Markdown("The unCLIP Image Variation model in diffusers comes from kakaobrain's karlo and the original codebase can be found [here](https://github.com/kakaobrain/karlo). Additionally, lucidrains has a DALL-E 2 recreation [here](https://github.com/lucidrains/DALLE2-pytorch)."),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("🐇  We'll see... ", on_click=close_unCLIP_image_variation_dlg)], actions_alignment=MainAxisAlignment.END)
      page.dialog = unCLIP_image_variation_help_dlg
      unCLIP_image_variation_help_dlg.open = True
      page.update()
    def file_picker_result(e: FilePickerResultEvent):
        if e.files != None:
          upload_files(e)
    def on_upload_progress(e: FilePickerUploadEvent):
      if e.progress == 1:
        if not slash in e.file_name:
          fname = os.path.join(root_dir, e.file_name)
          unCLIP_image_variation_prefs['file_name'] = e.file_name.rpartition('.')[0]
        else:
          fname = e.file_name
          unCLIP_image_variation_prefs['file_name'] = e.file_name.rpartition(slash)[2].rpartition('.')[0]
        init_image.value = fname
        init_image.update()
        unCLIP_image_variation_prefs['init_image'] = fname
        page.update()
    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)
    def upload_files(e):
        uf = []
        if file_picker.result != None and file_picker.result.files != None:
            for f in file_picker.result.files:
              if page.web:
                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))
              else:
                on_upload_progress(FilePickerUploadEvent(f.path, 1, ""))
            file_picker.upload(uf)
    page.overlay.append(file_picker)
    def pick_init(e):
        file_picker.pick_files(allow_multiple=False, allowed_extensions=["png", "PNG", "jpg", "jpeg"], dialog_title="Pick Init Image File")
    init_image = TextField(label="Initial Image", value=unCLIP_image_variation_prefs['init_image'], on_change=lambda e:changed(e,'init_image'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_init))
    def toggle_ESRGAN(e):
        ESRGAN_settings.height = None if e.control.value else 0
        unCLIP_image_variation_prefs['apply_ESRGAN_upscale'] = e.control.value
        ESRGAN_settings.update()
    def change_enlarge_scale(e):
        enlarge_scale_slider.controls[1].value = f" {float(e.control.value)}x"
        enlarge_scale_slider.update()
        changed(e, 'enlarge_scale', ptype="float")
    #prompt = TextField(label="Prompt Text", value=unCLIP_image_variation_prefs['prompt'], on_change=lambda e:changed(e,'prompt'))
    seed = TextField(label="Seed", width=90, value=str(unCLIP_image_variation_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    decoder_num_inference_row = SliderRow(label="Number of Decoder Inference Steps", min=1, max=100, divisions=99, pref=unCLIP_image_variation_prefs, key='decoder_num_inference_steps', tooltip="The number of Decoder denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.")
    super_res_num_inference_row = SliderRow(label="Number of Super-Res Inference Steps", min=1, max=100, divisions=99, pref=unCLIP_image_variation_prefs, key='decoder_num_inference_steps', tooltip="The number of Super-Res denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.")
    decoder_guidance = SliderRow(label="Decoder Guidance Scale", min=0, max=50, divisions=100, round=1, pref=unCLIP_image_variation_prefs, key='decoder_guidance_scale')
    batch_folder_name = TextField(label="Batch Folder Name", value=unCLIP_image_variation_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    #eta = TextField(label="ETA", value=str(unCLIP_image_variation_prefs['eta']), keyboard_type=KeyboardType.NUMBER, hint_text="Amount of Noise", on_change=lambda e:changed(e,'eta', ptype='float'))
    #eta = Slider(min=0.0, max=1.0, divisions=20, label="{value}", value=float(unCLIP_image_variation_prefs['eta']), tooltip="The weight of noise for added noise in a diffusion step. Its value is between 0.0 and 1.0 - 0.0 is DDIM and 1.0 is DDPM scheduler respectively.", expand=True, on_change=lambda e:changed(e,'eta', ptype='float'))
    #eta_row = Row([Text("DDIM ETA: "), eta])
    max_row = SliderRow(label="Max Resolution Size", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=unCLIP_image_variation_prefs, key='max_size')
    apply_ESRGAN_upscale = Switcher(label="Apply ESRGAN Upscale", value=unCLIP_image_variation_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)
    enlarge_scale_value = Text(f" {float(unCLIP_image_variation_prefs['enlarge_scale'])}x", weight=FontWeight.BOLD)
    enlarge_scale = Slider(min=1, max=4, divisions=6, label="{value}x", value=unCLIP_image_variation_prefs['enlarge_scale'], on_change=change_enlarge_scale, expand=True)
    enlarge_scale_slider = Row([Text("Enlarge Scale: "), enlarge_scale_value, enlarge_scale])
    display_upscaled_image = Checkbox(label="Display Upscaled Image", value=unCLIP_image_variation_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))
    ESRGAN_settings = Container(Column([enlarge_scale_slider, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    page.ESRGAN_block_unCLIP_image_variation = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    page.ESRGAN_block_unCLIP_image_variation.height = None if status['installed_ESRGAN'] else 0
    if not unCLIP_image_variation_prefs['apply_ESRGAN_upscale']:
        ESRGAN_settings.height = 0
    page.unCLIP_image_variation_output = Column([], auto_scroll=True)
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.unCLIP_image_variation_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🎆  unCLIP Image Variation Generator", "Generate Variations from an input image using unCLIP...", actions=[IconButton(icon=icons.HELP, tooltip="Help with unCLIP Image Variation Settings", on_click=unCLIP_image_variation_help)]),
        init_image,
        #Row([prompt, mask_image, invert_mask]),
        decoder_num_inference_row, super_res_num_inference_row,
        decoder_guidance,
        #eta_row,
        max_row,
        Row([NumberPicker(label="Number of Images: ", min=1, max=20, value=unCLIP_image_variation_prefs['num_images'], on_change=lambda e: changed(e, 'num_images')), seed, batch_folder_name]),
        page.ESRGAN_block_unCLIP_image_variation,
        Row([ElevatedButton(content=Text("🦄   Get unCLIP Image Variation", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_unCLIP_image_variation(page)),
             ElevatedButton(content=Text(value="📜   Run from Prompts List", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_unCLIP_image_variation(page, from_list=True))]),
        page.unCLIP_image_variation_output,
        clear_button,
      ]
    )),
    ], scroll=ScrollMode.AUTO, auto_scroll=False)
    return c

unCLIP_interpolation_prefs = {
    'prompt': '',
    'end_prompt': '',
    'steps': 5,
    'batch_folder_name': '',
    'prior_guidance_scale': 4.0,
    'decoder_guidance_scale': 8.0,
    'prior_num_inference_steps': 25,
    'decoder_num_inference_steps': 25,
    'super_res_num_inference_steps': 7,
    'seed': 0,
    'num_images': 1,
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": 4.0,
    "display_upscaled_image": True,
}
def buildUnCLIP_Interpolation(page):
    global unCLIP_interpolation_prefs, prefs, pipe_unCLIP_interpolation
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          if ptype == "int":
            unCLIP_interpolation_prefs[pref] = int(e.control.value)
          elif ptype == "float":
            unCLIP_interpolation_prefs[pref] = float(e.control.value)
          else:
            unCLIP_interpolation_prefs[pref] = e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def add_to_unCLIP_interpolation_output(o):
      page.unCLIP_interpolation_output.controls.append(o)
      page.unCLIP_interpolation_output.update()
      if not clear_button.visible:
        clear_button.visible = True
        clear_button.update()
    page.add_to_unCLIP_interpolation_output = add_to_unCLIP_interpolation_output
    def clear_output(e):
      if prefs['enable_sounds']: page.snd_delete.play()
      page.unCLIP_interpolation_output.controls = []
      page.unCLIP_interpolation_output.update()
      clear_button.visible = False
      clear_button.update()
    def unCLIP_interpolation_help(e):
      def close_unCLIP_interpolation_dlg(e):
        nonlocal unCLIP_interpolation_help_dlg
        unCLIP_interpolation_help_dlg.open = False
        page.update()
      unCLIP_interpolation_help_dlg = AlertDialog(title=Text("🙅   Help with unCLIP Text Interpolation Pipeline"), content=Column([
          Text("This Diffusion Pipeline takes two prompts and interpolates between the two input prompts using spherical interpolation ( slerp ). The input prompts are converted to text embeddings by the pipeline's text_encoder and the interpolation is done on the resulting text_embeddings over the number of steps specified. Defaults to 5 steps."),
          Text("Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we implemented a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples."),
          Text("The scheduler is a modified DDPM that has some minor variations in how it calculates the learned range variance and dynamically re-calculates betas based off the timesteps it is skipping. The scheduler also uses a slightly different step ratio when computing timesteps to use for inference."),
          Markdown("The unCLIP model in diffusers comes from kakaobrain's karlo and the original codebase can be found [here](https://github.com/kakaobrain/karlo). Additionally, lucidrains has a DALL-E 2 recreation [here](https://github.com/lucidrains/DALLE2-pytorch)."),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("😕  Transformative... ", on_click=close_unCLIP_interpolation_dlg)], actions_alignment=MainAxisAlignment.END)
      page.dialog = unCLIP_interpolation_help_dlg
      unCLIP_interpolation_help_dlg.open = True
      page.update()
    def toggle_ESRGAN(e):
        ESRGAN_settings.height = None if e.control.value else 0
        unCLIP_interpolation_prefs['apply_ESRGAN_upscale'] = e.control.value
        ESRGAN_settings.update()
    def change_enlarge_scale(e):
        enlarge_scale_slider.controls[1].value = f" {float(e.control.value)}x"
        enlarge_scale_slider.update()
        changed(e, 'enlarge_scale', ptype="float")
    prompt = TextField(label="Start Prompt Text", value=unCLIP_interpolation_prefs['prompt'], on_change=lambda e:changed(e,'prompt'))
    end_prompt = TextField(label="End Prompt Text", value=unCLIP_interpolation_prefs['end_prompt'], on_change=lambda e:changed(e,'end_prompt'))
    seed = TextField(label="Seed", width=90, value=str(unCLIP_interpolation_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    interpolation_steps_row = SliderRow(label="Number of Interpolation Step", min=1, max=50, divisions=49, pref=unCLIP_interpolation_prefs, key='steps', tooltip="The number of steps over which to interpolate from start_prompt to end_prompt.")
    prior_num_inference_row = SliderRow(label="Number of Prior Inference Steps", min=1, max=100, divisions=99, pref=unCLIP_interpolation_prefs, key='prior_num_inference_steps', tooltip="The number of Prior denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.")
    decoder_num_inference_row = SliderRow(label="Number of Decoder Inference Steps", min=1, max=100, divisions=99, pref=unCLIP_interpolation_prefs, key='decoder_num_inference_steps', tooltip="The number of Decoder denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.")
    super_res_num_inference_row = SliderRow(label="Number of Super-Res Inference Steps", min=1, max=100, divisions=99, pref=unCLIP_interpolation_prefs, key='decoder_num_inference_steps', tooltip="The number of Super-Res denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.")
    prior_guidance = SliderRow(label="Prior Guidance Scale", min=0, max=50, divisions=100, round=1, pref=unCLIP_interpolation_prefs, key='prior_guidance_scale')
    decoder_guidance = SliderRow(label="Decoder Guidance Scale", min=0, max=50, divisions=100, round=1, pref=unCLIP_interpolation_prefs, key='decoder_guidance_scale')
    batch_folder_name = TextField(label="Batch Folder Name", value=unCLIP_interpolation_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    #use_StableunCLIP_interpolation_pipeline = Tooltip(message="Combines prior model (generate clip image embedding from text, UnCLIPPipeline) and decoder pipeline (decode clip image embedding to image, StableDiffusionImageVariationPipeline)", content=Switcher(label="Use Stable UnCLIP Pipeline Instead", value=unCLIP_interpolation_prefs['use_StableunCLIP_interpolation_pipeline'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'use_StableunCLIP_interpolation_pipeline')))
    #eta = TextField(label="ETA", value=str(unCLIP_interpolation_prefs['eta']), keyboard_type=KeyboardType.NUMBER, hint_text="Amount of Noise", on_change=lambda e:changed(e,'eta', ptype='float'))
    #eta = Slider(min=0.0, max=1.0, divisions=20, label="{value}", value=float(unCLIP_interpolation_prefs['eta']), tooltip="The weight of noise for added noise in a diffusion step. Its value is between 0.0 and 1.0 - 0.0 is DDIM and 1.0 is DDPM scheduler respectively.", expand=True, on_change=lambda e:changed(e,'eta', ptype='float'))
    #eta_row = Row([Text("DDIM ETA: "), eta])
    #max_size = Slider(min=256, max=1280, divisions=64, label="{value}px", value=int(unCLIP_interpolation_prefs['max_size']), expand=True, on_change=lambda e:changed(e,'max_size', ptype='int'))
    #max_row = Row([Text("Max Resolution Size: "), max_size])
    apply_ESRGAN_upscale = Switcher(label="Apply ESRGAN Upscale", value=unCLIP_interpolation_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)
    enlarge_scale_value = Text(f" {float(unCLIP_interpolation_prefs['enlarge_scale'])}x", weight=FontWeight.BOLD)
    enlarge_scale = Slider(min=1, max=4, divisions=6, label="{value}x", value=unCLIP_interpolation_prefs['enlarge_scale'], on_change=change_enlarge_scale, expand=True)
    enlarge_scale_slider = Row([Text("Enlarge Scale: "), enlarge_scale_value, enlarge_scale])
    display_upscaled_image = Checkbox(label="Display Upscaled Image", value=unCLIP_interpolation_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))
    ESRGAN_settings = Container(Column([enlarge_scale_slider, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    page.ESRGAN_block_unCLIP_interpolation = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    page.ESRGAN_block_unCLIP_interpolation.height = None if status['installed_ESRGAN'] else 0
    if not unCLIP_interpolation_prefs['apply_ESRGAN_upscale']:
        ESRGAN_settings.height = 0
    page.unCLIP_interpolation_output = Column([], auto_scroll=True)
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.unCLIP_interpolation_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🌌  unCLIP Text Interpolation Generator", "Takes two prompts and interpolates between the two input prompts using spherical interpolation...", actions=[IconButton(icon=icons.HELP, tooltip="Help with unCLIP Settings", on_click=unCLIP_interpolation_help)]),
        prompt,
        end_prompt,
        #Row([prompt, mask_image, invert_mask]),
        interpolation_steps_row,
        prior_num_inference_row, decoder_num_inference_row, super_res_num_inference_row,
        prior_guidance, decoder_guidance,
        #eta_row, max_row,
        #use_StableunCLIP_interpolation_pipeline,
        #NumberPicker(label="Number of Images: ", min=1, max=20, value=unCLIP_interpolation_prefs['num_images'], on_change=lambda e: changed(e, 'num_images')),
        Row([seed, batch_folder_name]),
        page.ESRGAN_block_unCLIP_interpolation,
        Row([ElevatedButton(content=Text("🎆   Get unCLIP Interpolations", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_unCLIP_interpolation(page))]),
             #ElevatedButton(content=Text(value="📜   Run from Prompts List", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_unCLIP_interpolation(page, from_list=True))]),
        page.unCLIP_interpolation_output,
        clear_button,
      ]
    )),
    ], scroll=ScrollMode.AUTO, auto_scroll=False)
    return c

unCLIP_image_interpolation_prefs = {
    'init_image': '',
    'end_image': '',
    'file_name': '',
    'batch_folder_name': '',
    'interpolation_steps': 6,
    'decoder_guidance_scale': 8.0,
    'decoder_num_inference_steps': 25,
    'super_res_num_inference_steps': 7,
    'seed': 0,
    'num_images': 1,
    'max_size': 768,
    #'variance_type': 'learned_range',#fixed_small_log
    #'num_train_timesteps': 1000,
    #'prediction_type': 'epsilon',#sample
    #'clip_sample': True,
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": 4.0,
    "display_upscaled_image": True,
}
def buildUnCLIP_ImageInterpolation(page):
    global unCLIP_image_interpolation_prefs, prefs, pipe_unCLIP_image_interpolation
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          if ptype == "int":
            unCLIP_image_interpolation_prefs[pref] = int(e.control.value)
          elif ptype == "float":
            unCLIP_image_interpolation_prefs[pref] = float(e.control.value)
          else:
            unCLIP_image_interpolation_prefs[pref] = e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def add_to_unCLIP_image_interpolation_output(o):
      page.unCLIP_image_interpolation_output.controls.append(o)
      page.unCLIP_image_interpolation_output.update()
      if not clear_button.visible:
        clear_button.visible = True
        clear_button.update()
    page.add_to_unCLIP_image_interpolation_output = add_to_unCLIP_image_interpolation_output
    def clear_output(e):
      if prefs['enable_sounds']: page.snd_delete.play()
      page.unCLIP_image_interpolation_output.controls = []
      page.unCLIP_image_interpolation_output.update()
      clear_button.visible = False
      clear_button.update()
    def unCLIP_image_interpolation_help(e):
      def close_unCLIP_image_interpolation_dlg(e):
        nonlocal unCLIP_image_interpolation_help_dlg
        unCLIP_image_interpolation_help_dlg.open = False
        page.update()
      unCLIP_image_interpolation_help_dlg = AlertDialog(title=Text("🙅   Help with unCLIP Image Interpolation Pipeline"), content=Column([
          Text("This Diffusion Pipeline takes two images or an image_embeddings tensor of size 2 and interpolates between their embeddings using spherical interpolation ( slerp ). The input images/image_embeddings are converted to image embeddings by the pipeline's image_encoder and the interpolation is done on the resulting image_embeddings over the number of steps specified."),
          #Text(""),
          #Markdown("The unCLIP Image Interpolation model in diffusers comes from kakaobrain's karlo and the original codebase can be found [here](https://github.com/kakaobrain/karlo). Additionally, lucidrains has a DALL-E 2 recreation [here](https://github.com/lucidrains/DALLE2-pytorch)."),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("🦿  Transformers Activate... ", on_click=close_unCLIP_image_interpolation_dlg)], actions_alignment=MainAxisAlignment.END)
      page.dialog = unCLIP_image_interpolation_help_dlg
      unCLIP_image_interpolation_help_dlg.open = True
      page.update()
    pick_type = ""
    def file_picker_result(e: FilePickerResultEvent):
        if e.files != None:
          upload_files(e)
    def on_upload_progress(e: FilePickerUploadEvent):
      nonlocal pick_type
      if e.progress == 1:
        if not slash in e.file_name:
          fname = os.path.join(root_dir, e.file_name)
          unCLIP_image_interpolation_prefs['file_name'] = e.file_name.rpartition('.')[0]
        else:
          fname = e.file_name
          unCLIP_image_interpolation_prefs['file_name'] = e.file_name.rpartition(slash)[2].rpartition('.')[0]
        if pick_type == "init":
            init_image.value = fname
            init_image.update()
            unCLIP_image_interpolation_prefs['init_image'] = fname
        elif pick_type == "end":
            end_image.value = fname
            end_image.update()
            unCLIP_image_interpolation_prefs['end_image'] = fname
        page.update()
    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)
    def upload_files(e):
        uf = []
        if file_picker.result != None and file_picker.result.files != None:
            for f in file_picker.result.files:
              if page.web:
                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))
              else:
                on_upload_progress(FilePickerUploadEvent(f.path, 1, ""))
            file_picker.upload(uf)
    page.overlay.append(file_picker)
    def pick_init(e):
        nonlocal pick_type
        pick_type = "init"
        file_picker.pick_files(allow_multiple=False, allowed_extensions=["png", "PNG", "jpg", "jpeg"], dialog_title="Pick Init Image File")
    def pick_end(e):
        nonlocal pick_type
        pick_type = "end"
        file_picker.pick_files(allow_multiple=False, allowed_extensions=["png", "PNG", "jpg", "jpeg"], dialog_title="Pick Ending Image File")
    init_image = TextField(label="Initial Image", value=unCLIP_image_interpolation_prefs['init_image'], on_change=lambda e:changed(e,'init_image'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_init))
    end_image = TextField(label="Ending Image", value=unCLIP_image_interpolation_prefs['end_image'], on_change=lambda e:changed(e,'end_image'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_end))
    def toggle_ESRGAN(e):
        ESRGAN_settings.height = None if e.control.value else 0
        unCLIP_image_interpolation_prefs['apply_ESRGAN_upscale'] = e.control.value
        ESRGAN_settings.update()
    def change_enlarge_scale(e):
        enlarge_scale_slider.controls[1].value = f" {float(e.control.value)}x"
        enlarge_scale_slider.update()
        changed(e, 'enlarge_scale', ptype="float")
    #prompt = TextField(label="Prompt Text", value=unCLIP_image_interpolation_prefs['prompt'], on_change=lambda e:changed(e,'prompt'))
    seed = TextField(label="Seed", width=90, value=str(unCLIP_image_interpolation_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    interpolation_steps = SliderRow(label="Interpolation Steps", min=1, max=100, divisions=99, pref=unCLIP_image_interpolation_prefs, key='interpolation_steps', tooltip="The number of interpolation images to generate.")
    decoder_num_inference_row = SliderRow(label="Number of Decoder Inference Steps", min=1, max=100, divisions=99, pref=unCLIP_image_interpolation_prefs, key='decoder_num_inference_steps', tooltip="The number of Decoder denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.")
    super_res_num_inference_row = SliderRow(label="Number of Super-Res Inference Steps", min=1, max=100, divisions=99, pref=unCLIP_image_interpolation_prefs, key='decoder_num_inference_steps', tooltip="The number of Super-Res denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.")
    decoder_guidance = SliderRow(label="Decoder Guidance Scale", min=0, max=50, divisions=100, round=1, pref=unCLIP_image_interpolation_prefs, key='decoder_guidance_scale')
    batch_folder_name = TextField(label="Batch Folder Name", value=unCLIP_image_interpolation_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    #eta = TextField(label="ETA", value=str(unCLIP_image_interpolation_prefs['eta']), keyboard_type=KeyboardType.NUMBER, hint_text="Amount of Noise", on_change=lambda e:changed(e,'eta', ptype='float'))
    #eta = Slider(min=0.0, max=1.0, divisions=20, label="{value}", value=float(unCLIP_image_interpolation_prefs['eta']), tooltip="The weight of noise for added noise in a diffusion step. Its value is between 0.0 and 1.0 - 0.0 is DDIM and 1.0 is DDPM scheduler respectively.", expand=True, on_change=lambda e:changed(e,'eta', ptype='float'))
    #eta_row = Row([Text("DDIM ETA: "), eta])
    max_row = SliderRow(label="Max Resolution Size", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=unCLIP_image_interpolation_prefs, key='max_size')
    apply_ESRGAN_upscale = Switcher(label="Apply ESRGAN Upscale", value=unCLIP_image_interpolation_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)
    enlarge_scale_value = Text(f" {float(unCLIP_image_interpolation_prefs['enlarge_scale'])}x", weight=FontWeight.BOLD)
    enlarge_scale = Slider(min=1, max=4, divisions=6, label="{value}x", value=unCLIP_image_interpolation_prefs['enlarge_scale'], on_change=change_enlarge_scale, expand=True)
    enlarge_scale_slider = Row([Text("Enlarge Scale: "), enlarge_scale_value, enlarge_scale])
    display_upscaled_image = Checkbox(label="Display Upscaled Image", value=unCLIP_image_interpolation_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))
    ESRGAN_settings = Container(Column([enlarge_scale_slider, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    page.ESRGAN_block_unCLIP_image_interpolation = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    page.ESRGAN_block_unCLIP_image_interpolation.height = None if status['installed_ESRGAN'] else 0
    if not unCLIP_image_interpolation_prefs['apply_ESRGAN_upscale']:
        ESRGAN_settings.height = 0
    page.unCLIP_image_interpolation_output = Column([], auto_scroll=True)
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.unCLIP_image_interpolation_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🤖  unCLIP Image Interpolation Generator", "Pass two images and produces in-betweens while interpolating between their image-embeddings...", actions=[IconButton(icon=icons.HELP, tooltip="Help with unCLIP Image Interpolation Settings", on_click=unCLIP_image_interpolation_help)]),
        init_image, end_image,
        interpolation_steps,
        #Row([prompt, mask_image, invert_mask]),
        decoder_num_inference_row, super_res_num_inference_row,
        decoder_guidance,
        #eta_row,
        max_row,
        Row([NumberPicker(label="Number of Images: ", min=1, max=20, value=unCLIP_image_interpolation_prefs['num_images'], on_change=lambda e: changed(e, 'num_images')), seed, batch_folder_name]),
        page.ESRGAN_block_unCLIP_image_interpolation,
        Row([ElevatedButton(content=Text("🦾   Get unCLIP Image Interpolation", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_unCLIP_image_interpolation(page)),
             #ElevatedButton(content=Text(value="📜   Run from Prompts List", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_unCLIP_image_interpolation(page, from_list=True))
             ]),

      ]
    )), page.unCLIP_image_interpolation_output,
        clear_button,
    ], scroll=ScrollMode.AUTO, auto_scroll=False)
    return c


magic_mix_prefs = {
    'init_image': '',
    'prompt': '',
    'guidance_scale': 7.5,
    'num_inference_steps': 50,
    'mix_factor': 0.5,
    'kmin': 0.3,
    'kmax': 0.6,
    'seed': 0,
    'num_images': 1,
    'max_size': 1024,
    'scheduler_mode': 'DDIM',
    'scheduler_last': '',
    'batch_folder_name': '',
    'file_name': '',
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": 4.0,
    "display_upscaled_image": False,
}
def buildMagicMix(page):
    global magic_mix_prefs, prefs, pipe_magic_mix
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          if ptype == "int":
            magic_mix_prefs[pref] = int(e.control.value)
          elif ptype == "float":
            magic_mix_prefs[pref] = float(e.control.value)
          else:
            magic_mix_prefs[pref] = e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def add_to_magic_mix_output(o):
      page.MagicMix.controls.append(o)
      page.MagicMix.update()
      if not clear_button.visible:
        clear_button.visible = True
        clear_button.update()
    page.add_to_magic_mix_output = add_to_magic_mix_output
    def clear_output(e):
      if prefs['enable_sounds']: page.snd_delete.play()
      for i, c in enumerate(page.MagicMix.controls):
        if i == 0: continue
        else: del page.MagicMix.controls[i]
      page.MagicMix.update()
      clear_button.visible = False
      clear_button.update()
    def magic_mix_help(e):
      def close_magic_mix_dlg(e):
        nonlocal magic_mix_help_dlg
        magic_mix_help_dlg.open = False
        page.update()
      magic_mix_help_dlg = AlertDialog(title=Text("🙅   Help with MagicMix"), content=Column([
          Text("Have you ever imagined what a corgi-alike coffee machine or a tiger-alike rabbit would look like? In this work, we attempt to answer these questions by exploring a new task called semantic mixing, aiming at blending two different semantics to create a new concept (e.g., corgi + coffee machine -- > corgi-alike coffee machine). Unlike style transfer, where an image is stylized according to the reference style without changing the image content, semantic blending mixes two different concepts in a semantic manner to synthesize a novel concept while preserving the spatial layout and geometry. To this end, we present MagicMix, a simple yet effective solution based on pre-trained text-conditioned diffusion models. Motivated by the progressive generation property of diffusion models where layout/shape emerges at early denoising steps while semantically meaningful details appear at later steps during the denoising process, our method first obtains a coarse layout (either by corrupting an image or denoising from a pure Gaussian noise given a text prompt), followed by injection of conditional prompt for semantic mixing. Our method does not require any spatial mask or re-training, yet is able to synthesize novel objects with high fidelity. To improve the mixing quality, we further devise two simple strategies to provide better control and flexibility over the synthesized content. With our method, we present our results over diverse downstream applications, including semantic style transfer, novel object synthesis, breed mixing, and concept removal, demonstrating the flexibility of our method."),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("🧙  Sounds like magic... ", on_click=close_magic_mix_dlg)], actions_alignment=MainAxisAlignment.END)
      page.dialog = magic_mix_help_dlg
      magic_mix_help_dlg.open = True
      page.update()
    def file_picker_result(e: FilePickerResultEvent):
        if e.files != None:
          upload_files(e)
    def on_upload_progress(e: FilePickerUploadEvent):
      if e.progress == 1:
        if not slash in e.file_name:
          fname = os.path.join(root_dir, e.file_name)
          magic_mix_prefs['file_name'] = e.file_name.rpartition('.')[0]
        else:
          fname = e.file_name
          magic_mix_prefs['file_name'] = e.file_name.rpartition(slash)[2].rpartition('.')[0]
        init_image.value = fname
        init_image.update()
        magic_mix_prefs['init_image'] = fname
        page.update()
    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)
    def upload_files(e):
        uf = []
        if file_picker.result != None and file_picker.result.files != None:
            for f in file_picker.result.files:
              if page.web:
                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))
              else:
                on_upload_progress(FilePickerUploadEvent(f.path, 1, ""))
            file_picker.upload(uf)
    page.overlay.append(file_picker)
    def pick_init(e):
        file_picker.pick_files(allow_multiple=False, allowed_extensions=["png", "PNG", "jpg", "jpeg"], dialog_title="Pick Init Image File")
    prompt = TextField(label="Prompt Text", value=magic_mix_prefs['prompt'], on_change=lambda e:changed(e,'prompt'))
    def toggle_ESRGAN(e):
        ESRGAN_settings.height = None if e.control.value else 0
        magic_mix_prefs['apply_ESRGAN_upscale'] = e.control.value
        ESRGAN_settings.update()
    init_image = TextField(label="Initial Image", value=magic_mix_prefs['init_image'], on_change=lambda e:changed(e,'init_image'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_init))
    seed = TextField(label="Seed", width=90, value=str(magic_mix_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    scheduler_mode = Dropdown(label="Scheduler/Sampler Mode", hint_text="They're very similar, with minor differences in the noise", width=200,
            options=[
                dropdown.Option("DDIM"),
                dropdown.Option("LMS Discrete"),
                dropdown.Option("PNDM"),
            ], value=magic_mix_prefs['scheduler_mode'], autofocus=False, on_change=lambda e:changed(e, 'scheduler_mode'),
        )
    num_inference_row = SliderRow(label="Number of Inference Steps", min=1, max=100, divisions=99, pref=magic_mix_prefs, key='num_inference_steps', tooltip="The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.")
    guidance = SliderRow(label="Guidance Scale", min=0, max=50, divisions=100, round=1, pref=magic_mix_prefs, key='guidance_scale')
    mix_factor_row = SliderRow(label="Mix Factor", min=0.0, max=1.0, divisions=20, round=2, pref=magic_mix_prefs, key='mix_factor', tooltip="Interpolation constant used in the layout generation phase. The greater the value of `mix_factor`, the greater the influence of the prompt on the layout generation process.")
    kmin_row = SliderRow(label="k-Min", min=0.0, max=1.0, divisions=20, round=2, pref=magic_mix_prefs, key='kmin', tooltip="A higher value of kmin results in more steps for content generation process. Determine the range for the layout and content generation process.")
    kmax_row = SliderRow(label="k-Max", min=0.0, max=1.0, divisions=20, round=2, pref=magic_mix_prefs, key='kmax', tooltip="A higher value of kmax results in loss of more information about the layout of the original image. Determine the range for the layout and content generation process.")
    max_row = SliderRow(label="Max Resolution Size", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=magic_mix_prefs, key='max_size')
    batch_folder_name = TextField(label="Batch Folder Name", value=magic_mix_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    apply_ESRGAN_upscale = Switcher(label="Apply ESRGAN Upscale", value=magic_mix_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)
    enlarge_scale_slider = SliderRow(label="Enlarge Scale", min=1, max=4, divisions=6, round=1, suffix="x", pref=magic_mix_prefs, key='enlarge_scale')
    display_upscaled_image = Checkbox(label="Display Upscaled Image", value=magic_mix_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))
    ESRGAN_settings = Container(Column([enlarge_scale_slider, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    page.ESRGAN_block_magic_mix = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    page.ESRGAN_block_magic_mix.height = None if status['installed_ESRGAN'] else 0
    if not magic_mix_prefs['apply_ESRGAN_upscale']:
        ESRGAN_settings.height = 0

    page.magic_mix_output = Column([], auto_scroll=True)
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.magic_mix_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🧚  MagicMix Init Image with Prompt", "Diffusion Pipeline for semantic mixing of an image and a text prompt...", actions=[IconButton(icon=icons.HELP, tooltip="Help with MagicMix Settings", on_click=magic_mix_help)]),
        init_image,
        prompt,
        scheduler_mode,
        num_inference_row,
        guidance,
        mix_factor_row,
        ResponsiveRow([kmin_row, kmax_row]),
        max_row,
        Row([NumberPicker(label="Number of Images: ", min=1, max=8, value=magic_mix_prefs['num_images'], on_change=lambda e: changed(e, 'num_images')), seed, batch_folder_name]),
        page.ESRGAN_block_magic_mix,
        Row([ElevatedButton(content=Text("🪄  Make MagicMix", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_magic_mix(page)),
             ElevatedButton(content=Text(value="📜   Run from Prompts List", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_magic_mix(page, from_list=True))]),
        page.magic_mix_output,
        clear_button,
      ]
    ))], scroll=ScrollMode.AUTO, auto_scroll=False)
    return c

paint_by_example_prefs = {
    'original_image': '',
    'mask_image': '',
    'example_image': '',
    'num_inference_steps': 50,
    'guidance_scale': 7.5,
    'eta': 0.0,
    'seed': 0,
    'max_size': 768,
    'alpha_mask': False,
    'invert_mask': False,
    'num_images': 1,
    'batch_folder_name': '',
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": 2.0,
    "display_upscaled_image": False,
}
def buildPaintByExample(page):
    global paint_by_example_prefs, prefs, pipe_paint_by_example
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          if ptype == "int":
            paint_by_example_prefs[pref] = int(e.control.value)
          elif ptype == "float":
            paint_by_example_prefs[pref] = float(e.control.value)
          else:
            paint_by_example_prefs[pref] = e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def add_to_paint_by_example_output(o):
      page.paint_by_example_output.controls.append(o)
      page.paint_by_example_output.update()
      if not clear_button.visible:
        clear_button.visible = True
        clear_button.update()
    def clear_output(e):
      if prefs['enable_sounds']: page.snd_delete.play()
      page.paint_by_example_output.controls = []
      page.paint_by_example_output.update()
      clear_button.visible = False
      clear_button.update()
    def paint_by_example_help(e):
      def close_paint_by_example_dlg(e):
        nonlocal paint_by_example_help_dlg
        paint_by_example_help_dlg.open = False
        page.update()
      paint_by_example_help_dlg = AlertDialog(title=Text("💁   Help with Paint-by-Example"), content=Column([
          Text("Language-guided image editing has achieved great success recently. In this pipeline, we use exemplar-guided image editing for more precise control. We achieve this goal by leveraging self-supervised training to disentangle and re-organize the source image and the exemplar. However, the naive approach will cause obvious fusing artifacts. We carefully analyze it and propose an information bottleneck and strong augmentations to avoid the trivial solution of directly copying and pasting the exemplar image. Meanwhile, to ensure the controllability of the editing process, we design an arbitrary shape mask for the exemplar image and leverage the classifier-free guidance to increase the similarity to the exemplar image. The whole framework involves a single forward of the diffusion model without any iterative optimization. We demonstrate that our method achieves an impressive performance and enables controllable editing on in-the-wild images with high fidelity.  Credit goes to https://github.com/Fantasy-Studio/Paint-by-Example"),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("😸  Sweetness... ", on_click=close_paint_by_example_dlg)], actions_alignment=MainAxisAlignment.END)
      page.dialog = paint_by_example_help_dlg
      paint_by_example_help_dlg.open = True
      page.update()
    def file_picker_result(e: FilePickerResultEvent):
        if e.files != None:
          upload_files(e)
    def on_upload_progress(e: FilePickerUploadEvent):
      nonlocal pick_type
      if e.progress == 1:
        if not slash in e.file_name:
          fname = os.path.join(root_dir, e.file_name)
          paint_by_example_prefs['file_name'] = e.file_name.rpartition('.')[0]
        else:
          fname = e.file_name
          paint_by_example_prefs['file_name'] = e.file_name.rpartition(slash)[2].rpartition('.')[0]
        if pick_type == "original":
          original_image.value = fname
          original_image.update()
          paint_by_example_prefs['original_image'] = fname
        elif pick_type == "mask":
          mask_image.value = fname
          mask_image.update()
          paint_by_example_prefs['mask_image'] = fname
        elif pick_type == "example":
          example_image.value = fname
          example_image.update()
          paint_by_example_prefs['example_image'] = fname
        page.update()
    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)
    def upload_files(e):
        uf = []
        if file_picker.result != None and file_picker.result.files != None:
            for f in file_picker.result.files:
              if page.web:
                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))
              else:
                on_upload_progress(FilePickerUploadEvent(f.path, 1, ""))
            file_picker.upload(uf)
    page.overlay.append(file_picker)
    pick_type = ""
    #page.overlay.append(pick_files_dialog)
    def pick_original(e):
        nonlocal pick_type
        pick_type = "original"
        file_picker.pick_files(allow_multiple=False, allowed_extensions=["png", "PNG", "jpg", "jpeg"], dialog_title="Pick Original Image File")
    def pick_mask(e):
        nonlocal pick_type
        pick_type = "mask"
        file_picker.pick_files(allow_multiple=False, allowed_extensions=["png", "PNG", "jpg", "jpeg"], dialog_title="Pick Black & White Mask Image")
    def pick_example(e):
        nonlocal pick_type
        pick_type = "example"
        file_picker.pick_files(allow_multiple=False, allowed_extensions=["png", "PNG", "jpg", "jpeg"], dialog_title="Pick Example Style Image")
    def toggle_ESRGAN(e):
        ESRGAN_settings.height = None if e.control.value else 0
        paint_by_example_prefs['apply_ESRGAN_upscale'] = e.control.value
        ESRGAN_settings.update()
    def change_eta(e):
        changed(e, 'eta', ptype="float")
        eta_value.value = f" {paint_by_example_prefs['eta']}"
        eta_value.update()
        eta_row.update()
    original_image = TextField(label="Original Image", value=paint_by_example_prefs['original_image'], expand=1, on_change=lambda e:changed(e,'original_image'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_original))
    mask_image = TextField(label="Mask Image", value=paint_by_example_prefs['mask_image'], expand=1, on_change=lambda e:changed(e,'mask_image'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD_OUTLINED, on_click=pick_mask))
    alpha_mask = Checkbox(label="Alpha Mask", value=paint_by_example_prefs['alpha_mask'], tooltip="Use Transparent Alpha Channel of Init as Mask", fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'alpha_mask'))
    invert_mask = Checkbox(label="Invert", tooltip="Swaps the Black & White of your Mask Image", value=paint_by_example_prefs['invert_mask'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'invert_mask'))
    example_image = TextField(label="Example Style Image", value=paint_by_example_prefs['example_image'], on_change=lambda e:changed(e,'example_image'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_example))
    seed = TextField(label="Seed", width=90, value=str(paint_by_example_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    num_inference_row = SliderRow(label="Number of Inference Steps", min=1, max=100, divisions=99, pref=paint_by_example_prefs, key='num_inference_steps', tooltip="The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.")
    guidance = SliderRow(label="Guidance Scale", min=0, max=50, divisions=100, round=1, pref=paint_by_example_prefs, key='guidance_scale')
    #eta = TextField(label="ETA", value=str(paint_by_example_prefs['eta']), keyboard_type=KeyboardType.NUMBER, hint_text="Amount of Noise", on_change=lambda e:changed(e,'eta', ptype='float'))
    eta = Slider(min=0.0, max=1.0, divisions=20, label="{value}", value=float(paint_by_example_prefs['eta']), tooltip="The weight of noise for added noise in a diffusion step. Its value is between 0.0 and 1.0 - 0.0 is DDIM and 1.0 is DDPM scheduler respectively.", expand=True, on_change=change_eta)
    eta_value = Text(f" {paint_by_example_prefs['eta']}", weight=FontWeight.BOLD)
    eta_row = Row([Text("ETA:"), eta_value, Text("  DDIM"), eta, Text("DDPM")])
    page.etas.append(eta_row)
    max_row = SliderRow(label="Max Resolution Size", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=paint_by_example_prefs, key='max_size')
    batch_folder_name = TextField(label="Batch Folder Name", value=paint_by_example_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    apply_ESRGAN_upscale = Switcher(label="Apply ESRGAN Upscale", value=paint_by_example_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)
    enlarge_scale_slider = SliderRow(label="Enlarge Scale", min=1, max=4, divisions=6, round=1, suffix="x", pref=paint_by_example_prefs, key='enlarge_scale')
    display_upscaled_image = Checkbox(label="Display Upscaled Image", value=paint_by_example_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))
    ESRGAN_settings = Container(Column([enlarge_scale_slider, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    page.ESRGAN_block_paint_by_example = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    page.ESRGAN_block_paint_by_example.height = None if status['installed_ESRGAN'] else 0
    page.paint_by_example_output = Column([])
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.paint_by_example_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🦁  Paint-by-Example", "Image-guided Inpainting using an Example Image to Transfer Subject to Masked area...", actions=[IconButton(icon=icons.HELP, tooltip="Help with Paint-by-Example Settings", on_click=paint_by_example_help)]),
        ResponsiveRow([Row([original_image, alpha_mask], col={'lg':6}), Row([mask_image, invert_mask], col={'lg':6})]),
        example_image,
        num_inference_row,
        guidance,
        eta_row,
        max_row,
        Row([NumberPicker(label="Number of Images: ", min=1, max=8, value=paint_by_example_prefs['num_images'], on_change=lambda e: changed(e, 'num_images')), seed, batch_folder_name]),
        page.ESRGAN_block_paint_by_example,
        #Row([jump_length, jump_n_sample, seed]),
        ElevatedButton(content=Text("🐾  Run Paint-by-Example", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_paint_by_example(page)),
        page.paint_by_example_output,
        clear_button,
      ]
    ))], scroll=ScrollMode.AUTO, auto_scroll=False)
    return c

instruct_pix2pix_prefs = {
    'original_image': '',
    'prompt': '',
    'negative_prompt': '',
    'num_inference_steps': 100,
    'guidance_scale': 7.5,
    'image_guidance_scale': 1.5,
    'eta': 0.0,
    'seed': 0,
    'max_size': 768,
    'num_images': 1,
    'use_init_video': False,
    'init_video': '',
    'fps': 12,
    'start_time': 0,
    'end_time': 0,
    'control_v': 'v1.1',
    'use_SDXL':False,
    'batch_folder_name': '',
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": 2.0,
    "display_upscaled_image": False,
}

def buildInstructPix2Pix(page):
    global instruct_pix2pix_prefs, prefs, pipe_instruct_pix2pix
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          if ptype == "int":
            instruct_pix2pix_prefs[pref] = int(e.control.value)
          elif ptype == "float":
            instruct_pix2pix_prefs[pref] = float(e.control.value)
          else:
            instruct_pix2pix_prefs[pref] = e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def add_to_instruct_pix2pix_output(o):
      page.instruct_pix2pix_output.controls.append(o)
      page.instruct_pix2pix_output.update()
      if not clear_button.visible:
        clear_button.visible = True
        clear_button.update()
    def clear_output(e):
      if prefs['enable_sounds']: page.snd_delete.play()
      page.instruct_pix2pix_output.controls = []
      page.instruct_pix2pix_output.update()
      clear_button.visible = False
      clear_button.update()
    def instruct_pix2pix_help(e):
      def close_instruct_pix2pix_dlg(e):
        nonlocal instruct_pix2pix_help_dlg
        instruct_pix2pix_help_dlg.open = False
        page.update()
      instruct_pix2pix_help_dlg = AlertDialog(title=Text("💁   Help with Instruct-Pix2Pix"), content=Column([
          Text("A method for editing images from human instructions: given an input image and a written instruction that tells the model what to do, our model follows these instructions to edit the image. To obtain training data for this problem, we combine the knowledge of two large pretrained models -- a language model (GPT-3) and a text-to-image model (Stable Diffusion) -- to generate a large dataset of image editing examples. Our conditional diffusion model, InstructPix2Pix, is trained on our generated data, and generalizes to real images and user-written instructions at inference time. Since it performs edits in the forward pass and does not require per example fine-tuning or inversion, our model edits images quickly, in a matter of seconds. We show compelling editing results for a diverse collection of input images and written instructions."),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("😎  Fun2Fun... ", on_click=close_instruct_pix2pix_dlg)], actions_alignment=MainAxisAlignment.END)
      page.dialog = instruct_pix2pix_help_dlg
      instruct_pix2pix_help_dlg.open = True
      page.update()
    def file_picker_result(e: FilePickerResultEvent):
        if e.files != None:
          upload_files(e)
    def on_upload_progress(e: FilePickerUploadEvent):
      nonlocal pick_type
      if e.progress == 1:
        if not slash in e.file_name:
          fname = os.path.join(root_dir, e.file_name)
          instruct_pix2pix_prefs['file_name'] = e.file_name.rpartition('.')[0]
        else:
          fname = e.file_name
          instruct_pix2pix_prefs['file_name'] = e.file_name.rpartition(slash)[2].rpartition('.')[0]
        if pick_type == "image":
          original_image.value = fname
          original_image.update()
          instruct_pix2pix_prefs['original_image'] = fname
        elif pick_type == "video":
          init_video.value = fname
          init_video.update()
          instruct_pix2pix_prefs['init_video'] = fname
        page.update()
    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)
    def upload_files(e):
        uf = []
        if file_picker.result != None and file_picker.result.files != None:
            for f in file_picker.result.files:
              if page.web:
                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))
              else:
                on_upload_progress(FilePickerUploadEvent(f.path, 1, ""))
            file_picker.upload(uf)
    page.overlay.append(file_picker)
    pick_type = ""
    def pick_original(e):
        nonlocal pick_type
        pick_type = "image"
        file_picker.pick_files(allow_multiple=False, allowed_extensions=["png", "PNG", "jpg", "jpeg"], dialog_title="Pick Original Image File")
    def pick_video(e):
        nonlocal pick_type
        pick_type = "video"
        file_picker.pick_files(allow_multiple=False, allowed_extensions=["mp4", "avi"], dialog_title="Pick Initial Video File")
    def toggle_ESRGAN(e):
        ESRGAN_settings.height = None if e.control.value else 0
        instruct_pix2pix_prefs['apply_ESRGAN_upscale'] = e.control.value
        ESRGAN_settings.update()
    def change_eta(e):
        changed(e, 'eta', ptype="float")
        eta_value.value = f" {instruct_pix2pix_prefs['eta']}"
        eta_value.update()
        eta_row.update()
    def toggle_init_video(e):
        changed(e, 'use_init_video')
        show = e.control.value
        original_image.visible = not show
        original_image.update()
        init_video.visible = show
        init_video.update()
        vid_params.height = None if show else 0
        vid_params.update()
        run_prompt_list.visible = not show
        run_prompt_list.update()
    original_image = TextField(label="Original Image", value=instruct_pix2pix_prefs['original_image'], expand=True, on_change=lambda e:changed(e,'original_image'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_original))
    prompt = TextField(label="Editing Instructions Prompt Text", value=instruct_pix2pix_prefs['prompt'], col={'md': 9}, multiline=True, on_change=lambda e:changed(e,'prompt'))
    negative_prompt  = TextField(label="Negative Prompt Text", value=instruct_pix2pix_prefs['negative_prompt'], col={'md':3}, multiline=True, on_change=lambda e:changed(e,'negative_prompt'))
    seed = TextField(label="Seed", width=90, value=str(instruct_pix2pix_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    use_init_video = Tooltip(message="Input a short mp4 file to animate with.", content=Switcher(label="Use Init Video", value=instruct_pix2pix_prefs['use_init_video'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_init_video))
    init_video = TextField(label="Init Video Clip", value=instruct_pix2pix_prefs['init_video'], expand=True, visible=instruct_pix2pix_prefs['use_init_video'], on_change=lambda e:changed(e,'init_video'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_video))
    fps = SliderRow(label="Frames per Second", min=1, max=30, divisions=29, suffix='fps', pref=instruct_pix2pix_prefs, key='fps', tooltip="The FPS to extract from the init video clip.")
    start_time = TextField(label="Start Time (s)", value=instruct_pix2pix_prefs['start_time'], width=145, keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'start_time', ptype="float"))
    end_time = TextField(label="End Time (0 for all)", value=instruct_pix2pix_prefs['end_time'], width=145, keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'end_time', ptype="float"))
    vid_params = Container(content=Column([fps, Row([start_time, end_time])]), animate_size=animation.Animation(800, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE, height=None if instruct_pix2pix_prefs['use_init_video'] else 0)
    num_inference_row = SliderRow(label="Number of Inference Steps", min=1, max=150, divisions=149, pref=instruct_pix2pix_prefs, key='num_inference_steps', tooltip="The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.")
    guidance = SliderRow(label="Guidance Scale", min=0, max=50, divisions=100, round=1, pref=instruct_pix2pix_prefs, key='guidance_scale')
    image_guidance = SliderRow(label="Image Guidance Scale", min=0, max=200, divisions=400, round=1, pref=instruct_pix2pix_prefs, key='image_guidance_scale', tooltip="Image guidance scale is to push the generated image towards the inital image `image`. Higher image guidance scale encourages to generate images that are closely linked to the source image `image`, usually at the expense of lower image quality.")
    #eta = TextField(label="ETA", value=str(instruct_pix2pix_prefs['eta']), keyboard_type=KeyboardType.NUMBER, hint_text="Amount of Noise", on_change=lambda e:changed(e,'eta', ptype='float'))
    eta = Slider(min=0.0, max=1.0, divisions=20, label="{value}", value=float(instruct_pix2pix_prefs['eta']), tooltip="The weight of noise for added noise in a diffusion step. Its value is between 0.0 and 1.0 - 0.0 is DDIM and 1.0 is DDPM scheduler respectively.", expand=True, on_change=change_eta)
    eta_value = Text(f" {instruct_pix2pix_prefs['eta']}", weight=FontWeight.BOLD)
    eta_row = Row([Text("ETA:"), eta_value, Text("  DDIM"), eta, Text("DDPM")])
    page.etas.append(eta_row)
    max_row = SliderRow(label="Max Resolution Size", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=instruct_pix2pix_prefs, key='max_size')
    batch_folder_name = TextField(label="Batch Folder Name", value=instruct_pix2pix_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    apply_ESRGAN_upscale = Switcher(label="Apply ESRGAN Upscale", value=instruct_pix2pix_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)
    enlarge_scale_slider = SliderRow(label="Enlarge Scale", min=1, max=4, divisions=6, round=1, suffix="x", pref=instruct_pix2pix_prefs, key='enlarge_scale')
    display_upscaled_image = Checkbox(label="Display Upscaled Image", value=instruct_pix2pix_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))
    ESRGAN_settings = Container(Column([enlarge_scale_slider, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    page.ESRGAN_block_instruct_pix2pix = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    page.ESRGAN_block_instruct_pix2pix.height = None if status['installed_ESRGAN'] else 0
    page.instruct_pix2pix_output = Column([])
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.instruct_pix2pix_output.controls) > 0
    run_prompt_list = ElevatedButton(content=Text(value="📜   Run from Prompts List", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_instruct_pix2pix(page, from_list=True))
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🏜️  Instruct-Pix2Pix", "Text-Based Image Editing - Learning to Follow Image Editing Instructions...", actions=[IconButton(icon=icons.HELP, tooltip="Help with Instruct-Pix2Pix Settings", on_click=instruct_pix2pix_help)]),
        #ResponsiveRow([Row([original_image, alpha_mask], col={'lg':6}), Row([mask_image, invert_mask], col={'lg':6})]),
        Row([original_image, init_video, use_init_video]),
        vid_params,
        ResponsiveRow([prompt, negative_prompt]),
        num_inference_row,
        guidance,
        image_guidance,
        eta_row,
        max_row,
        Row([NumberPicker(label="Number of Images: ", min=1, max=8, value=instruct_pix2pix_prefs['num_images'], on_change=lambda e: changed(e, 'num_images')), seed, batch_folder_name]),
        page.ESRGAN_block_instruct_pix2pix,
        #Row([jump_length, jump_n_sample, seed]),

        Row([ElevatedButton(content=Text("🏖️  Run Instruct Pix2Pix", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_instruct_pix2pix(page)),
             run_prompt_list]),
        page.instruct_pix2pix_output,
        clear_button,
      ]))], scroll=ScrollMode.AUTO, auto_scroll=False)
    return c

controlnet_prefs = {
    'original_image': '',
    'prompt': '',
    'negative_prompt': 'lowres, text, watermark, cropped, low quality',
    'control_task': 'Scribble',
    'conditioning_scale': 1.0,
    'control_guidance_start': 0.0,
    'control_guidance_end': 1.0,
    'multi_controlnets': [],
    'batch_size': 1,
    'max_size': 768,
    'low_threshold': 100, #1-255
    'high_threshold': 200, #1-255
    'steps': 50, #100
    'guidance_scale': 9, #30
    'seed': 0,
    'eta': 0,
    'show_processed_image': False,
    'use_init_video': False,
    'init_video': '',
    'fps': 12,
    'start_time': 0,
    'end_time': 0,
    'file_prefix': 'controlnet-',
    'batch_folder_name': '',
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": 2.0,
    "display_upscaled_image": False,
}

def buildControlNet(page):
    global controlnet_prefs, prefs
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          if ptype == "int":
            controlnet_prefs[pref] = int(e.control.value)
          elif ptype == "float":
            controlnet_prefs[pref] = float(e.control.value)
          else:
            controlnet_prefs[pref] = e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def add_to_controlnet_output(o):
      page.controlnet_output.controls.append(o)
      page.controlnet_output.update()
      if not clear_button.visible:
        clear_button.visible = True
        clear_button.update()
    def clear_output(e):
      if prefs['enable_sounds']: page.snd_delete.play()
      page.controlnet_output.controls = []
      page.controlnet_output.update()
      clear_button.visible = False
      clear_button.update()
    def controlnet_help(e):
      def close_controlnet_dlg(e):
        nonlocal controlnet_help_dlg
        controlnet_help_dlg.open = False
        page.update()
      controlnet_help_dlg = AlertDialog(title=Text("💁   Help with ControlNet"), content=Column([
          Text('ControlNet is a neural network structure to control diffusion models by adding extra conditions. It copys the weights of neural network blocks into a "locked" copy and a "trainable" copy. The "trainable" one learns your condition. The "locked" one preserves your model. Thanks to this, training with small dataset of image pairs will not destroy the production-ready diffusion models. The "zero convolution" is 1×1 convolution with both weight and bias initialized as zeros. Before training, all zero convolutions output zeros, and ControlNet will not cause any distortion.  No layer is trained from scratch. You are still fine-tuning. Your original model is safe.  This allows training on small-scale or even personal devices. This is also friendly to merge/replacement/offsetting of models/weights/blocks/layers.'),
          Markdown("This is an interface for running the [official codebase](https://github.com/lllyasviel/ControlNet#readme) for models described in [Adding Conditional Control to Text-to-Image Diffusion Models](https://arxiv.org/abs/2302.05543).", on_tap_link=lambda e: e.page.launch_url(e.data)),
          Text("Scribble - A hand-drawn monochrome image with white outlines on a black background."),
          Text("Canny Map Edge - A monochrome image with white edges on a black background."),
          Text("OpenPose - A OpenPose bone image."),
          Text("Depth - A grayscale image with black representing deep areas and white representing shallow areas."),
          Text("HED - A monochrome image with white soft edges on a black background."),
          Text("M-LSD - A monochrome image composed only of white straight lines on a black background."),
          Text("Normal Map - A normal mapped image."),
          Text("Segmented - An ADE20K's semantic segmentation protocol image."),
          Text("LineArt - An image with line art, usually black lines on a white background."),
          Text("Shuffle - An image with shuffled patches or regions."),
          Text("Brightness - An image based on brightness of init."),
          Text("Instruct Pix2Pix - Trained with pixel to pixel instruction."),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("🍄  Too much control... ", on_click=close_controlnet_dlg)], actions_alignment=MainAxisAlignment.END)
      page.dialog = controlnet_help_dlg
      controlnet_help_dlg.open = True
      page.update()
    def file_picker_result(e: FilePickerResultEvent):
        if e.files != None:
          upload_files(e)
    def on_upload_progress(e: FilePickerUploadEvent):
      if e.progress == 1:
        if not slash in e.file_name:
          fname = os.path.join(root_dir, e.file_name)
          controlnet_prefs['file_name'] = e.file_name.rpartition('.')[0]
        else:
          fname = e.file_name
          controlnet_prefs['file_name'] = e.file_name.rpartition(slash)[2].rpartition('.')[0]
        if pick_type == "image":
          original_image.value = fname
          original_image.update()
          controlnet_prefs['original_image'] = fname
        elif pick_type == "video":
          init_video.value = fname
          init_video.update()
          controlnet_prefs['init_video'] = fname
        page.update()
    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)
    def upload_files(e):
        uf = []
        if file_picker.result != None and file_picker.result.files != None:
            for f in file_picker.result.files:
              if page.web:
                uf.append(FilePickerUploadFile(f.name, upload_url=e.page.get_upload_url(f.name, 600)))
              else:
                on_upload_progress(FilePickerUploadEvent(f.path, 1, ""))
            file_picker.upload(uf)
    page.overlay.append(file_picker)
    pick_type = ""
    def pick_original(e):
        nonlocal pick_type
        pick_type = "image"
        file_picker.pick_files(allow_multiple=False, allowed_extensions=["png", "PNG", "jpg", "jpeg"], dialog_title="Pick Original Image File")
    def pick_video(e):
        nonlocal pick_type
        pick_type = "video"
        file_picker.pick_files(allow_multiple=False, allowed_extensions=["mp4", "avi"], dialog_title="Pick Initial Video File")
    def toggle_ESRGAN(e):
        ESRGAN_settings.height = None if e.control.value else 0
        controlnet_prefs['apply_ESRGAN_upscale'] = e.control.value
        ESRGAN_settings.update()
    def change_task(e):
        task = e.control.value
        show = task.startswith("Video")# or task == "Video OpenPose"
        update = controlnet_prefs['use_init_video'] != show
        changed(e,'control_task')
        threshold.height = None if controlnet_prefs['control_task'] == "Canny Map Edge" or controlnet_prefs['control_task'] == "Video Canny Edge" else 0
        threshold.update()
        if update:
            original_image.visible = not show
            original_image.update()
            init_video.visible = show
            init_video.update()
            vid_params.height = None if show else 0
            vid_params.update()
            conditioning_scale.visible = not show
            conditioning_scale.update()
            add_layer_btn.visible = not show
            add_layer_btn.update()
            multi_layers.visible = not show
            multi_layers.update()
            run_prompt_list.visible = not show
            run_prompt_list.update()
            controlnet_prefs['use_init_video'] = show
    def change_eta(e):
        changed(e, 'eta', ptype="float")
        eta_value.value = f" {controlnet_prefs['eta']}"
        eta_value.update()
        eta_row.update()
    def add_layer(e):
        layer = {'control_task': controlnet_prefs['control_task'], 'original_image': controlnet_prefs['original_image'], 'conditioning_scale': controlnet_prefs['conditioning_scale'], 'control_guidance_start': controlnet_prefs['control_guidance_start'], 'control_guidance_end': controlnet_prefs['control_guidance_end'], 'use_init_video': False}
        if controlnet_prefs['control_task'] == "Video Canny Edge" or controlnet_prefs['control_task'] == "Video OpenPose":
          layer['use_init_video'] = True
          layer['init_video'] = controlnet_prefs['init_video']
          layer['fps'] = controlnet_prefs['fps']
          layer['start_time'] = controlnet_prefs['start_time']
          layer['end_time'] = controlnet_prefs['end_time']
          controlnet_prefs['init_video'] = ""
          init_video.value = ""
          original_image.update()
        controlnet_prefs['multi_controlnets'].append(layer)
        multi_layers.controls.append(ListTile(title=Row([Text(layer['control_task'] + " - ", weight=FontWeight.BOLD), Text(layer['init_video'] if layer['use_init_video'] else layer['original_image']), Text(f"- Conditioning Scale: {layer['conditioning_scale']} - Start: {layer['control_guidance_start']}, End: {layer['control_guidance_end']}")]), dense=True, trailing=PopupMenuButton(icon=icons.MORE_VERT,
          items=[
              PopupMenuItem(icon=icons.DELETE, text="Delete Control Layer", on_click=delete_layer, data=layer),
              PopupMenuItem(icon=icons.DELETE_SWEEP, text="Delete All Layers", on_click=delete_all_layers, data=layer),
          ]), data=layer))
        multi_layers.update()
        controlnet_prefs['original_image'] = ""
        original_image.value = ""
        original_image.update()
    def delete_layer(e):
        controlnet_prefs['multi_controlnets'].remove(e.control.data)
        for c in multi_layers.controls:
          if c.data['original_image'] == e.control.data['original_image']:
             multi_layers.controls.remove(c)
             break
        multi_layers.update()
    def delete_all_layers(e):
        controlnet_prefs['multi_controlnets'].clear()
        multi_layers.controls.clear()
        multi_layers.update()
    original_image = TextField(label="Original Drawing", value=controlnet_prefs['original_image'], expand=True, on_change=lambda e:changed(e,'original_image'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_original))
    prompt = TextField(label="Prompt Text", value=controlnet_prefs['prompt'], col={'md': 8}, multiline=True, on_change=lambda e:changed(e,'prompt'))
    #a_prompt  = TextField(label="Added Prompt Text", value=controlnet_prefs['a_prompt'], col={'md':3}, on_change=lambda e:changed(e,'a_prompt'))
    negative_prompt  = TextField(label="Negative Prompt Text", value=controlnet_prefs['negative_prompt'], col={'md':4}, multiline=True, on_change=lambda e:changed(e,'negative_prompt'))
    control_task = Dropdown(label="ControlNet Task", width=200, options=[dropdown.Option("Scribble"), dropdown.Option("Canny Map Edge"), dropdown.Option("OpenPose"), dropdown.Option("Depth"), dropdown.Option("HED"), dropdown.Option("M-LSD"), dropdown.Option("Normal Map"), dropdown.Option("Segmentation"), dropdown.Option("LineArt"), dropdown.Option("Shuffle"), dropdown.Option("Instruct Pix2Pix"), dropdown.Option("Brightness"), dropdown.Option("Video Canny Edge"), dropdown.Option("Video OpenPose")], value=controlnet_prefs['control_task'], on_change=change_task)
    conditioning_scale = SliderRow(label="Conditioning Scale", min=0, max=2, divisions=20, round=1, pref=controlnet_prefs, key='conditioning_scale', tooltip="The outputs of the controlnet are multiplied by `controlnet_conditioning_scale` before they are added to the residual in the original unet.")
    control_guidance_start = SliderRow(label="Control Guidance Start", min=0.0, max=1.0, divisions=10, round=1, expand=True, pref=controlnet_prefs, key='control_guidance_start', tooltip="The percentage of total steps at which the controlnet starts applying.")
    control_guidance_end = SliderRow(label="Control Guidance End", min=0.0, max=1.0, divisions=10, round=1, expand=True, pref=controlnet_prefs, key='control_guidance_end', tooltip="The percentage of total steps at which the controlnet stops applying.")
    #add_layer_btn = IconButton(icons.ADD, tooltip="Add Multi-ControlNet Layer", on_click=add_layer)
    add_layer_btn = ft.FilledButton("➕ Add Layer", width=140, on_click=add_layer)
    multi_layers = Column([], spacing=0)
    seed = TextField(label="Seed", width=90, value=str(controlnet_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    #use_init_video = Tooltip(message="Input a short mp4 file to animate with.", content=Switcher(label="Use Init Video", value=controlnet_prefs['use_init_video'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_init_video))
    init_video = TextField(label="Init Video Clip", value=controlnet_prefs['init_video'], expand=True, visible=controlnet_prefs['use_init_video'], on_change=lambda e:changed(e,'init_video'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_video))
    fps = SliderRow(label="Frames per Second", min=1, max=30, divisions=29, suffix='fps', pref=controlnet_prefs, key='fps', tooltip="The FPS to extract from the init video clip.")
    start_time = TextField(label="Start Time (s)", value=controlnet_prefs['start_time'], width=145, keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'start_time', ptype="float"))
    end_time = TextField(label="End Time (0 for all)", value=controlnet_prefs['end_time'], width=145, keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'end_time', ptype="float"))
    vid_params = Container(content=Column([fps, Row([start_time, end_time])]), animate_size=animation.Animation(800, AnimationCurve.EASE_OUT), clip_behavior=ClipBehavior.HARD_EDGE, height=None if controlnet_prefs['use_init_video'] else 0)
    num_inference_row = SliderRow(label="Number of Steps", min=1, max=100, divisions=99, pref=controlnet_prefs, key='steps', tooltip="The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.")
    guidance = SliderRow(label="Guidance Scale", min=0, max=30, divisions=60, round=1, pref=controlnet_prefs, key='guidance_scale')
    low_threshold_row = SliderRow(label="Canny Low Threshold", min=1, max=255, divisions=254, pref=controlnet_prefs, key='low_threshold')
    high_threshold_row = SliderRow(label="Canny High Threshold", min=1, max=255, divisions=254, pref=controlnet_prefs, key='high_threshold')
    threshold = Container(Column([low_threshold_row, high_threshold_row]), animate_size=animation.Animation(1000, AnimationCurve.EASE_IN), clip_behavior=ClipBehavior.HARD_EDGE)
    threshold.height = None if controlnet_prefs['control_task'] == "Canny Map Edge" else 0
    eta = Slider(min=0.0, max=1.0, divisions=20, label="{value}", value=float(controlnet_prefs['eta']), tooltip="The weight of noise for added noise in a diffusion step. Its value is between 0.0 and 1.0 - 0.0 is DDIM and 1.0 is DDPM scheduler respectively.", expand=True, on_change=change_eta)
    eta_value = Text(f" {controlnet_prefs['eta']}", weight=FontWeight.BOLD)
    eta_row = Row([Text("ETA:"), eta_value, Text("  DDIM"), eta])
    page.etas.append(eta_row)
    max_row = SliderRow(label="Max Resolution Size", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=controlnet_prefs, key='max_size')
    file_prefix = TextField(label="Filename Prefix",  value=controlnet_prefs['file_prefix'], width=150, height=60, on_change=lambda e:changed(e, 'file_prefix'))
    show_processed_image = Checkbox(label="Show Pre-Processed Image", value=controlnet_prefs['show_processed_image'], tooltip="Displays the Init-Image after being process by Canny, Depth, etc.", fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'show_processed_image'))
    batch_folder_name = TextField(label="Batch Folder Name", value=controlnet_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    apply_ESRGAN_upscale = Switcher(label="Apply ESRGAN Upscale", value=controlnet_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)
    enlarge_scale_slider = SliderRow(label="Enlarge Scale", min=1, max=4, divisions=6, round=1, suffix="x", pref=controlnet_prefs, key='enlarge_scale')
    display_upscaled_image = Checkbox(label="Display Upscaled Image", value=controlnet_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))
    ESRGAN_settings = Container(Column([enlarge_scale_slider, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    page.ESRGAN_block_controlnet = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    page.ESRGAN_block_controlnet.height = None if status['installed_ESRGAN'] else 0
    page.controlnet_output = Column([])
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.controlnet_output.controls) > 0
    run_prompt_list = ElevatedButton(content=Text(value="📜   Run from Prompts List", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_controlnet(page, from_list=True))
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🕸️  ControlNet Image+Text-to-Image+Video-to-Video", "Adding Input Conditions To Pretrained Text-to-Image Diffusion Models...", actions=[IconButton(icon=icons.HELP, tooltip="Help with ControlNet Settings", on_click=controlnet_help)]),
        Row([control_task, original_image, init_video, add_layer_btn]),
        conditioning_scale,
        Row([control_guidance_start, control_guidance_end]),
        multi_layers,
        vid_params,
        Divider(thickness=2, height=4),
        ResponsiveRow([prompt, negative_prompt]),
        threshold,
        num_inference_row,
        guidance,
        eta_row,
        max_row,
        show_processed_image,
        Row([NumberPicker(label="Batch Size: ", min=1, max=8, value=controlnet_prefs['batch_size'], on_change=lambda e: changed(e, 'batch_size')), seed, batch_folder_name, file_prefix]),
        page.ESRGAN_block_controlnet,
        Row([ElevatedButton(content=Text("🏸  Run ControlNet", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_controlnet(page)),
             run_prompt_list]),
        page.controlnet_output,
        clear_button,
      ]
    ))], scroll=ScrollMode.AUTO, auto_scroll=False)
    return c

controlnet_xl_prefs = {
    'original_image': '',
    'prompt': '',
    'negative_prompt': 'lowres, text, watermark, cropped, low quality',
    'control_task': 'Canny Map Edge',
    'conditioning_scale': 1.0,
    'control_guidance_start': 0.0,
    'control_guidance_end': 1.0,
    'multi_controlnets': [],
    'batch_size': 1,
    'max_size': 1024,
    'low_threshold': 100, #1-255
    'high_threshold': 200, #1-255
    'steps': 50, #100
    'guidance_scale': 7.5, #30
    'seed': 0,
    'eta': 0,
    'show_processed_image': False,
    'use_init_video': False,
    'init_video': '',
    'fps': 12,
    'start_time': 0,
    'end_time': 0,
    'file_prefix': 'controlnet-',
    'batch_folder_name': '',
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": 2.0,
    "display_upscaled_image": False,
}

def buildControlNetXL(page):
    global controlnet_xl_prefs, prefs
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          if ptype == "int":
            controlnet_xl_prefs[pref] = int(e.control.value)
          elif ptype == "float":
            controlnet_xl_prefs[pref] = float(e.control.value)
          else:
            controlnet_xl_prefs[pref] = e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def add_to_controlnet_xl_output(o):
      page.controlnet_xl_output.controls.append(o)
      page.controlnet_xl_output.update()
      if not clear_button.visible:
        clear_button.visible = True
        clear_button.update()
    def clear_output(e):
      if prefs['enable_sounds']: page.snd_delete.play()
      page.controlnet_xl_output.controls = []
      page.controlnet_xl_output.update()
      clear_button.visible = False
      clear_button.update()
    def controlnet_xl_help(e):
      def close_controlnet_xl_dlg(e):
        nonlocal controlnet_xl_help_dlg
        controlnet_xl_help_dlg.open = False
        page.update()
      controlnet_xl_help_dlg = AlertDialog(title=Text("💁   Help with ControlNet SDXL"), content=Column([
          Text('ControlNet-XL is a neural network structure to control diffusion models by adding extra conditions, using the latest StableDiffusion XL models. It copys the weights of neural network blocks into a "locked" copy and a "trainable" copy. The "trainable" one learns your condition. The "locked" one preserves your model. Thanks to this, training with small dataset of image pairs will not destroy the production-ready diffusion models. The "zero convolution" is 1×1 convolution with both weight and bias initialized as zeros. Before training, all zero convolutions output zeros, and ControlNetXL will not cause any distortion.  No layer is trained from scratch. You are still fine-tuning. Your original model is safe.  This allows training on small-scale or even personal devices. This is also friendly to merge/replacement/offsetting of models/weights/blocks/layers.'),
          Markdown("This is an interface for running the [official codebase](https://github.com/lllyasviel/ControlNet#readme) for models described in [Adding Conditional Control to Text-to-Image Diffusion Models](https://arxiv.org/abs/2302.05543).", on_tap_link=lambda e: e.page.launch_url(e.data)),
          #Text("Scribble - A hand-drawn monochrome image with white outlines on a black background."),
          Text("Canny Map Edge - A monochrome image with white edges on a black background."),
          Text("OpenPose - A OpenPose bone image."),
          Text("Depth - A grayscale image with black representing deep areas and white representing shallow areas."),
          Text("Softedge - A monochrome image with white soft edges on a black background."),
          #Text("M-LSD - A monochrome image composed only of white straight lines on a black background."),
          #Text("Normal Map - A normal mapped image."),
          Text("Segmented - An ADE20K's semantic segmentation protocol image."),
          Text("LineArt - An image with line art, usually black lines on a white background."),
          #Text("Shuffle - An image with shuffled patches or regions."),
          #Text("Brightness - An image based on brightness of init."),
          #Text("Instruct Pix2Pix - Trained with pixel to pixel instruction."),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("🍄  Too much control... ", on_click=close_controlnet_xl_dlg)], actions_alignment=MainAxisAlignment.END)
      page.dialog = controlnet_xl_help_dlg
      controlnet_xl_help_dlg.open = True
      page.update()
    def file_picker_result(e: FilePickerResultEvent):
        if e.files != None:
          upload_files(e)
    def on_upload_progress(e: FilePickerUploadEvent):
      if e.progress == 1:
        if not slash in e.file_name:
          fname = os.path.join(root_dir, e.file_name)
          controlnet_xl_prefs['file_name'] = e.file_name.rpartition('.')[0]
        else:
          fname = e.file_name
          controlnet_xl_prefs['file_name'] = e.file_name.rpartition(slash)[2].rpartition('.')[0]
        if pick_type == "image":
          original_image.value = fname
          original_image.update()
          controlnet_xl_prefs['original_image'] = fname
        elif pick_type == "video":
          init_video.value = fname
          init_video.update()
          controlnet_xl_prefs['init_video'] = fname
        page.update()
    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)
    def upload_files(e):
        uf = []
        if file_picker.result != None and file_picker.result.files != None:
            for f in file_picker.result.files:
              if page.web:
                uf.append(FilePickerUploadFile(f.name, upload_url=e.page.get_upload_url(f.name, 600)))
              else:
                on_upload_progress(FilePickerUploadEvent(f.path, 1, ""))
            file_picker.upload(uf)
    page.overlay.append(file_picker)
    pick_type = ""
    def pick_original(e):
        nonlocal pick_type
        pick_type = "image"
        file_picker.pick_files(allow_multiple=False, allowed_extensions=["png", "PNG", "jpg", "jpeg"], dialog_title="Pick Original Image File")
    def pick_video(e):
        nonlocal pick_type
        pick_type = "video"
        file_picker.pick_files(allow_multiple=False, allowed_extensions=["mp4", "avi"], dialog_title="Pick Initial Video File")
    def toggle_ESRGAN(e):
        ESRGAN_settings.height = None if e.control.value else 0
        controlnet_xl_prefs['apply_ESRGAN_upscale'] = e.control.value
        ESRGAN_settings.update()
    def change_task(e):
        task = e.control.value
        show = task.startswith("Video")# or task == "Video OpenPose"
        update = controlnet_xl_prefs['use_init_video'] != show
        changed(e,'control_task')
        threshold.height = None if "Canny" in controlnet_xl_prefs['control_task'] else 0
        threshold.update()
        if update:
            original_image.visible = not show
            original_image.update()
            init_video.visible = show
            init_video.update()
            vid_params.height = None if show else 0
            vid_params.update()
            conditioning_scale.visible = not show
            conditioning_scale.update()
            add_layer_btn.visible = not show
            add_layer_btn.update()
            multi_layers.visible = not show
            multi_layers.update()
            run_prompt_list.visible = not show
            run_prompt_list.update()
            controlnet_xl_prefs['use_init_video'] = show
    def change_eta(e):
        changed(e, 'eta', ptype="float")
        eta_value.value = f" {controlnet_xl_prefs['eta']}"
        eta_value.update()
        eta_row.update()
    def add_layer(e):
        layer = {'control_task': controlnet_xl_prefs['control_task'], 'original_image': controlnet_xl_prefs['original_image'], 'conditioning_scale': controlnet_xl_prefs['conditioning_scale'], 'control_guidance_start': controlnet_xl_prefs['control_guidance_start'], 'control_guidance_end': controlnet_xl_prefs['control_guidance_end'], 'use_init_video': False}
        if controlnet_xl_prefs['control_task'] == "Video Canny Edge" or controlnet_xl_prefs['control_task'] == "Video OpenPose":
          layer['use_init_video'] = True
          layer['init_video'] = controlnet_xl_prefs['init_video']
          layer['fps'] = controlnet_xl_prefs['fps']
          layer['start_time'] = controlnet_xl_prefs['start_time']
          layer['end_time'] = controlnet_xl_prefs['end_time']
          controlnet_xl_prefs['init_video'] = ""
          init_video.value = ""
          original_image.update()
        controlnet_xl_prefs['multi_controlnets'].append(layer)
        multi_layers.controls.append(ListTile(title=Row([Text(layer['control_task'] + " - ", weight=FontWeight.BOLD), Text(layer['init_video'] if layer['use_init_video'] else layer['original_image']), Text(f"- Conditioning Scale: {layer['conditioning_scale']} - Start: {layer['control_guidance_start']}, End: {layer['control_guidance_end']}")]), dense=True, trailing=PopupMenuButton(icon=icons.MORE_VERT,
          items=[
              PopupMenuItem(icon=icons.DELETE, text="Delete Control Layer", on_click=delete_layer, data=layer),
              PopupMenuItem(icon=icons.DELETE_SWEEP, text="Delete All Layers", on_click=delete_all_layers, data=layer),
          ]), data=layer))
        multi_layers.update()
        controlnet_xl_prefs['original_image'] = ""
        original_image.value = ""
        original_image.update()
    def delete_layer(e):
        controlnet_xl_prefs['multi_controlnets'].remove(e.control.data)
        for c in multi_layers.controls:
          if c.data['original_image'] == e.control.data['original_image']:
             multi_layers.controls.remove(c)
             break
        multi_layers.update()
    def delete_all_layers(e):
        controlnet_xl_prefs['multi_controlnets'].clear()
        multi_layers.controls.clear()
        multi_layers.update()
    original_image = TextField(label="Original Drawing", value=controlnet_xl_prefs['original_image'], expand=True, on_change=lambda e:changed(e,'original_image'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_original))
    prompt = TextField(label="Prompt Text", value=controlnet_xl_prefs['prompt'], col={'md': 8}, multiline=True, on_change=lambda e:changed(e,'prompt'))
    #a_prompt  = TextField(label="Added Prompt Text", value=controlnet_xl_prefs['a_prompt'], col={'md':3}, on_change=lambda e:changed(e,'a_prompt'))
    negative_prompt  = TextField(label="Negative Prompt Text", value=controlnet_xl_prefs['negative_prompt'], col={'md':4}, multiline=True, on_change=lambda e:changed(e,'negative_prompt'))
    control_task = Dropdown(label="ControlNet-SDXL Task", width=210, options=[dropdown.Option("Canny Map Edge"), dropdown.Option("Canny Map Edge mid"), dropdown.Option("Canny Map Edge small"), dropdown.Option("Depth"), dropdown.Option("Depth mid"), dropdown.Option("Depth small"), dropdown.Option("Segmentation"), dropdown.Option("LineArt"), dropdown.Option("Softedge"), dropdown.Option("OpenPose")], value=controlnet_xl_prefs['control_task'], on_change=change_task)
    #, dropdown.Option("Scribble"), dropdown.Option("HED"), dropdown.Option("M-LSD"), dropdown.Option("Normal Map"), dropdown.Option("Shuffle"), dropdown.Option("Instruct Pix2Pix"), dropdown.Option("Brightness"), dropdown.Option("Video Canny Edge"), dropdown.Option("Video OpenPose")
    conditioning_scale = SliderRow(label="Conditioning Scale", min=0, max=2, divisions=20, round=1, pref=controlnet_xl_prefs, key='conditioning_scale', tooltip="The outputs of the controlnet are multiplied by `controlnet_xl_conditioning_scale` before they are added to the residual in the original unet.")
    control_guidance_start = SliderRow(label="Control Guidance Start", min=0.0, max=1.0, divisions=10, round=1, expand=True, pref=controlnet_xl_prefs, key='control_guidance_start', tooltip="The percentage of total steps at which the controlnet starts applying.")
    control_guidance_end = SliderRow(label="Control Guidance End", min=0.0, max=1.0, divisions=10, round=1, expand=True, pref=controlnet_xl_prefs, key='control_guidance_end', tooltip="The percentage of total steps at which the controlnet stops applying.")
    #add_layer_btn = IconButton(icons.ADD, tooltip="Add Multi-ControlNetXL Layer", on_click=add_layer)
    add_layer_btn = ft.FilledButton("➕ Add Layer", width=140, on_click=add_layer)
    multi_layers = Column([], spacing=0)
    seed = TextField(label="Seed", width=90, value=str(controlnet_xl_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    #use_init_video = Tooltip(message="Input a short mp4 file to animate with.", content=Switcher(label="Use Init Video", value=controlnet_xl_prefs['use_init_video'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_init_video))
    init_video = TextField(label="Init Video Clip", value=controlnet_xl_prefs['init_video'], expand=True, visible=controlnet_xl_prefs['use_init_video'], on_change=lambda e:changed(e,'init_video'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_video))
    fps = SliderRow(label="Frames per Second", min=1, max=30, divisions=29, suffix='fps', pref=controlnet_xl_prefs, key='fps', tooltip="The FPS to extract from the init video clip.")
    start_time = TextField(label="Start Time (s)", value=controlnet_xl_prefs['start_time'], width=145, keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'start_time', ptype="float"))
    end_time = TextField(label="End Time (0 for all)", value=controlnet_xl_prefs['end_time'], width=145, keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'end_time', ptype="float"))
    vid_params = Container(content=Column([fps, Row([start_time, end_time])]), animate_size=animation.Animation(800, AnimationCurve.EASE_OUT), clip_behavior=ClipBehavior.HARD_EDGE, height=None if controlnet_xl_prefs['use_init_video'] else 0)

    num_inference_row = SliderRow(label="Number of Steps", min=1, max=100, divisions=99, pref=controlnet_xl_prefs, key='steps', tooltip="The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.")
    guidance = SliderRow(label="Guidance Scale", min=0, max=30, divisions=60, round=1, pref=controlnet_xl_prefs, key='guidance_scale')
    low_threshold_row = SliderRow(label="Canny Low Threshold", min=1, max=255, divisions=254, pref=controlnet_xl_prefs, key='low_threshold')
    high_threshold_row = SliderRow(label="Canny High Threshold", min=1, max=255, divisions=254, pref=controlnet_xl_prefs, key='high_threshold')
    threshold = Container(Column([low_threshold_row, high_threshold_row]), animate_size=animation.Animation(1000, AnimationCurve.EASE_IN), clip_behavior=ClipBehavior.HARD_EDGE)
    threshold.height = None if controlnet_xl_prefs['control_task'] == "Canny Map Edge" else 0
    eta = Slider(min=0.0, max=1.0, divisions=20, label="{value}", value=float(controlnet_xl_prefs['eta']), tooltip="The weight of noise for added noise in a diffusion step. Its value is between 0.0 and 1.0 - 0.0 is DDIM and 1.0 is DDPM scheduler respectively.", expand=True, on_change=change_eta)
    eta_value = Text(f" {controlnet_xl_prefs['eta']}", weight=FontWeight.BOLD)
    eta_row = Row([Text("ETA:"), eta_value, Text("  DDIM"), eta])
    page.etas.append(eta_row)
    max_row = SliderRow(label="Max Resolution Size", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=controlnet_xl_prefs, key='max_size')
    file_prefix = TextField(label="Filename Prefix",  value=controlnet_xl_prefs['file_prefix'], width=150, height=60, on_change=lambda e:changed(e, 'file_prefix'))
    show_processed_image = Checkbox(label="Show Pre-Processed Image", value=controlnet_xl_prefs['show_processed_image'], tooltip="Displays the Init-Image after being process by Canny, Depth, etc.", fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'show_processed_image'))
    batch_folder_name = TextField(label="Batch Folder Name", value=controlnet_xl_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    apply_ESRGAN_upscale = Switcher(label="Apply ESRGAN Upscale", value=controlnet_xl_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)
    enlarge_scale_slider = SliderRow(label="Enlarge Scale", min=1, max=4, divisions=6, round=1, suffix="x", pref=controlnet_xl_prefs, key='enlarge_scale')
    display_upscaled_image = Checkbox(label="Display Upscaled Image", value=controlnet_xl_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))
    ESRGAN_settings = Container(Column([enlarge_scale_slider, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    page.ESRGAN_block_controlnet = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    page.ESRGAN_block_controlnet.height = None if status['installed_ESRGAN'] else 0
    page.controlnet_xl_output = Column([])
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.controlnet_xl_output.controls) > 0
    run_prompt_list = ElevatedButton(content=Text(value="📜   Run from Prompts List", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_controlnet_xl(page, from_list=True))
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🕷  ControlNet SDXL Image+Text-to-Image", "Adding Input Conditions To Pretrained Text-to-Image Diffusion Models...", actions=[IconButton(icon=icons.HELP, tooltip="Help with ControlNetXL Settings", on_click=controlnet_xl_help)]),
        Row([control_task, original_image, init_video, add_layer_btn]),
        conditioning_scale,
        Row([control_guidance_start, control_guidance_end]),
        multi_layers,
        vid_params,
        Divider(thickness=2, height=4),
        ResponsiveRow([prompt, negative_prompt]),
        threshold,
        num_inference_row,
        guidance,
        eta_row,
        max_row,
        show_processed_image,
        Row([NumberPicker(label="Batch Size: ", min=1, max=8, value=controlnet_xl_prefs['batch_size'], on_change=lambda e: changed(e, 'batch_size')), seed, batch_folder_name, file_prefix]),
        page.ESRGAN_block_controlnet,
        Row([ElevatedButton(content=Text("🛃  Run ControlNet-XL", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_controlnet_xl(page)),
             run_prompt_list]),
        page.controlnet_xl_output,
        clear_button,
      ]
    ))], scroll=ScrollMode.AUTO, auto_scroll=False)
    return c


controlnet_video2video_prefs = {
    'init_video': '',
    'prompt': '',
    'negative_prompt': '',
    'control_task': 'Canny21',
    'controlnet_strength': 1.0,
    'init_image_strength': 0.5,
    'feedthrough_strength': 0.0,
    'motion_alpha': 0.1,
    'motion_sigma': 0.3,
    'color_fix': 'Lab', #'none', 'rgb', 'hsv', 'lab'
    'color_amount': 0.0,
    'max_size': 768,
    'low_threshold': 100, #1-255 canny
    'high_threshold': 200, #1-255
    'mlsd_score_thr': 0.1, #mlsd line detector v threshold
    'mlsd_dist_thr': 0.1, #mlsd line detector d threshold
    'steps': 25,
    'prompt_strength': 7.5,
    'start_time': 0.0,
    'end_time': 0.0,
    'duration': 0.0,
    'max_dimension': 832,
    'min_dimension': 512,
    'round_dims_to': 64,
    'no_audio': False,
    'skip_dumped_frames': False,
    'save_frames': False,
    'show_console': True,
    'fix_orientation': True,
    'file_prefix': 'controlnet-',
    'output_name': '',
    'batch_folder_name': '',
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": 2.0,
    "display_upscaled_image": False,
}

def buildControlNet_Video2Video(page):
    global controlnet_video2video_prefs, prefs
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          if ptype == "int":
            controlnet_video2video_prefs[pref] = int(e.control.value)
          elif ptype == "float":
            controlnet_video2video_prefs[pref] = float(e.control.value)
          else:
            controlnet_video2video_prefs[pref] = e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def clear_output(e):
      if prefs['enable_sounds']: page.snd_delete.play()
      page.controlnet_video2video_output.controls = []
      page.controlnet_video2video_output.update()
      clear_button.visible = False
      clear_button.update()
    def controlnet_video2video_help(e):
      def close_controlnet_video2video_dlg(e):
        nonlocal controlnet_video2video_help_dlg
        controlnet_video2video_help_dlg.open = False
        page.update()
      controlnet_video2video_help_dlg = AlertDialog(title=Text("💁   Help with ControlNet Video2Video"), content=Column([
          Text("Can apply Stable Diffusion to a video, while maintaining frame-to-frame consistency. It is based on the Stable Diffusion img2img model, but adds a motion estimator and motion compensator to maintain consistency between frames. It will process each input frame with some preprocessing (motion transfer/compensation of the output feedback), followed by a detector and diffusion models in a pipeline configured by the ControlNet Type option."),
          Text("Feedback strength, set with init-image-strength controls frame-to-frame consistency, by changing how much the motion-compensated previous output frame is fed into the next frame's diffusion pipeline in place of initial latent noise, a la img2img latent diffusion (citation needed). Values around 0.3 to 0.5 and sometimes much higher (closer to 1.0, the maximum which means no noise is added and no denoising steps will be run)."),
          Markdown("This is an interface for running the [ControlNet Video codebase](https://github.com/un1tz3r0/controlnetvideo) by [Victor Condino](un1tz3r0@gmail.com).", on_tap_link=lambda e: e.page.launch_url(e.data)),
          Text('ControlNet is a neural network structure to control diffusion models by adding extra conditions. It copys the weights of neural network blocks into a "locked" copy and a "trainable" copy. The "trainable" one learns your condition. The "locked" one preserves your model. Thanks to this, training with small dataset of image pairs will not destroy the production-ready diffusion models. The "zero convolution" is 1×1 convolution with both weight and bias initialized as zeros. Before training, all zero convolutions output zeros, and ControlNet will not cause any distortion.  No layer is trained from scratch. You are still fine-tuning. Your original model is safe.  This allows training on small-scale or even personal devices. This is also friendly to merge/replacement/offsetting of models/weights/blocks/layers.'),
          Text("Aesthetic - Uses image features extracted using a Canny edge detector trained on a large aesthetic dataset."),
          Text("Canny Map Edge - A monochrome image with white edges on a black background."),
          Text("OpenPose - A OpenPose bone image."),
          Text("Depth - A grayscale image with black representing deep areas and white representing shallow areas."),
          Text("HED - A monochrome image with white soft edges on a black background."),
          Text("M-LSD - A monochrome image composed only of white straight lines on a black background."),
          Text("Normal Map - A normal mapped image."),
          Text("LineArt - An image with line art, usually black lines on a white background."),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("😸  Could get crazy... ", on_click=close_controlnet_video2video_dlg)], actions_alignment=MainAxisAlignment.END)
      page.dialog = controlnet_video2video_help_dlg
      controlnet_video2video_help_dlg.open = True
      page.update()
    def file_picker_result(e: FilePickerResultEvent):
        if e.files != None:
          upload_files(e)
    def on_upload_progress(e: FilePickerUploadEvent):
      if e.progress == 1:
        if not slash in e.file_name:
          fname = os.path.join(root_dir, e.file_name)
          controlnet_video2video_prefs['file_name'] = e.file_name.rpartition('.')[0]
        else:
          fname = e.file_name
          controlnet_video2video_prefs['file_name'] = e.file_name.rpartition(slash)[2].rpartition('.')[0]
        if pick_type == "video":
          init_video.value = fname
          init_video.update()
          controlnet_video2video_prefs['init_video'] = fname
        page.update()
    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)
    def upload_files(e):
        uf = []
        if file_picker.result != None and file_picker.result.files != None:
            for f in file_picker.result.files:
              if page.web:
                uf.append(FilePickerUploadFile(f.name, upload_url=e.page.get_upload_url(f.name, 600)))
              else:
                on_upload_progress(FilePickerUploadEvent(f.path, 1, ""))
            file_picker.upload(uf)
    page.overlay.append(file_picker)
    pick_type = ""
    def pick_video(e):
        nonlocal pick_type
        pick_type = "video"
        file_picker.pick_files(allow_multiple=False, allowed_extensions=["mp4", "avi"], dialog_title="Pick Initial Video File")
    def toggle_ESRGAN(e):
        ESRGAN_settings.height = None if e.control.value else 0
        controlnet_video2video_prefs['apply_ESRGAN_upscale'] = e.control.value
        ESRGAN_settings.update()
    def change_task(e):
        changed(e,'control_task')
        canny_threshold.height = None if controlnet_video2video_prefs['control_task'] == "Canny" or controlnet_video2video_prefs['control_task'] == "Canny21" else 0
        canny_threshold.update()
        mlsd_threshold.height = None if controlnet_video2video_prefs['control_task'] == "MLSD" else 0
        mlsd_threshold.update()
    prompt = TextField(label="Prompt Text", value=controlnet_video2video_prefs['prompt'], col={'md': 8}, multiline=True, on_change=lambda e:changed(e,'prompt'))
    negative_prompt  = TextField(label="Negative Prompt Text", value=controlnet_video2video_prefs['negative_prompt'], col={'md':4}, multiline=True, on_change=lambda e:changed(e,'negative_prompt'))
    #'aesthetic', 'lineart21', 'hed', 'hed21', 'canny', 'canny21', 'openpose', 'openpose21', 'depth', 'depth21', 'normal', 'mlsd'
    control_task = Dropdown(label="ControlNet Task", width=150, options=[dropdown.Option("Aesthetic"), dropdown.Option("Lineart21"), dropdown.Option("HED"), dropdown.Option("HED21"), dropdown.Option("HED"), dropdown.Option("Canny"), dropdown.Option("Canny21"), dropdown.Option("OpenPose"), dropdown.Option("OpenPose21"), dropdown.Option("Depth"), dropdown.Option("Depth21"), dropdown.Option("Normal"), dropdown.Option("MLSD")], value=controlnet_video2video_prefs['control_task'], on_change=change_task)
    #conditioning_scale = SliderRow(label="Conditioning Scale", min=0, max=2, divisions=20, round=1, pref=controlnet_video2video_prefs, key='conditioning_scale', tooltip="The outputs of the controlnet are multiplied by `controlnet_video2video_conditioning_scale` before they are added to the residual in the original unet.")
    controlnet_strength = SliderRow(label="ControlNet Strength", min=0.0, max=1.0, divisions=20, round=2, pref=controlnet_video2video_prefs, key='controlnet_strength', tooltip="How much influence the controlnet annotator's output is used to guide the denoising process.")
    init_image_strength = SliderRow(label="Init-Image Strength", min=0.0, max=1.0, divisions=20, round=2, pref=controlnet_video2video_prefs, key='init_image_strength', tooltip="The init-image strength, or how much of the prompt-guided denoising process to skip in favor of starting with an existing image.")
    feedthrough_strength = SliderRow(label="Feedthrough Strength", min=0.0, max=1.0, divisions=20, round=2, pref=controlnet_video2video_prefs, key='feedthrough_strength', tooltip="The ratio of input to motion compensated prior output to feed through to the next frame.")
    init_video = TextField(label="Init Video Clip", value=controlnet_video2video_prefs['init_video'], expand=True, on_change=lambda e:changed(e,'init_video'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_video))
    #fps = SliderRow(label="Frames per Second", min=1, max=30, divisions=29, suffix='fps', pref=controlnet_video2video_prefs, key='fps', tooltip="The FPS to extract from the init video clip.")
    start_time = TextField(label="Start Time (s)", value=controlnet_video2video_prefs['start_time'], width=145, keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'start_time', ptype="float"))
    end_time = TextField(label="End Time (0 for all)", value=controlnet_video2video_prefs['end_time'], width=145, keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'end_time', ptype="float"))
    duration = TextField(label="Duration (0 for all)", value=controlnet_video2video_prefs['duration'], width=145, keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'duration', ptype="float"))
    vid_params = Container(content=Column([Row([start_time, end_time, duration])]), animate_size=animation.Animation(800, AnimationCurve.EASE_OUT), clip_behavior=ClipBehavior.HARD_EDGE, padding=padding.only(top=5))
    num_inference_row = SliderRow(label="Number of Steps", min=1, max=100, divisions=99, pref=controlnet_video2video_prefs, key='steps', tooltip="Number of inference steps, depends on the scheduler, trades off speed for quality.")
    prompt_strength = SliderRow(label="Prompt Strength", min=0, max=30, divisions=60, round=1, pref=controlnet_video2video_prefs, key='prompt_strength', tooltip="How much influence the prompt has on the output. Guidance Scale.")
    low_threshold_row = SliderRow(label="Canny Low Threshold", min=1, max=255, divisions=254, pref=controlnet_video2video_prefs, key='low_threshold', expand=True, col={'lg':6})
    high_threshold_row = SliderRow(label="Canny High Threshold", min=1, max=255, divisions=254, pref=controlnet_video2video_prefs, key='high_threshold', expand=True, col={'lg':6})
    canny_threshold = Container(ResponsiveRow([low_threshold_row, high_threshold_row]), animate_size=animation.Animation(1000, AnimationCurve.EASE_IN), clip_behavior=ClipBehavior.HARD_EDGE)
    canny_threshold.height = None if controlnet_video2video_prefs['control_task'] == "Canny Map Edge" else 0
    mlsd_score_thr = SliderRow(label="MLSD Score Threshold", min=0.0, max=1.0, divisions=10, round=1, pref=controlnet_video2video_prefs, key='mlsd_score_thr', expand=True, col={'lg':6})
    mlsd_dist_thr = SliderRow(label="MLSD Dist Threshold", min=0.0, max=1.0, divisions=10, round=1, pref=controlnet_video2video_prefs, key='mlsd_dist_thr', expand=True, col={'lg':6})
    mlsd_threshold = Container(ResponsiveRow([mlsd_score_thr, mlsd_dist_thr]), animate_size=animation.Animation(1000, AnimationCurve.EASE_IN), clip_behavior=ClipBehavior.HARD_EDGE)
    mlsd_threshold.height = None if controlnet_video2video_prefs['control_task'] == "MLSD" else 0
    motion_alpha = SliderRow(label="Motion Alpha", min=0.0, max=1.0, divisions=10, round=1, pref=controlnet_video2video_prefs, key='motion_alpha', expand=True, col={'lg':6}, tooltip="Smooth the motion vectors over time, 0.0 is no smoothing, 1.0 is maximum smoothing.")
    motion_sigma = SliderRow(label="Motion Sigma", min=0.0, max=1.0, divisions=10, round=1, pref=controlnet_video2video_prefs, key='motion_sigma', expand=True, col={'lg':6}, tooltip="Smooth the motion estimate spatially, 0.0 is no smoothing, used as sigma for gaussian blur.")
    max_dimension = SliderRow(label="Max Dimension", min=256, max=1280, divisions=32, multiple=32, suffix="px", pref=controlnet_video2video_prefs, key='max_dimension', expand=True, col={'lg':6})
    min_dimension = SliderRow(label="Mix Dimension", min=256, max=1280, divisions=32, multiple=32, suffix="px", pref=controlnet_video2video_prefs, key='min_dimension', expand=True, col={'lg':6})
    color_fix = Dropdown(label="Color Fix", width=150, options=[dropdown.Option("None"), dropdown.Option("RGB"), dropdown.Option("HSV"), dropdown.Option("Lab")], value=controlnet_video2video_prefs['color_fix'], on_change=lambda e:changed(e,'color_fix'), tooltip="Prevent color from drifting due to feedback and model bias by fixing the histogram to the first frame. Specify colorspace for histogram matching")
    color_amount = SliderRow(label="Color Amount", min=0.0, max=1.0, divisions=10, round=1, pref=controlnet_video2video_prefs, key='color_amount', expand=True, tooltip="Blend between the original color and the color matched version.")
    #max_row = SliderRow(label="Max Resolution Size", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=controlnet_video2video_prefs, key='max_size')
    no_audio = Switcher(label="No Audio", value=controlnet_video2video_prefs['no_audio'], tooltip="Don't include audio in the output video, even if the input video has audio", active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'no_audio'))
    skip_dumped_frames = Switcher(label="Skip Dumped Frames", value=controlnet_video2video_prefs['skip_dumped_frames'], tooltip="Read dumped frames from a previous run instead of processing the input video.", active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'skip_dumped_frames'))
    save_frames = Switcher(label="Save all Frames", value=controlnet_video2video_prefs['save_frames'], tooltip="Save the dumped frames to images_out batch folder. Otherwise only saves final video, keeping pngs in temp folder.", active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'save_frames'))
    fix_orientation = Switcher(label="Fix Orientation", value=controlnet_video2video_prefs['fix_orientation'], tooltip="Resize videos shot in portrait mode on some devices to fix incorrect aspect ratio bug.", active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'fix_orientation'))
    show_console = Switcher(label="Show Console Output", value=controlnet_video2video_prefs['show_console'], tooltip="Outputs the progress run log in the console window. Gets messy, but useful.", active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'show_console'))
    file_prefix = TextField(label="Filename Prefix",  value=controlnet_video2video_prefs['file_prefix'], width=150, height=60, on_change=lambda e:changed(e, 'file_prefix'))
    output_name = TextField(label="Output Name", value=controlnet_video2video_prefs['output_name'], on_change=lambda e:changed(e,'output_name'))
    batch_folder_name = TextField(label="Batch Folder Name", value=controlnet_video2video_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    apply_ESRGAN_upscale = Switcher(label="Apply ESRGAN Upscale", value=controlnet_video2video_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)
    enlarge_scale_slider = SliderRow(label="Enlarge Scale", min=1, max=4, divisions=6, round=1, suffix="x", pref=controlnet_video2video_prefs, key='enlarge_scale')
    display_upscaled_image = Checkbox(label="Display Upscaled Image", value=controlnet_video2video_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))
    ESRGAN_settings = Container(Column([enlarge_scale_slider, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    page.ESRGAN_block_controlnet = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    page.ESRGAN_block_controlnet.height = None if status['installed_ESRGAN'] else 0
    page.controlnet_video2video_output = Column([])
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.controlnet_video2video_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🤪  ControlNet Video2Video", "Apply Stable Diffusion to a video, while maintaining frame-to-frame consistency with motion estimator & compensator...", actions=[IconButton(icon=icons.HELP, tooltip="Help with ControlNet Vid2Vid Settings", on_click=controlnet_video2video_help)]),
        ResponsiveRow([prompt, negative_prompt]),
        Row([control_task, init_video]),
        canny_threshold,
        mlsd_threshold,
        vid_params,
        controlnet_strength,
        init_image_strength,
        feedthrough_strength,
        prompt_strength,
        num_inference_row,
        Row([color_fix, color_amount]),
        ResponsiveRow([motion_alpha, motion_sigma]),
        ResponsiveRow([max_dimension, min_dimension]),
        Row([no_audio, skip_dumped_frames, save_frames, fix_orientation, show_console]),
        Row([output_name, batch_folder_name, file_prefix]),
        page.ESRGAN_block_controlnet,
        Row([ElevatedButton(content=Text("🍃  Run ControlNet Vid2Vid", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_controlnet_video2video(page))]),
        page.controlnet_video2video_output,
        clear_button,
      ]
    ))], scroll=ScrollMode.AUTO, auto_scroll=False)
    return c

deepfloyd_prefs = {
    'prompt': '',
    'negative_prompt': '',
    'init_image': '',
    'mask_image': '',
    'alpha_mask': False,
    'invert_mask': False,
    'num_inference_steps': 100,
    'guidance_scale': 10,
    'image_strength': 0.7,
    'superres_num_inference_steps': 50,
    'superres_guidance_scale': 4,
    'upscale_num_inference_steps': 75,
    'upscale_guidance_scale': 9,
    'eta': 0.0,
    'seed': 0,
    'max_size': 768,
    'model_size': 'L (900M)',
    'apply_watermark': True,
    'low_memory': True,
    'keep_pipelines': False,
    'num_images': 1,
    'batch_folder_name': '',
    'file_prefix': 'IF-',
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": 2.0,
    "display_upscaled_image": False,
}

def buildDeepFloyd(page):
    global deepfloyd_prefs, prefs, pipe_deepfloyd
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          if ptype == "int":
            deepfloyd_prefs[pref] = int(e.control.value)
          elif ptype == "float":
            deepfloyd_prefs[pref] = float(e.control.value)
          else:
            deepfloyd_prefs[pref] = e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def add_to_deepfloyd_output(o):
      page.deepfloyd_output.controls.append(o)
      page.deepfloyd_output.update()
      if not clear_button.visible:
        clear_button.visible = True
        clear_button.update()
    def clear_output(e):
      if prefs['enable_sounds']: page.snd_delete.play()
      page.deepfloyd_output.controls = []
      page.deepfloyd_output.update()
      clear_button.visible = False
      clear_button.update()
    def deepfloyd_help(e):
      def close_deepfloyd_dlg(e):
        nonlocal deepfloyd_help_dlg
        deepfloyd_help_dlg.open = False
        page.update()
      deepfloyd_help_dlg = AlertDialog(title=Text("💁   Help with DeepFloyd-IF"), content=Column([
          Markdown("**You must accept the license on the model card of [DeepFloyd/IF-I-XL-v1.0](https://huggingface.co/DeepFloyd/IF-I-XL-v1.0) before using.**", on_tap_link=lambda e: e.page.launch_url(e.data)),
          Text('DeepFloyd IF is a novel state-of-the-art open-source text-to-image model with a high degree of photorealism and language understanding. Built by the team behind ruDALL-E (the Russian-language version of OpenAI\'s DALL-E algorithm), inspired by Google\'s "Imagen", and backed by the company behind Stable Diffusion, DeepFloyd\'s IF outperforms all of those algorithms. DeepFloyd IF is particularly good at understanding complex prompts and relationships between objects. It is also very good at inserting legible text into images - even more so than Stable Diffusion XL. It can even understand prompts in multiple languages. IF, or "Intelligent Fiction", is a text2image generator that is designed to create text and captions in the images in response to a prompt. The model is a modular composed of a frozen text encoder and three cascaded pixel diffusion modules:'),
          Markdown("""* Stage 1: a base model that generates 64x64 px image based on text prompt,
* Stage 2: a 64x64 px => 256x256 px super-resolution model, and a
* Stage 3: a 256x256 px => 1024x1024 px super-resolution model Stage 1 and Stage 2 utilize a frozen text encoder based on the T5 transformer to extract text embeddings, which are then fed into a UNet architecture enhanced with cross-attention and attention pooling. Stage 3 is Stability's x4 Upscaling model. The result is a highly efficient model that outperforms current state-of-the-art models, achieving a zero-shot FID score of 6.66 on the COCO dataset. Our work underscores the potential of larger UNet architectures in the first stage of cascaded diffusion models and depicts a promising future for text-to-image synthesis.""", on_tap_link=lambda e: e.page.launch_url(e.data)),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("😌  Let's go Deep... ", on_click=close_deepfloyd_dlg)], actions_alignment=MainAxisAlignment.END)
      page.dialog = deepfloyd_help_dlg
      deepfloyd_help_dlg.open = True
      page.update()
    def file_picker_result(e: FilePickerResultEvent):
        if e.files != None:
          upload_files(e)
    def on_upload_progress(e: FilePickerUploadEvent):
      nonlocal pick_type
      if e.progress == 1:
        if not slash in e.file_name:
          fname = os.path.join(root_dir, e.file_name)
          deepfloyd_prefs['file_name'] = e.file_name.rpartition('.')[0]
        else:
          fname = e.file_name
          deepfloyd_prefs['file_name'] = e.file_name.rpartition(slash)[2].rpartition('.')[0]
        if pick_type == "image":
          init_image.value = fname
          init_image.update()
          deepfloyd_prefs['init_image'] = fname
        elif pick_type == "mask":
          mask_image.value = fname
          mask_image.update()
          deepfloyd_prefs['mask_image'] = fname
        page.update()
    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)
    def upload_files(e):
        uf = []
        if file_picker.result != None and file_picker.result.files != None:
            for f in file_picker.result.files:
              if page.web:
                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))
              else:
                on_upload_progress(FilePickerUploadEvent(f.path, 1, ""))
            file_picker.upload(uf)
    page.overlay.append(file_picker)
    pick_type = ""
    def pick_init(e):
        nonlocal pick_type
        pick_type = "image"
        file_picker.pick_files(allow_multiple=False, allowed_extensions=["png", "PNG", "jpg", "jpeg"], dialog_title="Pick Original Image File")
    def pick_mask(e):
        nonlocal pick_type
        pick_type = "mask"
        file_picker.pick_files(allow_multiple=False, allowed_extensions=["png", "PNG", "jpg", "jpeg"], dialog_title="Pick Mask Image File")
    def toggle_ESRGAN(e):
        ESRGAN_settings.height = None if e.control.value else 0
        deepfloyd_prefs['apply_ESRGAN_upscale'] = e.control.value
        ESRGAN_settings.update()
    def change_eta(e):
        changed(e, 'eta', ptype="float")
        eta_value.value = f" {deepfloyd_prefs['eta']}"
        eta_value.update()
        eta_row.update()
    init_image = FileInput(label="Original Image (optional)", pref=deepfloyd_prefs, key='init_image', expand=True, page=page)
    #init_image = TextField(label="Original Image (optional)", value=deepfloyd_prefs['init_image'], expand=True, on_change=lambda e:changed(e,'init_image'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_init))
    mask_image = FileInput(label="Mask Image (optional)", pref=deepfloyd_prefs, key='mask_image', expand=1, page=page)
    #mask_image = TextField(label="Mask Image (optional)", value=deepfloyd_prefs['mask_image'], expand=1, on_change=lambda e:changed(e,'mask_image'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD_OUTLINED, on_click=pick_mask))
    invert_mask = Checkbox(label="Invert", tooltip="Swaps the Black & White of your Mask Image", value=deepfloyd_prefs['invert_mask'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'invert_mask'))
    alpha_mask = Checkbox(label="Alpha Mask", value=deepfloyd_prefs['alpha_mask'], tooltip="Use Transparent Alpha Channel of Init as Mask", fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'alpha_mask'))

    prompt = TextField(label="Prompt Text", value=deepfloyd_prefs['prompt'], col={'md': 9}, multiline=True, on_change=lambda e:changed(e,'prompt'))
    negative_prompt  = TextField(label="Negative Prompt Text", value=deepfloyd_prefs['negative_prompt'], col={'md':3}, multiline=True, on_change=lambda e:changed(e,'negative_prompt'))
    seed = TextField(label="Seed", width=90, value=str(deepfloyd_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    num_inference_row = SliderRow(label="Number of Inference Steps", min=1, max=200, divisions=199, pref=deepfloyd_prefs, key='num_inference_steps', tooltip="The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.")
    guidance = SliderRow(label="Guidance Scale", min=0, max=50, divisions=100, round=1, pref=deepfloyd_prefs, key='guidance_scale')

    superres_num_inference_row = SliderRow(label="Super Res Inference Steps", min=1, max=200, divisions=199, pref=deepfloyd_prefs, key='superres_num_inference_steps', tooltip="The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.")
    superres_guidance = SliderRow(label="Super Res Guidance Scale", min=0, max=50, divisions=100, round=1, pref=deepfloyd_prefs, key='superres_guidance_scale')
    upscale_num_inference_row = SliderRow(label="Upscale Inference Steps", min=1, max=200, divisions=199, pref=deepfloyd_prefs, key='upscale_num_inference_steps', tooltip="The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.")
    upscale_guidance = SliderRow(label="Upscale Guidance Scale", min=0, max=50, divisions=100, round=1, pref=deepfloyd_prefs, key='upscale_guidance_scale')
    image_strength = SliderRow(label="Image Strength", min=0, max=1, divisions=20, round=2, pref=deepfloyd_prefs, key='image_strength', tooltip="Conceptually, indicates how much to transform the reference `image`. Denoising steps depends on the amount of noise initially added.")
    #eta = TextField(label="ETA", value=str(deepfloyd_prefs['eta']), keyboard_type=KeyboardType.NUMBER, hint_text="Amount of Noise", on_change=lambda e:changed(e,'eta', ptype='float'))
    eta = Slider(min=0.0, max=1.0, divisions=20, label="{value}", value=float(deepfloyd_prefs['eta']), tooltip="The weight of noise for added noise in a diffusion step. Its value is between 0.0 and 1.0 - 0.0 is DDIM and 1.0 is DDPM scheduler respectively.", expand=True, on_change=change_eta)
    eta_value = Text(f" {deepfloyd_prefs['eta']}", weight=FontWeight.BOLD)
    eta_row = Row([Text("ETA:"), eta_value, Text("  DDIM"), eta, Text("DDPM")])
    #page.etas.append(eta_row)
    max_row = SliderRow(label="Max Resolution Size", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=deepfloyd_prefs, key='max_size', tooltip="Resizes your Init and Mask Image to save memory.")
    apply_watermark = Tooltip(message="Under the license, you are legally required to include the watermark on bottom right corner.", content=Switcher(label="Apply IF Watermark", value=deepfloyd_prefs['apply_watermark'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e, 'apply_watermark')))
    low_memory = Tooltip(message="Needed for < 16GB VRAM to run. If you have more power, disable for faster runs.", content=Switcher(label="Lower Memory", value=deepfloyd_prefs['low_memory'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e, 'low_memory')))
    keep_pipelines = Tooltip(message="Keeps all 3 Pipelines loaded persistantly. Faster reuse, but needs a lot of VRAM.", content=Switcher(label="Keep Pipelines Loaded", value=deepfloyd_prefs['keep_pipelines'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e, 'keep_pipelines')))
    model_size = Dropdown(label="Model Size", hint_text="Amount of Parameters trained on. Depends on your available memory.", width=240, options=[dropdown.Option("XL (4.3B)"), dropdown.Option("L (900M)"), dropdown.Option("M (400M)")], value=deepfloyd_prefs['model_size'], autofocus=False, on_change=lambda e:changed(e, 'model_size'))
    file_prefix = TextField(label="Filename Prefix",  value=deepfloyd_prefs['file_prefix'], width=150, height=60, on_change=lambda e:changed(e, 'file_prefix'))
    batch_folder_name = TextField(label="Batch Folder Name", value=deepfloyd_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    apply_ESRGAN_upscale = Switcher(label="Apply ESRGAN Upscale", value=deepfloyd_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)
    enlarge_scale_slider = SliderRow(label="Enlarge Scale", min=1, max=4, divisions=6, round=1, suffix="x", pref=deepfloyd_prefs, key='enlarge_scale')
    display_upscaled_image = Checkbox(label="Display Upscaled Image", value=deepfloyd_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))
    ESRGAN_settings = Container(Column([enlarge_scale_slider, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    page.ESRGAN_block_deepfloyd = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    page.ESRGAN_block_deepfloyd.height = None if status['installed_ESRGAN'] else 0
    page.deepfloyd_output = Column([])
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.deepfloyd_output.controls) > 0
    run_prompt_list = ElevatedButton(content=Text(value="📜   Run from Prompts List", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_deepfloyd(page, from_list=True))
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🌈  DeepFloyd IF (under construction, may not work)", "A new AI image generator that achieves state-of-the-art results on numerous image-generation tasks...", actions=[IconButton(icon=icons.HELP, tooltip="Help with IF-DeepFloyd Settings", on_click=deepfloyd_help)]),
        ResponsiveRow([prompt, negative_prompt]),
        ResponsiveRow([Row([init_image, alpha_mask], col={'lg':6}), Row([mask_image, invert_mask], col={'lg':6})]),
        #Row([init_image, mask_image, invert_mask]),
        image_strength,
        num_inference_row,
        guidance,
        superres_num_inference_row,
        superres_guidance,
        upscale_num_inference_row,
        upscale_guidance,
        eta_row,
        max_row,
        Row([apply_watermark, low_memory, keep_pipelines, model_size]),
        Row([NumberPicker(label="Number of Images: ", min=1, max=8, value=deepfloyd_prefs['num_images'], on_change=lambda e: changed(e, 'num_images')), seed, batch_folder_name, file_prefix]),
        page.ESRGAN_block_deepfloyd,
        Row([ElevatedButton(content=Text("🎈  Run DeepFloyd", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_deepfloyd(page)),
             run_prompt_list]),
        page.deepfloyd_output,
        clear_button,
      ]))], scroll=ScrollMode.AUTO, auto_scroll=False)
    return c


text_to_video_prefs = {
    'prompt': '',
    'negative_prompt': 'text, words, watermark, shutterstock',
    'num_inference_steps': 50,
    'guidance_scale': 9.0,
    'export_to_video': True,
    'eta': 0.0,
    'seed': 0,
    'width': 256,
    'height': 256,
    'num_frames': 16,
    'model': 'damo-vilab/text-to-video-ms-1.7b',
    'batch_folder_name': '',
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": 2.0,
    "display_upscaled_image": False,
    "lower_memory": True,
}

def buildTextToVideo(page):
    global text_to_video_prefs, prefs, pipe_text_to_video, editing_prompt
    editing_prompt = {'editing_prompt':'', 'edit_warmup_steps':10, 'edit_guidance_scale':5, 'edit_threshold':0.9, 'edit_weights':1, 'reverse_editing_direction': False}
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          if ptype == "int":
            text_to_video_prefs[pref] = int(e.control.value)
          elif ptype == "float":
            text_to_video_prefs[pref] = float(e.control.value)
          else:
            text_to_video_prefs[pref] = e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def clear_output(e):
      if prefs['enable_sounds']: page.snd_delete.play()
      page.text_to_video_output.controls = []
      page.text_to_video_output.update()
      clear_button.visible = False
      clear_button.update()
    def text_to_video_help(e):
      def close_text_to_video_dlg(e):
        nonlocal text_to_video_help_dlg
        text_to_video_help_dlg.open = False
        page.update()
      text_to_video_help_dlg = AlertDialog(title=Text("💁   Help with Text-To-Video"), content=Column([
          Text("Text-to-video synthesis from [ModelScope](https://modelscope.cn/) can be considered the same as Stable Diffusion structure-wise but it is extended to videos instead of static images. More specifically, this system allows us to generate videos from a natural language text prompt."),
          Markdown("""From the [model summary](https://huggingface.co/damo-vilab/modelscope-damo-text-to-video-synthesis):
*This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. Only English input is supported.*
Resources:
* [Website](https://modelscope.cn/models/damo/text-to-video-synthesis/summary)
* [GitHub repository](https://github.com/modelscope/modelscope/)""", on_tap_link=lambda e: e.page.launch_url(e.data)),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("🎞  What'll be next... ", on_click=close_text_to_video_dlg)], actions_alignment=MainAxisAlignment.END)
      page.dialog = text_to_video_help_dlg
      text_to_video_help_dlg.open = True
      page.update()
    def toggle_ESRGAN(e):
        ESRGAN_settings.height = None if e.control.value else 0
        text_to_video_prefs['apply_ESRGAN_upscale'] = e.control.value
        ESRGAN_settings.update()
    prompt = TextField(label="Animation Prompt Text", value=text_to_video_prefs['prompt'], col={'md': 9}, multiline=True, on_change=lambda e:changed(e,'prompt'))
    negative_prompt  = TextField(label="Negative Prompt Text", value=text_to_video_prefs['negative_prompt'], col={'md':3}, on_change=lambda e:changed(e,'negative_prompt'))
    num_frames = SliderRow(label="Number of Frames", min=1, max=300, divisions=299, pref=text_to_video_prefs, key='num_frames', tooltip="The number of video frames that are generated. Defaults to 16 frames which at 8 frames per seconds amounts to 2 seconds of video.")
    num_inference_row = SliderRow(label="Number of Inference Steps", min=1, max=150, divisions=149, pref=text_to_video_prefs, key='num_inference_steps', tooltip="The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.")
    guidance = SliderRow(label="Guidance Scale", min=0, max=50, divisions=100, round=1, pref=text_to_video_prefs, key='guidance_scale')
    eta_slider = SliderRow(label="ETA", min=0, max=1.0, divisions=20, round=1, pref=text_to_video_prefs, key='eta', tooltip="The weight of noise for added noise in a diffusion step. Its value is between 0.0 and 1.0 - 0.0 is DDIM and 1.0 is DDPM scheduler respectively.")
    #width_slider = SliderRow(label="Width", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=text_to_video_prefs, key='width')
    #height_slider = SliderRow(label="Height", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=text_to_video_prefs, key='height')
    export_to_video = Tooltip(message="Save mp4 file along with Image Sequence", content=Switcher(label="Export to Video", value=text_to_video_prefs['export_to_video'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'export_to_video')))
    lower_memory = Tooltip(message="Enable CPU offloading, VAE Tiling & Stitching", content=Switcher(label="Lower Memory Mode", value=text_to_video_prefs['lower_memory'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'lower_memory')))
    model = Dropdown(label="Video Model", hint_text="", expand=True, options=[dropdown.Option("damo-vilab/text-to-video-ms-1.7b"), dropdown.Option("modelscope-damo-text2video-synthesis"), dropdown.Option("modelscope-damo-text2video-pruned-weights"), dropdown.Option("cerspense/zeroscope_v2_XL"), dropdown.Option("cerspense/zeroscope_v2_576w")], value=text_to_video_prefs['model'], autofocus=False, on_change=lambda e:changed(e, 'model'))
    batch_folder_name = TextField(label="Batch Folder Name", value=text_to_video_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    seed = TextField(label="Seed", width=90, value=str(text_to_video_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    apply_ESRGAN_upscale = Switcher(label="Apply ESRGAN Upscale", value=text_to_video_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)
    enlarge_scale_slider = SliderRow(label="Enlarge Scale", min=1, max=4, divisions=6, round=1, suffix="x", pref=text_to_video_prefs, key='enlarge_scale')
    display_upscaled_image = Checkbox(label="Display Upscaled Image", value=text_to_video_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))
    ESRGAN_settings = Container(Column([enlarge_scale_slider, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    page.ESRGAN_block_text_to_video = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    page.ESRGAN_block_text_to_video.height = None if status['installed_ESRGAN'] else 0
    page.text_to_video_output = Column([], scroll=ScrollMode.AUTO, auto_scroll=False)
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.text_to_video_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🎥  Text-To-Video Synthesis", "Modelscope's Text-to-video-synthesis Model to Animate Diffusion", actions=[IconButton(icon=icons.HELP, tooltip="Help with Text-to-Video Settings", on_click=text_to_video_help)]),
        #ResponsiveRow([Row([original_image, alpha_mask], col={'lg':6}), Row([mask_image, invert_mask], col={'lg':6})]),
        ResponsiveRow([prompt, negative_prompt]),
        #Row([NumberPicker(label="Number of Frames: ", min=1, max=8, value=text_to_video_prefs['num_frames'], tooltip="The number of video frames that are generated. Defaults to 16 frames which at 8 frames per seconds amounts to 2 seconds of video.", on_change=lambda e: changed(e, 'num_frames')), seed, batch_folder_name]),
        Row([export_to_video, lower_memory, model]),
        num_frames,
        num_inference_row,
        guidance,
        eta_slider,
        #width_slider, height_slider,
        page.ESRGAN_block_text_to_video,
        Row([seed, batch_folder_name]),
        #Row([jump_length, jump_n_sample, seed]),
        Row([
            ElevatedButton(content=Text("📹  Run Text-To-Video", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_text_to_video(page)),
             #ElevatedButton(content=Text(value="📜   Run from Prompts List", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_text_to_video(page, from_list=True))
        ]),
        page.text_to_video_output,
        clear_button,
      ]
    ))], scroll=ScrollMode.AUTO, auto_scroll=False)
    return c

text_to_video_zero_prefs = {
    'prompt': '',
    'negative_prompt': 'text, words, watermark, shutterstock',
    'num_inference_steps': 50,
    'guidance_scale': 9.0,
    'export_to_video': True,
    'eta': 0.0,
    'seed': 0,
    'width': 1024,
    'height': 1024,
    'num_frames': 8,
    'motion_field_strength_x': 12,
    'motion_field_strength_y': 12,
    't0': 42,
    't1': 47,
    'input_video': '',
    'prep_type': 'Zero Shot',
    'batch_folder_name': '',
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": 2.0,
    "display_upscaled_image": False,
    "lower_memory": True,
}

def buildTextToVideoZero(page):
    global text_to_video_zero_prefs, prefs, pipe_text_to_video_zero
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          if ptype == "int":
            text_to_video_zero_prefs[pref] = int(e.control.value)
          elif ptype == "float":
            text_to_video_zero_prefs[pref] = float(e.control.value)
          else:
            text_to_video_zero_prefs[pref] = e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def clear_output(e):
      if prefs['enable_sounds']: page.snd_delete.play()
      page.text_to_video_zero_output.controls = []
      page.text_to_video_zero_output.update()
      clear_button.visible = False
      clear_button.update()
    def text_to_video_zero_help(e):
      def close_text_to_video_zero_dlg(e):
        nonlocal text_to_video_zero_help_dlg
        text_to_video_zero_help_dlg.open = False
        page.update()
      text_to_video_zero_help_dlg = AlertDialog(title=Text("💁   Help with Text-To-Video Zero"), content=Column([
          Text("Recent text-to-video generation approaches rely on computationally heavy training and require large-scale video datasets. In this paper, we introduce a new task of zero-shot text-to-video generation and propose a low-cost approach (without any training or optimization) by leveraging the power of existing text-to-image synthesis methods (e.g., Stable Diffusion), making them suitable for the video domain. Our key modifications include (i) enriching the latent codes of the generated frames with motion dynamics to keep the global scene and the background time consistent; and (ii) reprogramming frame-level self-attention using a new cross-frame attention of each frame on the first frame, to preserve the context, appearance, and identity of the foreground object."),
          Text("Experiments show that this leads to low overhead, yet high-quality and remarkably consistent video generation. Moreover, our approach is not limited to text-to-video synthesis but is also applicable to other tasks such as conditional and content-specialized video generation, and Video Instruct-Pix2Pix, i.e., instruction-guided video editing. As experiments show, our method performs comparably or sometimes better than recent approaches, despite not being trained on additional video data."),
          Markdown("""* [Project Page](https://text2video-zero.github.io/)
* [Paper](https://arxiv.org/abs/2303.13439)
* [Original Code](https://github.com/Picsart-AI-Research/Text2Video-Zero)""", on_tap_link=lambda e: e.page.launch_url(e.data)),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("🎞  Let's go crazy... ", on_click=close_text_to_video_zero_dlg)], actions_alignment=MainAxisAlignment.END)
      page.dialog = text_to_video_zero_help_dlg
      text_to_video_zero_help_dlg.open = True
      page.update()
    def toggle_ESRGAN(e):
        ESRGAN_settings.height = None if e.control.value else 0
        text_to_video_zero_prefs['apply_ESRGAN_upscale'] = e.control.value
        ESRGAN_settings.update()
    def change_steps(e):
        steps = e.control.value
        t0.set_max(steps - 1)
        t0.set_max(steps - 1)
        t0.set_divisions(steps - 1)
        if text_to_video_zero_prefs['t0'] > steps - 1:
            text_to_video_zero_prefs['t0'] = steps - 1
            t0.set_value(text_to_video_zero_prefs['t0'])
        t1.set_min(text_to_video_zero_prefs['t0'] + 1)
        t1.set_max(steps - 1)
        if text_to_video_zero_prefs['t1'] > steps - 1:
            text_to_video_zero_prefs['t1'] = steps - 1
            t1.set_value(text_to_video_zero_prefs['t1'])
        if text_to_video_zero_prefs['t1'] < t1.min:
            text_to_video_zero_prefs['t1'] = t1.min
            t1.set_value(text_to_video_zero_prefs['t1'])
        t1.set_divisions(t1.max - t1.min)
        t0.update_slider()
        t1.update_slider()
    def change_t0(e):
        t0_value = e.control.value
        steps = num_inference_row.value
        t1.set_min(t0_value + 1)
        if text_to_video_zero_prefs['t1'] > steps - 1:
            text_to_video_zero_prefs['t1'] = steps - 1
            t1.set_value(text_to_video_zero_prefs['t1'])
        if text_to_video_zero_prefs['t1'] < t1.min:
            text_to_video_zero_prefs['t1'] = t1.min
            t1.set_value(text_to_video_zero_prefs['t1'])
        t1.set_divisions(t1.max - t1.min)
        t1.update_slider()
    prompt = TextField(label="Animation Prompt Text", value=text_to_video_zero_prefs['prompt'], col={'md': 9}, multiline=True, on_change=lambda e:changed(e,'prompt'))
    negative_prompt  = TextField(label="Negative Prompt Text", value=text_to_video_zero_prefs['negative_prompt'], col={'md':3}, on_change=lambda e:changed(e,'negative_prompt'))
    num_frames = SliderRow(label="Number of Frames", min=1, max=300, divisions=299, pref=text_to_video_zero_prefs, key='num_frames', tooltip="The number of video frames that are generated. Defaults to 16 frames which at 8 frames per seconds amounts to 2 seconds of video.")
    num_inference_row = SliderRow(label="Number of Inference Steps", min=1, max=150, divisions=149, pref=text_to_video_zero_prefs, key='num_inference_steps', on_change=change_steps, tooltip="The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.")
    guidance = SliderRow(label="Guidance Scale", min=0, max=50, divisions=100, round=1, pref=text_to_video_zero_prefs, key='guidance_scale')
    eta_slider = SliderRow(label="ETA", min=0, max=1.0, divisions=20, round=1, pref=text_to_video_zero_prefs, key='eta', tooltip="The weight of noise for added noise in a diffusion step. Its value is between 0.0 and 1.0 - 0.0 is DDIM and 1.0 is DDPM scheduler respectively.")
    motion_field_strength_x = SliderRow(label="Motion Field Strength X", min=1, max=30, divisions=29, pref=text_to_video_zero_prefs, key='motion_field_strength_x', tooltip="Strength of motion in generated video along x-axis")
    motion_field_strength_y = SliderRow(label="Motion Field Strength Y", min=1, max=30, divisions=29, pref=text_to_video_zero_prefs, key='motion_field_strength_y', tooltip="Strength of motion in generated video along y-axis")
    t0 = SliderRow(label="Timestep t0", min=0, max=50, divisions=50, pref=text_to_video_zero_prefs, key='t0', on_change=change_t0, tooltip="Should be in the range [0, num_inference_steps - 1]")
    t1 = SliderRow(label="Timestep t1", min=43, max=50, divisions=7, pref=text_to_video_zero_prefs, key='t1', tooltip="Should be in the range [t0 + 1, num_inference_steps - 1]")
    #width_slider = SliderRow(label="Width", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=text_to_video_zero_prefs, key='width')
    #height_slider = SliderRow(label="Height", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=text_to_video_zero_prefs, key='height')
    export_to_video = Tooltip(message="Save mp4 file along with Image Sequence", content=Switcher(label="Export to Video", value=text_to_video_zero_prefs['export_to_video'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'export_to_video')))
    #lower_memory = Tooltip(message="Enable CPU offloading, VAE Tiling & Stitching", content=Switcher(label="Lower Memory Mode", value=text_to_video_zero_prefs['lower_memory'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'lower_memory')))
    batch_folder_name = TextField(label="Batch Folder Name", value=text_to_video_zero_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    seed = TextField(label="Seed", width=90, value=str(text_to_video_zero_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    apply_ESRGAN_upscale = Switcher(label="Apply ESRGAN Upscale", value=text_to_video_zero_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)
    enlarge_scale_slider = SliderRow(label="Enlarge Scale", min=1, max=4, divisions=6, round=1, suffix="x", pref=text_to_video_zero_prefs, key='enlarge_scale')
    display_upscaled_image = Checkbox(label="Display Upscaled Image", value=text_to_video_zero_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))
    ESRGAN_settings = Container(Column([enlarge_scale_slider, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    page.ESRGAN_block_text_to_video_zero = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    page.ESRGAN_block_text_to_video_zero.height = None if status['installed_ESRGAN'] else 0
    page.text_to_video_zero_output = Column([], scroll=ScrollMode.AUTO, auto_scroll=False)
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.text_to_video_zero_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🎥  Text-To-Video Zero", "Text-to-Image Diffusion Models for Zero-Shot Video Generators", actions=[IconButton(icon=icons.HELP, tooltip="Help with Text-To-Video Zero Settings", on_click=text_to_video_zero_help)]),
        #ResponsiveRow([Row([original_image, alpha_mask], col={'lg':6}), Row([mask_image, invert_mask], col={'lg':6})]),
        ResponsiveRow([prompt, negative_prompt]),
        #Row([NumberPicker(label="Number of Frames: ", min=1, max=8, value=text_to_video_zero_prefs['num_frames'], tooltip="The number of video frames that are generated. Defaults to 16 frames which at 8 frames per seconds amounts to 2 seconds of video.", on_change=lambda e: changed(e, 'num_frames')), seed, batch_folder_name]),
        Row([export_to_video]),
        num_frames,
        num_inference_row,
        guidance,
        eta_slider,
        motion_field_strength_x, motion_field_strength_y,
        t0, t1,
        #width_slider, height_slider,
        page.ESRGAN_block_text_to_video_zero,
        Row([seed, batch_folder_name]),
        #Row([jump_length, jump_n_sample, seed]),
        Row([
            ElevatedButton(content=Text("📹  Run Text2Video-Zero", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_text_to_video_zero(page)),
             #ElevatedButton(content=Text(value="📜   Run from Prompts List", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_text_to_video_zero(page, from_list=True))
        ]),
        page.text_to_video_zero_output,
        clear_button,
      ]
    ))], scroll=ScrollMode.AUTO, auto_scroll=False)
    return c

video_to_video_prefs = {
    'init_video': '',
    'fps': 12,
    'start_time': 0,
    'end_time': 0,
    'prompt': '',
    'negative_prompt': '',
    'num_inference_steps': 50,
    'guidance_scale': 15.0,
    'strength': 0.6,
    'export_to_video': True,
    'eta': 0.0,
    'seed': 0,
    'max_size': 1024,
    'width': 256,
    'height': 256,
    'num_frames': 16,
    'model': 'cerspense/zeroscope_v2_576w',
    'batch_folder_name': '',
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": 2.0,
    "display_upscaled_image": False,
    "lower_memory": True,
}

def buildVideoToVideo(page):
    global video_to_video_prefs, prefs, pipe_video_to_video, editing_prompt
    editing_prompt = {'editing_prompt':'', 'edit_warmup_steps':10, 'edit_guidance_scale':5, 'edit_threshold':0.9, 'edit_weights':1, 'reverse_editing_direction': False}
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          if ptype == "int":
            video_to_video_prefs[pref] = int(e.control.value)
          elif ptype == "float":
            video_to_video_prefs[pref] = float(e.control.value)
          else:
            video_to_video_prefs[pref] = e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def clear_output(e):
      if prefs['enable_sounds']: page.snd_delete.play()
      page.video_to_video_output.controls = []
      page.video_to_video_output.update()
      clear_button.visible = False
      clear_button.update()
    def video_to_video_help(e):
      def close_video_to_video_dlg(e):
        nonlocal video_to_video_help_dlg
        video_to_video_help_dlg.open = False
        page.update()
      video_to_video_help_dlg = AlertDialog(title=Text("💁   Help with Video-To-Video"), content=Column([
          Text("Cerspense Zeroscope are watermark-free model and have been trained on images that are 1024x576. Text-to-video synthesis from [ModelScope](https://modelscope.cn/) can be considered the same as Stable Diffusion structure-wise but it is extended to videos instead of static images. More specifically, this system allows us to generate videos from a natural language text prompt."),
          Markdown("""From the [model summary](https://huggingface.co/damo-vilab/modelscope-damo-text-to-video-synthesis):
*This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. Only English input is supported.*
Resources:
* [Website](https://modelscope.cn/models/damo/text-to-video-synthesis/summary)
* [GitHub repository](https://github.com/modelscope/modelscope/)""", on_tap_link=lambda e: e.page.launch_url(e.data)),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("🎞  What'll be next... ", on_click=close_video_to_video_dlg)], actions_alignment=MainAxisAlignment.END)
      page.dialog = video_to_video_help_dlg
      video_to_video_help_dlg.open = True
      page.update()
    def file_picker_result(e: FilePickerResultEvent):
        if e.files != None:
          upload_files(e)
    def on_upload_progress(e: FilePickerUploadEvent):
      if e.progress == 1:
        if not slash in e.file_name:
          fname = os.path.join(root_dir, e.file_name)
          video_to_video_prefs['file_name'] = e.file_name.rpartition('.')[0]
        else:
          fname = e.file_name
          video_to_video_prefs['file_name'] = e.file_name.rpartition(slash)[2].rpartition('.')[0]
        init_video.value = fname
        init_video.update()
        video_to_video_prefs['init_video'] = fname
        page.update()
    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)
    def upload_files(e):
        uf = []
        if file_picker.result != None and file_picker.result.files != None:
            for f in file_picker.result.files:
              if page.web:
                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))
              else:
                on_upload_progress(FilePickerUploadEvent(f.path, 1, ""))
            file_picker.upload(uf)
    page.overlay.append(file_picker)
    def pick_video(e):
        file_picker.pick_files(allow_multiple=False, allowed_extensions=["avi", "mp4", "mov"], dialog_title="Pick Video File")
    def toggle_ESRGAN(e):
        ESRGAN_settings.height = None if e.control.value else 0
        video_to_video_prefs['apply_ESRGAN_upscale'] = e.control.value
        ESRGAN_settings.update()
    prompt = TextField(label="Video Prompt Text", value=video_to_video_prefs['prompt'], col={'md': 9}, multiline=True, on_change=lambda e:changed(e,'prompt'))
    negative_prompt  = TextField(label="Negative Prompt Text", value=video_to_video_prefs['negative_prompt'], col={'md':3}, on_change=lambda e:changed(e,'negative_prompt'))
    init_video = TextField(label="Init Video File", value=video_to_video_prefs['init_video'], on_change=lambda e:changed(e,'init_video'), height=60, suffix=IconButton(icon=icons.VIDEO_CALL, on_click=pick_video))
    fps = SliderRow(label="Frames per Second", min=1, max=30, divisions=29, suffix='fps', pref=video_to_video_prefs, key='fps', tooltip="The FPS to extract from the init video clip.")
    start_time = TextField(label="Start Time (s)", value=controlnet_prefs['start_time'], width=145, keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'start_time', ptype="float"))
    end_time = TextField(label="End Time (0 for all)", value=controlnet_prefs['end_time'], width=145, keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'end_time', ptype="float"))
    vid_params = Container(content=Column([fps, Row([start_time, end_time])]), animate_size=animation.Animation(800, AnimationCurve.EASE_OUT), clip_behavior=ClipBehavior.HARD_EDGE, height=None if controlnet_prefs['use_init_video'] else 0)
    #num_frames = SliderRow(label="Number of Frames", min=1, max=300, divisions=299, pref=video_to_video_prefs, key='num_frames', tooltip="The number of video frames that are generated. Defaults to 16 frames which at 8 frames per seconds amounts to 2 seconds of video.")
    num_inference_row = SliderRow(label="Number of Inference Steps", min=1, max=150, divisions=149, pref=video_to_video_prefs, key='num_inference_steps', tooltip="The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.")
    guidance = SliderRow(label="Guidance Scale", min=0, max=50, divisions=100, round=1, pref=video_to_video_prefs, key='guidance_scale')
    strength = SliderRow(label="Init Image Strength", min=0.0, max=1.0, divisions=20, round=2, pref=video_to_video_prefs, key='strength', tooltip="Conceptually, indicates how much to transform the Reference Image over the Vid Generation. Higher value give less influence.")
    eta_slider = SliderRow(label="ETA", min=0, max=1.0, divisions=20, round=1, pref=video_to_video_prefs, key='eta', tooltip="The weight of noise for added noise in a diffusion step. Its value is between 0.0 and 1.0 - 0.0 is DDIM and 1.0 is DDPM scheduler respectively.")
    max_row = SliderRow(label="Max Resolution Size", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=video_to_video_prefs, key='max_size')
    #width_slider = SliderRow(label="Width", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=video_to_video_prefs, key='width')
    #height_slider = SliderRow(label="Height", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=video_to_video_prefs, key='height')
    export_to_video = Tooltip(message="Save mp4 file along with Image Sequence", content=Switcher(label="Export to Video", value=video_to_video_prefs['export_to_video'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'export_to_video')))
    lower_memory = Tooltip(message="Enable CPU offloading, VAE Tiling & Stitching", content=Switcher(label="Lower Memory Mode", value=video_to_video_prefs['lower_memory'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'lower_memory')))
    model = Dropdown(label="Video Model", hint_text="", expand=True, options=[dropdown.Option("damo-vilab/text-to-video-ms-1.7b"), dropdown.Option("modelscope-damo-text2video-synthesis"), dropdown.Option("modelscope-damo-text2video-pruned-weights"), dropdown.Option("cerspense/zeroscope_v2_XL"), dropdown.Option("cerspense/zeroscope_v2_576w")], value=video_to_video_prefs['model'], autofocus=False, on_change=lambda e:changed(e, 'model'))
    batch_folder_name = TextField(label="Batch Folder Name", value=video_to_video_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    seed = TextField(label="Seed", width=90, value=str(video_to_video_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    apply_ESRGAN_upscale = Switcher(label="Apply ESRGAN Upscale", value=video_to_video_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)
    enlarge_scale_slider = SliderRow(label="Enlarge Scale", min=1, max=4, divisions=6, round=1, suffix="x", pref=video_to_video_prefs, key='enlarge_scale')
    display_upscaled_image = Checkbox(label="Display Upscaled Image", value=video_to_video_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))
    ESRGAN_settings = Container(Column([enlarge_scale_slider, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    page.ESRGAN_block_video_to_video = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    page.ESRGAN_block_video_to_video.height = None if status['installed_ESRGAN'] else 0
    page.video_to_video_output = Column([], scroll=ScrollMode.AUTO, auto_scroll=False)
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.video_to_video_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("📽  Video-To-Video Synthesis", "Note: Uses more than 16GB VRAM, may crash session. Video-to-video-synthesis Model to Reanimate Video Clips", actions=[IconButton(icon=icons.HELP, tooltip="Help with Instruct-Pix2Pix Settings", on_click=video_to_video_help)]),
        #ResponsiveRow([Row([original_image, alpha_mask], col={'lg':6}), Row([mask_image, invert_mask], col={'lg':6})]),
        ResponsiveRow([prompt, negative_prompt]),
        init_video,
        fps,
        Row([start_time, end_time]),
        #Row([NumberPicker(label="Number of Frames: ", min=1, max=8, value=video_to_video_prefs['num_frames'], tooltip="The number of video frames that are generated. Defaults to 16 frames which at 8 frames per seconds amounts to 2 seconds of video.", on_change=lambda e: changed(e, 'num_frames')), seed, batch_folder_name]),
        Row([export_to_video, lower_memory, model]),
        #num_frames,
        num_inference_row,
        guidance,
        strength,
        eta_slider,
        max_row,
        #width_slider, height_slider,
        page.ESRGAN_block_video_to_video,
        Row([seed, batch_folder_name]),
        #Row([jump_length, jump_n_sample, seed]),
        Row([
            ElevatedButton(content=Text("🎦  Run Video-To-Video", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_video_to_video(page)),
             #ElevatedButton(content=Text(value="📜   Run from Prompts List", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_video_to_video(page, from_list=True))
        ]),
        page.video_to_video_output,
        clear_button,
      ]
    ))], scroll=ScrollMode.AUTO, auto_scroll=False)
    return c


potat1_prefs = {
    'prompt': '',
    'negative_prompt': 'text, watermark, copyright, blurry, low resolution, blur, low quality',
    'num_inference_steps': 25,
    'guidance_scale': 23.0,
    'fps': 24,
    'num_frames': 16,
    'export_to_video': False,
    'remove_watermark': True,
    'eta': 0.0,
    'seed': 0,
    'width': 1024,
    'height': 576,
    'init_video': '',
    'init_weight': 0.5,
    'batch_folder_name': '',
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": 2.0,
    "display_upscaled_image": False,
}

def buildPotat1(page):
    global potat1_prefs, prefs
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          if ptype == "int":
            potat1_prefs[pref] = int(e.control.value)
          elif ptype == "float":
            potat1_prefs[pref] = float(e.control.value)
          else:
            potat1_prefs[pref] = e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def clear_output(e):
      if prefs['enable_sounds']: page.snd_delete.play()
      page.potat1_output.controls = []
      page.potat1_output.update()
      clear_button.visible = False
      clear_button.update()
    def potat1_help(e):
      def close_potat1_dlg(e):
        nonlocal potat1_help_dlg
        potat1_help_dlg.open = False
        page.update()
      potat1_help_dlg = AlertDialog(title=Text("💁   Help with Potat1 Text-To-Video"), content=Column([
          Text("This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. The text-to-video generation diffusion model consists of three sub-networks: text feature extraction, text feature-to-video latent space diffusion model, and video latent space to video visual space. The overall model parameters are about 1.7 billion. Support English input. The diffusion model adopts the Unet3D structure, and realizes the function of video generation through the iterative denoising process from the pure Gaussian noise video. Trained with https://lambdalabs.com ❤ 1xA100 (40GB) 2197 clips, 68388 tagged frames ( salesforce/blip2-opt-6.7b-coco ) train_steps: 10000"),
          Markdown("[Huggingface Website](https://huggingface.co/camenduru/potat1) | [GitHub repository](https://github.com/camenduru/text-to-video-synthesis-colab)", on_tap_link=lambda e: e.page.launch_url(e.data)),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("🍠  Spuds up... ", on_click=close_potat1_dlg)], actions_alignment=MainAxisAlignment.END)
      page.dialog = potat1_help_dlg
      potat1_help_dlg.open = True
      page.update()
    def toggle_ESRGAN(e):
        ESRGAN_settings.height = None if e.control.value else 0
        potat1_prefs['apply_ESRGAN_upscale'] = e.control.value
        ESRGAN_settings.update()
    prompt = TextField(label="Animation Prompt Text", value=potat1_prefs['prompt'], col={'md': 9}, multiline=True, on_change=lambda e:changed(e,'prompt'))
    negative_prompt  = TextField(label="Negative Prompt Text", value=potat1_prefs['negative_prompt'], col={'md':3}, on_change=lambda e:changed(e,'negative_prompt'))
    num_frames = SliderRow(label="Number of Frames", min=1, max=300, divisions=299, pref=potat1_prefs, key='num_frames', tooltip="The number of video frames that are generated. Defaults to 16 frames which at 8 frames per seconds amounts to 2 seconds of video.")
    num_inference_row = SliderRow(label="Number of Inference Steps", min=1, max=150, divisions=149, pref=potat1_prefs, key='num_inference_steps', tooltip="The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.")
    guidance = SliderRow(label="Guidance Scale", min=0, max=50, divisions=100, round=1, pref=potat1_prefs, key='guidance_scale')
    fps = SliderRow(label="Frames per Second", min=1, max=30, divisions=29, suffix='fps', pref=potat1_prefs, key='fps')
    #eta_slider = SliderRow(label="ETA", min=0, max=1.0, divisions=20, round=1, pref=potat1_prefs, key='eta', tooltip="The weight of noise for added noise in a diffusion step. Its value is between 0.0 and 1.0 - 0.0 is DDIM and 1.0 is DDPM scheduler respectively.")
    #width_slider = SliderRow(label="Width", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=potat1_prefs, key='width')
    #height_slider = SliderRow(label="Height", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=potat1_prefs, key='height')
    export_to_video = Tooltip(message="Save mp4 file along with Image Sequence", content=Switcher(label="Export to Video", value=potat1_prefs['export_to_video'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'export_to_video')))
    width_slider = SliderRow(label="Width", min=256, max=1024, divisions=12, multiple=32, suffix="px", pref=potat1_prefs, key='width')
    height_slider = SliderRow(label="Height", min=256, max=1024, divisions=12, multiple=32, suffix="px", pref=potat1_prefs, key='height')
    batch_folder_name = TextField(label="Video Folder Name", value=potat1_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    seed = TextField(label="Seed", width=90, value=str(potat1_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    apply_ESRGAN_upscale = Switcher(label="Apply ESRGAN Upscale", value=potat1_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)
    enlarge_scale_slider = SliderRow(label="Enlarge Scale", min=1, max=4, divisions=6, round=1, suffix="x", pref=potat1_prefs, key='enlarge_scale')
    display_upscaled_image = Checkbox(label="Display Upscaled Image", value=potat1_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))
    ESRGAN_settings = Container(Column([enlarge_scale_slider, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    page.ESRGAN_block_potat1 = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    page.ESRGAN_block_potat1.height = None if status['installed_ESRGAN'] else 0
    page.potat1_output = Column([], scroll=ScrollMode.AUTO, auto_scroll=False)
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.potat1_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🥔  Potat1️⃣ Text-To-Video Synthesis", "CamenDuru's Open-Source 1024x576 Text-To-Video Model ", actions=[IconButton(icon=icons.HELP, tooltip="Help with Potat1 Settings", on_click=potat1_help)]),
        #ResponsiveRow([Row([original_image, alpha_mask], col={'lg':6}), Row([mask_image, invert_mask], col={'lg':6})]),
        ResponsiveRow([prompt, negative_prompt]),
        #Row([export_to_video, lower_memory]),
        num_frames,
        fps,
        num_inference_row,
        guidance,
        #eta_slider,
        width_slider, height_slider,
        #page.ESRGAN_block_potat1,
        Row([seed, batch_folder_name]),
        Row([
            ElevatedButton(content=Text("🍟  Run Potat1", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_potat1(page)),
        ]),
        page.potat1_output,
        clear_button,
      ]
    ))], scroll=ScrollMode.AUTO, auto_scroll=False)
    return c

stable_animation_prefs = {
    #'animation_prompt': '{\n0:"",\n}',
    'animation_prompts': {},
    'negative_prompt': 'text, words, watermark, shutterstock',
    'negative_prompt_weight': -1.0,
    #'prompt': '',
    #'frame': 0,
    'num_inference_steps': 50,
    'guidance_scale': 9.0,
    'export_to_video': True,
    'seed': 0,
    'width': 512,
    'height': 512,
    'max_frames': 300,
    'steps_curve': "0:(30)",
    'model': 'stable-diffusion-v1-5',
    'style_preset': "None",
    'sampler': 'K_dpmpp_2m',
    'clip_guidance': "None",
    'steps_strength_adj': True,
    'interpolate_prompts': False,
    'locked_seed': False,
    'noise_add_curve': "0:(0.02)",
    'noise_scale_curve': "0:(0.99)",
    'strength_curve': "0:(0.65)",
    'diffusion_cadence_curve': "0:(1.0)",
    'cadence_interp': 'Mix',
    'cadence_spans': False,
    'inpaint_border': False,
    'border': "replicate",
    'use_inpainting_model': False,
    'fps': 12,
    'mask_min_value': "0:(0.25)",
    'mask_binarization_thr': 0.5,
    #Colour & Depth Parameters
    'color_coherence': 'LAB',
    'brightness_curve': "0:(1.0)",
    'contrast_curve': "0:(1.0)",
    'hue_curve': "0:(0.0)",
    'saturation_curve': "0:(1.0)",
    'lightness_curve': "0:(0.0)",
    'color_match_animate': True,
    'depth_model_weight': 0.3,
    'near_plane': 200,
    'far_plane': 10000,
    'fov_curve': "0:(25)", #-180
    'depth_blur_curve': "0:(0.0)", #-7
    'depth_warp_curve': "0:(1.0)", #-1
    #2D & 3D Parameters
    'translation_x': "0:(0)",
    'translation_y': "0:(0)",
    'translation_z': "0:(0)",
    'angle': "0:(0)",
    'zoom': "0:(1)",
    'rotation_x': "0:(0)",
    'rotation_y': "0:(0)",
    'rotation_z': "0:(0)",
    'camera_type': "Perspective",#Orthographic
    'render_mode': "Mesh",#Pointcloud
    'animation_mode': "3D warp",
    'mask_power': 0.3, #-4
    #Input Parameters
    'init_image': "",
    'init_sizing': "Stretch",
    'mask_image': "",
    'mask_invert': False,
    'video_init_path': "",
    'video_flow_warp': True,
    'video_mix_in_curve': "0:(0.02)",
    'extract_nth_frame': 1,
    'video_init_fps': 12,
    'resume': False,
    'resume_from': '-1',
    #Post-Processor Parameters
    'output_fps': 24,
    'frame_interpolation_mode': "Rife", #film, none
    'frame_interpolation_factor': "2", #4, 8
    'batch_folder_name': '',
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": 2.0,
    "display_upscaled_image": False,
    "lower_memory": True,
}
stable_animation_prefs_default = stable_animation_prefs.copy()

def buildStableAnimation(page):
    global stable_animation_prefs, prefs, pipe_stable_animation
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          if ptype == "int":
            stable_animation_prefs[pref] = int(e.control.value)
          elif ptype == "float":
            stable_animation_prefs[pref] = float(e.control.value)
          else:
            stable_animation_prefs[pref] = e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def clear_output(e):
      if prefs['enable_sounds']: page.snd_delete.play()
      page.stable_animation_output.controls = []
      page.stable_animation_output.update()
      clear_button.visible = False
      clear_button.update()
    def stable_animation_help(e):
      def close_stable_animation_dlg(e):
        nonlocal stable_animation_help_dlg
        stable_animation_help_dlg.open = False
        page.update()
      stable_animation_help_dlg = AlertDialog(title=Text("💁   Help with Stable Animation"), content=Column([
          Text("With our Animation SDK, artists and developers have the ability to use the Stable Diffusion family of models to generate stunning animations. Create animations purely from prompts, start with an initial image, or drive an animation from a source video. "),
          Text("Artists have the ability to use all of our available inference models to generate animations. We currently support text-to-animation, image-to-animation, and video-to-animation. To see animated previews of how these parameters affect the resulting animation, please check out our Animation Handbook."),
          Markdown(" [Project Page](https://platform.stability.ai/docs/features/animation) | [Colab Gradio](https://colab.research.google.com/github/Stability-AI/stability-sdk/blob/animation/nbs/animation_gradio.ipynb) | [Animation Handbook](https://docs.google.com/document/d/1iHcAu_5rG11guGFie8sXBXPGuM4yKzqdd13MJ_1LU8U/edit?usp=sharing)", on_tap_link=lambda e: e.page.launch_url(e.data)),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("💸  Worth it... ", on_click=close_stable_animation_dlg)], actions_alignment=MainAxisAlignment.END)
      page.dialog = stable_animation_help_dlg
      stable_animation_help_dlg.open = True
      page.update()
    def toggle_ESRGAN(e):
        ESRGAN_settings.height = None if e.control.value else 0
        stable_animation_prefs['apply_ESRGAN_upscale'] = e.control.value
        ESRGAN_settings.update()
    def file_picker_result(e: FilePickerResultEvent):
        if e.files != None:
          upload_files(e)
    def on_upload_progress(e: FilePickerUploadEvent):
      nonlocal pick_type
      if e.progress == 1:
        if not slash in e.file_name:
          fname = os.path.join(root_dir, e.file_name)
          stable_animation_prefs['file_name'] = e.file_name.rpartition('.')[0]
        else:
          fname = e.file_name
          stable_animation_prefs['file_name'] = e.file_name.rpartition(slash)[2].rpartition('.')[0]
        if pick_type == "image":
          init_image.value = fname
          init_image.update()
          stable_animation_prefs['init_image'] = fname
        if pick_type == "mask":
          mask_image.value = fname
          mask_image.update()
          stable_animation_prefs['mask_image'] = fname
        elif pick_type == "video":
          init_video.value = fname
          init_video.update()
          stable_animation_prefs['init_video'] = fname
        page.update()
    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)
    def upload_files(e):
        uf = []
        if file_picker.result != None and file_picker.result.files != None:
            for f in file_picker.result.files:
              if page.web:
                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))
              else:
                on_upload_progress(FilePickerUploadEvent(f.path, 1, ""))
            file_picker.upload(uf)
    page.overlay.append(file_picker)
    pick_type = ""
    def pick_init(e):
        nonlocal pick_type
        pick_type = "image"
        file_picker.pick_files(allow_multiple=False, allowed_extensions=["png", "PNG", "jpg", "jpeg"], dialog_title="Pick Init Image File")
    def pick_mask(e):
        nonlocal pick_type
        pick_type = "mask"
        file_picker.pick_files(allow_multiple=False, allowed_extensions=["png", "PNG", "jpg", "jpeg"], dialog_title="Pick Mask Image File")
    def pick_video(e):
        nonlocal pick_type
        pick_type = "video"
        file_picker.pick_files(allow_multiple=False, allowed_extensions=["mp4", "avi"], dialog_title="Pick Initial Video File")
    def save_preset(e):
      def copy_preset(pl):
        nonlocal text_list, enter_text
        page.set_clipboard(enter_text.value)
        page.snack_bar = SnackBar(content=Text(f"📋   Animation Preset copied to clipboard..."))
        page.snack_bar.open = True
        close_dlg(e)
      def close_dlg(e):
          dlg_copy.open = False
          page.update()
      stable_animation_prefs['animation_prompts'] = {str(k):v for k,v in stable_animation_prefs['animation_prompts'].items()}
      text_list = json.dumps(stable_animation_prefs, indent = 4)
      enter_text = TextField(label="Stable Animation Preset JSON", value=text_list.strip(), expand=True, filled=True, multiline=True, autofocus=True)
      dlg_copy = AlertDialog(modal=False, title=Text("📝   Stable Animation as JSON"), content=Container(Column([enter_text], alignment=MainAxisAlignment.START, tight=True, width=(page.width if page.web else page.window_width) - 180, height=(page.height if page.web else page.window_height) - 100, scroll="none"), width=(page.width if page.web else page.window_width) - 180, height=(page.height if page.web else page.window_height) - 100), actions=[TextButton(content=Text("Cancel", size=18), on_click=close_dlg), ElevatedButton(content=Text(value=emojize(":floppy_disk:") + "  Copy Preset JSON to Clipboard", size=19, weight=FontWeight.BOLD), data=text_list, on_click=lambda ev: copy_preset(text_list))], actions_alignment=MainAxisAlignment.END)
      page.dialog = dlg_copy
      dlg_copy.open = True
      page.update()
    def toggle_resume(e):
      stable_animation_prefs['resume'] = e.control.value
      resume_from.visible = stable_animation_prefs['resume']
      resume_from.update()
    def paste_preset(e):
      def save_preset(e):
        try:
          preset_json = json.loads(enter_text.value.strip())
        except UnicodeDecodeError:
          close_dlg(e)
          alert_msg("Error Parsing JSON Data...")
          return
        load_preset(preset_json)
        close_dlg(e)
      def paste_clipboard(e):
          enter_text.value = page.get_clipboard()
          enter_text.update()
      def close_dlg(e):
          dlg_paste.open = False
          page.update()
      enter_text = TextField(label="Enter Stable Animation Preset JSON", expand=True, filled=True, min_lines=30, multiline=True, autofocus=True)
      dlg_paste = AlertDialog(modal=False, title=Text("📝  Paste Saved Preset JSON"), content=Container(Column([enter_text], alignment=MainAxisAlignment.START, tight=True, width=(page.width if page.web else page.window_width) - 180, height=(page.height if page.web else page.window_height) - 100, scroll="none"), width=(page.width if page.web else page.window_width) - 180, height=(page.height if page.web else page.window_height) - 100), actions=[TextButton(content=Text("Cancel", size=18), on_click=close_dlg), TextButton(content=Text("📋  Paste from Clipboard", size=18), on_click=paste_clipboard), ElevatedButton(content=Text(value=emojize(":floppy_disk:") + "  Load Preset JSON Values ", size=19, weight=FontWeight.BOLD), on_click=save_preset)], actions_alignment=MainAxisAlignment.END)
      page.dialog = dlg_paste
      dlg_paste.open = True
      page.update()
    def load_preset(d):
      global stable_animation_prefs
      stable_animation_prefs = d.copy()
      stable_animation_prefs['animation_prompts'] = {}
      #animation_prompt.value = d['animation_prompt']
      animation_prompts.controls.clear()
      prompts = d['animation_prompts'].copy()
      for k, v in prompts.items():
        add_prompt(d, int(k), v, False)
      negative_prompt.value = d['negative_prompt']
      negative_prompt_weight.value = d['negative_prompt_weight']
      max_frames.value = d['max_frames']
      fps.set_value(d['fps'])
      num_inference_row.set_value(d['num_inference_steps'])
      guidance.set_value(d['guidance_scale'])
      model_checkpoint.value = d['model']
      generation_sampler.value = d['sampler']
      style_preset.value = d['style_preset']
      clip_guidance.value = d['clip_guidance']
      steps_strength_adj.value = d['steps_strength_adj']
      interpolate_prompts.value = d['interpolate_prompts']
      locked_seed.value = d['locked_seed']
      noise_add_curve.value = d['noise_add_curve']
      noise_scale_curve.value = d['noise_scale_curve']
      strength_curve.value = d['strength_curve']
      diffusion_cadence_curve.value = d['diffusion_cadence_curve']
      cadence_interp.value = d['cadence_interp']
      cadence_spans.value = d['cadence_spans']
      inpaint_border.value = d['inpaint_border']
      border.value = d['border']
      use_inpainting_model.value = d['use_inpainting_model']
      mask_min_value.value = d['mask_min_value']
      mask_binarization_thr.set_value(d['mask_binarization_thr'])
      color_coherence.value = d['color_coherence']
      brightness_curve.value = d['brightness_curve']
      contrast_curve.value = d['contrast_curve']
      hue_curve.value = d['hue_curve']
      saturation_curve.value = d['saturation_curve']
      lightness_curve.value = d['lightness_curve']
      color_match_animate.value = d['color_match_animate']
      depth_model_weight.set_value(d['depth_model_weight'])
      fov_curve.value = d['fov_curve']
      depth_blur_curve.value = d['depth_blur_curve']
      depth_warp_curve.value = d['depth_warp_curve']
      translation_x.value = d['translation_x']
      translation_y.value = d['translation_y']
      translation_z.value = d['translation_z']
      angle.value = d['angle']
      zoom.value = d['zoom']
      rotation_x.value = d['rotation_x']
      rotation_y.value = d['rotation_y']
      rotation_z.value = d['rotation_z']
      camera_type.value = d['camera_type']
      render_mode.value = d['render_mode']
      mask_power.set_value(d['mask_power'])
      init_image.value = d['init_image']
      init_sizing.value = d['init_sizing']
      mask_image.value = d['mask_image']
      mask_invert.value = d['mask_invert']
      video_mix_in_curve.value = d['video_mix_in_curve']
      animation_mode.value = d['animation_mode']
      init_video.value = d['video_init_path']
      video_mix_in_curve.value = d['video_mix_in_curve']
      video_flow_warp.value = d['video_flow_warp']
      extract_nth_frame.value = d['extract_nth_frame']
      video_init_fps.value = d['video_init_fps']
      output_fps.value = d['output_fps']
      resume.value = d['resume']
      resume_from.value = d['resume_from']
      frame_interpolation_mode.value = d['frame_interpolation_mode']
      frame_interpolation_factor.value = d['frame_interpolation_factor']
      width_slider.set_value(d['width'])
      height_slider.set_value(d['height'])
      seed.value = d['seed']
      batch_folder_name.value = d['batch_folder_name']
      #.set_value(d[''])
      page.update()
    def load_default(e):
      global stable_animation_prefs_default
      load_preset(stable_animation_prefs_default)
    def edit_prompt(e):
      nonlocal animation_prompts
      f = int(e.control.data)
      edit_prompt = stable_animation_prefs['animation_prompts'][str(f)]
      def close_dlg(e):
        dlg_edit.open = False
        page.update()
      def save_prompt(e):
        fr = int(editing_frame.value)
        pline = f'{fr}: "{editing_prompt.value}"'
        for p in animation_prompts.controls:
          if p.data == f:
            p.title.value = pline
            p.data = fr
            for button in p.trailing.items:
              button.data = fr
            break
        stable_animation_prefs['animation_prompts'][str(editing_frame.value)] = editing_prompt.value.strip()
        if f != int(editing_frame.value):
          del stable_animation_prefs['animation_prompts'][str(f)]
          sorted_dict = {}
          for key in sorted(stable_animation_prefs['animation_prompts'].keys()):
              sorted_dict[key] = stable_animation_prefs['animation_prompts'][key]
          stable_animation_prefs['animation_prompts'] = sorted_dict
          animation_prompts.controls = sorted(animation_prompts.controls, key=lambda tile: tile.data)
        animation_prompts.update()
        close_dlg(e)
      editing_frame = TextField(label="Frame", width=90, value=str(f), keyboard_type=KeyboardType.NUMBER, tooltip="")
      editing_prompt = TextField(label="Keyframe Prompt Animation", expand=True, multiline=True, value=edit_prompt, autofocus=True)
      dlg_edit = AlertDialog(modal=False, title=Text(f"♟️ Edit Prompt Keyframe"), content=Container(Column([
          Row([editing_frame, editing_prompt])
      ], alignment=MainAxisAlignment.START, tight=True, scroll=ScrollMode.AUTO), width=(page.width if page.web else page.window_width) - 180), actions=[TextButton(content=Text("Cancel", size=18), on_click=close_dlg), ElevatedButton(content=Text(value=emojize(":floppy_disk:") + "  Save Prompt ", size=19, weight=FontWeight.BOLD), on_click=save_prompt)], actions_alignment=MainAxisAlignment.END)
      page.dialog = dlg_edit
      dlg_edit.open = True
      page.update()
    def del_prompt(e):
      f = e.control.data
      for i, p in enumerate(animation_prompts.controls):
        if p.data == f:
          del animation_prompts.controls[i]
          break
      animation_prompts.update()
      del stable_animation_prefs['animation_prompts'][str(f)]
      if prefs['enable_sounds']: page.snd_delete.play()
    def clear_prompts(e):
      animation_prompts.controls.clear()
      animation_prompts.update()
      stable_animation_prefs['animation_prompts'] = {}
      clear_prompt(e)
      if prefs['enable_sounds']: page.snd_delete.play()
    def clear_prompt(e):
      prompt.value = ""
      prompt.update()
    def copy_prompt(e):
      p = stable_animation_prefs['animation_prompts'][str(e.control.data)]
      page.set_clipboard(p)
      page.snack_bar = SnackBar(content=Text(f"📋  Prompt Text copied to clipboard..."))
      page.snack_bar.open = True
      page.update()
    def add_prompt(e, f=None, p=None, sound=True):
      if (not bool(prompt.value) or not bool(frame.value)) and f == None: return
      if f == None: f = int(frame.value)
      if p == None: p = prompt.value.strip()
      pline = f'{f}: "{p}"'
      if f in stable_animation_prefs['animation_prompts']:
        for i, p in enumerate(animation_prompts.controls):
          if p.data == f:
            p.title.value = pline
      else:
        animation_prompts.controls.append(ListTile(title=Text(pline, size=14), data=f, trailing=PopupMenuButton(icon=icons.MORE_VERT,
            items=[PopupMenuItem(icon=icons.EDIT, text="Edit Animation Prompt", on_click=edit_prompt, data=f),
                  PopupMenuItem(icon=icons.COPY, text="Copy Prompt Text", on_click=copy_prompt, data=f),
                  PopupMenuItem(icon=icons.DELETE, text="Delete Animation Prompt", on_click=del_prompt, data=f), PopupMenuItem(icon=icons.DELETE_SWEEP, text="Delete All Prompts", on_click=clear_prompts)]), dense=True, on_click=edit_prompt))
      stable_animation_prefs['animation_prompts'][str(f)] = p
      #stable_animation_prefs['animation_prompts'] = {int(k):v for k,v in stable_animation_prefs['animation_prompts'].items()}
      #stable_animation_prefs['animation_prompts'] = sorted(stable_animation_prefs['animation_prompts'].keys())
      #stable_animation_prefs['animation_prompts'] = {i: stable_animation_prefs['animation_prompts'][i] for i in list(stable_animation_prefs['animation_prompts'].keys()).sort()}
      sorted_dict = {}
      for key in sorted(stable_animation_prefs['animation_prompts'].keys()):
          sorted_dict[key] = stable_animation_prefs['animation_prompts'][key]
      stable_animation_prefs['animation_prompts'] = sorted_dict
      animation_prompts.controls = sorted(animation_prompts.controls, key=lambda tile: tile.data)
      animation_prompts.update()
      if prefs['enable_sounds'] and sound: page.snd_drop.play()
    copy_preset_button = IconButton(icons.COPY_ALL, tooltip="Save Animation Presets as JSON", on_click=save_preset)
    paste_preset_button = IconButton(icons.CONTENT_PASTE, tooltip="Load Animation Presets as JSON", on_click=paste_preset)
    default_preset_button = IconButton(icons.REFRESH, tooltip="Reset Animation Presets to Default", on_click=load_default) #, suffix=IconButton(icons.CLEAR, on_click=clear_prompt), width=50, height=50
    prompt = TextField(label="Keyframe Prompt Text", value="", filled=True, expand=True, multiline=True)
    frame = TextField(label="Frame", width=76, value="0", filled=True, keyboard_type=KeyboardType.NUMBER, tooltip="")
    negative_prompt_weight = TextField(label="Negative Weight", width=120, value=stable_animation_prefs['negative_prompt_weight'], keyboard_type=KeyboardType.NUMBER, tooltip="", on_change=lambda e:changed(e,'negative_prompt_weight'))
    add_prompt_keyframe = ft.FilledButton("➕  Add Keyframe", on_click=add_prompt)
    #animation_prompt = TextField(label="Animation Prompt Text", value=stable_animation_prefs['animation_prompt'], col={'md': 9}, multiline=True, on_change=lambda e:changed(e,'animation_prompt'))
    negative_prompt  = TextField(label="Negative Prompt Text", value=stable_animation_prefs['negative_prompt'], col={'md':3}, expand=True, multiline=True, on_change=lambda e:changed(e,'negative_prompt'))
    max_frames  = TextField(label="Max Number of Frames", width=200, hint_text="The number of video frames that are generated. Defaults to 16 frames which at 8 frames per seconds amounts to 2 seconds of video.", value=stable_animation_prefs['max_frames'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, on_change=lambda e:changed(e,'max_frames', ptype='int'))
    #max_frames = SliderRow(label="Max Number of Frames", min=1, max=300, divisions=299, pref=stable_animation_prefs, key='max_frames', tooltip="The number of video frames that are generated. Defaults to 16 frames which at 8 frames per seconds amounts to 2 seconds of video.")
    fps = SliderRow(label="Frames per Second", min=1, max=30, divisions=29, suffix='fps', expand=True, pref=stable_animation_prefs, key='fps', tooltip="The FPS to extract from the init video clip.")
    num_inference_row = SliderRow(label="Number of Inference Steps", min=1, max=150, divisions=149, pref=stable_animation_prefs, key='num_inference_steps', on_change=lambda e:changed(e,'num_inference_steps'), tooltip="The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.")
    guidance = SliderRow(label="Guidance Scale", min=0, max=50, divisions=100, round=1, pref=stable_animation_prefs, key='guidance_scale')
    #eta_slider = SliderRow(label="ETA", min=0, max=1.0, divisions=20, round=1, pref=stable_animation_prefs, key='eta', tooltip="The weight of noise for added noise in a diffusion step. Its value is between 0.0 and 1.0 - 0.0 is DDIM and 1.0 is DDPM scheduler respectively.")
    model_checkpoint = Dropdown(label="Model Checkpoint", hint_text="", width=270, options=[dropdown.Option("stable-diffusion-xl-beta-v2-2-2"), dropdown.Option("stable-diffusion-768-v2-1"), dropdown.Option("stable-diffusion-512-v2-1"), dropdown.Option("stable-diffusion-768-v2-0"), dropdown.Option("stable-diffusion-512-v2-0"), dropdown.Option("stable-diffusion-v1-5"), dropdown.Option("stable-diffusion-v1"), dropdown.Option("stable-inpainting-512-v2-0"), dropdown.Option("stable-inpainting-v1-0")], value=stable_animation_prefs['model'], autofocus=False, on_change=lambda e:changed(e, 'model'))
    generation_sampler = Dropdown(label="Generation Sampler", hint_text="", width=230, options=[dropdown.Option("DDIM"), dropdown.Option("PLMS"), dropdown.Option("K_euler"), dropdown.Option("K_euler_ancestral"), dropdown.Option("K_heun"), dropdown.Option("K_dpmpp_2m"), dropdown.Option("K_dpm_2_ancestral"), dropdown.Option("K_lms"), dropdown.Option("K_dpmpp_2s_ancestral"), dropdown.Option("K_dpm_2")], value=stable_animation_prefs['sampler'], autofocus=False, on_change=lambda e:changed(e, 'sampler'))
    #DDIM, PLMS, K_euler, K_euler_ancestral, K_heun, K_dpm_2, K_dpm_2_ancestral, K_lms, K_dpmpp_2m, K_dpmpp_2s_ancestral
    style_preset = Dropdown(label="Style Preset", hint_text="", width=240, options=[dropdown.Option("None"), dropdown.Option("3d-model"), dropdown.Option("analog-film"), dropdown.Option("anime"), dropdown.Option("cinematic"), dropdown.Option("comic-book"), dropdown.Option("digital-art"), dropdown.Option("enhance fantasy-art"), dropdown.Option("isometric"), dropdown.Option("line-art"), dropdown.Option("low-poly"), dropdown.Option("modeling-compound"), dropdown.Option("neon-punk"), dropdown.Option("origami"), dropdown.Option("photographic"), dropdown.Option("pixel-art")], value=stable_animation_prefs['style_preset'], autofocus=False, on_change=lambda e:changed(e, 'style_preset'))
    clip_guidance = Dropdown(label="Clip Guidance", hint_text="", width=240, options=[dropdown.Option("None"), dropdown.Option("Simple"), dropdown.Option("FastBlue"), dropdown.Option("FastGreen")], value=stable_animation_prefs['clip_guidance'], autofocus=False, on_change=lambda e:changed(e, 'clip_guidance'))
    steps_strength_adj = Checkbox(label="Steps Strength Adjustment", tooltip="Adjusts number of diffusion steps based on current previous frame strength value.", value=stable_animation_prefs['steps_strength_adj'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'steps_strength_adj'), col={'xs':12, 'md':6, 'lg':4, 'xl':3})
    interpolate_prompts = Checkbox(label="Interpolate Prompts", tooltip="Smoothly interpolate prompts between keyframes.", value=stable_animation_prefs['interpolate_prompts'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'interpolate_prompts'), col={'xs':12, 'md':6, 'lg':4, 'xl':3})
    locked_seed = Checkbox(label="Locked Seed", tooltip="Keep the same seed for all frames.", value=stable_animation_prefs['locked_seed'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'locked_seed'), col={'xs':12, 'md':6, 'lg':4, 'xl':3})
    noise_add_curve  = TextField(label="Noise Add Curve", value=stable_animation_prefs['noise_add_curve'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, multiline=True, on_change=lambda e:changed(e,'noise_add_curve'))
    noise_scale_curve  = TextField(label="Noise Scale Curve", value=stable_animation_prefs['noise_scale_curve'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, multiline=True, on_change=lambda e:changed(e,'noise_scale_curve'))
    strength_curve  = TextField(label="Strength Curve", value=stable_animation_prefs['strength_curve'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, multiline=True, on_change=lambda e:changed(e,'strength_curve'))
    diffusion_cadence_curve  = TextField(label="Diffusion Cadence Curve", hint_text="One greater than the number of frames between diffusion operations. A cadence of 1 performs diffusion on each frame. Values greater than one will generate frames using interpolation methods.", value=stable_animation_prefs['diffusion_cadence_curve'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, multiline=True, on_change=lambda e:changed(e,'diffusion_cadence_curve'))
    cadence_interp = Dropdown(label="Cadence Interpolation", hint_text="", width=200, options=[dropdown.Option("Mix"), dropdown.Option("RIFE"), dropdown.Option("VAE-LERP"), dropdown.Option("VAE-SLERP")], value=stable_animation_prefs['cadence_interp'], autofocus=False, on_change=lambda e:changed(e, 'cadence_interp'))
    cadence_spans = Checkbox(label="Cadence Spans", tooltip="Experimental diffusion cadence mode for better outpainting", value=stable_animation_prefs['cadence_spans'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'cadence_spans'))
    inpaint_border = Checkbox(label="Inpaint Border", tooltip="Use inpainting on top of border regions for 2D and 3D warp modes.", value=stable_animation_prefs['inpaint_border'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'inpaint_border'))
    border = Dropdown(label="Border", hint_text="Method that will be used to fill empty regions, e.g. after a rotation transform.", width=200, options=[dropdown.Option("reflect"), dropdown.Option("replicate"), dropdown.Option("wrap"), dropdown.Option("zero"), dropdown.Option("prefill")], value=stable_animation_prefs['border'], autofocus=False, on_change=lambda e:changed(e, 'border'))
    use_inpainting_model = Checkbox(label="Use Inpainting Model", tooltip="", value=stable_animation_prefs['use_inpainting_model'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'use_inpainting_model'))
    mask_min_value  = TextField(label="Mask Minimum", value=stable_animation_prefs['mask_min_value'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, multiline=True, on_change=lambda e:changed(e,'mask_min_value'))
    mask_binarization_thr = SliderRow(label="Mask Binarization Threshold", min=0, max=1.0, divisions=20, round=1, pref=stable_animation_prefs, key='mask_binarization_thr', expand=True, tooltip="Grayscale mask values lower than this value will be set to 0, values that are higher — to 1.")
    color_coherence = Dropdown(label="Color Coherance", hint_text="Color space that will be used for inter-frame color adjustments.", width=350, col={'xs':12, 'md':6, 'lg':4, 'xl':3}, options=[dropdown.Option("None"), dropdown.Option("HSV"), dropdown.Option("LAB"), dropdown.Option("RGB")], value=stable_animation_prefs['color_coherence'], autofocus=False, on_change=lambda e:changed(e, 'color_coherence'))
    brightness_curve  = TextField(label="Brightness Curve", value=stable_animation_prefs['brightness_curve'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, multiline=True, on_change=lambda e:changed(e,'brightness_curve'))
    contrast_curve  = TextField(label="Contrast Curve", value=stable_animation_prefs['contrast_curve'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, multiline=True, on_change=lambda e:changed(e,'contrast_curve'))
    hue_curve  = TextField(label="Hue Curve", value=stable_animation_prefs['hue_curve'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, multiline=True, on_change=lambda e:changed(e,'hue_curve'))
    saturation_curve  = TextField(label="Saturation Curve", value=stable_animation_prefs['saturation_curve'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, multiline=True, on_change=lambda e:changed(e,'saturation_curve'))
    lightness_curve  = TextField(label="Lightness Curve", value=stable_animation_prefs['lightness_curve'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, multiline=True, on_change=lambda e:changed(e,'lightness_curve'))
    color_match_animate = Checkbox(label="Color Match Animate", tooltip="Animate color match between key frames.", value=stable_animation_prefs['color_match_animate'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'color_match_animate'))
    depth_model_weight = SliderRow(label="Depth Model Weight", min=0, max=1.0, divisions=20, round=1, pref=stable_animation_prefs, key='depth_model_weight', expand=True, tooltip="Blend factor between AdaBins and MiDaS depth models.")
    fov_curve  = TextField(label="FOV Curve", hint_text="FOV angle of camera volume in degrees. Max 180.", value=stable_animation_prefs['fov_curve'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, multiline=True, on_change=lambda e:changed(e,'fov_curve'))
    depth_blur_curve  = TextField(label="Depth Blur Curve", hint_text="", value=stable_animation_prefs['depth_blur_curve'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, multiline=True, on_change=lambda e:changed(e,'depth_blur_curve'))
    depth_warp_curve  = TextField(label="Depth Warp Curve", hint_text="", value=stable_animation_prefs['depth_warp_curve'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, multiline=True, on_change=lambda e:changed(e,'depth_warp_curve'))
    translation_x  = TextField(label="Translation X", hint_text="", value=stable_animation_prefs['translation_x'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, multiline=True, on_change=lambda e:changed(e,'translation_x'))
    translation_y  = TextField(label="Translation Y", hint_text="", value=stable_animation_prefs['translation_y'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, multiline=True, on_change=lambda e:changed(e,'translation_y'))
    translation_z  = TextField(label="Translation Z", hint_text="", value=stable_animation_prefs['translation_z'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, multiline=True, on_change=lambda e:changed(e,'translation_z'))
    angle  = TextField(label="Angle", hint_text="", value=stable_animation_prefs['angle'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, multiline=True, on_change=lambda e:changed(e,'angle'))
    zoom  = TextField(label="Zoom", hint_text="", value=stable_animation_prefs['zoom'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, multiline=True, on_change=lambda e:changed(e,'zoom'))
    rotation_x  = TextField(label="Rotation X", hint_text="", value=stable_animation_prefs['rotation_x'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, multiline=True, on_change=lambda e:changed(e,'rotation_x'))
    rotation_y  = TextField(label="Rotation Y", hint_text="", value=stable_animation_prefs['rotation_y'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, multiline=True, on_change=lambda e:changed(e,'rotation_y'))
    rotation_z  = TextField(label="Rotation Z", hint_text="", value=stable_animation_prefs['rotation_z'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, multiline=True, on_change=lambda e:changed(e,'rotation_z'))
    camera_type = Dropdown(label="Camera Type", hint_text="", width=200, options=[dropdown.Option("Perspective"), dropdown.Option("Orthographic")], value=stable_animation_prefs['camera_type'], autofocus=False, on_change=lambda e:changed(e, 'camera_type'))
    render_mode = Dropdown(label="Render Mode", hint_text="", width=200, options=[dropdown.Option("Mesh"), dropdown.Option("Pointcloud")], value=stable_animation_prefs['render_mode'], autofocus=False, col={'xs':12, 'md':6, 'lg':4, 'xl':3}, on_change=lambda e:changed(e, 'render_mode'))
    mask_power = SliderRow(label="Mesh Power", min=0, max=4.0, divisions=80, round=1, expand=True, pref=stable_animation_prefs, key='mask_power', tooltip="")
    init_image = TextField(label="Initial Image", value=stable_animation_prefs['init_image'], on_change=lambda e:changed(e,'init_image'), expand=True, height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_init))
    init_sizing = Dropdown(label="Init Sizing", hint_text="", width=200, options=[dropdown.Option("Cover"), dropdown.Option("Stretch"), dropdown.Option("Resize-Canvas")], value=stable_animation_prefs['init_sizing'], autofocus=False, on_change=lambda e:changed(e, 'init_sizing'))
    mask_image = TextField(label="Mask Image", value=stable_animation_prefs['mask_image'], on_change=lambda e:changed(e,'mask_image'), expand=True, height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_mask))
    mask_invert = Checkbox(label="Invert Mask", tooltip="White in mask marks areas to change by default.", value=stable_animation_prefs['mask_invert'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'mask_invert'))
    video_mix_in_curve  = TextField(label="Video Mixin Curve", hint_text="", value=stable_animation_prefs['video_mix_in_curve'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, multiline=True, on_change=lambda e:changed(e,'video_mix_in_curve'))
    animation_mode = Dropdown(label="Animation Mode", hint_text="", width=200, options=[dropdown.Option("2D"), dropdown.Option("3D warp"), dropdown.Option("3D render"), dropdown.Option("Video Input")], value=stable_animation_prefs['animation_mode'], autofocus=False, on_change=lambda e:changed(e, 'animation_mode'))
    init_video = TextField(label="Init Video File", value=stable_animation_prefs['video_init_path'], on_change=lambda e:changed(e,'video_init_path'), expand=True, height=60, suffix=IconButton(icon=icons.VIDEO_CALL, on_click=pick_video))
    video_mix_in_curve  = TextField(label="Video Mixin Curve", hint_text="", value=stable_animation_prefs['video_mix_in_curve'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, multiline=True, on_change=lambda e:changed(e,'video_mix_in_curve'))
    video_flow_warp = Checkbox(label="Video Flow Warp", tooltip="Whether or not to transfer the optical flow from the video to the generated animation as a warp effect.", value=stable_animation_prefs['video_flow_warp'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'video_flow_warp'))
    extract_nth_frame  = TextField(label="Extract Nth Frame", width=200, hint_text="Only use every Nth frame of the video", value=stable_animation_prefs['extract_nth_frame'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, on_change=lambda e:changed(e,'extract_nth_frame', ptype='int'))
    video_init_fps  = TextField(label="Init Video FPS", width=200, hint_text="", value=stable_animation_prefs['video_init_fps'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, on_change=lambda e:changed(e,'video_init_fps', ptype='int'))
    output_fps  = TextField(label="Output FPS", width=200, hint_text="Frame rate to use when generating video output.", value=stable_animation_prefs['output_fps'], col={'xs':12, 'md':6, 'lg':4, 'xl':3}, on_change=lambda e:changed(e,'output_fps', ptype='int'))
    frame_interpolation_mode = Dropdown(label="Frame Interpolation Mode", hint_text="", width=200, options=[dropdown.Option("Rife"), dropdown.Option("Film"), dropdown.Option("None")], value=stable_animation_prefs['frame_interpolation_mode'], autofocus=False, on_change=lambda e:changed(e, 'frame_interpolation_mode'))
    frame_interpolation_factor = Dropdown(label="Frame Interpolation Factor", hint_text="", width=200, options=[dropdown.Option("2"), dropdown.Option("4"), dropdown.Option("8")], value=stable_animation_prefs['frame_interpolation_factor'], autofocus=False, on_change=lambda e:changed(e, 'frame_interpolation_factor'))
    width_slider = SliderRow(label="Width", min=256, max=1024, divisions=6, multiple=64, suffix="px", pref=stable_animation_prefs, key='width')
    height_slider = SliderRow(label="Height", min=256, max=1024, divisions=6, multiple=64, suffix="px", pref=stable_animation_prefs, key='height')
    export_to_video = Tooltip(message="Save mp4 file along with Image Sequence", content=Switcher(label="Export to Video", value=stable_animation_prefs['export_to_video'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'export_to_video')))
    #lower_memory = Tooltip(message="Enable CPU offloading, VAE Tiling & Stitching", content=Switcher(label="Lower Memory Mode", value=stable_animation_prefs['lower_memory'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'lower_memory')))
    seed = TextField(label="Seed", width=90, value=str(stable_animation_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    resume = Switcher(label="Resume Previous Animation", value=stable_animation_prefs['resume'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_resume)
    resume_from = TextField(label="Resume from Frame", width=150, value=str(stable_animation_prefs['resume_from']), visible=stable_animation_prefs['resume'], keyboard_type=KeyboardType.NUMBER, tooltip="-1 resumes from last frame", on_change=lambda e:changed(e,'seed', ptype='int'))
    batch_folder_name = TextField(label="Animation Folder Name", value=stable_animation_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    apply_ESRGAN_upscale = Switcher(label="Apply ESRGAN Upscale", value=stable_animation_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)
    enlarge_scale_slider = SliderRow(label="Enlarge Scale", min=1, max=4, divisions=6, round=1, suffix="x", pref=stable_animation_prefs, key='enlarge_scale')
    display_upscaled_image = Checkbox(label="Display Upscaled Image", value=stable_animation_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))
    ESRGAN_settings = Container(Column([enlarge_scale_slider, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    page.ESRGAN_block_stable_animation = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    page.ESRGAN_block_stable_animation.height = None if status['installed_ESRGAN'] else 0
    page.stable_animation_output = Column([], scroll=ScrollMode.AUTO, auto_scroll=False)
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.stable_animation_output.controls) > 0
    animation_prompts = Column([], spacing=0)
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🥏  Stable Animation SDK", "Use Stability.ai API Credits for Advanced Video Generation, similar to Deforum & Disco Diffusion.", actions=[copy_preset_button, paste_preset_button, default_preset_button, IconButton(icon=icons.HELP, tooltip="Help with Stable Animation Settings", on_click=stable_animation_help)]),
        #ResponsiveRow([animation_prompt, negative_prompt], vertical_alignment=CrossAxisAlignment.START),
        Row([frame, prompt, add_prompt_keyframe]),
        animation_prompts,
        Divider(thickness=2, height=4),
        Row([negative_prompt, negative_prompt_weight]),
        Row([max_frames, fps]),
        num_inference_row,
        guidance,
        Row([model_checkpoint, generation_sampler, style_preset, clip_guidance], wrap=True),
        ResponsiveRow([steps_strength_adj, interpolate_prompts, locked_seed]),
        ResponsiveRow([noise_add_curve, noise_scale_curve, strength_curve, diffusion_cadence_curve]),
        Row([cadence_interp, cadence_spans]),
        Row([border, inpaint_border, use_inpainting_model]),
        Row([mask_min_value, mask_binarization_thr]),
        ResponsiveRow([color_coherence, brightness_curve, contrast_curve, hue_curve, saturation_curve, lightness_curve]),
        Row([color_match_animate, depth_model_weight]),
        ResponsiveRow([fov_curve, depth_blur_curve, depth_warp_curve]),
        ResponsiveRow([translation_x, translation_y, translation_z, zoom]),
        ResponsiveRow([rotation_x, rotation_y, rotation_z, angle]),
        Row([camera_type, render_mode, mask_power]),
        Row([init_image, init_sizing]),
        Row([mask_image, mask_invert]),
        Row([init_video, animation_mode]),
        Row([video_flow_warp, video_mix_in_curve]),
        Row([extract_nth_frame, video_init_fps, output_fps, Row([resume, resume_from])], wrap=True),
        Row([export_to_video, frame_interpolation_mode, frame_interpolation_factor]),
        width_slider, height_slider,
        page.ESRGAN_block_stable_animation,
        Row([seed, batch_folder_name]),
        Row([
            ElevatedButton(content=Text("🧌  Run Stable Animation", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_stable_animation(page)),
             #ElevatedButton(content=Text(value="📜   Run from Prompts List", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_stable_animation(page, from_list=True))
        ]),
        page.stable_animation_output,
        clear_button,
      ]
    ))], scroll=ScrollMode.AUTO, auto_scroll=False)
    return c

roop_prefs = {
    'source_image': '',
    'target_image': '',
    'frame_processor': 'face_swapper',
    'keep_fps': True,
    'keep_audio': True,
    'keep_frames': True,
    'many_faces': False,
    'video_encoder': 'libx264',
    'video_quality': 18,
    'output_name': '',
    'max_size': 768,
    'num_images': 1,
    'batch_folder_name': '',
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": 2.0,
    "display_upscaled_image": False,
}
def buildROOP(page):
    global roop_prefs, prefs
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          if ptype == "int":
            roop_prefs[pref] = int(e.control.value)
          elif ptype == "float":
            roop_prefs[pref] = float(e.control.value)
          else:
            roop_prefs[pref] = e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def add_to_roop_output(o):
      page.roop_output.controls.append(o)
      page.roop_output.update()
      if not clear_button.visible:
        clear_button.visible = True
        clear_button.update()
    def clear_output(e):
      if prefs['enable_sounds']: page.snd_delete.play()
      page.roop_output.controls = []
      page.roop_output.update()
      clear_button.visible = False
      clear_button.update()
    def roop_help(e):
      def close_roop_dlg(e):
        nonlocal roop_help_dlg
        roop_help_dlg.open = False
        page.update()
      roop_help_dlg = AlertDialog(title=Text("💁   Help with ROOP Face Swap"), content=Column([
          Text("Choose a face (image with desired face) and the target image/video (image/video in which you want to replace the face) and click on Start. Open file explorer and navigate to the directory you select your output to be in. You will find a directory named <video_title> where you can see the frames being swapped in realtime. Once the processing is done, it will create the output file. That's it."),
          Text("Roop is such a powerful tool that can be used for many purposes, so it’s crucial to know the potential risks of using Roop. Roop Deepfake is an experimental project that aims to make deep fake technology more accessible and easy to use. It uses a library called insightface and some models to detect and replace faces. You can also use GPU acceleration to speed up the process."),
          Text("Disclaimer: This software is meant to be a productive contribution to the rapidly growing AI-generated media industry. It will help artists with tasks such as animating a custom character or using the character as a model for clothing etc."),
          Markdown(" [GitHub Page](https://github.com/s0md3v/roop) | [HuggingFace Space](https://huggingface.co/spaces/zhsso/roop) | [Colab](https://colab.research.google.com/drive/1uX5k33KNXprOeu_P9iov1byLOd4XGo1i#scrollTo=aN1XeEX_tsra)", on_tap_link=lambda e: e.page.launch_url(e.data)),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("👶  Promise not to abuse... ", on_click=close_roop_dlg)], actions_alignment=MainAxisAlignment.END)
      page.dialog = roop_help_dlg
      roop_help_dlg.open = True
      page.update()
    def file_picker_result(e: FilePickerResultEvent):
        if e.files != None:
          upload_files(e)
    def on_upload_progress(e: FilePickerUploadEvent):
      nonlocal pick_type
      if e.progress == 1:
        if not slash in e.file_name:
          fname = os.path.join(root_dir, e.file_name)
          roop_prefs['file_name'] = e.file_name.rpartition('.')[0]
        else:
          fname = e.file_name
          roop_prefs['file_name'] = e.file_name.rpartition(slash)[2].rpartition('.')[0]
        if pick_type == "source":
          source_image.value = fname
          source_image.update()
          roop_prefs['source_image'] = fname
        elif pick_type == "target":
          target_image.value = fname
          target_image.update()
          roop_prefs['target_image'] = fname
        page.update()
    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)
    def upload_files(e):
        uf = []
        if file_picker.result != None and file_picker.result.files != None:
            for f in file_picker.result.files:
              if page.web:
                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))
              else:
                on_upload_progress(FilePickerUploadEvent(f.path, 1, ""))
            file_picker.upload(uf)
    page.overlay.append(file_picker)
    pick_type = ""
    #page.overlay.append(pick_files_dialog)
    def pick_source(e):
        nonlocal pick_type
        pick_type = "source"
        file_picker.pick_files(allow_multiple=False, allowed_extensions=["png", "PNG", "jpg", "jpeg"], dialog_title="Pick Source Image File")
    def pick_target(e):
        nonlocal pick_type
        pick_type = "target"
        file_picker.pick_files(allow_multiple=False, allowed_extensions=["mp4", "avi", "png", "PNG", "jpg", "jpeg"], dialog_title="Pick Target Video or Image")
    def toggle_ESRGAN(e):
        ESRGAN_settings.height = None if e.control.value else 0
        roop_prefs['apply_ESRGAN_upscale'] = e.control.value
        ESRGAN_settings.update()
    source_image = TextField(label="Source Image of Face", value=roop_prefs['source_image'], on_change=lambda e:changed(e,'source_image'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_source))
    target_image = TextField(label="Target Video or Image", value=roop_prefs['target_image'], on_change=lambda e:changed(e,'target_image'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_target))
    keep_fps = Switcher(label="Keep FPS", value=roop_prefs['keep_fps'], on_change=lambda e:changed(e,'keep_fps'))
    keep_audio = Switcher(label="Keep Audio", value=roop_prefs['keep_audio'], on_change=lambda e:changed(e,'keep_audio'))
    keep_frames = Switcher(label="Keep Frames", value=roop_prefs['keep_frames'], on_change=lambda e:changed(e,'keep_frames'))
    many_faces = Switcher(label="Many Faces", value=roop_prefs['many_faces'], on_change=lambda e:changed(e,'many_faces'))
    frame_processor = Dropdown(label="Frame Processor", width=160, options=[dropdown.Option("face_swapper"), dropdown.Option("face_enhancer")], value=roop_prefs['frame_processor'], on_change=lambda e: changed(e, 'frame_processor'))
    video_encoder = Dropdown(label="Video Encoder", width=160, options=[dropdown.Option("libx264"), dropdown.Option("libx265"), dropdown.Option("libvpx-vp9")], value=roop_prefs['video_encoder'], on_change=lambda e: changed(e, 'video_encoder'))
    video_quality = SliderRow(label="Video Quality", min=0, max=50, divisions=49, pref=roop_prefs, key='video_quality')
    max_row = SliderRow(label="Max Image Size", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=roop_prefs, key='max_size')
    output_name = TextField(label="Output File Name", value=roop_prefs['output_name'], on_change=lambda e:changed(e,'output_name'))
    batch_folder_name = TextField(label="Batch Folder Name", value=roop_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    num_images = NumberPicker(label="Number of Images: ", min=1, max=8, value=roop_prefs['num_images'], on_change=lambda e: changed(e, 'num_images'))
    apply_ESRGAN_upscale = Switcher(label="Apply ESRGAN Upscale", value=roop_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)
    enlarge_scale_slider = SliderRow(label="Enlarge Scale", min=1, max=4, divisions=6, round=1, suffix="x", pref=roop_prefs, key='enlarge_scale')
    display_upscaled_image = Checkbox(label="Display Upscaled Image", value=roop_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))
    ESRGAN_settings = Container(Column([enlarge_scale_slider, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    page.ESRGAN_block_roop = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    page.ESRGAN_block_roop.height = None if status['installed_ESRGAN'] else 0
    page.roop_output = Column([])
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.roop_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🎭  ROOP Face Swapper", "Take a Video or Image and Replace the Face in it with a face of your choice, no dataset, no training needed...", actions=[IconButton(icon=icons.HELP, tooltip="Help with ROOP", on_click=roop_help)]),
        #ResponsiveRow([Row([source_image, alpha_mask], col={'lg':6}), Row([mask_image, invert_mask], col={'lg':6})]),
        source_image,
        target_image,
        ResponsiveRow([Column([keep_fps, keep_frames, frame_processor], col={'md':6, 'lg':4, 'xl':3}), Column([keep_audio, many_faces, video_encoder], col={'md':6, 'lg':4, 'xl':3})]),
        video_quality,
        max_row,
        Row([output_name, batch_folder_name]),
        page.ESRGAN_block_roop,
        #Row([jump_length, jump_n_sample, seed]),
        ElevatedButton(content=Text("😷  Run ROOP Swap", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_roop(page)),
        page.roop_output,
        #clear_button,
      ]
    ))], scroll=ScrollMode.AUTO, auto_scroll=False)
    return c

animate_diff_prefs = {
    'prompt': '',
    'negative_prompt': '',
    'steps': 25,
    'guidance_scale': 7.5,
    'editing_prompts': [],
    'dreambooth_lora': 'realisticVisionV40_v20Novae',
    'lora_alpha': 0.8,
    'custom_lora': '',
    'motion_module': 'mm_sd_v15',
    'scheduler': 'k_dpmpp_2m',
    'seed': 0,
    'video_length': 16,
    'width': 512,
    'height': 512,
    'overlap': 12,
    'stride': 4,
    'context': 16,
    'clip_skip': 1,
    'save_frames': True,
    'save_gif': True,
    'save_video': False,
    'num_images': 1,
    'batch_folder_name': '',
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": 2.0,
    "display_upscaled_image": False,
}

animate_diff_loras = [
    {'name': 'toonyou_beta3', 'file': 'toonyou_beta3.safetensors', 'path': 'https://huggingface.co/camenduru/AnimateDiff/resolve/main/toonyou_beta3.safetensors'},
    {'name': 'CounterfeitV30_v30', 'file': 'CounterfeitV30_v30.safetensors', 'path': 'https://huggingface.co/camenduru/AnimateDiff/resolve/main/CounterfeitV30_v30.safetensors'},
    {'name': 'FilmVelvia2', 'file': 'FilmVelvia2.safetensors', 'path': 'https://huggingface.co/camenduru/AnimateDiff/resolve/main/FilmVelvia2.safetensors'},
    {'name': 'Pyramid lora_Ghibli_n3', 'file': 'Pyramid%20lora_Ghibli_n3.safetensors', 'path': 'https://huggingface.co/camenduru/AnimateDiff/resolve/main/Pyramid%20lora_Ghibli_n3.safetensors'},
    {'name': 'TUSUN', 'file': 'TUSUN.safetensors', 'path': 'https://huggingface.co/camenduru/AnimateDiff/resolve/main/TUSUN.safetensors'},
    {'name': 'lyriel_v16', 'file': 'lyriel_v16.safetensors', 'path': 'https://huggingface.co/camenduru/AnimateDiff/resolve/main/lyriel_v16.safetensors'},
    {'name': 'majicmixRealistic_v5Preview', 'file': 'majicmixRealistic_v5Preview.safetensors', 'path': 'https://huggingface.co/camenduru/AnimateDiff/resolve/main/majicmixRealistic_v5Preview.safetensors'},
    {'name': 'moonfilm_filmGrain10', 'file': 'moonfilm_filmGrain10.safetensors', 'path': 'https://huggingface.co/camenduru/AnimateDiff/resolve/main/moonfilm_filmGrain10.safetensors'},
    {'name': 'moonfilm_reality20', 'file': 'moonfilm_reality20.safetensors', 'path': 'https://huggingface.co/camenduru/AnimateDiff/resolve/main/moonfilm_reality20.safetensors'},
    {'name': 'rcnzCartoon3d_v10', 'file': 'rcnzCartoon3d_v10.safetensors', 'path': 'https://huggingface.co/camenduru/AnimateDiff/resolve/main/rcnzCartoon3d_v10.safetensors'},
    {'name': 'realisticVisionV40_v20Novae', 'file': 'realisticVisionV40_v20Novae.safetensors', 'path': 'https://huggingface.co/camenduru/AnimateDiff/resolve/main/realisticVisionV40_v20Novae.safetensors'},
    #{'name': 'Custom', 'file': '', 'path': ''},
]
def buildAnimateDiff(page):
    global animate_diff_prefs, prefs, pipe_animate_diff, editing_prompt
    editing_prompt = {'prompt':'', 'negative_prompt':'', 'seed':0}
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          if ptype == "int":
            animate_diff_prefs[pref] = int(e.control.value)
          elif ptype == "float":
            animate_diff_prefs[pref] = float(e.control.value)
          else:
            animate_diff_prefs[pref] = e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def add_to_animate_diff_output(o):
      page.animate_diff_output.controls.append(o)
      page.animate_diff_output.update()
      if not clear_button.visible:
        clear_button.visible = True
        clear_button.update()
    def clear_output(e):
      if prefs['enable_sounds']: page.snd_delete.play()
      page.animate_diff_output.controls = []
      page.animate_diff_output.update()
      clear_button.visible = False
      clear_button.update()
    def animate_diff_help(e):
      def close_animate_diff_dlg(e):
        nonlocal animate_diff_help_dlg
        animate_diff_help_dlg.open = False
        page.update()
      animate_diff_help_dlg = AlertDialog(title=Text("💁   Help with AnimateDiff"), content=Column([
          Text("With the advance of text-to-image models (e.g., Stable Diffusion) and corresponding personalization techniques such as DreamBooth and LoRA, everyone can manifest their imagination into high-quality images at an affordable cost. Subsequently, there is a great demand for image animation techniques to further combine generated static images with motion dynamics. In this report, we propose a practical framework to animate most of the existing personalized text-to-image models once and for all, saving efforts in model-specific tuning. At the core of the proposed framework is to insert a newly initialized motion modeling module into the frozen text-to-image model and train it on video clips to distill reasonable motion priors. Once trained, by simply injecting this motion modeling module, all personalized versions derived from the same base T2I readily become text-driven models that produce diverse and personalized animated images. We conduct our evaluation on several public representative personalized text-to-image models across anime pictures and realistic photographs, and demonstrate that our proposed framework helps these models generate temporally smooth animation clips while preserving the domain and diversity of their outputs."),
          Text("Credits: Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu Qiao, Dahua Lin, Bo Dai. Also Neggles for refactoring, Camenduru and UI by Alan Bedian"),
          Markdown("[Neggles GitHub Code](https://github.com/neggles/animatediff-cli) - [Original GitHub Code](https://github.com/guoyww/animatediff/) - [Project Page](https://animatediff.github.io/) - [Arxiv Paper](https://arxiv.org/abs/2307.04725)", on_tap_link=lambda e: e.page.launch_url(e.data)),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("🧞  Make a Wish... ", on_click=close_animate_diff_dlg)], actions_alignment=MainAxisAlignment.END)
      page.dialog = animate_diff_help_dlg
      animate_diff_help_dlg.open = True
      page.update()
    def changed_lora(e):
      animate_diff_prefs['dreambooth_lora'] = e.control.value
      custom_lora.visible = e.control.value == "Custom"
      custom_lora.update()
    def toggle_ESRGAN(e):
        ESRGAN_settings.height = None if e.control.value else 0
        animate_diff_prefs['apply_ESRGAN_upscale'] = e.control.value
        ESRGAN_settings.update()
    def animate_diff_tile(animate_prompt):
        params = []
        for k, v in animate_prompt.items():
            if k == 'prompt': continue
            params.append(f'{to_title(k)}: {v}')
        sub = ', '.join(params)
        return ListTile(title=Text(animate_prompt['prompt'], max_lines=6, style=TextThemeStyle.BODY_LARGE), subtitle=Text(sub), dense=True, data=animate_prompt, trailing=PopupMenuButton(icon=icons.MORE_VERT,
          items=[PopupMenuItem(icon=icons.EDIT, text="Edit Animation Prompt", on_click=lambda e: edit_animate_diff(animate_prompt), data=animate_prompt),
                 PopupMenuItem(icon=icons.DELETE, text="Delete Animation Prompt", on_click=lambda e: del_animate_diff(animate_prompt), data=animate_prompt)]), on_click=lambda e: edit_animate_diff(animate_prompt))
    def edit_animate_diff(edit=None):
        animate_diff_prompt = edit if bool(edit) else editing_prompt.copy()
        edit_prompt = edit['prompt'] if bool(edit) else animate_diff_prefs['prompt']
        #negative_prompt = edit['negative_prompt'] if bool(edit) else animate_diff_prefs['negative_prompt']
        if not bool(edit):
            animate_diff_prompt['prompt'] = animate_diff_prefs['prompt']
            animate_diff_prompt['negative_prompt'] = animate_diff_prefs['negative_prompt']
            animate_diff_prompt['seed'] = animate_diff_prefs['seed']
        def close_dlg(e):
            dlg_edit.open = False
            page.update()
        def changed_p(e, pref=None):
            if pref is not None:
                animate_diff_prompt[pref] = e.control.value
        def save_animate_diff_prompt(e):
            if edit == None:
                animate_diff_prefs['editing_prompts'].append(animate_diff_prompt)
                page.animate_diff_prompts.controls.append(animate_diff_tile(animate_diff_prompt))
                page.animate_diff_prompts.update()
            else:
                for s in animate_diff_prefs['editing_prompts']:
                    if s['prompt'] == edit_prompt:
                        s = animate_diff_prompt
                        break
                for t in page.animate_diff_prompts.controls:
                    #print(f"{t.data['prompt']} == {edit_prompt} - {t.title.value}")
                    if t.title.value == edit_prompt: #t.data['prompt']
                        params = []
                        for k, v in animate_diff_prompt.items():
                            if k == 'prompt': continue
                            params.append(f'{to_title(k)}: {v}')
                        sub = ', '.join(params)
                        t.title = Text(animate_diff_prompt['prompt'], max_lines=6, style=TextThemeStyle.BODY_LARGE)
                        t.subtitle = Text(sub)
                        t.data = animate_diff_prompt
                        t.update()
                        break
                dlg_edit.open = False
                e.control.update()
                page.update()
        animate_diff_editing_prompt = TextField(label="AnimateDiff Prompt Modifier", value=animate_diff_prompt['prompt'], autofocus=True, on_change=lambda e:changed_p(e,'prompt'))
        animate_diff_negative_prompt = TextField(label="Negative Prompt", value=animate_diff_prompt['negative_prompt'], autofocus=True, on_change=lambda e:changed_p(e,'negative_prompt'))
        animate_diff_seed = TextField(label="Seed", width=90, value=str(animate_diff_prompt['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed_p(e,'seed'), col={'md':1})
        #reverse_editing_direction = Checkbox(label="Reverse Editing Direction", value=animate_diff_prompt['reverse_editing_direction'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed_p(e,'reverse_editing_direction'), tooltip="Whether the corresponding prompt in `editing_prompt` should be increased or decreased.")
        if edit != None:
            dlg_edit = AlertDialog(modal=False, title=Text(f"♟️ {'Edit' if bool(edit) else 'Add'} Animated Prompt"), content=Container(Column([
                animate_diff_editing_prompt,
                animate_diff_negative_prompt, animate_diff_seed,
            ], alignment=MainAxisAlignment.START, tight=True, scroll=ScrollMode.AUTO), width=(page.width if page.web else page.window_width) - 180), actions=[TextButton(content=Text("Cancel", size=18), on_click=close_dlg), ElevatedButton(content=Text(value=emojize(":floppy_disk:") + "  Save Prompt ", size=19, weight=FontWeight.BOLD), on_click=save_animate_diff_prompt)], actions_alignment=MainAxisAlignment.END)
            page.dialog = dlg_edit
            dlg_edit.open = True
            page.update()
        else:
            save_animate_diff_prompt(None)
    def del_animate_diff(edit=None):
        for s in animate_diff_prefs['editing_prompts']:
            if s['prompt'] == edit['prompt']:
                animate_diff_prefs['editing_prompts'].remove(s)
                break
        for t in page.animate_diff_prompts.controls:
            if t.data['prompt'] == edit['prompt']:
                page.animate_diff_prompts.controls.remove(t)
                break
        page.animate_diff_prompts.update()
        if prefs['enable_sounds']: page.snd_delete.play()
    def clear_animate_diff_prompts(e):
        animate_diff_prefs['editing_prompts'].clear()
        page.animate_diff_prompts.controls.clear()
        page.animate_diff_prompts.update()
        if prefs['enable_sounds']: page.snd_delete.play()
    prompt = TextField(label="Animation Prompt Text", value=animate_diff_prefs['prompt'], filled=True, col={'md': 8}, multiline=True, on_change=lambda e:changed(e,'prompt'))
    negative_prompt  = TextField(label="Negative Prompt Text", value=animate_diff_prefs['negative_prompt'], filled=True, col={'md':3}, multiline=True, on_change=lambda e:changed(e,'negative_prompt'))
    seed = TextField(label="Seed", width=90, value=str(animate_diff_prefs['seed']), keyboard_type=KeyboardType.NUMBER, filled=True, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'), col={'md':1})
    video_length = SliderRow(label="Video Length", min=1, max=500, divisions=499, pref=animate_diff_prefs, key='video_length', tooltip="The number of frames to animate.")
    num_inference_row = SliderRow(label="Number of Inference Steps", min=1, max=150, divisions=149, pref=animate_diff_prefs, key='steps', tooltip="The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.")
    guidance = SliderRow(label="Guidance Scale", min=0, max=50, divisions=100, round=1, pref=animate_diff_prefs, key='guidance_scale')
    context = SliderRow(label="Context Frames to Condition", min=1, max=24, divisions=23, pref=animate_diff_prefs, key='context', expand=True, col={'md': 6}, tooltip="Number of frames to condition on. Drop to 8 on cards with less than 8GB VRAM, can raise it to 20-24 on cards with more. (default: max of <length> or 24)")
    stride = SliderRow(label="Max Motion Stride", min=1, max=8, divisions=7, pref=animate_diff_prefs, key='stride', expand=True, col={'md': 6}, tooltip="Max motion stride as a power of 2 (default: 4)")
    clip_skip = SliderRow(label="Clip Skip", min=0, max=4, divisions=4, pref=animate_diff_prefs, key='clip_skip', expand=True, col={'md': 6}, tooltip="Skips part of the image generation process, leading to slightly different results from the CLIP model.")
    width_slider = SliderRow(label="Width", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=animate_diff_prefs, key='width')
    height_slider = SliderRow(label="Height", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=animate_diff_prefs, key='height')
    scheduler = Dropdown(label="Scheduler", options=[dropdown.Option("ddim"), dropdown.Option("pndm"), dropdown.Option("lms"), dropdown.Option("euler"), dropdown.Option("euler_a"), dropdown.Option("dpm_2"), dropdown.Option("k_dpm_2"), dropdown.Option("dpm_2_a"), dropdown.Option("k_dpm_2_a"), dropdown.Option("dpmpp_2m"), dropdown.Option("k_dpmpp_2m"), dropdown.Option("unipc"), dropdown.Option("dpmpp_sde"), dropdown.Option("k_dpmpp_sde"), dropdown.Option("dpmpp_2m_sde"), dropdown.Option("k_dpmpp_2m_sde")], width=162, value=animate_diff_prefs['scheduler'], on_change=lambda e: changed(e, 'scheduler'))
    motion_module = Dropdown(label="Motion Module", options=[dropdown.Option("mm_sd_v15"), dropdown.Option("mm_sd_v14")], width=150, value=animate_diff_prefs['motion_module'], on_change=lambda e: changed(e, 'motion_module'))
    dreambooth_lora = Dropdown(label="DreamBooth LoRA", options=[dropdown.Option("Custom")], value=animate_diff_prefs['dreambooth_lora'], on_change=changed_lora)
    custom_lora = TextField(label="Custom LoRA Safetensor (URL or Path)", value=animate_diff_prefs['custom_lora'], visible=animate_diff_prefs['dreambooth_lora']=="Custom", on_change=lambda e:changed(e,'custom_lora'))
    for lora in animate_diff_loras:
        dreambooth_lora.options.insert(1, dropdown.Option(lora['name']))
    lora_alpha = SliderRow(label="LoRA Alpha", min=0, max=1, divisions=10, round=1, expand=True, pref=animate_diff_prefs, key='lora_alpha', tooltip="The Weight of the custom LoRA Model to influence diffusion.")
    save_frames = Switcher(label="Save Frames", value=animate_diff_prefs['save_frames'], on_change=lambda e:changed(e,'save_frames'))
    save_gif = Switcher(label="Save Animated GIF", value=animate_diff_prefs['save_gif'], on_change=lambda e:changed(e,'save_gif'))
    save_video = Switcher(label="Save Video", value=animate_diff_prefs['save_video'], on_change=lambda e:changed(e,'save_video'))
    batch_folder_name = TextField(label="Batch Folder Name", value=animate_diff_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    num_videos = NumberPicker(label="Number of Videos: ", min=1, max=8, value=animate_diff_prefs['num_images'], on_change=lambda e: changed(e, 'num_images'))
    apply_ESRGAN_upscale = Switcher(label="Apply ESRGAN Upscale", value=animate_diff_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)
    enlarge_scale_slider = SliderRow(label="Enlarge Scale", min=1, max=4, divisions=6, round=1, suffix="x", pref=animate_diff_prefs, key='enlarge_scale')
    display_upscaled_image = Checkbox(label="Display Upscaled Image", value=animate_diff_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))
    ESRGAN_settings = Container(Column([enlarge_scale_slider, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    page.ESRGAN_block_animate_diff = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    page.ESRGAN_block_animate_diff.height = None if status['installed_ESRGAN'] else 0
    page.animate_diff_prompts = Column([], spacing=0)
    page.animate_diff_output = Column([])
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.animate_diff_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("👫  AnimateDiff Text-to-Video", "Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning...", actions=[IconButton(icon=icons.HELP, tooltip="Help with AnimateDiff Settings", on_click=animate_diff_help)]),
        #ResponsiveRow([Row([original_image, alpha_mask], col={'lg':6}), Row([mask_image, invert_mask], col={'lg':6})]),
        ResponsiveRow([prompt, negative_prompt, seed]),
        Row([Text("AnimateDiff Prompts", style=TextThemeStyle.TITLE_LARGE, weight=FontWeight.BOLD),
                    Row([ft.FilledTonalButton("Clear Prompts", on_click=clear_animate_diff_prompts), ft.FilledButton("Add Diff Prompt", on_click=lambda e: edit_animate_diff(None))])], alignment=MainAxisAlignment.SPACE_BETWEEN),
        page.animate_diff_prompts,
        Divider(thickness=2, height=4),
        num_inference_row,
        guidance,
        width_slider, height_slider,
        video_length,
        ResponsiveRow([context, stride]),
        Row([dreambooth_lora, lora_alpha]),
        custom_lora,
        Row([motion_module, scheduler, batch_folder_name]),
        Row([save_frames, save_gif, save_video]),
        page.ESRGAN_block_animate_diff,
        #Row([jump_length, jump_n_sample, seed]),
        Row([
            ElevatedButton(content=Text("💚  Run AnimateDiff", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_animate_diff(page)),
             #ElevatedButton(content=Text(value="📜   Run from Prompts List", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_animate_diff(page, from_list=True))
        ]),
        page.animate_diff_output,
        clear_button,
      ]
    ))], scroll=ScrollMode.AUTO, auto_scroll=False)
    return c

materialdiffusion_prefs = {
    "material_prompt": '',
    "batch_folder_name": '',
    "file_prefix": "material-",
    "num_outputs": 1,
    "steps":50,
    "eta":0.4,
    "width": 512,
    "height":512,
    "guidance_scale":7.5,
    "seed":0,
    "init_image": '',
    "prompt_strength": 0.5,
    "mask_image": '',
    "invert_mask": False,
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": prefs['enlarge_scale'],
    #"face_enhance": prefs['face_enhance'],
    "display_upscaled_image": prefs['display_upscaled_image'],
}

def buildMaterialDiffusion(page):
    global prefs, materialdiffusion_prefs, status

    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          if ptype == "int":
            materialdiffusion_prefs[pref] = int(e.control.value)
          elif ptype == "float":
            materialdiffusion_prefs[pref] = float(e.control.value)
          else:
            materialdiffusion_prefs[pref] = e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def pick_files_result(e: FilePickerResultEvent):
        if e.files:
            img = e.files
            uf = []
            fname = img[0]
            print(", ".join(map(lambda f: f.name, e.files)))
            src_path = page.get_upload_url(fname.name, 600)
            uf.append(FilePickerUploadFile(fname.name, upload_url=src_path))
            pick_files_dialog.upload(uf)
            print(str(src_path))
            #src_path = ''.join(src_path)
            print(str(uf[0]))
            dst_path = os.path.join(root_dir, fname.name)
            print(f'Copy {src_path} to {dst_path}')
            #shutil.copy(src_path, dst_path)
            # TODO: is init or mask?
            init_image.value = dst_path

    pick_files_dialog = FilePicker(on_result=pick_files_result)
    page.overlay.append(pick_files_dialog)
    #selected_files = Text()

    def file_picker_result(e: FilePickerResultEvent):
        if e.files != None:
            upload_files(e)
    def on_upload_progress(e: FilePickerUploadEvent):
        nonlocal pick_type
        if e.progress == 1:
            if not slash in e.file_name:
              fname = os.path.join(root_dir, e.file_name)
            else:
              fname = e.file_name
            if pick_type == "init":
                init_image.value = fname
                init_image.update()
                materialdiffusion_prefs['init_image'] = fname
            elif pick_type == "mask":
                mask_image.value = fname
                mask_image.update()
                materialdiffusion_prefs['mask_image'] = fname
            page.update()
    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)
    def upload_files(e):
        uf = []
        if file_picker.result != None and file_picker.result.files != None:
            for f in file_picker.result.files:
              if page.web:
                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))
              else:
                on_upload_progress(FilePickerUploadEvent(f.path, 1, ""))
            file_picker.upload(uf)
    page.overlay.append(file_picker)
    pick_type = ""
    #page.overlay.append(pick_files_dialog)
    def pick_init(e):
        nonlocal pick_type
        pick_type = "init"
        file_picker.pick_files(allow_multiple=False, allowed_extensions=["png", "PNG"], dialog_title="Pick Init Image File")
    def pick_mask(e):
        nonlocal pick_type
        pick_type = "mask"
        file_picker.pick_files(allow_multiple=False, allowed_extensions=["png", "PNG"], dialog_title="Pick Black & White Mask Image")
    def toggle_ESRGAN(e):
        ESRGAN_settings.height = None if e.control.value else 0
        materialdiffusion_prefs['apply_ESRGAN_upscale'] = e.control.value
        ESRGAN_settings.update()
        has_changed = True
    def change_strength(e):
        strength_value.value = f" {int(e.control.value * 100)}%"
        strength_value.update()
        guidance.update()
        changed(e, 'prompt_strength', ptype="float")

    material_prompt = TextField(label="Material Prompt", value=materialdiffusion_prefs['material_prompt'], multiline=True, on_change=lambda e:changed(e,'material_prompt'))
    batch_folder_name = TextField(label="Batch Folder Name", value=materialdiffusion_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    file_prefix = TextField(label="Filename Prefix", value=materialdiffusion_prefs['file_prefix'], width=150, on_change=lambda e:changed(e,'file_prefix'))
    #num_outputs = NumberPicker(label="Num of Outputs", min=1, max=4, step=4, value=materialdiffusion_prefs['num_outputs'], on_change=lambda e:changed(e,'num_outputs', ptype="int"))
    #num_outputs = TextField(label="num_outputs", value=materialdiffusion_prefs['num_outputs'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'num_outputs', ptype="int"))
    #n_iterations = TextField(label="Number of Iterations", value=materialdiffusion_prefs['n_iterations'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'n_iterations', ptype="int"))
    steps = TextField(label="Inference Steps", value=materialdiffusion_prefs['steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'steps', ptype="int"))
    eta = TextField(label="DDIM ETA", value=materialdiffusion_prefs['eta'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'eta', ptype="float"))
    steps = SliderRow(label="Inference Steps", min=0, max=200, divisions=200, pref=materialdiffusion_prefs, key='steps')
    eta = SliderRow(label="DDIM ETA", min=0, max=1, divisions=20, round=1, pref=materialdiffusion_prefs, key='eta')
    seed = TextField(label="Seed", value=materialdiffusion_prefs['seed'], keyboard_type=KeyboardType.NUMBER, width=120, on_change=lambda e:changed(e,'seed', ptype="int"))
    param_rows = ResponsiveRow([Column([batch_folder_name, file_prefix, NumberPicker(label="Output Images", min=1, max=8, step=1, value=materialdiffusion_prefs['num_outputs'], on_change=lambda e:changed(e,'num_outputs', ptype="int"))], col={'xs':12, 'md':6}),
                      Column([steps, eta, seed], col={'xs':12, 'lg':6})], vertical_alignment=CrossAxisAlignment.START)
    batch_row = Row([batch_folder_name, file_prefix], col={'xs':12, 'lg':6})
    number_row = Row([NumberPicker(label="Output Images", min=1, max=4, step=3, value=materialdiffusion_prefs['num_outputs'], on_change=lambda e:changed(e,'num_outputs', ptype="int")), seed], col={'xs':12, 'md':6})
    param_rows = ResponsiveRow([number_row, batch_row], vertical_alignment=CrossAxisAlignment.START)
    guidance = SliderRow(label="Guidance Scale", min=0, max=50, divisions=100, round=1, pref=materialdiffusion_prefs, key='guidance_scale')
    width_slider = SliderRow(label="Width", min=128, max=1024, divisions=14, multiple=32, suffix="px", pref=materialdiffusion_prefs, key='width')
    height_slider = SliderRow(label="Height", min=128, max=1024, divisions=14, multiple=32, suffix="px", pref=materialdiffusion_prefs, key='height')
    init_image = TextField(label="Init Image", value=materialdiffusion_prefs['init_image'], on_change=lambda e:changed(e,'init_image'), expand=True, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_init), col={'xs':12, 'md':6})
    mask_image = TextField(label="Mask Image", value=materialdiffusion_prefs['mask_image'], on_change=lambda e:changed(e,'mask_image'), expand=True, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD_OUTLINED, on_click=pick_mask), col={'xs':10, 'md':5})
    invert_mask = Checkbox(label="Invert", tooltip="Swaps the Black & White of your Mask Image", value=materialdiffusion_prefs['invert_mask'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'invert_mask'), col={'xs':2, 'md':1})
    image_pickers = Container(content=ResponsiveRow([init_image, mask_image, invert_mask]), padding=padding.only(top=5), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    prompt_strength = Slider(min=0.1, max=0.9, divisions=16, label="{value}%", value=materialdiffusion_prefs['prompt_strength'], on_change=change_strength, expand=True)
    strength_value = Text(f" {int(materialdiffusion_prefs['prompt_strength'] * 100)}%", weight=FontWeight.BOLD)
    strength_slider = Row([Text("Prompt Strength: "), strength_value, prompt_strength])
    #strength_slider = SliderRow(label="Prompt Strength", min=0.1, max=0.9, divisions=16, suffix="%", pref=materialdiffusion_prefs, key='prompt_strength')
    img_block = Container(Column([image_pickers, strength_slider, Divider(height=9, thickness=2)]), padding=padding.only(top=5), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    apply_ESRGAN_upscale = Switcher(label="Apply ESRGAN Upscale", value=materialdiffusion_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)
    enlarge_scale_slider = SliderRow(label="Enlarge Scale", min=1, max=4, divisions=6, round=1, suffix="x", pref=materialdiffusion_prefs, key='enlarge_scale')
    #face_enhance = Checkbox(label="Use Face Enhance GPFGAN", value=materialdiffusion_prefs['face_enhance'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'face_enhance'))
    display_upscaled_image = Checkbox(label="Display Upscaled Image", value=materialdiffusion_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))
    ESRGAN_settings = Container(Column([enlarge_scale_slider, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    page.ESRGAN_block_material = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    page.ESRGAN_block_material.height = None if status['installed_ESRGAN'] else 0
    if not materialdiffusion_prefs['apply_ESRGAN_upscale']:
        ESRGAN_settings.height = 0
    parameters_button = ElevatedButton(content=Text(value="💨   Run Material Diffusion", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_materialdiffusion(page))

    parameters_row = Row([parameters_button], alignment=MainAxisAlignment.SPACE_BETWEEN)
    page.materialdiffusion_output = Column([])
    c = Column([Container(
        padding=padding.only(18, 14, 20, 10), content=Column([
            Header("🧱  Replicate Material Diffusion", "Create Seamless Tiled Textures with your Prompt. Requires account at Replicate.com and your Key."),
            material_prompt,
            steps,
            guidance,
            eta,
            width_slider, height_slider, #Divider(height=9, thickness=2),
            img_block, page.ESRGAN_block_material,
            #(img_block if status['installed_img2img'] or status['installed_stability'] else Container(content=None)), (clip_block if prefs['install_CLIP_guided'] else Container(content=None)), (ESRGAN_block if prefs['install_ESRGAN'] else Container(content=None)),
            param_rows,
            #batch_row,
            #number_row,
            parameters_row,
            page.materialdiffusion_output
        ],
    ))], scroll=ScrollMode.AUTO)#batch_folder_name, batch_size, n_iterations, steps, eta, seed,
    return c

ImageNet_classes = {'ATM': 480, 'Acinonyx jubatus': 293, 'Aepyceros melampus': 352, 'Afghan': 160, 'Afghan hound': 160, 'African chameleon': 47, 'African crocodile': 49, 'African elephant': 386, 'African gray': 87, 'African grey': 87, 'African hunting dog': 275, 'Ailuropoda melanoleuca': 388, 'Ailurus fulgens': 387, 'Airedale': 191, 'Airedale terrier': 191, 'Alaska crab': 121, 'Alaska king crab': 121, 'Alaskan king crab': 121, 'Alaskan malamute': 249, 'Alligator mississipiensis': 50, 'Alopex lagopus': 279, 'Ambystoma maculatum': 28, 'Ambystoma mexicanum': 29, 'American Staffordshire terrier': 180, 'American alligator': 50, 'American black bear': 295, 'American chameleon': 40, 'American coot': 137, 'American eagle': 22, 'American egret': 132, 'American lobster': 122, 'American pit bull terrier': 180, 'American robin': 15, 'Angora': 332, 'Angora rabbit': 332, 'Anolis carolinensis': 40, 'Appenzeller': 240, 'Aptenodytes patagonica': 145, 'Arabian camel': 354, 'Aramus pictus': 135, 'Aranea diademata': 74, 'Araneus cavaticus': 73, 'Arctic fox': 279, 'Arctic wolf': 270, 'Arenaria interpres': 139, 'Argiope aurantia': 72, 'Ascaphus trui': 32, 'Asiatic buffalo': 346, 'Ateles geoffroyi': 381, 'Australian terrier': 193, 'Band Aid': 419, 'Bedlington terrier': 181, 'Bernese mountain dog': 239, 'Biro': 418, 'Blenheim spaniel': 156, 'Bonasa umbellus': 82, 'Border collie': 232, 'Border terrier': 182, 'Boston bull': 195, 'Boston terrier': 195, 'Bouvier des Flandres': 233, 'Bouviers des Flandres': 233, 'Brabancon griffon': 262, 'Bradypus tridactylus': 364, 'Brittany spaniel': 215, 'Bubalus bubalis': 346, 'CD player': 485, 'CRO': 688, 'CRT screen': 782, 'Cacatua galerita': 89, 'Camelus dromedarius': 354, 'Cancer irroratus': 119, 'Cancer magister': 118, 'Canis dingo': 273, 'Canis latrans': 272, 'Canis lupus': 269, 'Canis lupus tundrarum': 270, 'Canis niger': 271, 'Canis rufus': 271, 'Cape hunting dog': 275, 'Capra ibex': 350, 'Carassius auratus': 1, 'Carcharodon carcharias': 2, 'Cardigan': 264, 'Cardigan Welsh corgi': 264, 'Carduelis carduelis': 11, 'Caretta caretta': 33, 'Carphophis amoenus': 52, 'Carpodacus mexicanus': 12, 'Cavia cobaya': 338, 'Cebus capucinus': 378, 'Cerastes cornutus': 66, 'Chamaeleo chamaeleon': 47, 'Chesapeake Bay retriever': 209, 'Chihuahua': 151, 'Chlamydosaurus kingi': 43, 'Christmas stocking': 496, 'Ciconia ciconia': 127, 'Ciconia nigra': 128, 'Constrictor constrictor': 61, 'Crock Pot': 521, 'Crocodylus niloticus': 49, 'Crotalus adamanteus': 67, 'Crotalus cerastes': 68, 'Cuon alpinus': 274, 'Cygnus atratus': 100, 'Cypripedium calceolus': 986, 'Cypripedium parviflorum': 986, 'Danaus plexippus': 323, 'Dandie Dinmont': 194, 'Dandie Dinmont terrier': 194, 'Dermochelys coriacea': 34, 'Doberman': 236, 'Doberman pinscher': 236, 'Dugong dugon': 149, 'Dungeness crab': 118, 'Dutch oven': 544, 'Egretta albus': 132, 'Egretta caerulea': 131, 'Egyptian cat': 285, 'Elephas maximus': 385, 'English cocker spaniel': 219, 'English foxhound': 167, 'English setter': 212, 'English springer': 217, 'English springer spaniel': 217, 'EntleBucher': 241, 'Erolia alpina': 140, 'Erythrocebus patas': 371, 'Eschrichtius gibbosus': 147, 'Eschrichtius robustus': 147, 'Eskimo dog': 248, 'Euarctos americanus': 295, 'European fire salamander': 25, 'European gallinule': 136, 'Felis concolor': 286, 'Felis onca': 290, 'French bulldog': 245, 'French horn': 566, 'French loaf': 930, 'Fringilla montifringilla': 10, 'Fulica americana': 137, 'Galeocerdo cuvieri': 3, 'German police dog': 235, 'German shepherd': 235, 'German shepherd dog': 235, 'German short-haired pointer': 210, 'Gila monster': 45, 'Gordon setter': 214, 'Gorilla gorilla': 366, 'Granny Smith': 948, 'Great Dane': 246, 'Great Pyrenees': 257, 'Greater Swiss Mountain dog': 238, 'Grifola frondosa': 996, 'Haliaeetus leucocephalus': 22, 'Heloderma suspectum': 45, 'Hippopotamus amphibius': 344, 'Holocanthus tricolor': 392, 'Homarus americanus': 122, 'Hungarian pointer': 211, 'Hylobates lar': 368, 'Hylobates syndactylus': 369, 'Hypsiglena torquata': 60, 'Ibizan Podenco': 173, 'Ibizan hound': 173, 'Iguana iguana': 39, 'Indian cobra': 63, 'Indian elephant': 385, 'Indri brevicaudatus': 384, 'Indri indri': 384, 'Irish setter': 213, 'Irish terrier': 184, 'Irish water spaniel': 221, 'Irish wolfhound': 170, 'Italian greyhound': 171, 'Japanese spaniel': 152, 'Kakatoe galerita': 89, 'Kerry blue terrier': 183, 'Komodo dragon': 48, 'Komodo lizard': 48, 'Labrador retriever': 208, 'Lacerta viridis': 46, 'Lakeland terrier': 189, 'Latrodectus mactans': 75, 'Lemur catta': 383, 'Leonberg': 255, 'Lepisosteus osseus': 395, 'Lhasa': 204, 'Lhasa apso': 204, 'Loafer': 630, 'Loxodonta africana': 386, 'Lycaon pictus': 275, 'Madagascar cat': 383, 'Maine lobster': 122, 'Maltese': 153, 'Maltese dog': 153, 'Maltese terrier': 153, 'Melursus ursinus': 297, 'Mergus serrator': 98, 'Mexican hairless': 268, 'Model T': 661, 'Mustela nigripes': 359, 'Mustela putorius': 358, 'Naja naja': 63, 'Nasalis larvatus': 376, 'Newfoundland': 256, 'Newfoundland dog': 256, 'Nile crocodile': 49, 'Norfolk terrier': 185, 'Northern lobster': 122, 'Norwegian elkhound': 174, 'Norwich terrier': 186, 'Old English sheepdog': 229, 'Oncorhynchus kisutch': 391, 'Orcinus orca': 148, 'Ornithorhynchus anatinus': 103, 'Ovis canadensis': 349, 'Pan troglodytes': 367, 'Panthera leo': 291, 'Panthera onca': 290, 'Panthera pardus': 288, 'Panthera tigris': 292, 'Panthera uncia': 289, 'Paralithodes camtschatica': 121, 'Passerina cyanea': 14, 'Peke': 154, 'Pekinese': 154, 'Pekingese': 154, 'Pembroke': 263, 'Pembroke Welsh corgi': 263, 'Persian cat': 283, 'Petri dish': 712, 'Phalangium opilio': 70, 'Phascolarctos cinereus': 105, 'Polaroid Land camera': 732, 'Polaroid camera': 732, 'Polyporus frondosus': 996, 'Pomeranian': 259, 'Pongo pygmaeus': 365, 'Porphyrio porphyrio': 136, 'Psittacus erithacus': 87, 'Python sebae': 62, 'R.V.': 757, 'RV': 757, 'Rana catesbeiana': 30, 'Rhodesian ridgeback': 159, 'Rocky Mountain bighorn': 349, 'Rocky Mountain sheep': 349, 'Rottweiler': 234, 'Russian wolfhound': 169, 'Saimiri sciureus': 382, 'Saint Bernard': 247, 'Salamandra salamandra': 25, 'Saluki': 176, 'Samoyed': 258, 'Samoyede': 258, 'Sciurus niger': 335, 'Scotch terrier': 199, 'Scottie': 199, 'Scottish deerhound': 177, 'Scottish terrier': 199, 'Sealyham': 190, 'Sealyham terrier': 190, 'Shetland': 230, 'Shetland sheep dog': 230, 'Shetland sheepdog': 230, 'Shih-Tzu': 155, 'Siamese': 284, 'Siamese cat': 284, 'Siberian husky': 250, 'St Bernard': 247, 'Staffordshire bull terrier': 179, 'Staffordshire bullterrier': 179, 'Staffordshire terrier': 180, 'Strix nebulosa': 24, 'Struthio camelus': 9, 'Sus scrofa': 342, 'Sussex spaniel': 220, 'Sydney silky': 201, 'Symphalangus syndactylus': 369, 'T-shirt': 610, 'Thalarctos maritimus': 296, 'Tibetan mastiff': 244, 'Tibetan terrier': 200, 'Tinca tinca': 0, 'Tringa totanus': 141, 'Triturus vulgaris': 26, 'Turdus migratorius': 15, 'U-boat': 833, 'Urocyon cinereoargenteus': 280, 'Ursus Maritimus': 296, 'Ursus americanus': 295, 'Ursus arctos': 294, 'Ursus ursinus': 297, 'Varanus komodoensis': 48, 'Virginia fence': 912, 'Vulpes macrotis': 278, 'Vulpes vulpes': 277, 'Walker foxhound': 166, 'Walker hound': 166, 'Weimaraner': 178, 'Welsh springer spaniel': 218, 'West Highland white terrier': 203, 'Windsor tie': 906, 'Yorkshire terrier': 187, 'abacus': 398, 'abaya': 399, 'academic gown': 400, 'academic robe': 400, 'accordion': 401, 'acorn': 988, 'acorn squash': 941, 'acoustic guitar': 402, 'admiral': 321, 'aegis': 461, 'affenpinscher': 252, 'agama': 42, 'agaric': 992, 'ai': 364, 'aircraft carrier': 403, 'airliner': 404, 'airship': 405, 'albatross': 146, 'all-terrain bike': 671, 'alligator lizard': 44, 'alp': 970, 'alsatian': 235, 'altar': 406, 'ambulance': 407, 'amphibian': 408, 'amphibious vehicle': 408, 'analog clock': 409, 'ananas': 953, 'anemone': 108, 'anemone fish': 393, 'anole': 40, 'ant': 310, 'anteater': 102, 'apiary': 410, 'apron': 411, 'armadillo': 363, 'armored combat vehicle': 847, 'armoured combat vehicle': 847, 'army tank': 847, 'artichoke': 944, 'articulated lorry': 867, 'ash bin': 412, 'ash-bin': 412, 'ashbin': 412, 'ashcan': 412, 'assault gun': 413, 'assault rifle': 413, 'attack aircraft carrier': 403, 'automated teller': 480, 'automated teller machine': 480, 'automatic teller': 480, 'automatic teller machine': 480, 'automatic washer': 897, 'axolotl': 29, 'baboon': 372, 'back pack': 414, 'backpack': 414, 'badger': 362, 'bagel': 931, 'bakehouse': 415, 'bakery': 415, 'bakeshop': 415, 'balance beam': 416, 'bald eagle': 22, 'balloon': 417, 'ballpen': 418, 'ballplayer': 981, 'ballpoint': 418, 'ballpoint pen': 418, 'balusters': 421, 'balustrade': 421, 'banana': 954, 'bandeau': 459, 'banded gecko': 38, 'banister': 421, 'banjo': 420, 'bannister': 421, 'barbell': 422, 'barber chair': 423, 'barbershop': 424, 'barn': 425, 'barn spider': 73, 'barometer': 426, 'barracouta': 389, 'barrel': 427, 'barrow': 428, 'bars': 702, 'baseball': 429, 'baseball player': 981, 'basenji': 253, 'basketball': 430, 'basset': 161, 'basset hound': 161, 'bassinet': 431, 'bassoon': 432, 'bath': 435, 'bath towel': 434, 'bathing cap': 433, 'bathing trunks': 842, 'bathing tub': 435, 'bathroom tissue': 999, 'bathtub': 435, 'beach waggon': 436, 'beach wagon': 436, 'beacon': 437, 'beacon light': 437, 'beagle': 162, 'beaker': 438, 'beam': 416, 'bear cat': 387, 'bearskin': 439, 'beaver': 337, 'bee': 309, 'bee eater': 92, 'bee house': 410, 'beer bottle': 440, 'beer glass': 441, 'beigel': 931, 'bell': 494, 'bell cot': 442, 'bell cote': 442, 'bell pepper': 945, 'bell toad': 32, 'bib': 443, 'bicycle-built-for-two': 444, 'bighorn': 349, 'bighorn sheep': 349, 'bikini': 445, 'billfish': 395, 'billfold': 893, 'billiard table': 736, 'binder': 446, 'binoculars': 447, 'birdhouse': 448, 'bison': 347, 'bittern': 133, 'black Maria': 734, 'black and gold garden spider': 72, 'black bear': 295, 'black grouse': 80, 'black stork': 128, 'black swan': 100, 'black widow': 75, 'black-and-tan coonhound': 165, 'black-footed ferret': 359, 'bloodhound': 163, 'blow drier': 589, 'blow dryer': 589, 'blower': 545, 'blowfish': 397, 'blue jack': 391, 'blue jean': 608, 'bluetick': 164, 'boa': 552, 'boa constrictor': 61, 'boar': 342, 'board': 532, 'boat paddle': 693, 'boathouse': 449, 'bob': 450, 'bobsled': 450, 'bobsleigh': 450, 'bobtail': 229, 'bola': 451, 'bola tie': 451, 'bolete': 997, 'bolo': 451, 'bolo tie': 451, 'bonnet': 452, 'book jacket': 921, 'bookcase': 453, 'bookshop': 454, 'bookstall': 454, 'bookstore': 454, 'borzoi': 169, 'bottle screw': 512, 'bottlecap': 455, 'bow': 456, 'bow tie': 457, 'bow-tie': 457, 'bowtie': 457, 'box tortoise': 37, 'box turtle': 37, 'boxer': 242, 'bra': 459, 'brain coral': 109, 'brambling': 10, 'brass': 458, 'brassiere': 459, 'breakwater': 460, 'breastplate': 461, 'briard': 226, 'bridegroom': 982, 'broccoli': 937, 'broom': 462, 'brown bear': 294, 'bruin': 294, 'brush kangaroo': 104, 'brush wolf': 272, 'bubble': 971, 'bucket': 463, 'buckeye': 990, 'buckle': 464, 'buckler': 787, 'bulbul': 16, 'bull mastiff': 243, 'bullet': 466, 'bullet train': 466, 'bulletproof vest': 465, 'bullfrog': 30, 'bulwark': 460, 'burrito': 965, 'busby': 439, 'bustard': 138, 'butcher shop': 467, 'butternut squash': 942, 'cab': 468, 'cabbage butterfly': 324, 'cairn': 192, 'cairn terrier': 192, 'caldron': 469, 'can opener': 473, 'candle': 470, 'candy store': 509, 'cannon': 471, 'canoe': 472, 'capitulum': 998, 'capuchin': 378, 'car mirror': 475, 'car wheel': 479, 'carabid beetle': 302, 'carbonara': 959, 'cardigan': 474, 'cardoon': 946, 'carousel': 476, "carpenter's kit": 477, "carpenter's plane": 726, 'carriage': 705, 'carriage dog': 251, 'carrier': 403, 'carrion fungus': 994, 'carrousel': 476, 'carton': 478, 'cash dispenser': 480, 'cash machine': 480, 'cask': 427, 'cassette': 481, 'cassette player': 482, 'castle': 483, 'cat bear': 387, 'catamaran': 484, 'catamount': 287, 'cathode-ray oscilloscope': 688, 'cauldron': 469, 'cauliflower': 938, 'cell': 487, 'cello': 486, 'cellphone': 487, 'cellular phone': 487, 'cellular telephone': 487, 'centipede': 79, 'cerastes': 66, 'chain': 488, 'chain armor': 490, 'chain armour': 490, 'chain mail': 490, 'chain saw': 491, 'chainlink fence': 489, 'chainsaw': 491, 'chambered nautilus': 117, 'cheeseburger': 933, 'cheetah': 293, 'chest': 492, 'chetah': 293, 'chickadee': 19, 'chiffonier': 493, 'chime': 494, 'chimp': 367, 'chimpanzee': 367, 'china cabinet': 495, 'china closet': 495, 'chiton': 116, 'chocolate sauce': 960, 'chocolate syrup': 960, 'chopper': 499, 'chow': 260, 'chow chow': 260, 'chrysanthemum dog': 200, 'chrysomelid': 304, 'church': 497, 'church building': 497, 'chute': 701, 'cicada': 316, 'cicala': 316, 'cimarron': 349, 'cinema': 498, 'claw': 600, 'cleaver': 499, 'cliff': 972, 'cliff dwelling': 500, 'cloak': 501, 'clog': 502, 'closet': 894, 'clumber': 216, 'clumber spaniel': 216, 'coach': 705, 'coach dog': 251, 'coast': 978, 'coat-of-mail shell': 116, 'cock': 7, 'cocker': 219, 'cocker spaniel': 219, 'cockroach': 314, 'cocktail shaker': 503, 'coffee mug': 504, 'coffeepot': 505, 'coho': 391, 'coho salmon': 391, 'cohoe': 391, 'coil': 506, 'collie': 231, 'colobus': 375, 'colobus monkey': 375, 'combination lock': 507, 'comfort': 750, 'comforter': 750, 'comic book': 917, 'commode': 493, 'common iguana': 39, 'common newt': 26, 'computer keyboard': 508, 'computer mouse': 673, 'conch': 112, 'confectionary': 509, 'confectionery': 509, 'conker': 990, 'consomme': 925, 'container ship': 510, 'container vessel': 510, 'containership': 510, 'convertible': 511, 'coon bear': 388, 'coral fungus': 991, 'coral reef': 973, 'corkscrew': 512, 'corn': 987, 'cornet': 513, 'cot': 520, 'cottontail': 330, 'cottontail rabbit': 330, 'coucal': 91, 'cougar': 286, 'courgette': 939, 'cowboy boot': 514, 'cowboy hat': 515, 'coyote': 272, 'cradle': 516, 'crampfish': 5, 'crane': 517, 'crash helmet': 518, 'crate': 519, 'crawdad': 124, 'crawdaddy': 124, 'crawfish': 124, 'crayfish': 124, 'crib': 520, 'cricket': 312, 'crinoline': 601, 'croquet ball': 522, 'crossword': 918, 'crossword puzzle': 918, 'crutch': 523, 'cucumber': 943, 'cuirass': 524, 'cuke': 943, 'cup': 968, 'curly-coated retriever': 206, 'custard apple': 956, 'daddy longlegs': 70, 'daisy': 985, 'dalmatian': 251, 'dam': 525, 'damselfly': 320, 'dark glasses': 837, 'darning needle': 319, 'day bed': 831, 'deerhound': 177, 'denim': 608, 'desk': 526, 'desktop computer': 527, "devil's darning needle": 319, 'devilfish': 147, 'dhole': 274, 'dial phone': 528, 'dial telephone': 528, 'diamondback': 67, 'diamondback rattlesnake': 67, 'diaper': 529, 'digital clock': 530, 'digital watch': 531, 'dike': 525, 'dingo': 273, 'dining table': 532, 'dipper': 20, 'dirigible': 405, 'disc brake': 535, 'dish washer': 534, 'dishcloth': 533, 'dishrag': 533, 'dishwasher': 534, 'dishwashing machine': 534, 'disk brake': 535, 'dock': 536, 'dockage': 536, 'docking facility': 536, 'dog sled': 537, 'dog sleigh': 537, 'dogsled': 537, 'dome': 538, 'doormat': 539, 'dough': 961, 'dowitcher': 142, 'dragon lizard': 48, 'dragonfly': 319, 'drake': 97, 'drilling platform': 540, 'dromedary': 354, 'drop': 972, 'drop-off': 972, 'drum': 541, 'drumstick': 542, 'duck-billed platypus': 103, 'duckbill': 103, 'duckbilled platypus': 103, 'dugong': 149, 'dumbbell': 543, 'dung beetle': 305, 'dunlin': 140, 'dust cover': 921, 'dust jacket': 921, 'dust wrapper': 921, 'dustbin': 412, 'dustcart': 569, 'dyke': 525, 'ear': 998, 'earthstar': 995, 'eastern fox squirrel': 335, 'eatery': 762, 'eating house': 762, 'eating place': 762, 'echidna': 102, 'eel': 390, 'eft': 27, 'eggnog': 969, 'egis': 461, 'electric fan': 545, 'electric guitar': 546, 'electric locomotive': 547, 'electric ray': 5, 'electric switch': 844, 'electrical switch': 844, 'elkhound': 174, 'emmet': 310, 'entertainment center': 548, 'envelope': 549, 'espresso': 967, 'espresso maker': 550, 'essence': 711, 'estate car': 436, 'ewer': 725, 'face powder': 551, 'feather boa': 552, 'ferret': 359, 'fiddle': 889, 'fiddler crab': 120, 'field glasses': 447, 'fig': 952, 'file': 553, 'file cabinet': 553, 'filing cabinet': 553, 'fire engine': 555, 'fire screen': 556, 'fire truck': 555, 'fireboat': 554, 'fireguard': 556, 'fitch': 358, 'fixed disk': 592, 'flagpole': 557, 'flagstaff': 557, 'flamingo': 130, 'flat-coated retriever': 205, 'flattop': 403, 'flatworm': 110, 'flowerpot': 738, 'flute': 558, 'fly': 308, 'folding chair': 559, 'food market': 582, 'football helmet': 560, 'footstall': 708, 'foreland': 976, 'forklift': 561, 'foulmart': 358, 'foumart': 358, 'fountain': 562, 'fountain pen': 563, 'four-poster': 564, 'fox squirrel': 335, 'freight car': 565, 'frilled lizard': 43, 'frying pan': 567, 'frypan': 567, 'fur coat': 568, 'gar': 395, 'garbage can': 412, 'garbage truck': 569, 'garden cart': 428, 'garden spider': 74, 'garfish': 395, 'garpike': 395, 'garter snake': 57, 'gas helmet': 570, 'gas pump': 571, 'gasmask': 570, 'gasoline pump': 571, 'gazelle': 353, 'gazelle hound': 176, 'geta': 502, 'geyser': 974, 'giant lizard': 48, 'giant panda': 388, 'giant schnauzer': 197, 'gibbon': 368, 'glasshouse': 580, 'globe artichoke': 944, 'globefish': 397, 'go-kart': 573, 'goblet': 572, 'golden retriever': 207, 'goldfinch': 11, 'goldfish': 1, 'golf ball': 574, 'golf cart': 575, 'golfcart': 575, 'gondola': 576, 'gong': 577, 'goose': 99, 'gorilla': 366, 'gown': 578, 'grampus': 148, 'grand': 579, 'grand piano': 579, 'grass snake': 57, 'grasshopper': 311, 'gray fox': 280, 'gray whale': 147, 'gray wolf': 269, 'great gray owl': 24, 'great grey owl': 24, 'great white heron': 132, 'great white shark': 2, 'green lizard': 46, 'green mamba': 64, 'green snake': 55, 'greenhouse': 580, 'grey fox': 280, 'grey whale': 147, 'grey wolf': 269, 'grille': 581, 'grocery': 582, 'grocery store': 582, 'groenendael': 224, 'groin': 460, 'groom': 982, 'ground beetle': 302, 'groyne': 460, 'grunter': 341, 'guacamole': 924, 'guenon': 370, 'guenon monkey': 370, 'guillotine': 583, 'guinea pig': 338, 'gyromitra': 993, 'hack': 468, 'hair drier': 589, 'hair dryer': 589, 'hair slide': 584, 'hair spray': 585, 'half track': 586, 'hammer': 587, 'hammerhead': 4, 'hammerhead shark': 4, 'hamper': 588, 'hamster': 333, 'hand blower': 589, 'hand-held computer': 590, 'hand-held microcomputer': 590, 'handbasin': 896, 'handkerchief': 591, 'handrail': 421, 'hankey': 591, 'hankie': 591, 'hanky': 591, 'hard disc': 592, 'hard disk': 592, 'hare': 331, 'harmonica': 593, 'harp': 594, 'hartebeest': 351, 'harvester': 595, 'harvestman': 70, 'hatchet': 596, 'hautbois': 683, 'hautboy': 683, 'haversack': 414, 'hay': 958, 'head': 976, 'head cabbage': 936, 'headland': 976, 'hedgehog': 334, 'helix': 506, 'hen': 8, 'hen of the woods': 996, 'hen-of-the-woods': 996, 'hermit crab': 125, 'high bar': 602, 'hip': 989, 'hippo': 344, 'hippopotamus': 344, 'hockey puck': 746, 'hodometer': 685, 'hog': 341, 'hognose snake': 54, 'holothurian': 329, 'holster': 597, 'home theater': 598, 'home theatre': 598, 'honeycomb': 599, 'hook': 600, 'hoopskirt': 601, 'hopper': 311, 'horizontal bar': 602, 'horn': 566, 'hornbill': 93, 'horned asp': 66, 'horned rattlesnake': 68, 'horned viper': 66, 'horse cart': 603, 'horse chestnut': 990, 'horse-cart': 603, 'hot dog': 934, 'hot pot': 926, 'hotdog': 934, 'hotpot': 926, 'hourglass': 604, 'house finch': 12, 'howler': 379, 'howler monkey': 379, 'hummingbird': 94, 'hunting spider': 77, 'husky': 248, 'hussar monkey': 371, 'hyaena': 276, 'hyena': 276, 'hyena dog': 275, 'iPod': 605, 'ibex': 350, 'ice bear': 296, 'ice cream': 928, 'ice lolly': 929, 'icebox': 760, 'icecream': 928, 'igniter': 626, 'ignitor': 626, 'iguana': 39, 'impala': 352, 'indigo bird': 14, 'indigo bunting': 14, 'indigo finch': 14, 'indri': 384, 'indris': 384, 'internet site': 916, 'iron': 606, 'island dispenser': 571, 'isopod': 126, 'jacamar': 95, 'jack': 955, "jack-o'-lantern": 607, 'jackfruit': 955, 'jaguar': 290, 'jak': 955, 'jammies': 697, 'jay': 17, 'jean': 608, 'jeep': 609, 'jellyfish': 107, 'jersey': 610, 'jetty': 460, "jeweler's loupe": 633, 'jigsaw puzzle': 611, 'jinrikisha': 612, 'joystick': 613, "judge's robe": 400, 'junco': 13, 'kangaroo bear': 105, 'keeshond': 261, 'kelpie': 227, 'keypad': 508, 'killer': 148, 'killer whale': 148, 'kimono': 614, 'king crab': 121, 'king of beasts': 291, 'king penguin': 145, 'king snake': 56, 'kingsnake': 56, 'kit fox': 278, 'kite': 21, 'knapsack': 414, 'knee pad': 615, 'knot': 616, 'koala': 105, 'koala bear': 105, 'komondor': 228, 'kuvasz': 222, 'lab coat': 617, 'laboratory coat': 617, 'labyrinth': 646, 'lacewing': 318, 'lacewing fly': 318, 'ladle': 618, 'lady beetle': 301, 'ladybeetle': 301, 'ladybird': 301, 'ladybird beetle': 301, 'ladybug': 301, 'lakeshore': 975, 'lakeside': 975, 'lamp shade': 619, 'lampshade': 619, 'landrover': 609, 'langouste': 123, 'langur': 374, 'laptop': 620, 'laptop computer': 620, 'lavabo': 896, 'lawn cart': 428, 'lawn mower': 621, 'leaf beetle': 304, 'leafhopper': 317, 'leatherback': 34, 'leatherback turtle': 34, 'leathery turtle': 34, 'lemon': 951, 'lens cap': 622, 'lens cover': 622, 'leopard': 288, 'lesser panda': 387, 'letter box': 637, 'letter opener': 623, 'library': 624, 'lifeboat': 625, 'light': 626, 'lighter': 626, 'lighthouse': 437, 'limo': 627, 'limousine': 627, 'limpkin': 135, 'liner': 628, 'linnet': 12, 'lion': 291, 'lionfish': 396, 'lip rouge': 629, 'lipstick': 629, 'little blue heron': 131, 'llama': 355, 'loggerhead': 33, 'loggerhead turtle': 33, 'lollipop': 929, 'lolly': 929, 'long-horned beetle': 303, 'longicorn': 303, 'longicorn beetle': 303, 'lorikeet': 90, 'lotion': 631, 'loudspeaker': 632, 'loudspeaker system': 632, 'loupe': 633, 'lumbermill': 634, 'lycaenid': 326, 'lycaenid butterfly': 326, 'lynx': 287, 'macaque': 373, 'macaw': 88, 'magnetic compass': 635, 'magpie': 18, 'mail': 490, 'mailbag': 636, 'mailbox': 637, 'maillot': 639, 'malamute': 249, 'malemute': 249, 'malinois': 225, 'man-eater': 2, 'man-eating shark': 2, 'maned wolf': 271, 'manhole cover': 640, 'mantid': 315, 'mantis': 315, 'manufactured home': 660, 'maraca': 641, 'marimba': 642, 'market': 582, 'marmoset': 377, 'marmot': 336, 'marsh hen': 137, 'mashed potato': 935, 'mask': 643, 'matchstick': 644, 'maypole': 645, 'maze': 646, 'measuring cup': 647, 'meat cleaver': 499, 'meat loaf': 962, 'meat market': 467, 'meatloaf': 962, 'medicine cabinet': 648, 'medicine chest': 648, 'meerkat': 299, 'megalith': 649, 'megalithic structure': 649, 'membranophone': 541, 'memorial tablet': 458, 'menu': 922, 'merry-go-round': 476, 'microphone': 650, 'microwave': 651, 'microwave oven': 651, 'mierkat': 299, 'mike': 650, 'mileometer': 685, 'military plane': 895, 'military uniform': 652, 'milk can': 653, 'milkweed butterfly': 323, 'milometer': 685, 'mini': 655, 'miniature pinscher': 237, 'miniature poodle': 266, 'miniature schnauzer': 196, 'minibus': 654, 'miniskirt': 655, 'minivan': 656, 'mink': 357, 'missile': 744, 'mitten': 658, 'mixing bowl': 659, 'mobile home': 660, 'mobile phone': 487, 'modem': 662, 'mole': 460, 'mollymawk': 146, 'monarch': 323, 'monarch butterfly': 323, 'monastery': 663, 'mongoose': 298, 'monitor': 664, 'monkey dog': 252, 'monkey pinscher': 252, 'monocycle': 880, 'mop': 840, 'moped': 665, 'mortar': 666, 'mortarboard': 667, 'mosque': 668, 'mosquito hawk': 319, 'mosquito net': 669, 'motor scooter': 670, 'mountain bike': 671, 'mountain lion': 286, 'mountain tent': 672, 'mouse': 673, 'mousetrap': 674, 'mouth harp': 593, 'mouth organ': 593, 'movie house': 498, 'movie theater': 498, 'movie theatre': 498, 'moving van': 675, 'mower': 621, 'mud hen': 137, 'mud puppy': 29, 'mud turtle': 35, 'mushroom': 947, 'muzzle': 676, 'nail': 677, 'napkin': 529, 'nappy': 529, 'native bear': 105, 'nautilus': 117, 'neck brace': 678, 'necklace': 679, 'nematode': 111, 'nematode worm': 111, 'night snake': 60, 'nipple': 680, 'notebook': 681, 'notebook computer': 681, 'notecase': 893, 'nudibranch': 115, 'numbfish': 5, 'nursery': 580, 'obelisk': 682, 'oboe': 683, 'ocarina': 684, 'ocean liner': 628, 'odometer': 685, 'off-roader': 671, 'offshore rig': 540, 'oil filter': 686, 'one-armed bandit': 800, 'opera glasses': 447, 'orang': 365, 'orange': 950, 'orangutan': 365, 'orangutang': 365, 'orca': 148, 'organ': 687, 'oscilloscope': 688, 'ostrich': 9, 'otter': 360, 'otter hound': 175, 'otterhound': 175, 'ounce': 289, 'overskirt': 689, 'ox': 345, 'oxcart': 690, 'oxygen mask': 691, 'oyster catcher': 143, 'oystercatcher': 143, 'packet': 692, 'packsack': 414, 'paddle': 693, 'paddle wheel': 694, 'paddlewheel': 694, 'paddy wagon': 734, 'padlock': 695, 'pail': 463, 'paintbrush': 696, 'painter': 286, 'pajama': 697, 'palace': 698, 'paling': 716, 'panda': 388, 'panda bear': 388, 'pandean pipe': 699, 'panpipe': 699, 'panther': 290, 'paper knife': 623, 'paper towel': 700, 'paperknife': 623, 'papillon': 157, 'parachute': 701, 'parallel bars': 702, 'park bench': 703, 'parking meter': 704, 'partridge': 86, 'passenger car': 705, 'patas': 371, 'patio': 706, 'patrol wagon': 734, 'patten': 502, 'pay-phone': 707, 'pay-station': 707, 'peacock': 84, 'pearly nautilus': 117, 'pedestal': 708, 'pelican': 144, 'pencil box': 709, 'pencil case': 709, 'pencil eraser': 767, 'pencil sharpener': 710, 'penny bank': 719, 'perfume': 711, 'petrol pump': 571, 'pharos': 437, 'photocopier': 713, 'piano accordion': 401, 'pick': 714, 'pickelhaube': 715, 'picket fence': 716, 'pickup': 717, 'pickup truck': 717, 'picture palace': 498, 'pier': 718, 'pig': 341, 'pigboat': 833, 'piggy bank': 719, 'pill bottle': 720, 'pillow': 721, 'pineapple': 953, 'ping-pong ball': 722, 'pinwheel': 723, 'pipe organ': 687, 'pirate': 724, 'pirate ship': 724, 'pismire': 310, 'pit bull terrier': 180, 'pitcher': 725, 'pizza': 963, 'pizza pie': 963, "pj's": 697, 'plane': 726, 'planetarium': 727, 'plaque': 458, 'plastic bag': 728, 'plate': 923, 'plate rack': 729, 'platyhelminth': 110, 'platypus': 103, 'plectron': 714, 'plectrum': 714, 'plinth': 708, 'plough': 730, 'plow': 730, "plumber's helper": 731, 'plunger': 731, 'pocketbook': 893, 'poke bonnet': 452, 'polar bear': 296, 'pole': 733, 'polecat': 361, 'police van': 734, 'police wagon': 734, 'polyplacophore': 116, 'pomegranate': 957, 'poncho': 735, 'pool table': 736, 'pop bottle': 737, 'popsicle': 929, 'porcupine': 334, 'postbag': 636, 'pot': 738, 'potpie': 964, "potter's wheel": 739, 'power drill': 740, 'prairie chicken': 83, 'prairie fowl': 83, 'prairie grouse': 83, 'prairie wolf': 272, 'prayer mat': 741, 'prayer rug': 741, 'press': 894, 'pretzel': 932, 'printer': 742, 'prison': 743, 'prison house': 743, 'proboscis monkey': 376, 'projectile': 744, 'projector': 745, 'promontory': 976, 'ptarmigan': 81, 'puck': 746, 'puff': 750, 'puff adder': 54, 'puffer': 397, 'pufferfish': 397, 'pug': 254, 'pug-dog': 254, 'puma': 286, 'punch bag': 747, 'punchball': 747, 'punching bag': 747, 'punching ball': 747, 'purse': 748, 'pyjama': 697, 'quail': 85, 'quill': 749, 'quill pen': 749, 'quilt': 750, 'race car': 751, 'racer': 751, 'racing car': 751, 'racket': 752, 'racquet': 752, 'radiator': 753, 'radiator grille': 581, 'radio': 754, 'radio reflector': 755, 'radio telescope': 755, 'rain barrel': 756, 'ram': 348, 'rapeseed': 984, 'reaper': 595, 'recreational vehicle': 757, 'red fox': 277, 'red hot': 934, 'red panda': 387, 'red setter': 213, 'red wine': 966, 'red wolf': 271, 'red-backed sandpiper': 140, 'red-breasted merganser': 98, 'redbone': 168, 'redshank': 141, 'reel': 758, 'reflex camera': 759, 'refrigerator': 760, 'remote': 761, 'remote control': 761, 'respirator': 570, 'restaurant': 762, 'revolver': 763, 'rhinoceros beetle': 306, 'ribbed toad': 32, 'ricksha': 612, 'rickshaw': 612, 'rifle': 764, 'rig': 867, 'ring armor': 490, 'ring armour': 490, 'ring mail': 490, 'ring snake': 53, 'ring-binder': 446, 'ring-necked snake': 53, 'ring-tailed lemur': 383, 'ringlet': 322, 'ringlet butterfly': 322, 'ringneck snake': 53, 'ringtail': 378, 'river horse': 344, 'roach': 314, 'robin': 15, 'rock beauty': 392, 'rock crab': 119, 'rock lobster': 123, 'rock python': 62, 'rock snake': 62, 'rocker': 765, 'rocking chair': 765, 'rose hip': 989, 'rosehip': 989, 'rotisserie': 766, 'roundabout': 476, 'roundworm': 111, 'rubber': 767, 'rubber eraser': 767, 'rucksack': 414, 'ruddy turnstone': 139, 'ruffed grouse': 82, 'rugby ball': 768, 'rule': 769, 'ruler': 769, 'running shoe': 770, 'sabot': 502, 'safe': 771, 'safety pin': 772, 'salt shaker': 773, 'saltshaker': 773, 'sand bar': 977, 'sand viper': 66, 'sandal': 774, 'sandbar': 977, 'sarong': 775, 'sawmill': 634, 'sax': 776, 'saxophone': 776, 'scabbard': 777, 'scale': 778, 'schipperke': 223, 'school bus': 779, 'schooner': 780, 'scooter': 670, 'scope': 688, 'scoreboard': 781, 'scorpion': 71, 'screen': 782, 'screw': 783, 'screwdriver': 784, 'scuba diver': 983, 'sea anemone': 108, 'sea cradle': 116, 'sea crawfish': 123, 'sea cucumber': 329, 'sea lion': 150, 'sea slug': 115, 'sea snake': 65, 'sea star': 327, 'sea urchin': 328, 'sea wolf': 148, 'sea-coast': 978, 'seacoast': 978, 'seashore': 978, 'seat belt': 785, 'seatbelt': 785, 'seawall': 460, 'semi': 867, 'sewing machine': 786, 'sewing needle': 319, 'shades': 837, 'shako': 439, 'shield': 787, 'shoe shop': 788, 'shoe store': 788, 'shoe-shop': 788, 'shoji': 789, 'shopping basket': 790, 'shopping cart': 791, 'shovel': 792, 'shower cap': 793, 'shower curtain': 794, 'siamang': 369, 'sidewinder': 68, 'silky terrier': 201, 'silver salmon': 391, 'site': 916, 'six-gun': 763, 'six-shooter': 763, 'skeeter hawk': 319, 'ski': 795, 'ski mask': 796, 'skillet': 567, 'skunk': 361, 'sleeping bag': 797, 'sleuthhound': 163, 'slide rule': 798, 'sliding door': 799, 'slipstick': 798, 'slot': 800, 'sloth bear': 297, 'slug': 114, 'smoothing iron': 606, 'snail': 113, 'snake doctor': 319, 'snake feeder': 319, 'snake fence': 912, 'snake-rail fence': 912, 'snoek': 389, 'snooker table': 736, 'snorkel': 801, 'snow leopard': 289, 'snowbird': 13, 'snowmobile': 802, 'snowplough': 803, 'snowplow': 803, 'soap dispenser': 804, 'soccer ball': 805, 'sock': 806, 'soda bottle': 737, 'soft-coated wheaten terrier': 202, 'solar collector': 807, 'solar dish': 807, 'solar furnace': 807, 'sombrero': 808, 'sorrel': 339, 'soup bowl': 809, 'space bar': 810, 'space heater': 811, 'space shuttle': 812, 'spaghetti squash': 940, 'spatula': 813, 'speaker': 632, 'speaker system': 632, 'speaker unit': 632, 'speedboat': 814, 'spider monkey': 381, 'spider web': 815, "spider's web": 815, 'spike': 998, 'spindle': 816, 'spiny anteater': 102, 'spiny lobster': 123, 'spiral': 506, 'spoonbill': 129, 'sport car': 817, 'sports car': 817, 'spot': 818, 'spotlight': 818, 'spotted salamander': 28, 'squealer': 341, 'squeeze box': 401, 'squirrel monkey': 382, 'stage': 819, 'standard poodle': 267, 'standard schnauzer': 198, 'starfish': 327, 'station waggon': 436, 'station wagon': 436, 'steam locomotive': 820, 'steel arch bridge': 821, 'steel drum': 822, 'stethoscope': 823, 'stick insect': 313, 'stingray': 6, 'stinkhorn': 994, 'stole': 824, 'stone wall': 825, 'stop watch': 826, 'stoplight': 920, 'stopwatch': 826, 'stove': 827, 'strainer': 828, 'strawberry': 949, 'street sign': 919, 'streetcar': 829, 'stretcher': 830, 'studio couch': 831, 'stupa': 832, 'sturgeon': 394, 'sub': 833, 'submarine': 833, 'suit': 834, 'suit of clothes': 834, 'sulfur butterfly': 325, 'sulphur butterfly': 325, 'sulphur-crested cockatoo': 89, 'sun blocker': 838, 'sunblock': 838, 'sundial': 835, 'sunglass': 836, 'sunglasses': 837, 'sunscreen': 838, 'suspension bridge': 839, 'swab': 840, 'sweatshirt': 841, 'sweet potato': 684, 'swimming cap': 433, 'swimming trunks': 842, 'swing': 843, 'switch': 844, 'swob': 840, 'syringe': 845, 'syrinx': 699, 'tabby': 281, 'tabby cat': 281, 'table lamp': 846, 'tailed frog': 32, 'tailed toad': 32, 'tam-tam': 577, 'tandem': 444, 'tandem bicycle': 444, 'tank': 847, 'tank suit': 639, 'tape player': 848, 'taper': 470, 'tarantula': 76, 'taxi': 468, 'taxicab': 468, 'teapot': 849, 'teddy': 850, 'teddy bear': 850, 'tee shirt': 610, 'television': 851, 'television system': 851, 'ten-gallon hat': 515, 'tench': 0, 'tennis ball': 852, 'terrace': 706, 'terrapin': 36, 'thatch': 853, 'thatched roof': 853, 'theater curtain': 854, 'theatre curtain': 854, 'thimble': 855, 'thrasher': 856, 'three-toed sloth': 364, 'thresher': 856, 'threshing machine': 856, 'throne': 857, 'thunder snake': 52, 'tick': 78, 'tiger': 292, 'tiger beetle': 300, 'tiger cat': 282, 'tiger shark': 3, 'tile roof': 858, 'timber wolf': 269, 'tin opener': 473, 'titi': 380, 'titi monkey': 380, 'toaster': 859, 'tobacco shop': 860, 'tobacconist': 860, 'tobacconist shop': 860, 'toilet paper': 999, 'toilet seat': 861, 'toilet tissue': 999, 'tool kit': 477, 'tope': 832, 'torch': 862, 'torpedo': 5, 'totem pole': 863, 'toucan': 96, 'tow car': 864, 'tow truck': 864, 'toy poodle': 265, 'toy terrier': 158, 'toyshop': 865, 'trackless trolley': 874, 'tractor': 866, 'tractor trailer': 867, 'traffic light': 920, 'traffic signal': 920, 'trailer truck': 867, 'tram': 829, 'tramcar': 829, 'transverse flute': 558, 'trash barrel': 412, 'trash bin': 412, 'trash can': 412, 'tray': 868, 'tree frog': 31, 'tree-frog': 31, 'trench coat': 869, 'triceratops': 51, 'tricycle': 870, 'trifle': 927, 'trike': 870, 'trilobite': 69, 'trimaran': 871, 'tripod': 872, 'triumphal arch': 873, 'trolley': 829, 'trolley car': 829, 'trolley coach': 874, 'trolleybus': 874, 'trombone': 875, 'trucking rig': 867, 'trump': 513, 'trumpet': 513, 'tub': 876, 'tup': 348, 'turnstile': 877, 'tusker': 101, 'two-piece': 445, 'tympan': 541, 'typewriter keyboard': 878, 'umbrella': 879, 'unicycle': 880, 'upright': 881, 'upright piano': 881, 'vacuum': 882, 'vacuum cleaner': 882, 'vale': 979, 'valley': 979, 'vase': 883, 'vat': 876, 'vault': 884, 'velocipede': 870, 'velvet': 885, 'vending machine': 886, 'vestment': 887, 'viaduct': 888, 'vine snake': 59, 'violin': 889, 'violoncello': 486, 'vizsla': 211, 'volcano': 980, 'volleyball': 890, 'volute': 506, 'vulture': 23, 'waffle iron': 891, 'waggon': 436, 'wagon': 734, 'walking stick': 313, 'walkingstick': 313, 'wall clock': 892, 'wallaby': 104, 'wallet': 893, 'wardrobe': 894, 'warplane': 895, 'warragal': 273, 'warrigal': 273, 'warthog': 343, 'wash-hand basin': 896, 'washbasin': 896, 'washbowl': 896, 'washer': 897, 'washing machine': 897, 'wastebin': 412, 'water bottle': 898, 'water buffalo': 346, 'water hen': 137, 'water jug': 899, 'water ouzel': 20, 'water ox': 346, 'water snake': 58, 'water tower': 900, 'wax light': 470, 'weasel': 356, 'web site': 916, 'website': 916, 'weevil': 307, 'weighing machine': 778, 'welcome mat': 539, 'wheelbarrow': 428, 'whippet': 172, 'whiptail': 41, 'whiptail lizard': 41, 'whirligig': 476, 'whiskey jug': 901, 'whistle': 902, 'white fox': 279, 'white shark': 2, 'white stork': 127, 'white wolf': 270, 'whorl': 506, 'wig': 903, 'wild boar': 342, 'window screen': 904, 'window shade': 905, 'wine bottle': 907, 'wing': 908, 'wire-haired fox terrier': 188, 'wireless': 754, 'wok': 909, 'wolf spider': 77, 'wombat': 106, 'wood pussy': 361, 'wood rabbit': 330, 'wooden spoon': 910, 'woodworking plane': 726, 'wool': 911, 'woolen': 911, 'woollen': 911, 'worm fence': 912, 'worm snake': 52, 'wreck': 913, 'wrecker': 864, 'xylophone': 642, 'yawl': 914, "yellow lady's slipper": 986, 'yellow lady-slipper': 986, 'yurt': 915, 'zebra': 340, 'zucchini': 939}
DiT_prefs = {
    'prompt': '',
    'batch_folder_name': '',
    'guidance_scale': 4.0,
    'num_inference_steps': 50,
    'seed': 0,
    'num_images': 1,
    #'variance_type': 'learned_range',#fixed_small_log
    #'num_train_timesteps': 1000,
    #'prediction_type': 'epsilon',#sample
    #'clip_sample': True,
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": 4.0,
    "display_upscaled_image": True,
}
def buildDiT(page):
    global DiT_prefs, prefs, pipe_DiT
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          if ptype == "int":
            DiT_prefs[pref] = int(e.control.value)
          elif ptype == "float":
            DiT_prefs[pref] = float(e.control.value)
          else:
            DiT_prefs[pref] = e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def add_to_DiT_output(o):
      page.DiT_output.controls.append(o)
      page.DiT_output.update()
      if not clear_button.visible:
        clear_button.visible = True
        clear_button.update()
    page.add_to_DiT_output = add_to_DiT_output
    def clear_output(e):
      if prefs['enable_sounds']: page.snd_delete.play()
      page.DiT_output.controls = []
      page.DiT_output.update()
      clear_button.visible = False
      clear_button.update()
    def DiT_help(e):
      def close_DiT_dlg(e):
        nonlocal DiT_help_dlg
        DiT_help_dlg.open = False
        page.update()
      DiT_help_dlg = AlertDialog(title=Text("🙅   Help with DiT Pipeline"), content=Column([
          Text("Provide a comma separated list of general ImageNet Classes to create images. Press Class List to see availble classes, click to copy a token to clipboard then paste in textfield."),
          Text("We explore a new class of diffusion models based on the transformer architecture. We train latent diffusion models of images, replacing the commonly-used U-Net backbone with a transformer that operates on latent patches. We analyze the scalability of our Diffusion Transformers (DiTs) through the lens of forward pass complexity as measured by Gflops. We find that DiTs with higher Gflops -- through increased transformer depth/width or increased number of input tokens -- consistently have lower FID. In addition to possessing good scalability properties, our largest DiT-XL/2 models outperform all prior diffusion models on the class-conditional ImageNet 512x512 and 256x256 benchmarks, achieving a state-of-the-art FID of 2.27 on the latter."),
          Markdown("The DiT model in diffusers comes from  can be found here: [Scalable Diffusion Models with Transformers](https://www.wpeebles.com/DiT) (DiT) and [facebookresearch/dit](https://github.com/facebookresearch/dit)..", on_tap_link=lambda e: e.page.launch_url(e.data)),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("😕  Interesting... ", on_click=close_DiT_dlg)], actions_alignment=MainAxisAlignment.END)
      page.dialog = DiT_help_dlg
      DiT_help_dlg.open = True
      page.update()
    def copy_class(e):
      page.set_clipboard(e.control.text)
      page.snack_bar = SnackBar(content=Text(f"📋   Class {e.control.text} copied to clipboard..."))
      page.snack_bar.open = True
      page.update()
    def show_classes(e):
      classes = []
      for c in ImageNet_classes.keys():
        #TODO Copy to clipboard on click
        classes.append(TextButton(c, col={'sm':4, 'md':3, 'lg':2,}, on_click=copy_class))
      alert_msg(page, "ImageNET Class List", content=Container(Column([ResponsiveRow(
        controls=classes,
        expand=True,
      )], spacing=0), width=(page.width if page.web else page.window_width) - 150), okay="😲  That's a lot...", sound=False)
    guidance_scale = SliderRow(label="Guidance Scale", min=0, max=50, divisions=100, round=1, pref=DiT_prefs, key='guidance_scale')
    def toggle_ESRGAN(e):
        ESRGAN_settings.height = None if e.control.value else 0
        DiT_prefs['apply_ESRGAN_upscale'] = e.control.value
        ESRGAN_settings.update()
    prompt = TextField(label="ImageNet Class Names (separated by commas)", value=DiT_prefs['prompt'], on_change=lambda e:changed(e,'prompt'))
    seed = TextField(label="Seed", width=90, value=str(DiT_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    num_inference_row = SliderRow(label="Number of Inference Steps", min=1, max=100, divisions=99, pref=DiT_prefs, key='num_inference_steps', tooltip="The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.")
    batch_folder_name = TextField(label="Batch Folder Name", value=DiT_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    #eta = TextField(label="ETA", value=str(DiT_prefs['eta']), keyboard_type=KeyboardType.NUMBER, hint_text="Amount of Noise", on_change=lambda e:changed(e,'eta', ptype='float'))
    #eta = Slider(min=0.0, max=1.0, divisions=20, label="{value}", value=float(DiT_prefs['eta']), tooltip="The weight of noise for added noise in a diffusion step. Its value is between 0.0 and 1.0 - 0.0 is DDIM and 1.0 is DDPM scheduler respectively.", expand=True, on_change=lambda e:changed(e,'eta', ptype='float'))
    #eta_row = Row([Text("DDIM ETA: "), eta])
    #max_size = Slider(min=256, max=1280, divisions=64, label="{value}px", value=int(DiT_prefs['max_size']), expand=True, on_change=lambda e:changed(e,'max_size', ptype='int'))
    #max_row = Row([Text("Max Resolution Size: "), max_size])
    apply_ESRGAN_upscale = Switcher(label="Apply ESRGAN Upscale", value=DiT_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)
    enlarge_scale_slider = SliderRow(label="Enlarge Scale", min=1, max=4, divisions=6, round=1, suffix="x", pref=DiT_prefs, key='enlarge_scale')
    display_upscaled_image = Checkbox(label="Display Upscaled Image", value=DiT_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))
    ESRGAN_settings = Container(Column([enlarge_scale_slider, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    page.ESRGAN_block_DiT = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    page.ESRGAN_block_DiT.height = None if status['installed_ESRGAN'] else 0
    if not DiT_prefs['apply_ESRGAN_upscale']:
        ESRGAN_settings.height = 0
    page.DiT_output = Column([], auto_scroll=True)
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.DiT_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("⚧️  DiT Models with Transformers Class-to-Image Generator", "Scalable Diffusion Models with Transformers...", actions=[ft.OutlinedButton("Class List", on_click=show_classes), IconButton(icon=icons.HELP, tooltip="Help with DiT Settings", on_click=DiT_help)]),
        prompt,
        #Row([prompt, mask_image, invert_mask]),
        num_inference_row,
        guidance_scale,
        Row([NumberPicker(label="Number of Images: ", min=1, max=20, value=DiT_prefs['num_images'], on_change=lambda e: changed(e, 'num_images')), seed, batch_folder_name]),
        page.ESRGAN_block_DiT,
        Row([ElevatedButton(content=Text("🔀   Get DiT Generation", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_DiT(page)),
             #ElevatedButton(content=Text(value="📜   Run from Prompts List", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_DiT(page, from_list=True))
             ]),

      ]
    )), page.DiT_output,
        clear_button,
    ], scroll=ScrollMode.AUTO, auto_scroll=False)
    return c

dall_e_prefs = {
    'prompt': '',
    'size': '512x512',
    'num_images': 1,
    'init_image': '',
    'mask_image': '',
    'variation': False,
    "invert_mask": False,
    'file_prefix': 'dalle-',
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": prefs['enlarge_scale'],
    "face_enhance": prefs['face_enhance'],
    "display_upscaled_image": prefs['display_upscaled_image'],
    "batch_folder_name": '',
}

def buildDallE2(page):
    global dall_e_prefs
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          if ptype == "int":
            dall_e_prefs[pref] = int(e.control.value)
          elif ptype == "float":
            dall_e_prefs[pref] = float(e.control.value)
          else:
            dall_e_prefs[pref] = e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def pick_files_result(e: FilePickerResultEvent):
        if e.files:
            img = e.files
            dalle = []
            fname = img[0]
            print(", ".join(map(lambda f: f.name, e.files)))
            src_path = page.get_upload_url(fname.name, 600)
            dalle.append(FilePickerUploadFile(fname.name, upload_url=src_path))
            pick_files_dialog.upload(dalle)
            print(str(src_path))
            #src_path = ''.join(src_path)
            print(str(dalle[0]))
            dst_path = os.path.join(root_dir, fname.name)
            print(f'Copy {src_path} to {dst_path}')
            #shutil.copy(src_path, dst_path)
            # TODO: is init or mask?
            init_image.value = dst_path

    pick_files_dialog = FilePicker(on_result=pick_files_result)
    page.overlay.append(pick_files_dialog)
    #selected_files = Text()

    def file_picker_result(e: FilePickerResultEvent):
        if e.files != None:
            upload_files(e)
    def on_upload_progress(e: FilePickerUploadEvent):
        nonlocal pick_type
        if e.progress == 1:
            if not slash in e.file_name:
              fname = os.path.join(root_dir, e.file_name)
            else:
              fname = e.file_name
            if pick_type == "init":
                init_image.value = fname
                init_image.update()
                dall_e_prefs['init_image'] = fname
            elif pick_type == "mask":
                mask_image.value = fname
                mask_image.update()
                dall_e_prefs['mask_image'] = fname
            page.update()
    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)
    def upload_files(e):
        dalle = []
        if file_picker.result != None and file_picker.result.files != None:
            for f in file_picker.result.files:
              if page.web:
                dalle.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))
              else:
                on_upload_progress(FilePickerUploadEvent(f.path, 1, ""))
            file_picker.upload(dalle)
    page.overlay.append(file_picker)
    pick_type = ""
    #page.overlay.append(pick_files_dialog)
    def pick_init(e):
        nonlocal pick_type
        pick_type = "init"
        file_picker.pick_files(allow_multiple=False, allowed_extensions=["png", "PNG"], dialog_title="Pick Init Image File")
    def pick_mask(e):
        nonlocal pick_type
        pick_type = "mask"
        file_picker.pick_files(allow_multiple=False, allowed_extensions=["png", "PNG"], dialog_title="Pick Black & White Mask Image")
    def toggle_ESRGAN(e):
        ESRGAN_settings.height = None if e.control.value else 0
        dall_e_prefs['apply_ESRGAN_upscale'] = e.control.value
        ESRGAN_settings.update()
    def change_enlarge_scale(e):
        enlarge_scale_slider.controls[1].value = f" {float(e.control.value)}x"
        enlarge_scale_slider.update()
        changed(e, 'enlarge_scale', ptype="float")

    prompt = TextField(label="Prompt Text", value=dall_e_prefs['prompt'], multiline=True, on_change=lambda e:changed(e,'prompt'))
    batch_folder_name = TextField(label="Batch Folder Name", value=dall_e_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    file_prefix = TextField(label="Filename Prefix", value=dall_e_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))
    #num_images = NumberPicker(label="Num of Outputs", min=1, max=4, step=4, value=dall_e_prefs['num_images'], on_change=lambda e:changed(e,'num_images', ptype="int"))
    #num_images = TextField(label="num_images", value=dall_e_prefs['num_images'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'num_images', ptype="int"))
    #n_iterations = TextField(label="Number of Iterations", value=dall_e_prefs['n_iterations'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'n_iterations', ptype="int"))
    #steps = TextField(label="Inference Steps", value=dall_e_prefs['steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'steps', ptype="int"))
    #eta = TextField(label="DDIM ETA", value=dall_e_prefs['eta'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'eta', ptype="float"))
    #seed = TextField(label="Seed", value=dall_e_prefs['seed'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'seed', ptype="int"))
    size = Dropdown(label="Image Size", width=120, options=[dropdown.Option("256x256"), dropdown.Option("512x512"), dropdown.Option("1024x1024")], value=dall_e_prefs['size'], on_change=lambda e:changed(e,'size'))
    param_rows = ResponsiveRow([Row([batch_folder_name, file_prefix], col={'lg':6}), Row([size, NumberPicker(label=" Number of Images", min=1, max=10, step=1, value=dall_e_prefs['num_images'], on_change=lambda e:changed(e,'num_images', ptype="int"))], col={'lg':6})])

    #width = Slider(min=128, max=1024, divisions=6, label="{value}px", value=dall_e_prefs['width'], on_change=change_width, expand=True)
    #width_value = Text(f" {int(dall_e_prefs['width'])}px", weight=FontWeight.BOLD)
    #width_slider = Row([Text(f"Width: "), width_value, width])
    #height = Slider(min=128, max=1024, divisions=6, label="{value}px", value=dall_e_prefs['height'], on_change=change_height, expand=True)
    #height_value = Text(f" {int(dall_e_prefs['height'])}px", weight=FontWeight.BOLD)
    #height_slider = Row([Text(f"Height: "), height_value, height])
    init_image = TextField(label="Init Image", value=dall_e_prefs['init_image'], on_change=lambda e:changed(e,'init_image'), expand=True, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_init, col={"*":1, "md":3}))
    mask_image = TextField(label="Mask Image", value=dall_e_prefs['mask_image'], on_change=lambda e:changed(e,'mask_image'), expand=True, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD_OUTLINED, on_click=pick_mask, col={"*":1, "md":3}))
    variation = Checkbox(label="Variation   ", tooltip="Creates Variation of Init Image. Disregards the Prompt and Mask.", value=dall_e_prefs['variation'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'variation'))
    invert_mask = Checkbox(label="Invert", tooltip="Swaps the Black & White of your Mask Image", value=dall_e_prefs['invert_mask'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'invert_mask'))
    image_pickers = Container(content=ResponsiveRow([Row([init_image, variation], col={"md":6}), Row([mask_image, invert_mask], col={"md":6})], run_spacing=2), padding=padding.only(top=5), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    #prompt_strength = Slider(min=0.1, max=0.9, divisions=16, label="{value}%", value=dall_e_prefs['prompt_strength'], on_change=change_strength, expand=True)
    #strength_value = Text(f" {int(dall_e_prefs['prompt_strength'] * 100)}%", weight=FontWeight.BOLD)
    #strength_slider = Row([Text("Prompt Strength: "), strength_value, prompt_strength])
    img_block = Container(Column([image_pickers, Divider(height=9, thickness=2)]), padding=padding.only(top=5), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    apply_ESRGAN_upscale = Switcher(label="Apply ESRGAN Upscale", value=dall_e_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)
    enlarge_scale_value = Text(f" {float(dall_e_prefs['enlarge_scale'])}x", weight=FontWeight.BOLD)
    enlarge_scale = Slider(min=1, max=4, divisions=6, label="{value}x", value=dall_e_prefs['enlarge_scale'], on_change=change_enlarge_scale, expand=True)
    enlarge_scale_slider = Row([Text("Enlarge Scale: "), enlarge_scale_value, enlarge_scale])
    face_enhance = Checkbox(label="Use Face Enhance GPFGAN", value=dall_e_prefs['face_enhance'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'face_enhance'))
    display_upscaled_image = Checkbox(label="Display Upscaled Image", value=dall_e_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))
    ESRGAN_settings = Container(Column([enlarge_scale_slider, face_enhance, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    page.ESRGAN_block_dalle = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    page.ESRGAN_block_dalle.height = None if status['installed_ESRGAN'] else 0
    if not dall_e_prefs['apply_ESRGAN_upscale']:
        ESRGAN_settings.height = 0
    list_button = ElevatedButton(content=Text(value="📜   Run from Prompts List", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_dall_e(page, from_list=True))
    parameters_button = ElevatedButton(content=Text(value="🖼️   Run Dall-E 2", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_dall_e(page))

    parameters_row = Row([parameters_button, list_button], spacing=22)#, alignment=MainAxisAlignment.SPACE_BETWEEN)
    page.dall_e_output = Column([])
    c = Column([Container(
        padding=padding.only(18, 14, 20, 10), content=Column([
            Header("👺  OpenAI Dall-E 2", "Generates Images using your OpenAI API Key. Note: Uses same credits as official website."),
            prompt,
            param_rows,
            img_block, page.ESRGAN_block_dalle,
            #(img_block if status['installed_img2img'] or status['installed_stability'] else Container(content=None)), (clip_block if prefs['install_CLIP_guided'] else Container(content=None)), (ESRGAN_block if prefs['install_ESRGAN'] else Container(content=None)),
            parameters_row,
            page.dall_e_output
        ],
    ))], scroll=ScrollMode.AUTO)
    return c

kandinsky_prefs = {
    "prompt": '',
    "negative_prompt": '',
    "batch_folder_name": '',
    "file_prefix": "kandinsky-",
    "num_images": 1,
    "steps":25,
    #"ddim_eta":0.05,
    "width": 512,
    "height":512,
    "guidance_scale":4,
    'prior_guidance_scale': 4.0,
    'prior_steps': 25,
    "init_image": '',
    "strength": 0.3,
    "mask_image": '',
    "invert_mask": False,
    "seed": 0,
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": prefs['enlarge_scale'],
    "face_enhance": prefs['face_enhance'],
    "display_upscaled_image": prefs['display_upscaled_image'],
}

def buildKandinsky(page):
    global prefs, kandinsky_prefs, status
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          if ptype == "int":
            kandinsky_prefs[pref] = int(e.control.value)
          elif ptype == "float":
            kandinsky_prefs[pref] = float(e.control.value)
          else:
            kandinsky_prefs[pref] = e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def kandinsky_help(e):
      def close_kandinsky_dlg(e):
        nonlocal kandinsky_help_dlg
        kandinsky_help_dlg.open = False
        page.update()
      kandinsky_help_dlg = AlertDialog(title=Text("🙅   Help with Kandinsky Pipeline"), content=Column([
          #Text("NOTE: Right now, installing this may be incompatible with Diffusers packages, so it may not work if you first installed HuggingFace & Stable Diffusion. It's recommended to run this on a fresh runtime, only installing ESRGAN to upscale. We hope to fix this soon, but works great."),
          #Text("Kandinsky 2.1 inherits best practicies from Dall-E 2 and Latent diffusion, while introducing some new ideas."),
          Markdown("Kandinsky 2.2 inherits best practices from [DALL-E 2](https://arxiv.org/abs/2204.06125) and [Latent Diffusion](https://huggingface.co/docs/diffusers/api/pipelines/latent_diffusion), while introducing some new ideas.\nIt uses [CLIP](https://huggingface.co/docs/transformers/model_doc/clip) for encoding images and text, and a diffusion image prior (mapping) between latent spaces of CLIP modalities. This approach enhances the visual performance of the model and unveils new horizons in blending images and text-guided image manipulation.\nThe Kandinsky model is created by [Arseniy Shakhmatov](https://github.com/cene555), [Anton Razzhigaev](https://github.com/razzant), [Aleksandr Nikolich](https://github.com/AlexWortega), [Igor Pavlov](https://github.com/boomb0om), [Andrey Kuznetsov](https://github.com/kuznetsoffandrey) and [Denis Dimitrov](https://github.com/denndimitrov) and the original codebase can be found [here](https://github.com/ai-forever/Kandinsky-2)", on_tap_link=lambda e: e.page.launch_url(e.data)),
          Text("As text and image encoder it uses CLIP model and diffusion image prior (mapping) between latent spaces of CLIP modalities. This approach increases the visual performance of the model and unveils new horizons in blending images and text-guided image manipulation. For diffusion mapping of latent spaces we use transformer with num_layers=20, num_heads=32 and hidden_size=2048. Kandinsky 2.1 was trained on a large-scale image-text dataset LAION HighRes and fine-tuned on our internal datasets. These encoders and multilingual training datasets unveil the real multilingual text-to-image generation experience!"),
          Text("The decision to make changes to the architecture came after continuing to learn the Kandinsky 2.0 version and trying to get stable text embeddings of the mT5 multilingual language model. The logical conclusion was that the use of only text embedding was not enough for high-quality image synthesis. After analyzing once again the existing DALL-E 2 solution from OpenAI, it was decided to experiment with the image prior model (allows you to generate visual embedding CLIP by text prompt or text embedding CLIP), while remaining in the latent visual space paradigm, so that you do not have to retrain the diffusion part of the UNet model Kandinsky 2.0. Now a little more details about the learning process of Kandinsky 2.1."),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("🤤  Quality... ", on_click=close_kandinsky_dlg)], actions_alignment=MainAxisAlignment.END)
      page.dialog = kandinsky_help_dlg
      kandinsky_help_dlg.open = True
      page.update()
    def pick_files_result(e: FilePickerResultEvent):
        if e.files:
            img = e.files
            uf = []
            fname = img[0]
            #print(", ".join(map(lambda f: f.name, e.files)))
            src_path = page.get_upload_url(fname.name, 600)
            uf.append(FilePickerUploadFile(fname.name, upload_url=src_path))
            pick_files_dialog.upload(uf)
            #print(str(src_path))
            #src_path = ''.join(src_path)
            #print(str(uf[0]))
            dst_path = os.path.join(root_dir, fname.name)
            #print(f'Copy {src_path} to {dst_path}')
            #shutil.copy(src_path, dst_path)
            # TODO: is init or mask?
            init_image.value = dst_path
    pick_files_dialog = FilePicker(on_result=pick_files_result)
    page.overlay.append(pick_files_dialog)
    #selected_files = Text()
    def file_picker_result(e: FilePickerResultEvent):
        if e.files != None:
            upload_files(e)
    def on_upload_progress(e: FilePickerUploadEvent):
        nonlocal pick_type
        if e.progress == 1:
            if not slash in e.file_name:
              fname = os.path.join(root_dir, e.file_name)
            else:
              fname = e.file_name
            if pick_type == "init":
                init_image.value = fname
                init_image.update()
                kandinsky_prefs['init_image'] = fname
            elif pick_type == "mask":
                mask_image.value = fname
                mask_image.update()
                kandinsky_prefs['mask_image'] = fname
            page.update()
    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)
    def upload_files(e):
        uf = []
        if file_picker.result != None and file_picker.result.files != None:
            for f in file_picker.result.files:
              if page.web:
                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))
              else:
                on_upload_progress(FilePickerUploadEvent(f.path, 1, ""))
            file_picker.upload(uf)
    page.overlay.append(file_picker)
    pick_type = ""
    #page.overlay.append(pick_files_dialog)
    def pick_init(e):
        nonlocal pick_type
        pick_type = "init"
        file_picker.pick_files(allow_multiple=False, allowed_extensions=["png", "PNG"], dialog_title="Pick Init Image File")
    def pick_mask(e):
        nonlocal pick_type
        pick_type = "mask"
        file_picker.pick_files(allow_multiple=False, allowed_extensions=["png", "PNG"], dialog_title="Pick Black & White Mask Image")
    def toggle_ESRGAN(e):
        ESRGAN_settings.height = None if e.control.value else 0
        kandinsky_prefs['apply_ESRGAN_upscale'] = e.control.value
        ESRGAN_settings.update()
    prompt = TextField(label="Prompt Text", value=kandinsky_prefs['prompt'], multiline=True, col={'md':9}, on_change=lambda e:changed(e,'prompt'))
    negative_prompt = TextField(label="Negative Prompt Text", value=kandinsky_prefs['negative_prompt'], multiline=True, col={'md':3}, on_change=lambda e:changed(e,'negative_prompt'))
    batch_folder_name = TextField(label="Batch Folder Name", value=kandinsky_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    file_prefix = TextField(label="Filename Prefix", value=kandinsky_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))
    #num_outputs = NumberPicker(label="Num of Outputs", min=1, max=4, step=4, value=kandinsky_prefs['num_outputs'], on_change=lambda e:changed(e,'num_outputs', ptype="int"))
    #num_outputs = TextField(label="num_outputs", value=kandinsky_prefs['num_outputs'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'num_outputs', ptype="int"))
    #n_iterations = TextField(label="Number of Iterations", value=kandinsky_prefs['n_iterations'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'n_iterations', ptype="int"))
    steps = TextField(label="Number of Steps", value=kandinsky_prefs['steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'steps', ptype="int"))
    #ddim_eta = TextField(label="DDIM ETA", value=kandinsky_prefs['ddim_eta'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'ddim_eta', ptype="float"))
    #dynamic_threshold_v = TextField(label="Dynamic Threshold", value=kandinsky_prefs['dynamic_threshold_v'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'dynamic_threshold_v', ptype="float"))
    #sampler = Dropdown(label="Sampler", width=200, options=[dropdown.Option("ddim_sampler"), dropdown.Option("p_sampler")], value=kandinsky_prefs['sampler'], on_change=lambda e:changed(e,'sampler'), col={'xs':12, 'md':6})
    n_images = NumberPicker(label="Number of Images", min=1, max=9, step=1, value=kandinsky_prefs['num_images'], on_change=lambda e:changed(e,'num_images', ptype="int"))
    #param_rows = ResponsiveRow([Column([batch_folder_name, n_images], col={'xs':12, 'md':6}),
                      #Column([file_prefix, sampler], col={'xs':12, 'md':6})
                      #Column([steps, ddim_eta, dynamic_threshold_v], col={'xs':12, 'md':6})
                      #], vertical_alignment=CrossAxisAlignment.START)
    #denoised_type = Dropdown(label="Denoised Type", width=180, options=[dropdown.Option("dynamic_threshold"), dropdown.Option("clip_denoised")], value=kandinsky_prefs['denoised_type'], on_change=lambda e:changed(e,'denoised_type'), col={'xs':12, 'md':6})
    #dropdown_row = ResponsiveRow([sampler])#, denoised_type])
    steps = SliderRow(label="Number of Steps", min=0, max=200, divisions=200, pref=kandinsky_prefs, key='steps')
    prior_guidance_scale = SliderRow(label="Prior Guidance Scale", min=0, max=10, divisions=20, round=1, expand=True, pref=kandinsky_prefs, key='prior_guidance_scale', col={'xs':12, 'md':6})
    prior_steps = SliderRow(label="Prior Steps", min=0, max=50, divisions=50, expand=True, pref=kandinsky_prefs, key='prior_steps', col={'xs':12, 'md':6})
    guidance = SliderRow(label="Guidance Scale", min=0, max=50, divisions=50, pref=kandinsky_prefs, key='guidance_scale')
    width_slider = SliderRow(label="Width", min=128, max=1024, divisions=14, multiple=32, suffix="px", pref=kandinsky_prefs, key='width')
    height_slider = SliderRow(label="Height", min=128, max=1024, divisions=14, multiple=32, suffix="px", pref=kandinsky_prefs, key='height')
    init_image = TextField(label="Init Image", value=kandinsky_prefs['init_image'], on_change=lambda e:changed(e,'init_image'), expand=True, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_init), col={'xs':12, 'md':6})
    mask_image = TextField(label="Mask Image", value=kandinsky_prefs['mask_image'], on_change=lambda e:changed(e,'mask_image'), expand=True, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD_OUTLINED, on_click=pick_mask), col={'xs':10, 'md':5})
    invert_mask = Checkbox(label="Invert", tooltip="Swaps the Black & White of your Mask Image", value=kandinsky_prefs['invert_mask'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'invert_mask'), col={'xs':2, 'md':1})
    image_pickers = Container(content=ResponsiveRow([init_image, mask_image, invert_mask]), padding=padding.only(top=5), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    strength_slider = SliderRow(label="Init Image Strength", min=0.1, max=0.9, divisions=16, round=2, pref=kandinsky_prefs, key='strength')
    img_block = Container(Column([image_pickers, strength_slider, Divider(height=9, thickness=2)]), padding=padding.only(top=5), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    seed = TextField(label="Seed", width=90, value=str(kandinsky_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    apply_ESRGAN_upscale = Switcher(label="Apply ESRGAN Upscale", value=kandinsky_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)
    enlarge_scale_slider = SliderRow(label="Enlarge Scale", min=1, max=4, divisions=6, round=1, suffix="x", pref=kandinsky_prefs, key='enlarge_scale')
    face_enhance = Checkbox(label="Use Face Enhance GPFGAN", value=kandinsky_prefs['face_enhance'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'face_enhance'))
    display_upscaled_image = Checkbox(label="Display Upscaled Image", value=kandinsky_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))
    ESRGAN_settings = Container(Column([enlarge_scale_slider, face_enhance, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    page.ESRGAN_block_kandinsky = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    page.ESRGAN_block_kandinsky.height = None if status['installed_ESRGAN'] else 0
    if not kandinsky_prefs['apply_ESRGAN_upscale']:
        ESRGAN_settings.height = 0
    parameters_button = ElevatedButton(content=Text(value="✨   Run Kandinsky 2.2", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_kandinsky(page))
    from_list_button = ElevatedButton(content=Text(value="📜   Run from Prompts List", size=20), tooltip="Uses all queued Image Parameters per prompt in Prompt List", color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_kandinsky(page, from_list=True))
    from_list_with_params_button = ElevatedButton(content=Text(value="📜   Run from Prompts List /w these Parameters", size=20), tooltip="Uses above settings per prompt in Prompt List", color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_kandinsky(page, from_list=True, with_params=True))
    parameters_row = Row([parameters_button, from_list_button, from_list_with_params_button], wrap=True) #, alignment=MainAxisAlignment.SPACE_BETWEEN
    page.kandinsky_output = Column([])
    c = Column([Container(
        padding=padding.only(18, 14, 20, 10), content=Column([
            Header("🎎  Kandinsky 2.2", "A Latent Diffusion model with two Multilingual text encoders, supports 100+ languages, made in Russia.", actions=[IconButton(icon=icons.HELP, tooltip="Help with Kandinsky Settings", on_click=kandinsky_help)]),
            ResponsiveRow([prompt, negative_prompt]),
            #param_rows, #dropdown_row,
            ResponsiveRow([prior_steps, prior_guidance_scale]),
            steps,
            guidance, width_slider, height_slider, #Divider(height=9, thickness=2),
            img_block,
            #Row([batch_folder_name, file_prefix]),
            ResponsiveRow([Row([n_images, seed], col={'md':6}), Row([batch_folder_name, file_prefix], col={'md':6})]),
            page.ESRGAN_block_kandinsky,
            #(img_block if status['installed_img2img'] or status['installed_stability'] else Container(content=None)), (clip_block if prefs['install_CLIP_guided'] else Container(content=None)), (ESRGAN_block if prefs['install_ESRGAN'] else Container(content=None)),
            parameters_row,
            page.kandinsky_output
        ],
    ))], scroll=ScrollMode.AUTO)#batch_folder_name, batch_size, n_iterations, steps, ddim_eta, seed,
    return c

kandinsky2_prefs = {
    "prompt": '',
    "batch_folder_name": '',
    "file_prefix": "kandinsky-",
    "num_images": 1,
    "steps":100,
    "ddim_eta":0.05,
    "width": 512,
    "height":512,
    "guidance_scale":4,
    'prior_cf_scale': 4,
    'prior_steps': "25",
    "dynamic_threshold_v":99.5,
    "sampler": "ddim_sampler",
    "denoised_type": "dynamic_threshold",
    "init_image": '',
    "strength": 0.5,
    "mask_image": '',
    "invert_mask": False,
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": prefs['enlarge_scale'],
    "face_enhance": prefs['face_enhance'],
    "display_upscaled_image": prefs['display_upscaled_image'],
}

def buildKandinsky2(page):
    global prefs, kandinsky2_prefs, status
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          if ptype == "int":
            kandinsky2_prefs[pref] = int(e.control.value)
          elif ptype == "float":
            kandinsky2_prefs[pref] = float(e.control.value)
          else:
            kandinsky2_prefs[pref] = e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def kandinsky2_help(e):
      def close_kandinsky2_dlg(e):
        nonlocal kandinsky2_help_dlg
        kandinsky2_help_dlg.open = False
        page.update()
      kandinsky2_help_dlg = AlertDialog(title=Text("🙅   Help with Kandinsky Pipeline"), content=Column([
          Text("NOTE: Right now, installing this may be incompatible with Diffusers packages, so it may not work if you first installed HuggingFace & Stable Diffusion. It's recommended to run this on a fresh runtime, only installing ESRGAN to upscale. We hope to fix this soon, but works great."),
          Text("Kandinsky 2.1 inherits best practicies from Dall-E 2 and Latent diffusion, while introducing some new ideas."),
          Text("As text and image encoder it uses CLIP model and diffusion image prior (mapping) between latent spaces of CLIP modalities. This approach increases the visual performance of the model and unveils new horizons in blending images and text-guided image manipulation. For diffusion mapping of latent spaces we use transformer with num_layers=20, num_heads=32 and hidden_size=2048. Kandinsky 2.1 was trained on a large-scale image-text dataset LAION HighRes and fine-tuned on our internal datasets. These encoders and multilingual training datasets unveil the real multilingual text-to-image generation experience!"),
          Text("The decision to make changes to the architecture came after continuing to learn the Kandinsky 2.0 version and trying to get stable text embeddings of the mT5 multilingual language model. The logical conclusion was that the use of only text embedding was not enough for high-quality image synthesis. After analyzing once again the existing DALL-E 2 solution from OpenAI, it was decided to experiment with the image prior model (allows you to generate visual embedding CLIP by text prompt or text embedding CLIP), while remaining in the latent visual space paradigm, so that you do not have to retrain the diffusion part of the UNet model Kandinsky 2.0. Now a little more details about the learning process of Kandinsky 2.1."),
          Markdown("[Kandinsky GitHub](https://github.com/ai-forever/Kandinsky-2) | [Kandinsky 2.1 Blog](https://habr.com/ru/companies/sberbank/articles/725282/) | [FusionBrain Demo](https://fusionbrain.ai/diffusion)", on_tap_link=lambda e: e.page.launch_url(e.data)),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("🤤  Quality... ", on_click=close_kandinsky2_dlg)], actions_alignment=MainAxisAlignment.END)
      page.dialog = kandinsky2_help_dlg
      kandinsky2_help_dlg.open = True
      page.update()
    def pick_files_result(e: FilePickerResultEvent):
        if e.files:
            img = e.files
            uf = []
            fname = img[0]
            #print(", ".join(map(lambda f: f.name, e.files)))
            src_path = page.get_upload_url(fname.name, 600)
            uf.append(FilePickerUploadFile(fname.name, upload_url=src_path))
            pick_files_dialog.upload(uf)
            #print(str(src_path))
            #src_path = ''.join(src_path)
            #print(str(uf[0]))
            dst_path = os.path.join(root_dir, fname.name)
            #print(f'Copy {src_path} to {dst_path}')
            #shutil.copy(src_path, dst_path)
            # TODO: is init or mask?
            init_image.value = dst_path
    pick_files_dialog = FilePicker(on_result=pick_files_result)
    page.overlay.append(pick_files_dialog)
    #selected_files = Text()
    def file_picker_result(e: FilePickerResultEvent):
        if e.files != None:
            upload_files(e)
    def on_upload_progress(e: FilePickerUploadEvent):
        nonlocal pick_type
        if e.progress == 1:
            if not slash in e.file_name:
              fname = os.path.join(root_dir, e.file_name)
            else:
              fname = e.file_name
            if pick_type == "init":
                init_image.value = fname
                init_image.update()
                kandinsky2_prefs['init_image'] = fname
            elif pick_type == "mask":
                mask_image.value = fname
                mask_image.update()
                kandinsky2_prefs['mask_image'] = fname
            page.update()
    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)
    def upload_files(e):
        uf = []
        if file_picker.result != None and file_picker.result.files != None:
            for f in file_picker.result.files:
              if page.web:
                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))
              else:
                on_upload_progress(FilePickerUploadEvent(f.path, 1, ""))
            file_picker.upload(uf)
    page.overlay.append(file_picker)
    pick_type = ""
    #page.overlay.append(pick_files_dialog)
    def pick_init(e):
        nonlocal pick_type
        pick_type = "init"
        file_picker.pick_files(allow_multiple=False, allowed_extensions=["png", "PNG"], dialog_title="Pick Init Image File")
    def pick_mask(e):
        nonlocal pick_type
        pick_type = "mask"
        file_picker.pick_files(allow_multiple=False, allowed_extensions=["png", "PNG"], dialog_title="Pick Black & White Mask Image")
    def toggle_ESRGAN(e):
        ESRGAN_settings.height = None if e.control.value else 0
        kandinsky2_prefs['apply_ESRGAN_upscale'] = e.control.value
        ESRGAN_settings.update()
    prompt = TextField(label="Prompt Text", value=kandinsky2_prefs['prompt'], multiline=True, on_change=lambda e:changed(e,'prompt'))
    batch_folder_name = TextField(label="Batch Folder Name", value=kandinsky2_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    file_prefix = TextField(label="Filename Prefix", value=kandinsky2_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))
    #num_outputs = NumberPicker(label="Num of Outputs", min=1, max=4, step=4, value=kandinsky2_prefs['num_outputs'], on_change=lambda e:changed(e,'num_outputs', ptype="int"))
    #num_outputs = TextField(label="num_outputs", value=kandinsky2_prefs['num_outputs'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'num_outputs', ptype="int"))
    #n_iterations = TextField(label="Number of Iterations", value=kandinsky2_prefs['n_iterations'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'n_iterations', ptype="int"))
    steps = TextField(label="Number of Steps", value=kandinsky2_prefs['steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'steps', ptype="int"))
    ddim_eta = TextField(label="DDIM ETA", value=kandinsky2_prefs['ddim_eta'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'ddim_eta', ptype="float"))
    dynamic_threshold_v = TextField(label="Dynamic Threshold", value=kandinsky2_prefs['dynamic_threshold_v'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'dynamic_threshold_v', ptype="float"))
    sampler = Dropdown(label="Sampler", width=200, options=[dropdown.Option("ddim_sampler"), dropdown.Option("p_sampler")], value=kandinsky2_prefs['sampler'], on_change=lambda e:changed(e,'sampler'), col={'xs':12, 'md':6})
    n_images = NumberPicker(label="Number of Images", min=1, max=9, step=1, value=kandinsky2_prefs['num_images'], on_change=lambda e:changed(e,'num_images', ptype="int"))
    param_rows = ResponsiveRow([Column([batch_folder_name, n_images], col={'xs':12, 'md':6}),
                      Column([file_prefix, sampler], col={'xs':12, 'md':6})
                      #Column([steps, ddim_eta, dynamic_threshold_v], col={'xs':12, 'md':6})
                      ], vertical_alignment=CrossAxisAlignment.START)
    denoised_type = Dropdown(label="Denoised Type", width=180, options=[dropdown.Option("dynamic_threshold"), dropdown.Option("clip_denoised")], value=kandinsky2_prefs['denoised_type'], on_change=lambda e:changed(e,'denoised_type'), col={'xs':12, 'md':6})
    dropdown_row = ResponsiveRow([sampler])#, denoised_type])
    steps = SliderRow(label="Number of Steps", min=0, max=200, divisions=200, pref=kandinsky2_prefs, key='steps')
    prior_cf_scale = SliderRow(label="Prior CF Scale", min=0, max=10, divisions=10, pref=kandinsky2_prefs, key='prior_cf_scale')
    prior_steps = SliderRow(label="Prior Steps", min=0, max=50, divisions=50, pref=kandinsky2_prefs, key='prior_steps')
    guidance = SliderRow(label="Guidance Scale", min=0, max=50, divisions=50, pref=kandinsky2_prefs, key='guidance_scale')
    width_slider = SliderRow(label="Width", min=128, max=1024, divisions=14, multiple=32, suffix="px", pref=kandinsky2_prefs, key='width')
    height_slider = SliderRow(label="Height", min=128, max=1024, divisions=14, multiple=32, suffix="px", pref=kandinsky2_prefs, key='height')
    init_image = TextField(label="Init Image", value=kandinsky2_prefs['init_image'], on_change=lambda e:changed(e,'init_image'), expand=True, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_init), col={'xs':12, 'md':6})
    mask_image = TextField(label="Mask Image", value=kandinsky2_prefs['mask_image'], on_change=lambda e:changed(e,'mask_image'), expand=True, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD_OUTLINED, on_click=pick_mask), col={'xs':10, 'md':5})
    invert_mask = Checkbox(label="Invert", tooltip="Swaps the Black & White of your Mask Image", value=kandinsky2_prefs['invert_mask'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'invert_mask'), col={'xs':2, 'md':1})
    image_pickers = Container(content=ResponsiveRow([init_image, mask_image, invert_mask]), padding=padding.only(top=5), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    strength_slider = SliderRow(label="Init Image Strength", min=0.1, max=0.9, divisions=16, round=2, pref=kandinsky2_prefs, key='strength')
    img_block = Container(Column([image_pickers, strength_slider, Divider(height=9, thickness=2)]), padding=padding.only(top=5), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    apply_ESRGAN_upscale = Switcher(label="Apply ESRGAN Upscale", value=kandinsky2_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)
    enlarge_scale_slider = SliderRow(label="Enlarge Scale", min=1, max=4, divisions=6, round=1, suffix="x", pref=kandinsky2_prefs, key='enlarge_scale')
    face_enhance = Checkbox(label="Use Face Enhance GPFGAN", value=kandinsky2_prefs['face_enhance'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'face_enhance'))
    display_upscaled_image = Checkbox(label="Display Upscaled Image", value=kandinsky2_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))
    ESRGAN_settings = Container(Column([enlarge_scale_slider, face_enhance, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    page.ESRGAN_block_kandinsky2 = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    page.ESRGAN_block_kandinsky2.height = None if status['installed_ESRGAN'] else 0
    if not kandinsky2_prefs['apply_ESRGAN_upscale']:
        ESRGAN_settings.height = 0
    parameters_button = ElevatedButton(content=Text(value="✨   Run Kandinsky 2.2", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_kandinsky(page))

    parameters_row = Row([parameters_button], alignment=MainAxisAlignment.SPACE_BETWEEN)
    page.kandinsky2_output = Column([])
    c = Column([Container(
        padding=padding.only(18, 14, 20, 10), content=Column([
            Header("🎎  Kandinsky 2.1", "A Latent Diffusion model with two Multilingual text encoders, supports 100+ languages, made in Russia.", actions=[IconButton(icon=icons.HELP, tooltip="Help with Kandinsky Settings", on_click=kandinsky2_help)]),
            prompt,
            #param_rows, #dropdown_row,
            steps, prior_steps, prior_cf_scale,
            guidance, width_slider, height_slider, #Divider(height=9, thickness=2),
            img_block,
            Row([batch_folder_name, file_prefix]),
            Row([n_images, sampler]),
            page.ESRGAN_block_kandinsky2,
            #(img_block if status['installed_img2img'] or status['installed_stability'] else Container(content=None)), (clip_block if prefs['install_CLIP_guided'] else Container(content=None)), (ESRGAN_block if prefs['install_ESRGAN'] else Container(content=None)),
            parameters_row,
            page.kandinsky2_output
        ],
    ))], scroll=ScrollMode.AUTO)#batch_folder_name, batch_size, n_iterations, steps, ddim_eta, seed,
    return c

kandinsky_fuse_prefs = {
    "prompt": '',
    "batch_folder_name": '',
    "file_prefix": "kandinsky-",
    "num_images": 1,
    "mixes": [],
    "steps":25,
    "width": 512,
    "height":512,
    "guidance_scale":8,
    #'prior_cf_scale': 4,
    #'prior_steps': "25",
    #"sampler": "ddim_sampler",
    "init_image": '',
    "weight": 0.5,
    #"mask_image": '',
    #"invert_mask": False,
    "seed": 0,
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": prefs['enlarge_scale'],
    "face_enhance": prefs['face_enhance'],
    "display_upscaled_image": prefs['display_upscaled_image'],
}

def buildKandinskyFuse(page):
    global prefs, kandinsky_fuse_prefs, status
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          if ptype == "int":
            kandinsky_fuse_prefs[pref] = int(e.control.value)
          elif ptype == "float":
            kandinsky_fuse_prefs[pref] = float(e.control.value)
          else:
            kandinsky_fuse_prefs[pref] = e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def pick_files_result(e: FilePickerResultEvent):
        if e.files:
            img = e.files
            uf = []
            fname = img[0]
            #print(", ".join(map(lambda f: f.name, e.files)))
            src_path = page.get_upload_url(fname.name, 600)
            uf.append(FilePickerUploadFile(fname.name, upload_url=src_path))
            pick_files_dialog.upload(uf)
            #print(str(src_path))
            #src_path = ''.join(src_path)
            #print(str(uf[0]))
            dst_path = os.path.join(root_dir, fname.name)
            #print(f'Copy {src_path} to {dst_path}')
            #shutil.copy(src_path, dst_path)
            init_image.value = dst_path
    pick_files_dialog = FilePicker(on_result=pick_files_result)
    page.overlay.append(pick_files_dialog)
    #selected_files = Text()
    def file_picker_result(e: FilePickerResultEvent):
        if e.files != None:
            upload_files(e)
    def on_upload_progress(e: FilePickerUploadEvent):
        nonlocal pick_type
        if e.progress == 1:
            if not slash in e.file_name:
              fname = os.path.join(root_dir, e.file_name)
            else:
              fname = e.file_name
            init_image.value = fname
            init_image.update()
            kandinsky_fuse_prefs['init_image'] = fname
            page.update()
    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)
    def upload_files(e):
        uf = []
        if file_picker.result != None and file_picker.result.files != None:
            for f in file_picker.result.files:
              if page.web:
                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))
              else:
                on_upload_progress(FilePickerUploadEvent(f.path, 1, ""))
            file_picker.upload(uf)
    page.overlay.append(file_picker)
    pick_type = ""
    #page.overlay.append(pick_files_dialog)
    def pick_init(e):
        nonlocal pick_type
        pick_type = "init"
        file_picker.pick_files(allow_multiple=False, allowed_extensions=["png", "PNG"], dialog_title="Pick Init Image File")
    def toggle_ESRGAN(e):
        ESRGAN_settings.height = None if e.control.value else 0
        kandinsky_fuse_prefs['apply_ESRGAN_upscale'] = e.control.value
        ESRGAN_settings.update()
    def kandinsky_help(e):
      def close_kandinsky_dlg(e):
        nonlocal kandinsky_help_dlg
        kandinsky_help_dlg.open = False
        page.update()
      kandinsky_help_dlg = AlertDialog(title=Text("🙅   Help with Kandinsky Fuse Pipeline"), content=Column([
          #Text("NOTE: Right now, installing this may be incompatible with Diffusers packages, so it may not work if you first installed HuggingFace & Stable Diffusion. It's recommended to run this on a fresh runtime, only installing ESRGAN to upscale. We hope to fix this soon, but works great."),
          Text("This variation lets you fuse together many images together with multiple text prompts to create a mix. Set the weights of the prompts and images to adjust the amount of influence it has on the generated style. Get experimental"),
          Text("Kandinsky 2.2 inherits best practicies from Dall-E 2 and Latent diffusion, while introducing some new ideas."),
          Text("As text and image encoder it uses CLIP model and diffusion image prior (mapping) between latent spaces of CLIP modalities. This approach increases the visual performance of the model and unveils new horizons in blending images and text-guided image manipulation. For diffusion mapping of latent spaces we use transformer with num_layers=20, num_heads=32 and hidden_size=2048. Kandinsky 2.1 was trained on a large-scale image-text dataset LAION HighRes and fine-tuned on our internal datasets. These encoders and multilingual training datasets unveil the real multilingual text-to-image generation experience!"),
          Text("The decision to make changes to the architecture came after continuing to learn the Kandinsky 2.0 version and trying to get stable text embeddings of the mT5 multilingual language model. The logical conclusion was that the use of only text embedding was not enough for high-quality image synthesis. After analyzing once again the existing DALL-E 2 solution from OpenAI, it was decided to experiment with the image prior model (allows you to generate visual embedding CLIP by text prompt or text embedding CLIP), while remaining in the latent visual space paradigm, so that you do not have to retrain the diffusion part of the UNet model Kandinsky 2.0. Now a little more details about the learning process of Kandinsky 2.1."),
          Markdown("[Kandinsky GitHub](https://github.com/ai-forever/Kandinsky-2) | [Kandinsky 2.1 Blog](https://habr.com/ru/companies/sberbank/articles/725282/) | [FusionBrain Demo](https://fusionbrain.ai/diffusion)"),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("🫢  Possibility Overload... ", on_click=close_kandinsky_dlg)], actions_alignment=MainAxisAlignment.END)
      page.dialog = kandinsky_help_dlg
      kandinsky_help_dlg.open = True
      page.update()
    def add_image(e):
        if not bool(kandinsky_fuse_prefs['init_image']): return
        layer = {'init_image': kandinsky_fuse_prefs['init_image'], 'weight': kandinsky_fuse_prefs['weight']}
        kandinsky_fuse_prefs['mixes'].append(layer)
        fuse_layers.controls.append(ListTile(title=Row([Text(layer['init_image'], weight=FontWeight.BOLD), Text(f"Weight: {layer['weight']}")], alignment=MainAxisAlignment.SPACE_BETWEEN), dense=True, trailing=PopupMenuButton(icon=icons.MORE_VERT,
          items=[
              PopupMenuItem(icon=icons.EDIT, text="Edit Image Layer", on_click=edit_layer, data=layer),
              PopupMenuItem(icon=icons.DELETE, text="Delete Image Layer", on_click=delete_layer, data=layer),
              PopupMenuItem(icon=icons.DELETE_SWEEP, text="Delete All Layers", on_click=delete_all_layers, data=layer),
              PopupMenuItem(icon=icons.ARROW_UPWARD, text="Move Up", on_click=move_up, data=layer),
              PopupMenuItem(icon=icons.ARROW_DOWNWARD, text="Move Down", on_click=move_down, data=layer),
          ]), data=layer, on_click=edit_layer))
        fuse_layers.update()
        kandinsky_fuse_prefs['init_image'] = ""
        init_image.value = ""
        init_image.update()
    def add_prompt(e):
        if not bool(kandinsky_fuse_prefs['prompt']): return
        layer = {'prompt': kandinsky_fuse_prefs['prompt'], 'weight': kandinsky_fuse_prefs['weight']}
        kandinsky_fuse_prefs['mixes'].append(layer)
        fuse_layers.controls.append(ListTile(title=Row([Text(layer['prompt'], weight=FontWeight.BOLD), Text(f"Weight: {layer['weight']}")], alignment=MainAxisAlignment.SPACE_BETWEEN), dense=True, trailing=PopupMenuButton(icon=icons.MORE_VERT,
          items=[
              PopupMenuItem(icon=icons.EDIT, text="Edit Text Layer", on_click=edit_layer, data=layer),
              PopupMenuItem(icon=icons.DELETE, text="Delete Text Layer", on_click=delete_layer, data=layer),
              PopupMenuItem(icon=icons.DELETE_SWEEP, text="Delete All Layers", on_click=delete_all_layers, data=layer),
              PopupMenuItem(icon=icons.ARROW_UPWARD, text="Move Up", on_click=move_up, data=layer),
              PopupMenuItem(icon=icons.ARROW_DOWNWARD, text="Move Down", on_click=move_down, data=layer),
          ]), data=layer, on_click=edit_layer))
        fuse_layers.update()
        kandinsky_fuse_prefs['prompt'] = ""
        prompt.value = ""
        prompt.update()
    def delete_layer(e):
        kandinsky_fuse_prefs['mixes'].remove(e.control.data)
        for c in fuse_layers.controls:
            if 'prompt' in c.data:
                if c.data['prompt'] == e.control.data['prompt']:
                    fuse_layers.controls.remove(c)
                    break
            else:
                if c.data['init_image'] == e.control.data['init_image']:
                    fuse_layers.controls.remove(c)
                    break
        fuse_layers.update()
    def delete_all_layers(e):
        kandinsky_fuse_prefs['mixes'].clear()
        fuse_layers.controls.clear()
        fuse_layers.update()
    def move_down(e):
        idx = kandinsky_fuse_prefs['mixes'].index(e.control.data)
        if idx < (len(kandinsky_fuse_prefs['mixes']) - 1):
          d = kandinsky_fuse_prefs['mixes'].pop(idx)
          kandinsky_fuse_prefs['mixes'].insert(idx+1, d)
          dr = fuse_layers.controls.pop(idx)
          fuse_layers.controls.insert(idx+1, dr)
          fuse_layers.update()
    def move_up(e):
        idx = kandinsky_fuse_prefs['mixes'].index(e.control.data)
        if idx > 0:
          d = kandinsky_fuse_prefs['mixes'].pop(idx)
          kandinsky_fuse_prefs['mixes'].insert(idx-1, d)
          dr = fuse_layers.controls.pop(idx)
          fuse_layers.controls.insert(idx-1, dr)
          fuse_layers.update()
    def edit_layer(e):
        data = e.control.data
        layer_type = "prompt" if "prompt" in data else "image"
        def pick_files_result(e: FilePickerResultEvent):
            if e.files:
                img = e.files
                uf = []
                fname = img[0]
                src_path = page.get_upload_url(fname.name, 600)
                uf.append(FilePickerUploadFile(fname.name, upload_url=src_path))
                pick_files_dialog.upload(uf)
                dst_path = os.path.join(root_dir, fname.name)
                image_mix.value = dst_path
        pick_files_dialog = FilePicker(on_result=pick_files_result)
        page.overlay.append(pick_files_dialog)
        def file_picker_result(e: FilePickerResultEvent):
            if e.files != None:
                upload_files(e)
        def on_upload_progress(e: FilePickerUploadEvent):
            if e.progress == 1:
                if not slash in e.file_name:
                  fname = os.path.join(root_dir, e.file_name)
                else:
                  fname = e.file_name
                image_mix.value = fname
                image_mix.update()
                data['init_image'] = fname
                page.update()
        file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)
        def upload_files(e):
            uf = []
            if file_picker.result != None and file_picker.result.files != None:
                for f in file_picker.result.files:
                  if page.web:
                    uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))
                  else:
                    on_upload_progress(FilePickerUploadEvent(f.path, 1, ""))
                file_picker.upload(uf)
        page.overlay.append(file_picker)
        def pick_image(e):
            file_picker.pick_files(allow_multiple=False, allowed_extensions=["png", "PNG"], dialog_title="Pick Init Image File")
        #name = e.control.title.controls[0].value
        #path = e.control.title.controls[1].value
        if layer_type == "prompt":
            prompt_value = data["prompt"]
            image_value = ""
        else:
            prompt_value = ""
            image_value = data["init_image"]
        def close_dlg(e):
            dlg_edit.open = False
            page.update()
        def save_layer(e):
            layer = None
            for l in kandinsky_fuse_prefs['mixes']:
                if "prompt" in l:
                  if layer_type == "prompt":
                      if data["prompt"] == l["prompt"]:
                        layer = l
                        layer['prompt'] = prompt_text.value
                        break
                else:
                    if layer_type == "image":
                      if data["init_image"] == l["init_image"]:
                        layer = l
                        layer['init_image'] = image_mix.value
                        break
            for c in fuse_layers.controls:
                if 'prompt' in c.data:
                    if 'prompt' not in data: continue
                    if c.data['prompt'] == data['prompt']:
                        c.title.controls[0].value = layer['prompt']
                        c.title.controls[1].value = f"Weight: {layer['weight']}"
                        c.update()
                        break
                else:
                    if 'init_image' not in data: continue
                    if c.data['init_image'] == data['init_image']:
                        c.title.controls[0].value = layer['init_image']
                        c.title.controls[1].value = f"Weight: {layer['weight']}"
                        c.update()
                        break
            layer['prompt'] = prompt_text.value
            #layer['weight'] = model_path.value
            dlg_edit.open = False
            e.control.update()
            page.update()
        prompt_text = TextField(label="Fuse Prompt Text", value=prompt_value, multiline=True, visible=layer_type == "prompt")
        image_mix = TextField(label="Fuse Image Path", value=image_value, visible=layer_type == "image", height=65, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_image))
        edit_weights = SliderRow(label="Weight/Strength", min=0, max=1, divisions=20, round=1, pref=data, key='weight', tooltip="Indicates how much each individual concept should influence the overall guidance. If no weights are provided all concepts are applied equally.")
        dlg_edit = AlertDialog(modal=False, title=Text(f"🧳 Edit Kandinsky Fuse {layer_type.title()} Mix"), content=Container(Column([prompt_text, image_mix, edit_weights], alignment=MainAxisAlignment.START, tight=True, scroll=ScrollMode.AUTO, width=(page.width if page.web else page.window_width) - 100)), actions=[TextButton(content=Text("Cancel", size=18), on_click=close_dlg), ElevatedButton(content=Text(value=emojize(":floppy_disk:") + "  Save Layer ", size=19, weight=FontWeight.BOLD), on_click=save_layer)], actions_alignment=MainAxisAlignment.END)
        page.dialog = dlg_edit
        dlg_edit.open = True
        page.update()
    add_prompt_btn = ft.FilledButton("➕ Add Prompt", width=150, on_click=add_prompt)
    #add_prompt_btn = IconButton(icons.ADD, tooltip="Add Text Prompt", on_click=add_prompt)
    add_image_btn = ft.FilledButton("➕ Add Image", width=150, on_click=add_image)
    #add_image_btn = IconButton(icons.ADD, tooltip="Add Image to Mix", on_click=add_image)
    prompt = TextField(label="Mix Prompt Text", value=kandinsky_fuse_prefs['prompt'], expand=True, multiline=True, on_submit=add_prompt, on_change=lambda e:changed(e,'prompt'))
    prompt_row = Row([prompt, add_prompt_btn])
    init_image = TextField(label="Mixing Image", value=kandinsky_fuse_prefs['init_image'], on_change=lambda e:changed(e,'init_image'), expand=True, height=65, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_init), col={'xs':12, 'md':6})
    image_row = Row([init_image, add_image_btn])
    weight_slider = SliderRow(label="Text or Image Weight", min=0.1, max=0.9, divisions=16, round=2, pref=kandinsky_fuse_prefs, key='weight')
    fuse_layers = Column([], spacing=0)
    batch_folder_name = TextField(label="Batch Folder Name", value=kandinsky_fuse_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    file_prefix = TextField(label="Filename Prefix", value=kandinsky_fuse_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))
    #num_outputs = NumberPicker(label="Num of Outputs", min=1, max=4, step=4, value=kandinsky_fuse_prefs['num_outputs'], on_change=lambda e:changed(e,'num_outputs', ptype="int"))
    #num_outputs = TextField(label="num_outputs", value=kandinsky_fuse_prefs['num_outputs'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'num_outputs', ptype="int"))
    #n_iterations = TextField(label="Number of Iterations", value=kandinsky_fuse_prefs['n_iterations'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'n_iterations', ptype="int"))
    steps = TextField(label="Number of Steps", value=kandinsky_fuse_prefs['steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'steps', ptype="int"))
    #sampler = Dropdown(label="Sampler", width=200, options=[dropdown.Option("ddim_sampler"), dropdown.Option("p_sampler")], value=kandinsky_fuse_prefs['sampler'], on_change=lambda e:changed(e,'sampler'), col={'xs':12, 'md':6})
    n_images = NumberPicker(label="Number of Images", min=1, max=9, step=1, value=kandinsky_fuse_prefs['num_images'], on_change=lambda e:changed(e,'num_images', ptype="int"))
    #param_rows = ResponsiveRow([Column([batch_folder_name, n_images], col={'xs':12, 'md':6}),
    #                  Column([file_prefix, sampler], col={'xs':12, 'md':6})
    #                  #Column([steps, ddim_eta, dynamic_threshold_v], col={'xs':12, 'md':6})
    #                  ], vertical_alignment=CrossAxisAlignment.START)
    steps = SliderRow(label="Number of Steps", min=0, max=200, divisions=200, pref=kandinsky_fuse_prefs, key='steps')
    #prior_cf_scale = SliderRow(label="Prior CF Scale", min=0, max=10, divisions=10, pref=kandinsky_fuse_prefs, key='prior_cf_scale')
    #prior_steps = SliderRow(label="Prior Steps", min=0, max=50, divisions=50, pref=kandinsky_fuse_prefs, key='prior_steps')
    guidance = SliderRow(label="Guidance Scale", min=0, max=50, divisions=50, pref=kandinsky_fuse_prefs, key='guidance_scale')
    width_slider = SliderRow(label="Width", min=128, max=1024, divisions=14, multiple=32, suffix="px", pref=kandinsky_fuse_prefs, key='width')
    height_slider = SliderRow(label="Height", min=128, max=1024, divisions=14, multiple=32, suffix="px", pref=kandinsky_fuse_prefs, key='height')
    seed = TextField(label="Seed", width=90, value=str(kandinsky_fuse_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    #mask_image = TextField(label="Mask Image", value=kandinsky_fuse_prefs['mask_image'], on_change=lambda e:changed(e,'mask_image'), expand=True, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD_OUTLINED, on_click=pick_mask), col={'xs':10, 'md':5})
    #invert_mask = Checkbox(label="Invert", tooltip="Swaps the Black & White of your Mask Image", value=kandinsky_fuse_prefs['invert_mask'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'invert_mask'), col={'xs':2, 'md':1})
    #image_pickers = Container(content=ResponsiveRow([init_image, mask_image, invert_mask]), padding=padding.only(top=5), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    #img_block = Container(Column([image_pickers, weight_slider, Divider(height=9, thickness=2)]), padding=padding.only(top=5), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    apply_ESRGAN_upscale = Switcher(label="Apply ESRGAN Upscale", value=kandinsky_fuse_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)
    enlarge_scale_slider = SliderRow(label="Enlarge Scale", min=1, max=4, divisions=6, round=1, suffix="x", pref=kandinsky_fuse_prefs, key='enlarge_scale')
    face_enhance = Checkbox(label="Use Face Enhance GPFGAN", value=kandinsky_fuse_prefs['face_enhance'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'face_enhance'))
    display_upscaled_image = Checkbox(label="Display Upscaled Image", value=kandinsky_fuse_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))
    ESRGAN_settings = Container(Column([enlarge_scale_slider, face_enhance, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    page.ESRGAN_block_kandinsky_fuse = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    page.ESRGAN_block_kandinsky_fuse.height = None if status['installed_ESRGAN'] else 0
    if not kandinsky_fuse_prefs['apply_ESRGAN_upscale']:
        ESRGAN_settings.height = 0
    parameters_button = ElevatedButton(content=Text(value="💥   Run Kandinsky 2.2 Fuser", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_kandinsky_fuse(page))

    parameters_row = Row([parameters_button], alignment=MainAxisAlignment.SPACE_BETWEEN)
    page.kandinsky_fuse_output = Column([])
    c = Column([Container(
        padding=padding.only(18, 14, 20, 10), content=Column([
            Header("💣  Kandinsky 2.2 Fuse", "Mix multiple Images and Prompts together to Interpolate. A Latent Diffusion model with two Multilingual text encoders, supports 100+ languages, made in Russia.", actions=[IconButton(icon=icons.HELP, tooltip="Help with Kandinsky Settings", on_click=kandinsky_help)]),
            prompt_row,
            image_row,
            weight_slider,
            Divider(height=5, thickness=4),
            fuse_layers,
            #Divider(height=2, thickness=2),
            #param_rows, #dropdown_row,
            steps,
            #prior_steps, prior_cf_scale,
            guidance, width_slider, height_slider, #Divider(height=9, thickness=2),
            #Row([batch_folder_name, file_prefix]),
            #Row([n_images, sampler]),
            ResponsiveRow([Row([n_images, seed], col={'md':6}), Row([batch_folder_name, file_prefix], col={'md':6})]),
            page.ESRGAN_block_kandinsky_fuse,
            #(img_block if status['installed_img2img'] or status['installed_stability'] else Container(content=None)), (clip_block if prefs['install_CLIP_guided'] else Container(content=None)), (ESRGAN_block if prefs['install_ESRGAN'] else Container(content=None)),
            parameters_row,
            page.kandinsky_fuse_output
        ],
    ))], scroll=ScrollMode.AUTO)#batch_folder_name, batch_size, n_iterations, steps, ddim_eta, seed,
    return c

kandinsky2_fuse_prefs = {
    "prompt": '',
    "batch_folder_name": '',
    "file_prefix": "kandinsky-",
    "num_images": 1,
    "mixes": [],
    "steps":100,
    "width": 512,
    "height":512,
    "guidance_scale":8,
    'prior_cf_scale': 4,
    'prior_steps': "25",
    "sampler": "ddim_sampler",
    "init_image": '',
    "weight": 0.5,
    "mask_image": '',
    "invert_mask": False,
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": prefs['enlarge_scale'],
    "face_enhance": prefs['face_enhance'],
    "display_upscaled_image": prefs['display_upscaled_image'],
}

def buildKandinsky2Fuse(page):
    global prefs, kandinsky2_fuse_prefs, status
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          if ptype == "int":
            kandinsky2_fuse_prefs[pref] = int(e.control.value)
          elif ptype == "float":
            kandinsky2_fuse_prefs[pref] = float(e.control.value)
          else:
            kandinsky2_fuse_prefs[pref] = e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def pick_files_result(e: FilePickerResultEvent):
        if e.files:
            img = e.files
            uf = []
            fname = img[0]
            #print(", ".join(map(lambda f: f.name, e.files)))
            src_path = page.get_upload_url(fname.name, 600)
            uf.append(FilePickerUploadFile(fname.name, upload_url=src_path))
            pick_files_dialog.upload(uf)
            #print(str(src_path))
            #src_path = ''.join(src_path)
            #print(str(uf[0]))
            dst_path = os.path.join(root_dir, fname.name)
            #print(f'Copy {src_path} to {dst_path}')
            #shutil.copy(src_path, dst_path)
            init_image.value = dst_path
    pick_files_dialog = FilePicker(on_result=pick_files_result)
    page.overlay.append(pick_files_dialog)
    #selected_files = Text()
    def file_picker_result(e: FilePickerResultEvent):
        if e.files != None:
            upload_files(e)
    def on_upload_progress(e: FilePickerUploadEvent):
        nonlocal pick_type
        if e.progress == 1:
            if not slash in e.file_name:
              fname = os.path.join(root_dir, e.file_name)
            else:
              fname = e.file_name
            init_image.value = fname
            init_image.update()
            kandinsky2_fuse_prefs['init_image'] = fname
            page.update()
    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)
    def upload_files(e):
        uf = []
        if file_picker.result != None and file_picker.result.files != None:
            for f in file_picker.result.files:
              if page.web:
                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))
              else:
                on_upload_progress(FilePickerUploadEvent(f.path, 1, ""))
            file_picker.upload(uf)
    page.overlay.append(file_picker)
    pick_type = ""
    #page.overlay.append(pick_files_dialog)
    def pick_init(e):
        nonlocal pick_type
        pick_type = "init"
        file_picker.pick_files(allow_multiple=False, allowed_extensions=["png", "PNG"], dialog_title="Pick Init Image File")
    def toggle_ESRGAN(e):
        ESRGAN_settings.height = None if e.control.value else 0
        kandinsky2_fuse_prefs['apply_ESRGAN_upscale'] = e.control.value
        ESRGAN_settings.update()
    def kandinsky2_help(e):
      def close_kandinsky2_dlg(e):
        nonlocal kandinsky2_help_dlg
        kandinsky2_help_dlg.open = False
        page.update()
      kandinsky2_help_dlg = AlertDialog(title=Text("🙅   Help with Kandinsky Fuse Pipeline"), content=Column([
          Text("NOTE: Right now, installing this may be incompatible with Diffusers packages, so it may not work if you first installed HuggingFace & Stable Diffusion. It's recommended to run this on a fresh runtime, only installing ESRGAN to upscale. We hope to fix this soon, but works great."),
          Text("This variation lets you fuse together many images together with multiple text prompts to create a mix. Set the weights of the prompts and images to adjust the amount of influence it has on the generated style. Get experimental"),
          Text("Kandinsky 2.1 inherits best practicies from Dall-E 2 and Latent diffusion, while introducing some new ideas."),
          Text("As text and image encoder it uses CLIP model and diffusion image prior (mapping) between latent spaces of CLIP modalities. This approach increases the visual performance of the model and unveils new horizons in blending images and text-guided image manipulation. For diffusion mapping of latent spaces we use transformer with num_layers=20, num_heads=32 and hidden_size=2048. Kandinsky 2.1 was trained on a large-scale image-text dataset LAION HighRes and fine-tuned on our internal datasets. These encoders and multilingual training datasets unveil the real multilingual text-to-image generation experience!"),
          Text("The decision to make changes to the architecture came after continuing to learn the Kandinsky 2.0 version and trying to get stable text embeddings of the mT5 multilingual language model. The logical conclusion was that the use of only text embedding was not enough for high-quality image synthesis. After analyzing once again the existing DALL-E 2 solution from OpenAI, it was decided to experiment with the image prior model (allows you to generate visual embedding CLIP by text prompt or text embedding CLIP), while remaining in the latent visual space paradigm, so that you do not have to retrain the diffusion part of the UNet model Kandinsky 2.0. Now a little more details about the learning process of Kandinsky 2.1."),
          Markdown("[Kandinsky GitHub](https://github.com/ai-forever/Kandinsky-2) | [Kandinsky 2.1 Blog](https://habr.com/ru/companies/sberbank/articles/725282/) | [FusionBrain Demo](https://fusionbrain.ai/diffusion)"),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("🫢  Possibility Overload... ", on_click=close_kandinsky2_dlg)], actions_alignment=MainAxisAlignment.END)
      page.dialog = kandinsky2_help_dlg
      kandinsky2_help_dlg.open = True
      page.update()
    def add_image(e):
        if not bool(kandinsky2_fuse_prefs['init_image']): return
        layer = {'init_image': kandinsky2_fuse_prefs['init_image'], 'weight': kandinsky2_fuse_prefs['weight']}
        kandinsky2_fuse_prefs['mixes'].append(layer)
        fuse_layers.controls.append(ListTile(title=Row([Text(layer['init_image'], weight=FontWeight.BOLD), Text(f"Weight: {layer['weight']}")], alignment=MainAxisAlignment.SPACE_BETWEEN), dense=True, trailing=PopupMenuButton(icon=icons.MORE_VERT,
          items=[
              PopupMenuItem(icon=icons.EDIT, text="Edit Image Layer", on_click=edit_layer, data=layer),
              PopupMenuItem(icon=icons.DELETE, text="Delete Image Layer", on_click=delete_layer, data=layer),
              PopupMenuItem(icon=icons.DELETE_SWEEP, text="Delete All Layers", on_click=delete_all_layers, data=layer),
              PopupMenuItem(icon=icons.ARROW_UPWARD, text="Move Up", on_click=move_up, data=layer),
              PopupMenuItem(icon=icons.ARROW_DOWNWARD, text="Move Down", on_click=move_down, data=layer),
          ]), data=layer, on_click=edit_layer))
        fuse_layers.update()
        kandinsky2_fuse_prefs['init_image'] = ""
        init_image.value = ""
        init_image.update()
    def add_prompt(e):
        if not bool(kandinsky2_fuse_prefs['prompt']): return
        layer = {'prompt': kandinsky2_fuse_prefs['prompt'], 'weight': kandinsky2_fuse_prefs['weight']}
        kandinsky2_fuse_prefs['mixes'].append(layer)
        fuse_layers.controls.append(ListTile(title=Row([Text(layer['prompt'], weight=FontWeight.BOLD), Text(f"Weight: {layer['weight']}")], alignment=MainAxisAlignment.SPACE_BETWEEN), dense=True, trailing=PopupMenuButton(icon=icons.MORE_VERT,
          items=[
              PopupMenuItem(icon=icons.EDIT, text="Edit Text Layer", on_click=edit_layer, data=layer),
              PopupMenuItem(icon=icons.DELETE, text="Delete Text Layer", on_click=delete_layer, data=layer),
              PopupMenuItem(icon=icons.DELETE_SWEEP, text="Delete All Layers", on_click=delete_all_layers, data=layer),
              PopupMenuItem(icon=icons.ARROW_UPWARD, text="Move Up", on_click=move_up, data=layer),
              PopupMenuItem(icon=icons.ARROW_DOWNWARD, text="Move Down", on_click=move_down, data=layer),
          ]), data=layer, on_click=edit_layer))
        fuse_layers.update()
        kandinsky2_fuse_prefs['prompt'] = ""
        prompt.value = ""
        prompt.update()
    def delete_layer(e):
        kandinsky2_fuse_prefs['mixes'].remove(e.control.data)
        for c in fuse_layers.controls:
            if 'prompt' in c.data:
                if c.data['prompt'] == e.control.data['prompt']:
                    fuse_layers.controls.remove(c)
                    break
            else:
                if c.data['init_image'] == e.control.data['init_image']:
                    fuse_layers.controls.remove(c)
                    break
        fuse_layers.update()
    def delete_all_layers(e):
        kandinsky2_fuse_prefs['mixes'].clear()
        fuse_layers.controls.clear()
        fuse_layers.update()
    def move_down(e):
        idx = kandinsky2_fuse_prefs['mixes'].index(e.control.data)
        if idx < (len(kandinsky2_fuse_prefs['mixes']) - 1):
          d = kandinsky2_fuse_prefs['mixes'].pop(idx)
          kandinsky2_fuse_prefs['mixes'].insert(idx+1, d)
          dr = fuse_layers.controls.pop(idx)
          fuse_layers.controls.insert(idx+1, dr)
          fuse_layers.update()
    def move_up(e):
        idx = kandinsky2_fuse_prefs['mixes'].index(e.control.data)
        if idx > 0:
          d = kandinsky2_fuse_prefs['mixes'].pop(idx)
          kandinsky2_fuse_prefs['mixes'].insert(idx-1, d)
          dr = fuse_layers.controls.pop(idx)
          fuse_layers.controls.insert(idx-1, dr)
          fuse_layers.update()
    def edit_layer(e):
        data = e.control.data
        layer_type = "prompt" if "prompt" in data else "image"
        def pick_files_result(e: FilePickerResultEvent):
            if e.files:
                img = e.files
                uf = []
                fname = img[0]
                src_path = page.get_upload_url(fname.name, 600)
                uf.append(FilePickerUploadFile(fname.name, upload_url=src_path))
                pick_files_dialog.upload(uf)
                dst_path = os.path.join(root_dir, fname.name)
                image_mix.value = dst_path
        pick_files_dialog = FilePicker(on_result=pick_files_result)
        page.overlay.append(pick_files_dialog)
        def file_picker_result(e: FilePickerResultEvent):
            if e.files != None:
                upload_files(e)
        def on_upload_progress(e: FilePickerUploadEvent):
            if e.progress == 1:
                if not slash in e.file_name:
                  fname = os.path.join(root_dir, e.file_name)
                else:
                  fname = e.file_name
                image_mix.value = fname
                image_mix.update()
                data['init_image'] = fname
                page.update()
        file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)
        def upload_files(e):
            uf = []
            if file_picker.result != None and file_picker.result.files != None:
                for f in file_picker.result.files:
                  if page.web:
                    uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))
                  else:
                    on_upload_progress(FilePickerUploadEvent(f.path, 1, ""))
                file_picker.upload(uf)
        page.overlay.append(file_picker)
        def pick_image(e):
            file_picker.pick_files(allow_multiple=False, allowed_extensions=["png", "PNG"], dialog_title="Pick Init Image File")
        #name = e.control.title.controls[0].value
        #path = e.control.title.controls[1].value
        if layer_type == "prompt":
            prompt_value = data["prompt"]
            image_value = ""
        else:
            prompt_value = ""
            image_value = data["init_image"]
        def close_dlg(e):
            dlg_edit.open = False
            page.update()
        def save_layer(e):
            layer = None
            for l in kandinsky2_fuse_prefs['mixes']:
                if "prompt" in l:
                  if layer_type == "prompt":
                      if data["prompt"] == l["prompt"]:
                        layer = l
                        layer['prompt'] = prompt_text.value
                        break
                else:
                    if layer_type == "image":
                      if data["init_image"] == l["init_image"]:
                        layer = l
                        layer['init_image'] = image_mix.value
                        break
            for c in fuse_layers.controls:
                if 'prompt' in c.data:
                    if 'prompt' not in data: continue
                    if c.data['prompt'] == data['prompt']:
                        c.title.controls[0].value = layer['prompt']
                        c.title.controls[1].value = f"Weight: {layer['weight']}"
                        c.update()
                        break
                else:
                    if 'init_image' not in data: continue
                    if c.data['init_image'] == data['init_image']:
                        c.title.controls[0].value = layer['init_image']
                        c.title.controls[1].value = f"Weight: {layer['weight']}"
                        c.update()
                        break
            layer['prompt'] = prompt_text.value
            #layer['weight'] = model_path.value
            dlg_edit.open = False
            e.control.update()
            page.update()
        prompt_text = TextField(label="Fuse Prompt Text", value=prompt_value, multiline=True, visible=layer_type == "prompt")
        image_mix = TextField(label="Fuse Image Path", value=image_value, visible=layer_type == "image", height=65, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_image))
        edit_weights = SliderRow(label="Weight/Strength", min=0, max=1, divisions=20, round=1, pref=data, key='weight', tooltip="Indicates how much each individual concept should influence the overall guidance. If no weights are provided all concepts are applied equally.")
        dlg_edit = AlertDialog(modal=False, title=Text(f"🧳 Edit Kandinsky Fuse {layer_type.title()} Mix"), content=Container(Column([prompt_text, image_mix, edit_weights], alignment=MainAxisAlignment.START, tight=True, scroll=ScrollMode.AUTO, width=(page.width if page.web else page.window_width) - 100)), actions=[TextButton(content=Text("Cancel", size=18), on_click=close_dlg), ElevatedButton(content=Text(value=emojize(":floppy_disk:") + "  Save Layer ", size=19, weight=FontWeight.BOLD), on_click=save_layer)], actions_alignment=MainAxisAlignment.END)
        page.dialog = dlg_edit
        dlg_edit.open = True
        page.update()
    add_prompt_btn = ft.FilledButton("➕ Add Prompt", width=135, on_click=add_prompt)
    #add_prompt_btn = IconButton(icons.ADD, tooltip="Add Text Prompt", on_click=add_prompt)
    add_image_btn = ft.FilledButton("➕ Add Image", width=135, on_click=add_image)
    #add_image_btn = IconButton(icons.ADD, tooltip="Add Image to Mix", on_click=add_image)
    prompt = TextField(label="Mix Prompt Text", value=kandinsky2_fuse_prefs['prompt'], expand=True, multiline=True, on_submit=add_prompt, on_change=lambda e:changed(e,'prompt'))
    prompt_row = Row([prompt, add_prompt_btn])
    init_image = TextField(label="Mixing Image", value=kandinsky2_fuse_prefs['init_image'], on_change=lambda e:changed(e,'init_image'), expand=True, height=65, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_init), col={'xs':12, 'md':6})
    image_row = Row([init_image, add_image_btn])
    weight_slider = SliderRow(label="Text or Image Weight", min=0.1, max=0.9, divisions=16, round=2, pref=kandinsky2_fuse_prefs, key='weight')
    fuse_layers = Column([], spacing=0)
    batch_folder_name = TextField(label="Batch Folder Name", value=kandinsky2_fuse_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    file_prefix = TextField(label="Filename Prefix", value=kandinsky2_fuse_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))
    #num_outputs = NumberPicker(label="Num of Outputs", min=1, max=4, step=4, value=kandinsky2_fuse_prefs['num_outputs'], on_change=lambda e:changed(e,'num_outputs', ptype="int"))
    #num_outputs = TextField(label="num_outputs", value=kandinsky2_fuse_prefs['num_outputs'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'num_outputs', ptype="int"))
    #n_iterations = TextField(label="Number of Iterations", value=kandinsky2_fuse_prefs['n_iterations'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'n_iterations', ptype="int"))
    steps = TextField(label="Number of Steps", value=kandinsky2_fuse_prefs['steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'steps', ptype="int"))
    sampler = Dropdown(label="Sampler", width=200, options=[dropdown.Option("ddim_sampler"), dropdown.Option("p_sampler")], value=kandinsky2_fuse_prefs['sampler'], on_change=lambda e:changed(e,'sampler'), col={'xs':12, 'md':6})
    n_images = NumberPicker(label="Number of Images", min=1, max=9, step=1, value=kandinsky2_fuse_prefs['num_images'], on_change=lambda e:changed(e,'num_images', ptype="int"))
    param_rows = ResponsiveRow([Column([batch_folder_name, n_images], col={'xs':12, 'md':6}),
                      Column([file_prefix, sampler], col={'xs':12, 'md':6})
                      #Column([steps, ddim_eta, dynamic_threshold_v], col={'xs':12, 'md':6})
                      ], vertical_alignment=CrossAxisAlignment.START)
    steps = SliderRow(label="Number of Steps", min=0, max=200, divisions=200, pref=kandinsky2_fuse_prefs, key='steps')
    prior_cf_scale = SliderRow(label="Prior CF Scale", min=0, max=10, divisions=10, pref=kandinsky2_fuse_prefs, key='prior_cf_scale')
    prior_steps = SliderRow(label="Prior Steps", min=0, max=50, divisions=50, pref=kandinsky2_fuse_prefs, key='prior_steps')
    guidance = SliderRow(label="Guidance Scale", min=0, max=50, divisions=50, pref=kandinsky2_fuse_prefs, key='guidance_scale')
    width_slider = SliderRow(label="Width", min=128, max=1024, divisions=14, multiple=32, suffix="px", pref=kandinsky2_fuse_prefs, key='width')
    height_slider = SliderRow(label="Height", min=128, max=1024, divisions=14, multiple=32, suffix="px", pref=kandinsky2_fuse_prefs, key='height')
    #mask_image = TextField(label="Mask Image", value=kandinsky2_fuse_prefs['mask_image'], on_change=lambda e:changed(e,'mask_image'), expand=True, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD_OUTLINED, on_click=pick_mask), col={'xs':10, 'md':5})
    #invert_mask = Checkbox(label="Invert", tooltip="Swaps the Black & White of your Mask Image", value=kandinsky2_fuse_prefs['invert_mask'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'invert_mask'), col={'xs':2, 'md':1})
    #image_pickers = Container(content=ResponsiveRow([init_image, mask_image, invert_mask]), padding=padding.only(top=5), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    #img_block = Container(Column([image_pickers, weight_slider, Divider(height=9, thickness=2)]), padding=padding.only(top=5), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    apply_ESRGAN_upscale = Switcher(label="Apply ESRGAN Upscale", value=kandinsky2_fuse_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)
    enlarge_scale_slider = SliderRow(label="Enlarge Scale", min=1, max=4, divisions=6, round=1, suffix="x", pref=kandinsky2_fuse_prefs, key='enlarge_scale')
    face_enhance = Checkbox(label="Use Face Enhance GPFGAN", value=kandinsky2_fuse_prefs['face_enhance'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'face_enhance'))
    display_upscaled_image = Checkbox(label="Display Upscaled Image", value=kandinsky2_fuse_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))
    ESRGAN_settings = Container(Column([enlarge_scale_slider, face_enhance, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    page.ESRGAN_block_kandinsky2_fuse = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    page.ESRGAN_block_kandinsky2_fuse.height = None if status['installed_ESRGAN'] else 0
    if not kandinsky2_fuse_prefs['apply_ESRGAN_upscale']:
        ESRGAN_settings.height = 0
    parameters_button = ElevatedButton(content=Text(value="💥   Run Kandinsky 2.1 Fuser", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_kandinsky2_fuse(page))

    parameters_row = Row([parameters_button], alignment=MainAxisAlignment.SPACE_BETWEEN)
    page.kandinsky2_fuse_output = Column([])
    c = Column([Container(
        padding=padding.only(18, 14, 20, 10), content=Column([
            Header("💣  Kandinsky 2.1 Fuse", "Mix multiple Images and Prompts together. A Latent Diffusion model with two Multilingual text encoders, supports 100+ languages, made in Russia.", actions=[IconButton(icon=icons.HELP, tooltip="Help with Kandinsky Settings", on_click=kandinsky2_help)]),
            prompt_row,
            image_row,
            weight_slider,
            Divider(height=5, thickness=4),
            fuse_layers,
            #Divider(height=2, thickness=2),
            #param_rows, #dropdown_row,
            steps, prior_steps, prior_cf_scale,
            guidance, width_slider, height_slider, #Divider(height=9, thickness=2),
            Row([batch_folder_name, file_prefix]),
            Row([n_images, sampler]),
            page.ESRGAN_block_kandinsky2_fuse,
            #(img_block if status['installed_img2img'] or status['installed_stability'] else Container(content=None)), (clip_block if prefs['install_CLIP_guided'] else Container(content=None)), (ESRGAN_block if prefs['install_ESRGAN'] else Container(content=None)),
            parameters_row,
            page.kandinsky2_fuse_output
        ],
    ))], scroll=ScrollMode.AUTO)#batch_folder_name, batch_size, n_iterations, steps, ddim_eta, seed,
    return c

kandinsky_controlnet_prefs = {
    "prompt": '',
    "negative_prompt": '',
    "batch_folder_name": '',
    "file_prefix": "kandinsky-",
    "num_images": 1,
    "batch_size": 1,
    "steps": 50,
    "prior_steps": 25,
    #"ddim_eta":0.05,
    "width": 768,
    "height":768,
    "guidance_scale":4,
    "init_image": '',
    "strength": 0.5,
    "prior_strength": 0.8,
    #"mask_image": '',
    #"invert_mask": False,
    "seed": 0,
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": prefs['enlarge_scale'],
    "face_enhance": prefs['face_enhance'],
    "display_upscaled_image": prefs['display_upscaled_image'],
}

def buildKandinskyControlNet(page):
    global prefs, kandinsky_controlnet_prefs, status
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          if ptype == "int":
            kandinsky_controlnet_prefs[pref] = int(e.control.value)
          elif ptype == "float":
            kandinsky_controlnet_prefs[pref] = float(e.control.value)
          else:
            kandinsky_controlnet_prefs[pref] = e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def kandinsky_controlnet_help(e):
      def close_kandinsky_controlnet_dlg(e):
        nonlocal kandinsky_controlnet_help_dlg
        kandinsky_controlnet_help_dlg.open = False
        page.update()
      kandinsky_controlnet_help_dlg = AlertDialog(title=Text("🙅   Help with Kandinsky ControlNet Pipeline"), content=Column([
          #Text("NOTE: Right now, installing this may be incompatible with Diffusers packages, so it may not work if you first installed HuggingFace & Stable Diffusion. It's recommended to run this on a fresh runtime, only installing ESRGAN to upscale. We hope to fix this soon, but works great."),
          Text("Kandinsky 2.2 includes KandinskyV22ControlnetImg2ImgPipeline that will allow you to add control to the image generation process with both the image and its depth map. This pipeline works really well with KandinskyV22PriorEmb2EmbPipeline, which generates image embeddings based on both a text prompt and an image. For our robot cat example, we will pass the prompt and cat image together to the prior pipeline to generate an image embedding. We will then use that image embedding and the depth map of the cat to further control the image generation process."),
          Markdown("Kandinsky 2.2 inherits best practices from [DALL-E 2](https://arxiv.org/abs/2204.06125) and [Latent Diffusion](https://huggingface.co/docs/diffusers/api/pipelines/latent_diffusion), while introducing some new ideas.\nIt uses [CLIP](https://huggingface.co/docs/transformers/model_doc/clip) for encoding images and text, and a diffusion image prior (mapping) between latent spaces of CLIP modalities. This approach enhances the visual performance of the model and unveils new horizons in blending images and text-guided image manipulation.\nThe Kandinsky model is created by [Arseniy Shakhmatov](https://github.com/cene555), [Anton Razzhigaev](https://github.com/razzant), [Aleksandr Nikolich](https://github.com/AlexWortega), [Igor Pavlov](https://github.com/boomb0om), [Andrey Kuznetsov](https://github.com/kuznetsoffandrey) and [Denis Dimitrov](https://github.com/denndimitrov) and the original codebase can be found [here](https://github.com/ai-forever/Kandinsky-2)", on_tap_link=lambda e: e.page.launch_url(e.data)),
          Text("As text and image encoder it uses CLIP model and diffusion image prior (mapping) between latent spaces of CLIP modalities. This approach increases the visual performance of the model and unveils new horizons in blending images and text-guided image manipulation. For diffusion mapping of latent spaces we use transformer with num_layers=20, num_heads=32 and hidden_size=2048. Kandinsky 2.1 was trained on a large-scale image-text dataset LAION HighRes and fine-tuned on our internal datasets. These encoders and multilingual training datasets unveil the real multilingual text-to-image generation experience!"),
          Text("The decision to make changes to the architecture came after continuing to learn the Kandinsky 2.0 version and trying to get stable text embeddings of the mT5 multilingual language model. The logical conclusion was that the use of only text embedding was not enough for high-quality image synthesis. After analyzing once again the existing DALL-E 2 solution from OpenAI, it was decided to experiment with the image prior model (allows you to generate visual embedding CLIP by text prompt or text embedding CLIP), while remaining in the latent visual space paradigm, so that you do not have to retrain the diffusion part of the UNet model Kandinsky 2.0. Now a little more details about the learning process of Kandinsky 2.1."),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("🤤  Quality... ", on_click=close_kandinsky_controlnet_dlg)], actions_alignment=MainAxisAlignment.END)
      page.dialog = kandinsky_controlnet_help_dlg
      kandinsky_controlnet_help_dlg.open = True
      page.update()
    def pick_files_result(e: FilePickerResultEvent):
        if e.files:
            img = e.files
            uf = []
            fname = img[0]
            #print(", ".join(map(lambda f: f.name, e.files)))
            src_path = page.get_upload_url(fname.name, 600)
            uf.append(FilePickerUploadFile(fname.name, upload_url=src_path))
            pick_files_dialog.upload(uf)
            #print(str(src_path))
            #src_path = ''.join(src_path)
            #print(str(uf[0]))
            dst_path = os.path.join(root_dir, fname.name)
            #print(f'Copy {src_path} to {dst_path}')
            #shutil.copy(src_path, dst_path)
            # TODO: is init or mask?
            init_image.value = dst_path
    pick_files_dialog = FilePicker(on_result=pick_files_result)
    page.overlay.append(pick_files_dialog)
    #selected_files = Text()
    def file_picker_result(e: FilePickerResultEvent):
        if e.files != None:
            upload_files(e)
    def on_upload_progress(e: FilePickerUploadEvent):
        nonlocal pick_type
        if e.progress == 1:
            if not slash in e.file_name:
              fname = os.path.join(root_dir, e.file_name)
            else:
              fname = e.file_name
            if pick_type == "init":
                init_image.value = fname
                init_image.update()
                kandinsky_controlnet_prefs['init_image'] = fname
            page.update()
    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)
    def upload_files(e):
        uf = []
        if file_picker.result != None and file_picker.result.files != None:
            for f in file_picker.result.files:
              if page.web:
                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))
              else:
                on_upload_progress(FilePickerUploadEvent(f.path, 1, ""))
            file_picker.upload(uf)
    page.overlay.append(file_picker)
    pick_type = ""
    #page.overlay.append(pick_files_dialog)
    def pick_init(e):
        nonlocal pick_type
        pick_type = "init"
        file_picker.pick_files(allow_multiple=False, allowed_extensions=["png", "PNG"], dialog_title="Pick Init Image File")
    def toggle_ESRGAN(e):
        ESRGAN_settings.height = None if e.control.value else 0
        kandinsky_controlnet_prefs['apply_ESRGAN_upscale'] = e.control.value
        ESRGAN_settings.update()
    prompt = TextField(label="Prompt Text", value=kandinsky_controlnet_prefs['prompt'], multiline=True, col={'md':9}, on_change=lambda e:changed(e,'prompt'))
    negative_prompt = TextField(label="Negative Prompt Text", value=kandinsky_controlnet_prefs['negative_prompt'], multiline=True, col={'md':3}, on_change=lambda e:changed(e,'negative_prompt'))
    batch_folder_name = TextField(label="Batch Folder Name", value=kandinsky_controlnet_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    file_prefix = TextField(label="Filename Prefix", value=kandinsky_controlnet_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))
    #n_iterations = TextField(label="Number of Iterations", value=kandinsky_controlnet_prefs['n_iterations'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'n_iterations', ptype="int"))
    steps = TextField(label="Number of Steps", value=kandinsky_controlnet_prefs['steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'steps', ptype="int"))
    #sampler = Dropdown(label="Sampler", width=200, options=[dropdown.Option("ddim_sampler"), dropdown.Option("p_sampler")], value=kandinsky_controlnet_prefs['sampler'], on_change=lambda e:changed(e,'sampler'), col={'xs':12, 'md':6})
    batch_size = NumberPicker(label="Batch Size", min=1, max=6, step=1, value=kandinsky_controlnet_prefs['batch_size'], on_change=lambda e:changed(e,'batch_size', ptype="int"))
    n_images = NumberPicker(label="Number of Images", min=1, max=9, step=1, value=kandinsky_controlnet_prefs['num_images'], on_change=lambda e:changed(e,'num_images', ptype="int"))
    #param_rows = ResponsiveRow([Column([batch_folder_name, n_images], col={'xs':12, 'md':6}),
                      #Column([file_prefix, sampler], col={'xs':12, 'md':6})
                      #Column([steps, ddim_eta, dynamic_threshold_v], col={'xs':12, 'md':6})
                      #], vertical_alignment=CrossAxisAlignment.START)
    steps = SliderRow(label="Number of Steps", min=0, max=200, divisions=200, pref=kandinsky_controlnet_prefs, key='steps')
    prior_steps = SliderRow(label="Number of Prior Steps", min=0, max=200, divisions=200, pref=kandinsky_controlnet_prefs, key='prior_steps')
    #prior_cf_scale = SliderRow(label="Prior CF Scale", min=0, max=10, divisions=10, pref=kandinsky_controlnet_prefs, key='prior_cf_scale')
    #prior_steps = SliderRow(label="Prior Steps", min=0, max=50, divisions=50, pref=kandinsky_controlnet_prefs, key='prior_steps')
    guidance = SliderRow(label="Guidance Scale", min=0, max=50, divisions=50, pref=kandinsky_controlnet_prefs, key='guidance_scale')
    width_slider = SliderRow(label="Width", min=128, max=1024, divisions=14, multiple=32, suffix="px", pref=kandinsky_controlnet_prefs, key='width')
    height_slider = SliderRow(label="Height", min=128, max=1024, divisions=14, multiple=32, suffix="px", pref=kandinsky_controlnet_prefs, key='height')
    init_image = TextField(label="Init Image", value=kandinsky_controlnet_prefs['init_image'], on_change=lambda e:changed(e,'init_image'), suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_init), col={'xs':12, 'md':6})
    #mask_image = TextField(label="Mask Image", value=kandinsky_controlnet_prefs['mask_image'], on_change=lambda e:changed(e,'mask_image'), expand=True, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD_OUTLINED, on_click=pick_mask), col={'xs':10, 'md':5})
    strength_slider = SliderRow(label="Init Image Strength", min=0.1, max=0.9, divisions=16, round=2, pref=kandinsky_controlnet_prefs, key='strength', tooltip="Indicates how much to transform the reference image.")
    prior_strength_slider = SliderRow(label="Prior Image Strength", min=0.1, max=0.9, divisions=16, round=2, pref=kandinsky_controlnet_prefs, key='prior_strength', tooltip="Indicates how much to transform the reference text embeddings.")
    #img_block = Container(Column([init_image, strength_slider, prior_strength_slider, Divider(height=9, thickness=2)]), padding=padding.only(top=5), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    seed = TextField(label="Seed", width=90, value=str(kandinsky_controlnet_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    apply_ESRGAN_upscale = Switcher(label="Apply ESRGAN Upscale", value=kandinsky_controlnet_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)
    enlarge_scale_slider = SliderRow(label="Enlarge Scale", min=1, max=4, divisions=6, round=1, suffix="x", pref=kandinsky_controlnet_prefs, key='enlarge_scale')
    face_enhance = Checkbox(label="Use Face Enhance GPFGAN", value=kandinsky_controlnet_prefs['face_enhance'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'face_enhance'))
    display_upscaled_image = Checkbox(label="Display Upscaled Image", value=kandinsky_controlnet_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))
    ESRGAN_settings = Container(Column([enlarge_scale_slider, face_enhance, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    page.ESRGAN_block_kandinsky_controlnet = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    page.ESRGAN_block_kandinsky_controlnet.height = None if status['installed_ESRGAN'] else 0
    if not kandinsky_controlnet_prefs['apply_ESRGAN_upscale']:
        ESRGAN_settings.height = 0
    parameters_button = ElevatedButton(content=Text(value="🌷   Run Kandinsky ControlNet", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_kandinsky_controlnet(page))
    from_list_button = ElevatedButton(content=Text(value="📜   Run from Prompts List", size=20), tooltip="Uses all queued Image Parameters per prompt in Prompt List", color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_kandinsky_controlnet(page, from_list=True))
    from_list_with_params_button = ElevatedButton(content=Text(value="📜   Run from Prompts List /w these Parameters", size=20), tooltip="Uses above settings per prompt in Prompt List", color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_kandinsky_controlnet(page, from_list=True, with_params=True))
    parameters_row = Row([parameters_button, from_list_button, from_list_with_params_button], wrap=True) #, alignment=MainAxisAlignment.SPACE_BETWEEN
    page.kandinsky_controlnet_output = Column([])
    c = Column([Container(
        padding=padding.only(18, 14, 20, 10), content=Column([
            Header("🏩  Kandinsky 2.2 ControlNet Text+Image-to-Image", "Image-to-Image Generation with ControlNet Conditioning and depth-estimation from transformers.", actions=[IconButton(icon=icons.HELP, tooltip="Help with Kandinsky Settings", on_click=kandinsky_controlnet_help)]),
            ResponsiveRow([prompt, negative_prompt]),
            init_image, strength_slider, prior_strength_slider,
            #img_block,
            #param_rows, #dropdown_row,
            steps,
            prior_steps,
            #prior_cf_scale,
            guidance, width_slider, height_slider, #Divider(height=9, thickness=2),
            #Row([batch_folder_name, file_prefix]),
            ResponsiveRow([Row([n_images, batch_size, seed], col={'md':6}), Row([batch_folder_name, file_prefix], col={'md':6})]),
            page.ESRGAN_block_kandinsky_controlnet,
            #(img_block if status['installed_img2img'] or status['installed_stability'] else Container(content=None)), (clip_block if prefs['install_CLIP_guided'] else Container(content=None)), (ESRGAN_block if prefs['install_ESRGAN'] else Container(content=None)),
            parameters_row,
            page.kandinsky_controlnet_output
        ],
    ))], scroll=ScrollMode.AUTO)#batch_folder_name, batch_size, n_iterations, steps, ddim_eta, seed,
    return c


deep_daze_prefs = {
    'prompt': '',
    'num_layers': 32,
    'save_every': 20,
    'max_size': 512,
    'save_progress': False,
    'learning_rate': 1e-5,
    'iterations': 1050,
    'num_images': 1,
    'batch_folder_name': '',
    'file_prefix': 'daze-',
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": 2.0,
    "display_upscaled_image": False,
}

def buildDeepDaze(page):
    global deep_daze_prefs, prefs, pipe_deep_daze
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          if ptype == "int":
            deep_daze_prefs[pref] = int(e.control.value)
          elif ptype == "float":
            deep_daze_prefs[pref] = float(e.control.value)
          else:
            deep_daze_prefs[pref] = e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def add_to_deep_daze_output(o):
      page.deep_daze_output.controls.append(o)
      page.deep_daze_output.update()
      if not clear_button.visible:
        clear_button.visible = True
        clear_button.update()
    page.add_to_deep_daze_output = add_to_deep_daze_output
    def clear_output(e):
      if prefs['enable_sounds']: page.snd_delete.play()
      page.deep_daze_output.controls = []
      page.deep_daze_output.update()
      clear_button.visible = False
      clear_button.update()
    def deep_daze_help(e):
      def close_deep_daze_dlg(e):
        nonlocal deep_daze_help_dlg
        deep_daze_help_dlg.open = False
        page.update()
      deep_daze_help_dlg = AlertDialog(title=Text("🙅   Help with DeepDaze Pipeline"), content=Column([
          Text("Text to image generation using OpenAI's CLIP and Siren. Credit goes to Ryan Murdock for the discovery of this technique (and for coming up with the great name)!"),
          Text("Heavily influenced by Alexander Mordvintsev's Deep Dream, this work uses CLIP to match an image learned by a SIREN network with a given textual description."),
          #Markdown(""),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("😵‍💫  Why not... ", on_click=close_deep_daze_dlg)], actions_alignment=MainAxisAlignment.END)
      page.dialog = deep_daze_help_dlg
      deep_daze_help_dlg.open = True
      page.update()
    def toggle_ESRGAN(e):
        ESRGAN_settings.height = None if e.control.value else 0
        deep_daze_prefs['apply_ESRGAN_upscale'] = e.control.value
        ESRGAN_settings.update()
    prompt = TextField(label="Prompt Text", value=deep_daze_prefs['prompt'], on_change=lambda e:changed(e,'prompt'))
    max_row = SliderRow(label="Max Resolution Size", min=256, max=1024, divisions=12, multiple=32, suffix="px", pref=deep_daze_prefs, key='max_size')
    #num_layers = TextField(label="Inference Steps", value=str(deep_daze_prefs['num_layers']), keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'num_layers', ptype='int'))
    learning_rate = TextField(label="Learning Rate", width=130, value=float(deep_daze_prefs['learning_rate']), keyboard_type=KeyboardType.NUMBER, tooltip="The learning rate of the neural net.", on_change=lambda e:changed(e,'learning_rate', ptype='float'))
    num_layers_row = SliderRow(label="Number of Layers", min=1, max=100, divisions=99, pref=deep_daze_prefs, key='num_layers', tooltip="The number of hidden layers to use with Siren neural network")
    save_every_row = SliderRow(label="Save/Show Every x Steps", min=1, max=100, divisions=99, pref=deep_daze_prefs, key='save_every', tooltip="Generate an image every time iterations is a multiple of this number.")
    iterations_row = SliderRow(label="Number of Iterations", min=1, max=2000, divisions=1999, pref=deep_daze_prefs, key='iterations', tooltip="The number of times to calculate and backpropogate loss in a given epoch.")
    batch_folder_name = TextField(label="Batch Folder Name", value=deep_daze_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    file_prefix = TextField(label="Filename Prefix", value=deep_daze_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))
    save_progress = Tooltip(message="Whether or not to save images generated before training Siren is complete.", content=Switcher(label="Save Progress Steps", value=deep_daze_prefs['save_progress'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'save_progress')))
    #eta = TextField(label="ETA", value=str(deep_daze_prefs['eta']), keyboard_type=KeyboardType.NUMBER, hint_text="Amount of Noise", on_change=lambda e:changed(e,'eta', ptype='float'))
    #eta = Slider(min=0.0, max=1.0, divisions=20, label="{value}", value=float(deep_daze_prefs['eta']), tooltip="The weight of noise for added noise in a diffusion step. Its value is between 0.0 and 1.0 - 0.0 is DDIM and 1.0 is DDPM scheduler respectively.", expand=True, on_change=lambda e:changed(e,'eta', ptype='float'))
    #eta_row = Row([Text("DDIM ETA: "), eta])
    #max_size = Slider(min=256, max=1280, divisions=64, label="{value}px", value=int(deep_daze_prefs['max_size']), expand=True, on_change=lambda e:changed(e,'max_size', ptype='int'))
    #max_row = Row([Text("Max Resolution Size: "), max_size])
    apply_ESRGAN_upscale = Switcher(label="Apply ESRGAN Upscale", value=deep_daze_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)
    enlarge_scale_slider = SliderRow(label="Enlarge Scale", min=1, max=4, divisions=6, round=1, suffix="x", pref=deep_daze_prefs, key='enlarge_scale')
    display_upscaled_image = Checkbox(label="Display Upscaled Image", value=deep_daze_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))
    ESRGAN_settings = Container(Column([enlarge_scale_slider, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    page.ESRGAN_block_deep_daze = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    page.ESRGAN_block_deep_daze.height = None if status['installed_ESRGAN'] else 0
    if not deep_daze_prefs['apply_ESRGAN_upscale']:
        ESRGAN_settings.height = 0
    page.deep_daze_output = Column([], auto_scroll=True)
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.deep_daze_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("👀  DeepDaze Text-to-Image Generator", "An alternative method using OpenAI's CLIP and Siren. Made a few years ago but still facinating results....", actions=[IconButton(icon=icons.HELP, tooltip="Help with DeepDaze Settings", on_click=deep_daze_help)]),
        prompt,
        num_layers_row,
        iterations_row,
        Row([learning_rate, save_progress]),
        save_every_row,
        max_row,
        #NumberPicker(label="Number of Images: ", min=1, max=20, value=deep_daze_prefs['num_images'], on_change=lambda e: changed(e, 'num_images')),
        Row([batch_folder_name, file_prefix]),
        page.ESRGAN_block_deep_daze,
        Row([ElevatedButton(content=Text("😶   Get DeepDaze Generation", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_deep_daze(page)),
             #ElevatedButton(content=Text(value="📜   Run from Prompts List", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_deep_daze(page, from_list=True))
        ]),
      ]
    )), page.deep_daze_output,
        clear_button,
    ], scroll=ScrollMode.AUTO, auto_scroll=False)
    return c

CLIPstyler_prefs = {
    'source':'a photo',
    'prompt_text': 'Detailed oil painting',
    'batch_folder_name': 'clipstyler',
    'crop_size': 128,
    'num_crops': 64,
    'original_image': '',
    'image_dir': "",
    'training_iterations': 100,
    'width': 512,
    'height': 512,
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": prefs['enlarge_scale'],
    "display_upscaled_image": prefs['display_upscaled_image'],
}

def buildCLIPstyler(page):
    global CLIPstyler, prefs
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          if ptype == "int":
            CLIPstyler_prefs[pref] = int(e.control.value)
          elif ptype == "float":
            CLIPstyler_prefs[pref] = float(e.control.value)
          else:
            CLIPstyler_prefs[pref] = e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def pick_files_result(e: FilePickerResultEvent):
        if e.files:
            img = e.files
            uf = []
            fname = img[0]
            print(", ".join(map(lambda f: f.name, e.files)))
            src_path = page.get_upload_url(fname.name, 600)
            uf.append(FilePickerUploadFile(fname.name, upload_url=src_path))
            pick_files_dialog.upload(uf)
            print(str(src_path))
            #src_path = ''.join(src_path)
            print(str(uf[0]))
            dst_path = os.path.join(root_dir, fname.name)
            print(f'Copy {src_path} to {dst_path}')
            #shutil.copy(src_path, dst_path)
            # TODO: is original or mask?
            original_image.value = dst_path

    pick_files_dialog = FilePicker(on_result=pick_files_result)
    page.overlay.append(pick_files_dialog)
    #selected_files = Text()

    def file_picker_result(e: FilePickerResultEvent):
        if e.files != None:
            upload_files(e)
    def on_upload_progress(e: FilePickerUploadEvent):
        if e.progress == 1:
            if not slash in e.file_name:
              fname = os.path.join(root_dir, e.file_name)
            else:
              fname = e.file_name
            original_image.value = fname
            original_image.update()
            CLIPstyler_prefs['original_image'] = fname
            page.update()
    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)
    def upload_files(e):
        uf = []
        if file_picker.result != None and file_picker.result.files != None:
            for f in file_picker.result.files:
              if page.web:
                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))
              else:
                on_upload_progress(FilePickerUploadEvent(f.path, 1, ""))
            file_picker.upload(uf)
    page.overlay.append(file_picker)
    pick_type = ""
    #page.overlay.append(pick_files_dialog)
    def pick_original(e):
        file_picker.pick_files(allow_multiple=False, allowed_extensions=["png", "PNG", "jpg", "jpeg"], dialog_title="Pick original Image File")
    def toggle_ESRGAN(e):
        ESRGAN_settings.height = None if e.control.value else 0
        CLIPstyler_prefs['apply_ESRGAN_upscale'] = e.control.value
        ESRGAN_settings.update()
        has_changed = True
    prompt_text = TextField(label="Stylized Prompt Text", value=CLIPstyler_prefs['prompt_text'], on_change=lambda e:changed(e,'prompt_text'))
    batch_folder_name = TextField(label="Batch Folder Name", value=CLIPstyler_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    source = TextField(label="Source Type", value=CLIPstyler_prefs['source'], on_change=lambda e:changed(e,'source'))
    #training_iterations = TextField(label="Training Iterations", value=CLIPstyler_prefs['training_iterations'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'training_iterations', ptype="int"))
    crop_size = TextField(label="Crop Size", value=CLIPstyler_prefs['crop_size'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'crop_size', ptype="int"))
    num_crops = TextField(label="Number of Crops", value=CLIPstyler_prefs['num_crops'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e:changed(e,'num_crops', ptype="int"))
    param_rows = Column([Row([batch_folder_name, source]), Row([crop_size, num_crops])])
    iterations = SliderRow(label="Training Iterations", min=50, max=500, divisions=90, pref=CLIPstyler_prefs, key='training_iterations')

    width_slider = SliderRow(label="Width", min=128, max=1024, divisions=14, multiple=32, suffix="px", pref=CLIPstyler_prefs, key='width')
    height_slider = SliderRow(label="Height", min=128, max=1024, divisions=14, multiple=32, suffix="px", pref=CLIPstyler_prefs, key='height')
    original_image = TextField(label="Original Image", value=CLIPstyler_prefs['original_image'], on_change=lambda e:changed(e,'original_image'), expand=True, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_original, col={"*":1, "md":3}))
    #mask_image = TextField(label="Mask Image", value=CLIPstyler_prefs['mask_image'], on_change=lambda e:changed(e,'mask_image'), expand=True, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD_OUTLINED, on_click=pick_mask, col={"*":1, "md":3}))
    #invert_mask = Checkbox(label="Invert", tooltip="Swaps the Black & White of your Mask Image", value=CLIPstyler_prefs['invert_mask'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'invert_mask'))
    image_picker = Container(content=Row([original_image]), padding=padding.only(top=5), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    #prompt_strength = Slider(min=0.1, max=0.9, divisions=16, label="{value}%", value=CLIPstyler_prefs['prompt_strength'], on_change=change_strength, expand=True)
    #strength_value = Text(f" {int(CLIPstyler_prefs['prompt_strength'] * 100)}%", weight=FontWeight.BOLD)
    #strength_slider = Row([Text("Prompt Strength: "), strength_value, prompt_strength])
    #img_block = Container(Column([image_pickers, strength_slider, Divider(height=9, thickness=2)]), padding=padding.only(top=5), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    apply_ESRGAN_upscale = Switcher(label="Apply ESRGAN Upscale", value=CLIPstyler_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)
    enlarge_scale_slider = SliderRow(label="Enlarge Scale", min=1, max=4, divisions=6, round=1, suffix="x", pref=CLIPstyler_prefs, key='enlarge_scale')
    #face_enhance = Checkbox(label="Use Face Enhance GPFGAN", value=CLIPstyler_prefs['face_enhance'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'face_enhance'))
    display_upscaled_image = Checkbox(label="Display Upscaled Image", value=CLIPstyler_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))
    ESRGAN_settings = Container(Column([enlarge_scale_slider, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    page.ESRGAN_block_styler = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    page.ESRGAN_block_styler.height = None if status['installed_ESRGAN'] else 0
    if not CLIPstyler_prefs['apply_ESRGAN_upscale']:
        ESRGAN_settings.height = 0
    parameters_button = ElevatedButton(content=Text(value="📎   Run CLIP-Styler", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_CLIPstyler(page))

    parameters_row = Row([parameters_button], alignment=MainAxisAlignment.SPACE_BETWEEN)
    page.CLIPstyler_output = Column([])
    c = Column([Container(
        padding=padding.only(18, 14, 20, 10), content=Column([
            Header("😎   CLIP-Styler", "Transfers a Text Guided Style onto your Image From Prompt Description..."),
            image_picker, prompt_text,
            param_rows, iterations, width_slider, height_slider, #Divider(height=9, thickness=2),
            page.ESRGAN_block_styler,
            #(img_block if status['installed_img2img'] or status['installed_stability'] else Container(content=None)), (clip_block if prefs['install_CLIP_guided'] else Container(content=None)), (ESRGAN_block if prefs['install_ESRGAN'] else Container(content=None)),
            parameters_row,
            page.CLIPstyler_output
        ],
    ))], scroll=ScrollMode.AUTO)#batch_folder_name, batch_size, n_iterations, steps, crop_size, num_crops,
    return c

semantic_prefs = {
    'prompt': '',
    'negative_prompt': '',
    'num_inference_steps': 100,
    'guidance_scale': 7.5,
    'edit_momentum_scale': 0.1,
    'edit_mom_beta': 0.4,
    'editing_prompts': [],
    'eta': 0.0,
    'seed': 0,
    'width': 960,
    'height': 768,
    'num_images': 1,
    'batch_folder_name': '',
    "apply_ESRGAN_upscale": prefs['apply_ESRGAN_upscale'],
    "enlarge_scale": 2.0,
    "display_upscaled_image": False,
}

def buildSemanticGuidance(page):
    global semantic_prefs, prefs, pipe_semantic, editing_prompt
    editing_prompt = {'editing_prompt':'', 'edit_warmup_steps':10, 'edit_guidance_scale':5, 'edit_threshold':0.9, 'edit_weights':1, 'reverse_editing_direction': False}
    def changed(e, pref=None, ptype="str"):
      if pref is not None:
        try:
          if ptype == "int":
            semantic_prefs[pref] = int(e.control.value)
          elif ptype == "float":
            semantic_prefs[pref] = float(e.control.value)
          else:
            semantic_prefs[pref] = e.control.value
        except Exception:
          alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
          pass
    def add_to_semantic_output(o):
      page.semantic_output.controls.append(o)
      page.semantic_output.update()
      if not clear_button.visible:
        clear_button.visible = True
        clear_button.update()
    def clear_output(e):
      if prefs['enable_sounds']: page.snd_delete.play()
      page.semantic_output.controls = []
      page.semantic_output.update()
      clear_button.visible = False
      clear_button.update()
    def semantic_help(e):
      def close_semantic_dlg(e):
        nonlocal semantic_help_dlg
        semantic_help_dlg.open = False
        page.update()
      semantic_help_dlg = AlertDialog(title=Text("💁   Help with Semantic Guidance"), content=Column([
          Text("SEGA allows applying or removing one or more concepts from an image. The strength of the concept can also be controlled. I.e. the smile concept can be used to incrementally increase or decrease the smile of a portrait. Similar to how classifier free guidance provides guidance via empty prompt inputs, SEGA provides guidance on conceptual prompts. Multiple of these conceptual prompts can be applied simultaneously. Each conceptual prompt can either add or remove their concept depending on if the guidance is applied positively or negatively."),
          Text("Unlike Pix2Pix Zero or Attend and Excite, SEGA directly interacts with the diffusion process instead of performing any explicit gradient-based optimization."),
          Text("Semantic Guidance for Diffusion Models was proposed in SEGA: Instructing Diffusion using Semantic Dimensions and provides strong semantic control over the image generation. Small changes to the text prompt usually result in entirely different output images. However, with SEGA a variety of changes to the image are enabled that can be controlled easily and intuitively, and stay true to the original image composition."),
          Text("Text-to-image diffusion models have recently received a lot of interest for their astonishing ability to produce high-fidelity images from text only. However, achieving one-shot generation that aligns with the user's intent is nearly impossible, yet small changes to the input prompt often result in very different images. This leaves the user with little semantic control. To put the user in control, we show how to interact with the diffusion process to flexibly steer it along semantic directions. This semantic guidance (SEGA) allows for subtle and extensive edits, changes in composition and style, as well as optimizing the overall artistic conception. We demonstrate SEGA's effectiveness on a variety of tasks and provide evidence for its versatility and flexibility."),
          Markdown("[HuggingFace Documentation](https://huggingface.co/docs/diffusers/main/en/api/pipelines/semantic_stable_diffusion) - [Paper](https://arxiv.org/abs/2301.12247)", on_tap_link=lambda e: e.page.launch_url(e.data)),
        ], scroll=ScrollMode.AUTO), actions=[TextButton("😖  More Control Please... ", on_click=close_semantic_dlg)], actions_alignment=MainAxisAlignment.END)
      page.dialog = semantic_help_dlg
      semantic_help_dlg.open = True
      page.update()
    def toggle_ESRGAN(e):
        ESRGAN_settings.height = None if e.control.value else 0
        semantic_prefs['apply_ESRGAN_upscale'] = e.control.value
        ESRGAN_settings.update()
    def change_eta(e):
        changed(e, 'eta', ptype="float")
        eta_value.value = f" {semantic_prefs['eta']}"
        eta_value.update()
        eta_row.update()
    def semantic_tile(semantic_prompt):
        params = []
        for k, v in semantic_prompt.items():
            if k == 'editing_prompt': continue
            params.append(f'{to_title(k)}: {v}')
        sub = ', '.join(params)
        return ListTile(title=Text(semantic_prompt['editing_prompt'], max_lines=6, style=TextThemeStyle.BODY_LARGE), subtitle=Text(sub), dense=True, data=semantic_prompt, trailing=PopupMenuButton(icon=icons.MORE_VERT,
          items=[PopupMenuItem(icon=icons.EDIT, text="Edit Semantic Prompt", on_click=lambda e: edit_semantic(semantic_prompt), data=semantic_prompt),
                 PopupMenuItem(icon=icons.DELETE, text="Delete Semantic Prompt", on_click=lambda e: del_semantic(semantic_prompt), data=semantic_prompt)]), on_click=lambda e: edit_semantic(semantic_prompt))
    def edit_semantic(edit=None):
        semantic_prompt = edit if bool(edit) else editing_prompt.copy()
        edit_prompt = edit['editing_prompt'] if bool(edit) else ""
        def close_dlg(e):
            dlg_edit.open = False
            page.update()
        def changed_p(e, pref=None):
            if pref is not None:
                semantic_prompt[pref] = e.control.value
        def save_semantic_prompt(e):
            if edit == None:
                semantic_prefs['editing_prompts'].append(semantic_prompt)
                page.semantic_prompts.controls.append(semantic_tile(semantic_prompt))
                page.semantic_prompts.update()
            else:
                for s in semantic_prefs['editing_prompts']:
                    if s['editing_prompt'] == edit_prompt:
                        s = semantic_prompt
                        break
                for t in page.semantic_prompts.controls:
                    if t.data['editing_prompt'] == edit_prompt:
                        params = []
                        for k, v in semantic_prompt.items():
                            if k == 'editing_prompt': continue
                            params.append(f'{to_title(k)}: {v}')
                        sub = ', '.join(params)
                        t.title = Text(semantic_prompt['editing_prompt'], max_lines=6, style=TextThemeStyle.BODY_LARGE)
                        t.subtitle = Text(sub)
                        t.data = semantic_prompt
                        t.update()
                        break
            dlg_edit.open = False
            e.control.update()
            page.update()
        semantic_editing_prompt = TextField(label="Semantic Editing Prompt Modifier", value=semantic_prompt['editing_prompt'], autofocus=True, on_change=lambda e:changed_p(e,'editing_prompt'))
        edit_warmup_steps = SliderRow(label="Edit Warmup Steps", min=0, max=50, divisions=50, pref=semantic_prompt, key='edit_warmup_steps', tooltip="Number of diffusion steps (for each prompt) for which semantic guidance will not be applied. Momentum will still be calculated for those steps and applied once all warmup periods are over.")
        edit_guidance_scale = SliderRow(label="Edit Guidance Scale", min=0, max=20, divisions=40, round=1, pref=semantic_prompt, key='edit_guidance_scale', tooltip="Guidance scale for semantic guidance. If provided as list values should correspond to `editing_prompt`.")
        edit_threshold = SliderRow(label="Edit Threshold", min=0, max=1, divisions=40, round=3, pref=semantic_prompt, key='edit_threshold', tooltip="Threshold of semantic guidance.")
        edit_weights = SliderRow(label="Edit Weights", min=0, max=10, divisions=20, round=1, pref=semantic_prompt, key='edit_weights', tooltip="Indicates how much each individual concept should influence the overall guidance. If no weights are provided all concepts are applied equally.")
        reverse_editing_direction = Checkbox(label="Reverse Editing Direction", value=semantic_prompt['reverse_editing_direction'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed_p(e,'reverse_editing_direction'), tooltip="Whether the corresponding prompt in `editing_prompt` should be increased or decreased.")
        dlg_edit = AlertDialog(modal=False, title=Text(f"♟️ {'Edit' if bool(edit) else 'Add'} Semantic Prompt"), content=Container(Column([
            semantic_editing_prompt, edit_warmup_steps, edit_guidance_scale, edit_threshold, edit_weights, reverse_editing_direction,
        ], alignment=MainAxisAlignment.START, tight=True, scroll=ScrollMode.AUTO), width=(page.width if page.web else page.window_width) - 180), actions=[TextButton(content=Text("Cancel", size=18), on_click=close_dlg), ElevatedButton(content=Text(value=emojize(":floppy_disk:") + "  Save Prompt ", size=19, weight=FontWeight.BOLD), on_click=save_semantic_prompt)], actions_alignment=MainAxisAlignment.END)
        page.dialog = dlg_edit
        dlg_edit.open = True
        page.update()
    def del_semantic(edit=None):
        for s in semantic_prefs['editing_prompts']:
            if s['editing_prompt'] == edit['editing_prompt']:
                semantic_prefs['editing_prompts'].remove(s)
                break
        for t in page.semantic_prompts.controls:
            if t.data['editing_prompt'] == edit['editing_prompt']:
                page.semantic_prompts.controls.remove(t)
                break
        page.semantic_prompts.update()
        if prefs['enable_sounds']: page.snd_delete.play()
    def clear_semantic_prompts(e):
        semantic_prefs['editing_prompts'].clear()
        page.semantic_prompts.controls.clear()
        page.semantic_prompts.update()
        if prefs['enable_sounds']: page.snd_delete.play()
    prompt = TextField(label="Base Prompt Text", value=semantic_prefs['prompt'], col={'md': 9}, multiline=True, on_change=lambda e:changed(e,'prompt'))
    negative_prompt  = TextField(label="Negative Prompt Text", value=semantic_prefs['negative_prompt'], col={'md':3}, multiline=True, on_change=lambda e:changed(e,'negative_prompt'))
    seed = TextField(label="Seed", width=90, value=str(semantic_prefs['seed']), keyboard_type=KeyboardType.NUMBER, tooltip="0 or -1 picks a Random seed", on_change=lambda e:changed(e,'seed', ptype='int'))
    num_inference_row = SliderRow(label="Number of Inference Steps", min=1, max=150, divisions=149, pref=semantic_prefs, key='num_inference_steps', tooltip="The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.")
    guidance = SliderRow(label="Guidance Scale", min=0, max=50, divisions=100, round=1, pref=semantic_prefs, key='guidance_scale')
    edit_momentum_scale = SliderRow(label="Edit Momentum Scale", min=0, max=1, divisions=20, round=1, pref=semantic_prefs, key='edit_momentum_scale', tooltip="Scale of the momentum to be added to the semantic guidance at each diffusion step. Momentum is already built up during warmup, i.e. for diffusion steps smaller than `sld_warmup_steps`. Momentum will only be added to latent guidance once all warmup periods are finished.")
    edit_mom_beta = SliderRow(label="Edit Momentum Beta", min=0, max=1, divisions=20, round=1, pref=semantic_prefs, key='edit_mom_beta', tooltip="Defines how semantic guidance momentum builds up. `edit_mom_beta` indicates how much of the previous momentum will be kept. Momentum is already built up during warmup, i.e. for diffusion steps smaller than `edit_warmup_steps`.")
    #eta = TextField(label="ETA", value=str(semantic_prefs['eta']), keyboard_type=KeyboardType.NUMBER, hint_text="Amount of Noise", on_change=lambda e:changed(e,'eta', ptype='float'))
    eta = Slider(min=0.0, max=1.0, divisions=20, label="{value}", value=float(semantic_prefs['eta']), tooltip="The weight of noise for added noise in a diffusion step. Its value is between 0.0 and 1.0 - 0.0 is DDIM and 1.0 is DDPM scheduler respectively.", expand=True, on_change=change_eta)
    eta_value = Text(f" {semantic_prefs['eta']}", weight=FontWeight.BOLD)
    eta_row = Row([Text("ETA:"), eta_value, eta])
    page.etas.append(eta_row)
    width_slider = SliderRow(label="Width", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=semantic_prefs, key='width')
    height_slider = SliderRow(label="Height", min=256, max=1280, divisions=64, multiple=16, suffix="px", pref=semantic_prefs, key='height')
    batch_folder_name = TextField(label="Batch Folder Name", value=semantic_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    apply_ESRGAN_upscale = Switcher(label="Apply ESRGAN Upscale", value=semantic_prefs['apply_ESRGAN_upscale'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_ESRGAN)
    enlarge_scale_slider = SliderRow(label="Enlarge Scale", min=1, max=4, divisions=6, round=1, suffix="x", pref=semantic_prefs, key='enlarge_scale')
    display_upscaled_image = Checkbox(label="Display Upscaled Image", value=semantic_prefs['display_upscaled_image'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'display_upscaled_image'))
    ESRGAN_settings = Container(Column([enlarge_scale_slider, display_upscaled_image], spacing=0), padding=padding.only(left=32), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    page.ESRGAN_block_semantic = Container(Column([apply_ESRGAN_upscale, ESRGAN_settings]), animate_size=animation.Animation(1000, AnimationCurve.BOUNCE_OUT), clip_behavior=ClipBehavior.HARD_EDGE)
    page.ESRGAN_block_semantic.height = None if status['installed_ESRGAN'] else 0
    page.semantic_prompts = Column([], spacing=0)
    page.semantic_output = Column([])
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.semantic_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🧩  Semantic Guidance for Diffusion Models - SEGA", "Text-to-Image Generation with Latent Editing to apply or remove multiple concepts from an image with advanced controls....", actions=[IconButton(icon=icons.HELP, tooltip="Help with Semantic Guidance Settings", on_click=semantic_help)]),
        #ResponsiveRow([Row([original_image, alpha_mask], col={'lg':6}), Row([mask_image, invert_mask], col={'lg':6})]),
        ResponsiveRow([prompt, negative_prompt]),
        Row([Text("Editing Semantic Prompts", style=TextThemeStyle.TITLE_LARGE, weight=FontWeight.BOLD),
                    Row([ft.FilledTonalButton("Clear Prompts", on_click=clear_semantic_prompts), ft.FilledButton("Add Editing Prompt", on_click=lambda e: edit_semantic(None))])], alignment=MainAxisAlignment.SPACE_BETWEEN),
        page.semantic_prompts,
        Divider(thickness=2, height=4),
        num_inference_row,
        guidance,
        edit_momentum_scale, edit_mom_beta,
        eta_row,
        width_slider, height_slider,
        Row([NumberPicker(label="Number of Images: ", min=1, max=8, value=semantic_prefs['num_images'], on_change=lambda e: changed(e, 'num_images')), seed, batch_folder_name]),
        page.ESRGAN_block_semantic,
        #Row([jump_length, jump_n_sample, seed]),
        Row([
            ElevatedButton(content=Text("🎳  Run Semantic Guidance", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_semantic(page)),
             #ElevatedButton(content=Text(value="📜   Run from Prompts List", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_semantic(page, from_list=True))
        ]),
        page.semantic_output,
        clear_button,
      ]
    ))], scroll=ScrollMode.AUTO, auto_scroll=False)
    return c

#TODO: Waiting for Scribbler addon integration
def buildDreamMask(page):
    #prog_bars: Dict[str, ProgressRing] = {}
    files = Ref[Column]()
    #upload_button = Ref[ElevatedButton]()

    def file_picker_result(e: FilePickerResultEvent):
        files.current.controls.clear()
        if e.files != None:
          upload_files(e)
    def on_upload_progress(e: FilePickerUploadEvent):
      if e.progress == 1:
        if not slash in e.file_name:
          fname = os.path.join(root_dir, e.file_name)
        else:
          fname = e.file_name
        files.current.controls.append(Row([Text(f"Done uploading {root_dir}{e.file_name}")]))
        page.update()
    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)
    def upload_files(e):
        uf = []
        if file_picker.result != None and file_picker.result.files != None:
            for f in file_picker.result.files:
              if page.web:
                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))
              else:
                on_upload_progress(FilePickerUploadEvent(f.path, 1, ""))
            file_picker.upload(uf)
    page.overlay.append(file_picker)

    c = Column([
        ElevatedButton(
            "Select Init Image to Mask...",
            icon=icons.FOLDER_OPEN,
            on_click=lambda _: file_picker.pick_files(allow_multiple=False, allowed_extensions=["png", "PNG"], dialog_title="Pick Init Image File" ),
        ),
        Column(ref=files),
    ])
    return c

dreambooth_prefs = {
    'instance_prompt': '',
    'prior_preservation': False,
    'prior_preservation_class_prompt': "",
    'num_class_images': 12,
    'sample_batch_size': 2,
    'train_batch_size': 1,
    'prior_loss_weight': 0.5,
    'prior_preservation_class_folder': os.path.join(root_dir, "class_images"),
    'learning_rate': 5e-06,
    'max_train_steps': 450,
    'seed': 222476,
    'name_of_your_concept': "",
    'save_concept': True,
    'where_to_save_concept': "Public Library",
    'max_size': 512,
    'image_path': '',
    'readme_description': '',
    'urls': [],
}

def buildDreamBooth(page):
    global prefs, dreambooth_prefs
    def changed(e, pref=None, ptype="str"):
        if pref is not None:
          try:
            if ptype == "int":
              dreambooth_prefs[pref] = int(e.control.value)
            elif ptype == "float":
              dreambooth_prefs[pref] = float(e.control.value)
            else:
              dreambooth_prefs[pref] = e.control.value
          except Exception:
            alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
            pass
    def add_to_dreambooth_output(o):
        page.dreambooth_output.controls.append(o)
        page.dreambooth_output.update()
    def clear_output(e):
        if prefs['enable_sounds']: page.snd_delete.play()
        page.dreambooth_output.controls = []
        page.dreambooth_output.update()
        clear_button.visible = False
        clear_button.update()
    def db_help(e):
        def close_db_dlg(e):
          nonlocal db_help_dlg
          db_help_dlg.open = False
          page.update()
        db_help_dlg = AlertDialog(title=Text("💁   Help with DreamBooth"), content=Column([
            Text("First thing is to collect all your own images that you want to teach it to dream.  Feed it at least 5 square pictures of the object or style to learn, and it'll save your Custom Model Checkpoint."),
            Text("Fine-tune your perameters, but be aware that the training process takes a long time to run, so careful with the settings if you don't have the patience or processor. Dream at your own risk."),
          ], scroll=ScrollMode.AUTO), actions=[TextButton(emojize(':sleepy_face:') + "  Got it... ", on_click=close_db_dlg)], actions_alignment=MainAxisAlignment.END)
        page.dialog = db_help_dlg
        db_help_dlg.open = True
        page.update()
    def delete_image(e):
        f = e.control.data
        if os.path.isfile(f):
          os.remove(f)
          for i, fl in enumerate(page.db_file_list.controls):
            if fl.title.value == f:
              del page.db_file_list.controls[i]
              page.db_file_list.update()
              continue
    def delete_all_images(e):
        for fl in page.db_file_list.controls:
          f = fl.title.value
          if os.path.isfile(f):
            os.remove(f)
        page.db_file_list.controls.clear()
        page.db_file_list.update()
    def add_file(fpath, update=True):
        page.db_file_list.controls.append(ListTile(title=Text(fpath), dense=True, trailing=PopupMenuButton(icon=icons.MORE_VERT,
          items=[#TODO: View Image
              PopupMenuItem(icon=icons.DELETE, text="Delete Image", on_click=delete_image, data=fpath),
              PopupMenuItem(icon=icons.DELETE_SWEEP, text="Delete All", on_click=delete_all_images, data=fpath),
          ])))
        if update: page.db_file_list.update()
    def file_picker_result(e: FilePickerResultEvent):
        if e.files != None:
          upload_files(e)
    save_dir = os.path.join(root_dir, 'my_concept')
    def on_upload_progress(e: FilePickerUploadEvent):
        if e.progress == 1:
          if not os.path.exists(save_dir):
            os.mkdir(save_dir)
          if not slash in e.file_name:
            fname = os.path.join(root_dir, e.file_name)
            fpath = os.path.join(save_dir, e.file_name)
          else:
            fname = e.file_name
            fpath = os.path.join(save_dir, e.file_name.rpartition(slash)[2])
          original_img = PILImage.open(fname)
          width, height = original_img.size
          width, height = scale_dimensions(width, height, dreambooth_prefs['max_size'])
          original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert("RGB")
          original_img.save(fpath)
          if page.web: os.remove(fname)
          #shutil.move(fname, fpath)
          add_file(fpath)
    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)
    def pick_path(e):
        file_picker.pick_files(allow_multiple=True, allowed_extensions=["png", "PNG", "jpg", "jpeg"], dialog_title="Pick Image File to Enlarge")
    def upload_files(e):
        uf = []
        if file_picker.result != None and file_picker.result.files != None:
            for f in file_picker.result.files:
              if page.web:
                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))
              else:
                on_upload_progress(FilePickerUploadEvent(f.path, 1, ""))
            file_picker.upload(uf)
    page.overlay.append(file_picker)
    def add_image(e):
        save_dir = os.path.join(root_dir, 'my_concept')
        if not os.path.exists(save_dir):
          os.mkdir(save_dir)
        if image_path.value.startswith('http'):
          import requests
          from io import BytesIO
          response = requests.get(image_path.value)
          fpath = os.path.join(save_dir, image_path.value.rpartition(slash)[2])
          concept_image = PILImage.open(BytesIO(response.content)).convert("RGB")
          width, height = concept_image.size
          width, height = scale_dimensions(width, height, dreambooth_prefs['max_size'])
          concept_image = concept_image.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert("RGB")
          concept_image.save(fpath)
          add_file(fpath)
        elif os.path.isfile(image_path.value):
          fpath = os.path.join(save_dir, image_path.value.rpartition(slash)[2])
          original_img = PILImage.open(image_path.value)
          width, height = original_img.size
          width, height = scale_dimensions(width, height, dreambooth_prefs['max_size'])
          original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert("RGB")
          original_img.save(fpath)
          #shutil.copy(image_path.value, fpath)
          add_file(fpath)
        elif os.path.isdir(image_path.value):
          for f in os.listdir(image_path.value):
            file_path = os.path.join(image_path.value, f)
            if os.path.isdir(file_path): continue
            if f.lower().endswith(('.png', '.jpg', '.jpeg')):
              fpath = os.path.join(save_dir, f)
              original_img = PILImage.open(file_path)
              width, height = original_img.size
              width, height = scale_dimensions(width, height, dreambooth_prefs['max_size'])
              original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert("RGB")
              original_img.save(fpath)
              #shutil.copy(file_path, fpath)
              add_file(fpath)
        else:
          if bool(image_path.value):
            alert_msg(page, "Couldn't find a valid File, Path or URL...")
          else:
            pick_path(e)
          return
        image_path.value = ""
        image_path.update()
    def load_images():
        if os.path.exists(save_dir):
          for f in os.listdir(save_dir):
            existing = os.path.join(save_dir, f)
            if os.path.isdir(existing): continue
            if f.lower().endswith(('.png', '.jpg', '.jpeg')):
              add_file(existing, update=False)
    instance_prompt = TextField(label="Instance Prompt Token Text", value=dreambooth_prefs['instance_prompt'], on_change=lambda e:changed(e,'instance_prompt'))
    prior_preservation_class_prompt = TextField(label="Prior Preservation Class Prompt", value=dreambooth_prefs['prior_preservation_class_prompt'], on_change=lambda e:changed(e,'prior_preservation_class_prompt'))
    prior_preservation = Checkbox(label="Prior Preservation", tooltip="If you'd like class of the concept (e.g.: toy, dog, painting) is guaranteed to be preserved. This increases the quality and helps with generalization at the cost of training time", value=dreambooth_prefs['prior_preservation'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'prior_preservation'))
    num_class_images = TextField(label="Number of Class Images", value=dreambooth_prefs['num_class_images'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'num_class_images', ptype='int'), width = 160)
    sample_batch_size = TextField(label="Sample Batch Size", value=dreambooth_prefs['sample_batch_size'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'sample_batch_size', ptype='int'), width = 160)
    prior_loss_weight = TextField(label="Prior Loss Weight", value=dreambooth_prefs['prior_loss_weight'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'prior_loss_weight', ptype='float'), width = 160)
    max_train_steps = TextField(label="Max Training Steps", value=dreambooth_prefs['max_train_steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'max_train_steps', ptype='int'), width = 160)
    learning_rate = TextField(label="Learning Rate", value=dreambooth_prefs['learning_rate'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'learning_rate', ptype='float'), width = 160)
    seed = TextField(label="Seed", value=dreambooth_prefs['seed'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'seed', ptype='int'), width = 160)
    save_concept = Checkbox(label="Save Concept    ", tooltip="", value=dreambooth_prefs['save_concept'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'save_concept'))
    where_to_save_concept = Dropdown(label="Where to Save Concept", width=250, options=[dropdown.Option("Public Library"), dropdown.Option("Privately to my Profile")], value=dreambooth_prefs['where_to_save_concept'], on_change=lambda e: changed(e, 'where_to_save_concept'))
    prior_preservation_class_folder = TextField(label="Prior Preservation Class Folder", value=dreambooth_prefs['prior_preservation_class_folder'], on_change=lambda e:changed(e,'prior_preservation_class_folder'))
    name_of_your_concept = TextField(label="Name of your Concept", value=dreambooth_prefs['name_of_your_concept'], on_change=lambda e:changed(e,'name_of_your_concept'))
    readme_description = TextField(label="Extra README Description", value=dreambooth_prefs['readme_description'], on_change=lambda e:changed(e,'readme_description'))
    max_row = SliderRow(label="Max Resolution Size", min=256, max=1024, divisions=12, multiple=32, suffix="px", pref=dreambooth_prefs, key='max_size')
    image_path = TextField(label="Image File or Folder Path or URL to Train", value=dreambooth_prefs['image_path'], on_change=lambda e:changed(e,'image_path'), suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_path), expand=1)
    add_image_button = ElevatedButton(content=Text("Add File or Folder"), on_click=add_image)
    page.db_file_list = Column([], tight=True, spacing=0)
    load_images()
    #seed = TextField(label="Seed", value=dreambooth_prefs['seed'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'seed', ptype='int'), width = 160)
    #lambda_entropy = TextField(label="Lambda Entropy", value=dreamfusdreambooth_prefsion_prefs['lambda_entropy'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'lambda_entropy', ptype='float'), width = 160)
    #max_steps = TextField(label="Max Steps", value=dreambooth_prefs['max_steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'max_steps', ptype='int'), width = 160)
    page.dreambooth_output = Column([])
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.dreambooth_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("😶‍🌫️  Create Custom DreamBooth Concept Model", "Provide a collection of images to conceptualize. Warning: May take over an hour to run the training...", actions=[IconButton(icon=icons.HELP, tooltip="Help with DreamBooth Settings", on_click=db_help)]),
        Row([instance_prompt, name_of_your_concept]),
        Row([num_class_images, sample_batch_size, prior_loss_weight]),
        Row([max_train_steps, learning_rate, seed]),
        Row([save_concept, where_to_save_concept]),
        readme_description,
        #Row([prior_preservation_class_folder]),
        max_row,
        Row([image_path, add_image_button]),
        page.db_file_list,
        Row([ElevatedButton(content=Text("👨‍🎨️  Run DreamBooth", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_dreambooth(page)), ElevatedButton(content=Text("👨‍🎨️  Run DreamBooth 2", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_dreambooth2(page))]),
        page.dreambooth_output,
        clear_button,
      ]
    ))], scroll=ScrollMode.AUTO)
    return c

textualinversion_prefs = {
    'what_to_teach': 'object',
    'placeholder_token': '',
    'initializer_token': '',
    'scale_lr': True,
    'max_train_steps': 3000,
    'train_batch_size': 1,
    'gradient_accumulation_steps': 4,
    'seed': 22276,
    'repeats': 100,
    'validation_prompt': '',
    'validation_steps': 1,
    'num_vectors': 2,
    'output_dir': os.path.join(root_dir, "sd-concept-output"),
    'learning_rate': 5e-04,
    'name_of_your_concept': "",
    'save_concept': True,
    'where_to_save_concept': "Public Library",
    'max_size': 512,
    'image_path': '',
    'readme_description': '',
    'urls': [],
}
def buildTextualInversion(page):
    global prefs, textualinversion_prefs
    def changed(e, pref=None, ptype="str"):
        if pref is not None:
          try:
            if ptype == "int":
              textualinversion_prefs[pref] = int(e.control.value)
            elif ptype == "float":
              textualinversion_prefs[pref] = float(e.control.value)
            else:
              textualinversion_prefs[pref] = e.control.value
          except Exception:
            alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
            pass
    def add_to_textualinversion_output(o):
        page.textualinversion_output.controls.append(o)
        page.textualinversion_output.update()
    def clear_output(e):
        if prefs['enable_sounds']: page.snd_delete.play()
        page.textualinversion_output.controls = []
        page.textualinversion_output.update()
        clear_button.visible = False
        clear_button.update()
    def ti_help(e):
        def close_ti_dlg(e):
          nonlocal ti_help_dlg
          ti_help_dlg.open = False
          page.update()
        ti_help_dlg = AlertDialog(title=Text("💁   Help with Textual-Inversion"), content=Column([
            Text(""),
          ], scroll=ScrollMode.AUTO), actions=[TextButton("😪  I'll figure it out... ", on_click=close_ti_dlg)], actions_alignment=MainAxisAlignment.END)
        page.dialog = ti_help_dlg
        ti_help_dlg.open = True
        page.update()
    def delete_image(e):
        f = e.control.data
        if os.path.isfile(f):
          os.remove(f)
          for i, fl in enumerate(page.ti_file_list.controls):
            if fl.title.value == f:
              del page.ti_file_list.controls[i]
              page.ti_file_list.update()
              continue
    def delete_all_images(e):
        for fl in page.ti_file_list.controls:
          f = fl.title.value
          if os.path.isfile(f):
            os.remove(f)
        page.ti_file_list.controls.clear()
        page.ti_file_list.update()
    def add_file(fpath, update=True):
        page.ti_file_list.controls.append(ListTile(title=Text(fpath), dense=True, trailing=PopupMenuButton(icon=icons.MORE_VERT,
          items=[#TODO: View Image
              PopupMenuItem(icon=icons.DELETE, text="Delete Image", on_click=delete_image, data=fpath),
              PopupMenuItem(icon=icons.DELETE_SWEEP, text="Delete All", on_click=delete_all_images, data=fpath),
          ])))
        if update: page.ti_file_list.update()
    def file_picker_result(e: FilePickerResultEvent):
        if e.files != None:
          upload_files(e)
    save_dir = os.path.join(root_dir, 'my_concept')
    def on_upload_progress(e: FilePickerUploadEvent):
        if e.progress == 1:
          if not os.path.exists(save_dir):
            os.mkdir(save_dir)
          if not slash in e.file_name:
            fname = os.path.join(root_dir, e.file_name)
            fpath = os.path.join(save_dir, e.file_name)
          else:
            fname = e.file_name
            fpath = os.path.join(save_dir, e.file_name.rpartition(slash)[2])
          original_img = PILImage.open(fname)
          width, height = original_img.size
          width, height = scale_dimensions(width, height, textualinversion_prefs['max_size'])
          original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert("RGB")
          original_img.save(fpath)
          if page.web: os.remove(fname)
          #shutil.move(fname, fpath)
          add_file(fpath)
    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)
    def pick_path(e):
        file_picker.pick_files(allow_multiple=True, allowed_extensions=["png", "PNG", "jpg", "jpeg"], dialog_title="Pick Image File to Enlarge")
    def upload_files(e):
        uf = []
        if file_picker.result != None and file_picker.result.files != None:
            for f in file_picker.result.files:
              if page.web:
                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))
              else:
                on_upload_progress(FilePickerUploadEvent(f.path, 1, ""))
            file_picker.upload(uf)
    page.overlay.append(file_picker)
    def add_image(e):
        save_dir = os.path.join(root_dir, 'my_concept')
        if not os.path.exists(save_dir):
          os.mkdir(save_dir)
        if image_path.value.startswith('http'):
          import requests
          from io import BytesIO
          response = requests.get(image_path.value)
          fpath = os.path.join(save_dir, image_path.value.rpartition(slash)[2])
          concept_image = PILImage.open(BytesIO(response.content)).convert("RGB")
          width, height = concept_image.size
          width, height = scale_dimensions(width, height, textualinversion_prefs['max_size'])
          concept_image = concept_image.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert("RGB")
          concept_image.save(fpath)
          add_file(fpath)
        elif os.path.isfile(image_path.value):
          fpath = os.path.join(save_dir, image_path.value.rpartition(slash)[2])
          original_img = PILImage.open(image_path.value)
          width, height = original_img.size
          width, height = scale_dimensions(width, height, textualinversion_prefs['max_size'])
          original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert("RGB")
          original_img.save(fpath)
          #shutil.copy(image_path.value, fpath)
          add_file(fpath)
        elif os.path.isdir(image_path.value):
          for f in os.listdir(image_path.value):
            file_path = os.path.join(image_path.value, f)
            if os.path.isdir(file_path): continue
            if f.lower().endswith(('.png', '.jpg', '.jpeg')):
              fpath = os.path.join(save_dir, f)
              original_img = PILImage.open(file_path)
              width, height = original_img.size
              width, height = scale_dimensions(width, height, textualinversion_prefs['max_size'])
              original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert("RGB")
              original_img.save(fpath)
              #shutil.copy(file_path, fpath)
              add_file(fpath)
        else:
          if bool(image_path.value):
            alert_msg(page, "Couldn't find a valid File, Path or URL...")
          else:
            pick_path(e)
          return
        image_path.value = ""
        image_path.update()
    def load_images():
        if os.path.exists(save_dir):
          for f in os.listdir(save_dir):
            existing = os.path.join(save_dir, f)
            if os.path.isdir(existing): continue
            if f.lower().endswith(('.png', '.jpg', '.jpeg')):
              add_file(existing, update=False)
    what_to_teach = Dropdown(label="What to Teach", width=250, options=[dropdown.Option("object"), dropdown.Option("style")], value=textualinversion_prefs['what_to_teach'], on_change=lambda e: changed(e, 'what_to_teach'))
    placeholder_token = TextField(label="Placeholder <Token> Keyword", value=textualinversion_prefs['placeholder_token'], on_change=lambda e:changed(e,'placeholder_token'))
    initializer_token = TextField(label="Initializer Token Category Summary", value=textualinversion_prefs['initializer_token'], on_change=lambda e:changed(e,'initializer_token'))
    validation_prompt = TextField(label="Validation <Token> Prompt", value=textualinversion_prefs['validation_prompt'], on_change=lambda e:changed(e,'validation_prompt'))
    gradient_accumulation_steps = TextField(label="Gradient Accumulation Steps", value=textualinversion_prefs['gradient_accumulation_steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'gradient_accumulation_steps', ptype='int'), width = 160)
    scale_lr = Checkbox(label="Scale Learning Rate", tooltip="", value=textualinversion_prefs['scale_lr'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'scale_lr'))
    validation_steps = TextField(label="Validation Steps", value=textualinversion_prefs['validation_steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'validation_steps', ptype='int'), width = 145)
    num_vectors = TextField(label="Number of Vectors", value=textualinversion_prefs['num_vectors'], tooltip="How many textual inversion vectors shall be used to learn the concept.", keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'num_vectors', ptype='int'), width = 145)
    repeats = TextField(label="Repeats", value=textualinversion_prefs['repeats'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'repeats', ptype='int'), width = 160)
    train_batch_size = TextField(label="Train Batch Size", value=textualinversion_prefs['train_batch_size'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'train_batch_size', ptype='float'), width = 160)
    max_train_steps = TextField(label="Max Training Steps", value=textualinversion_prefs['max_train_steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'max_train_steps', ptype='int'), width = 160)
    learning_rate = TextField(label="Learning Rate", value=textualinversion_prefs['learning_rate'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'learning_rate', ptype='float'), width = 160)
    seed = TextField(label="Seed", value=textualinversion_prefs['seed'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'seed', ptype='int'), width = 160)
    save_concept = Checkbox(label="Save Concept    ", tooltip="", value=textualinversion_prefs['save_concept'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'save_concept'))
    where_to_save_concept = Dropdown(label="Where to Save Concept", width=250, options=[dropdown.Option("Public Library"), dropdown.Option("Privately to my Profile")], value=textualinversion_prefs['where_to_save_concept'], on_change=lambda e: changed(e, 'where_to_save_concept'))
    output_dir = TextField(label="Prior Preservation Class Folder", value=textualinversion_prefs['output_dir'], on_change=lambda e:changed(e,'output_dir'))
    name_of_your_concept = TextField(label="Name of your Concept", value=textualinversion_prefs['name_of_your_concept'], on_change=lambda e:changed(e,'name_of_your_concept'))
    readme_description = TextField(label="Extra README Description", value=textualinversion_prefs['readme_description'], on_change=lambda e:changed(e,'readme_description'))
    max_row = SliderRow(label="Max Resolution Size", min=256, max=1024, divisions=12, multiple=32, suffix="px", pref=textualinversion_prefs, key='max_size')
    image_path = TextField(label="Image File or Folder Path or URL to Train", value=textualinversion_prefs['image_path'], on_change=lambda e:changed(e,'image_path'), suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_path), expand=1)
    add_image_button = ElevatedButton(content=Text("Add File or Folder"), on_click=add_image)
    page.ti_file_list = Column([], tight=True, spacing=0)
    load_images()
    #seed = TextField(label="Seed", value=textualinversion_prefs['seed'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'seed', ptype='int'), width = 160)
    #lambda_entropy = TextField(label="Lambda Entropy", value=dreamfustextualinversion_prefsion_prefs['lambda_entropy'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'lambda_entropy', ptype='float'), width = 160)
    #max_steps = TextField(label="Max Steps", value=textualinversion_prefs['max_steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'max_steps', ptype='int'), width = 160)
    page.textualinversion_output = Column([])
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.textualinversion_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("😶‍🌫️  Create Cusom Textual-Inversion Concept Model", "Provide a collection of images to conceptualize. Warning: May take over an hour to run the training...", actions=[IconButton(icon=icons.HELP, tooltip="Help with Textual-Inversion Settings", on_click=ti_help)]),
        Row([what_to_teach, initializer_token]),
        Row([placeholder_token, name_of_your_concept]),
        Row([validation_prompt, validation_steps, num_vectors]),
        scale_lr,
        Row([gradient_accumulation_steps, repeats, train_batch_size]),
        Row([max_train_steps, learning_rate, seed]),
        Row([save_concept, where_to_save_concept]),
        readme_description,
        #Row([output_dir]),
        max_row,
        Row([image_path, add_image_button]),
        page.ti_file_list,
        ElevatedButton(content=Text("👨‍🎨️  Run Textual-Inversion", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_textualinversion(page)),
        page.textualinversion_output,
        clear_button,
      ]
    ))], scroll=ScrollMode.AUTO)
    return c

LoRA_dreambooth_prefs = {
    'instance_prompt': '', #The prompt with identifier specifying the instance
    'class_prompt': '',
    'prior_preservation': False, #Flag to add prior preservation loss.
    'num_class_images': 100, #Minimal class images for prior preservation loss. If there are not enough images already present in class_data_dir, additional images will be sampled with class_prompt.
    'sample_batch_size': 4, #Batch size (per device) for sampling images.
    'train_batch_size': 1, #"Batch size (per device) for the training dataloader.
    'gradient_accumulation_steps': 1, #Number of updates steps to accumulate before performing a backward/update pass.
    'gradient_checkpointing': True, #Whether or not to use gradient checkpointing to save memory at the expense of slower backward pass.
    'checkpointing_steps': 100,#Number of training steps between saving model checkpoints
    'lr_scheduler': 'constant', #["linear", "cosine", "cosine_with_restarts", "polynomial", "constant", "constant_with_warmup"]
    'lr_warmup_steps': 500, #Number of steps for the warmup in the lr scheduler.
    'lr_num_cycles': 1, #Number of hard resets of the lr in cosine_with_restarts scheduler.
    'lr_power': 1, #Power factor of the polynomial scheduler.
    'prior_loss_weight': 1.0, #The weight of prior preservation loss.
    'class_data_dir': os.path.join(root_dir, "class_images"),
    'learning_rate': 1e-4, #Initial learning rate (after the potential warmup period) to use.
    'scale_lr': False, #Scale the learning rate by the number of GPUs, gradient accumulation steps, and batch size.
    'max_train_steps': 500, #Total number of training steps to perform.  If provided, overrides num_train_epochs.
    'seed': 0,
    'name_of_your_model': '',
    'save_model': True,
    'where_to_save_model': 'Public HuggingFace',
    'resolution': 512,
    'image_path': '',
    'readme_description': '',
    'urls': [],
}

    #--lr_num_cycles=1 --lr_power=1 --prior_loss_weight=1.0 --sample_batch_size=4 --num_class_images=100
LoRA_prefs = {
    'instance_prompt': '', #The prompt with identifier specifying the instance
    'class_prompt': '',
    'prior_preservation': False, #Flag to add prior preservation loss.
    #'num_class_images': 100, #Minimal class images for prior preservation loss. If there are not enough images already present in class_data_dir, additional images will be sampled with class_prompt.
    #'sample_batch_size': 4, #Batch size (per device) for sampling images.
    'train_batch_size': 1, #"Batch size (per device) for the training dataloader.
    'gradient_accumulation_steps': 1, #Number of updates steps to accumulate before performing a backward/update pass.
    'gradient_checkpointing': True, #Whether or not to use gradient checkpointing to save memory at the expense of slower backward pass.
    'checkpointing_steps': 100,#Number of training steps between saving model checkpoints
    'resume_from_checkpoint': '', #Whether training should be resumed from a previous checkpoint. Use a path saved by" `--checkpointing_steps`, or `latest` to automatically select the last available checkpoint.
    'lr_scheduler': 'constant', #["linear", "cosine", "cosine_with_restarts", "polynomial", "constant", "constant_with_warmup"]
    'lr_warmup_steps': 500, #Number of steps for the warmup in the lr scheduler.
    #'lr_num_cycles': 1, #Number of hard resets of the lr in cosine_with_restarts scheduler.
    #'lr_power': 1, #Power factor of the polynomial scheduler.
    #'prior_loss_weight': 1.0, #The weight of prior preservation loss.
    'class_data_dir': os.path.join(root_dir, "class_images"),
    'learning_rate': 1e-4, #Initial learning rate (after the potential warmup period) to use.
    'scale_lr': False, #Scale the learning rate by the number of GPUs, gradient accumulation steps, and batch size.
    'max_train_steps': 500, #Total number of training steps to perform.  If provided, overrides num_train_epochs.
    'seed': 0,
    'validation_prompt': '', #A prompt that is sampled during training for inference.
    'num_validation_images': 4, #Number of images that should be generated during validation with `validation_prompt`.
    'validation_epochs': 1, #Run fine-tuning validation every X epochs. The validation process consists of running the prompt
    'use_SDXL': False,
    'name_of_your_model': '',
    'save_model': True,
    'where_to_save_model': 'Public HuggingFace',
    'resolution': 512,
    'image_path': '',
    'readme_description': '',
    'urls': [],
}

def buildLoRA_Dreambooth(page):
    global prefs, LoRA_dreambooth_prefs
    def changed(e, pref=None, ptype="str"):
        if pref is not None:
          try:
            if ptype == "int":
              LoRA_dreambooth_prefs[pref] = int(e.control.value)
            elif ptype == "float":
              LoRA_dreambooth_prefs[pref] = float(e.control.value)
            else:
              LoRA_dreambooth_prefs[pref] = e.control.value
          except Exception:
            alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
            pass
    def add_to_LoRA_dreambooth_output(o):
        page.LoRA_dreambooth_output.controls.append(o)
        page.LoRA_dreambooth_output.update()
    def clear_output(e):
        if prefs['enable_sounds']: page.snd_delete.play()
        page.LoRA_dreambooth_output.controls = []
        page.LoRA_dreambooth_output.update()
        clear_button.visible = False
        clear_button.update()
    def lora_dreambooth_help(e):
        def close_lora_dreambooth_dlg(e):
          nonlocal lora_dreambooth_help_dlg
          lora_dreambooth_help_dlg.open = False
          page.update()
        lora_dreambooth_help_dlg = AlertDialog(title=Text("💁   Help with LoRA DreamBooth"), content=Column([
            Text("First thing is to collect all your own images that you want to teach it to dream.  Feed it at least 5 square pictures of the object or style to learn, and it'll save your Custom Model Checkpoint."),
            Markdown("""Low-Rank Adaption of Large Language Models was first introduced by Microsoft in [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685) by *Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen*
In a nutshell, LoRA allows to adapt pretrained models by adding pairs of rank-decomposition matrices to existing weights and **only** training those newly added weights. This has a couple of advantages:
- Previous pretrained weights are kept frozen so that the model is not prone to [catastrophic forgetting](https://www.pnas.org/doi/10.1073/pnas.1611835114)
- Rank-decomposition matrices have significantly fewer parameters than the original model, which means that trained LoRA weights are easily portable.
- LoRA attention layers allow to control to which extent the model is adapted torwards new training images via a `scale` parameter.""", on_tap_link=lambda e: e.page.launch_url(e.data)),
            Text("Fine-tune your perameters, but be aware that the training process takes a long time to run, so careful with the settings if you don't have the patience or processor. Dream at your own risk."),
          ], scroll=ScrollMode.AUTO), actions=[TextButton(emojize(':sun_with_face:') + "  Neato... ", on_click=close_lora_dreambooth_dlg)], actions_alignment=MainAxisAlignment.END)
        page.dialog = lora_dreambooth_help_dlg
        lora_dreambooth_help_dlg.open = True
        page.update()
    def delete_image(e):
        f = e.control.data
        if os.path.isfile(f):
          os.remove(f)
          for i, fl in enumerate(page.lora_dreambooth_file_list.controls):
            if fl.title.value == f:
              del page.lora_dreambooth_file_list.controls[i]
              page.lora_dreambooth_file_list.update()
              continue
    def delete_all_images(e):
        for fl in page.lora_dreambooth_file_list.controls:
          f = fl.title.value
          if os.path.isfile(f):
            os.remove(f)
        page.lora_dreambooth_file_list.controls.clear()
        page.lora_dreambooth_file_list.update()
    def image_details(e):
        img = e.control.data
        alert_msg(e.page, "Image Details", content=Image(src=img), sound=False)
    def add_file(fpath, update=True):
        page.lora_dreambooth_file_list.controls.append(ListTile(title=Text(fpath), dense=False, trailing=PopupMenuButton(icon=icons.MORE_VERT,
          items=[#TODO: View Image
              PopupMenuItem(icon=icons.INFO, text="Image Details", on_click=image_details, data=fpath),
              PopupMenuItem(icon=icons.DELETE, text="Delete Image", on_click=delete_image, data=fpath),
              PopupMenuItem(icon=icons.DELETE_SWEEP, text="Delete All", on_click=delete_all_images, data=fpath),
          ]), data=fpath, on_click=image_details))
        if update: page.lora_dreambooth_file_list.update()
    def file_picker_result(e: FilePickerResultEvent):
        if e.files != None:
          upload_files(e)
    save_dir = os.path.join(root_dir, 'my_model')
    def on_upload_progress(e: FilePickerUploadEvent):
        if e.progress == 1:
          if not os.path.exists(save_dir):
            os.mkdir(save_dir)
          if not slash in e.file_name:
            fname = os.path.join(root_dir, e.file_name)
            fpath = os.path.join(save_dir, e.file_name)
          else:
            fname = e.file_name
            fpath = os.path.join(save_dir, e.file_name.rpartition(slash)[2])
          original_img = PILImage.open(fname)
          width, height = original_img.size
          width, height = scale_dimensions(width, height, LoRA_dreambooth_prefs['resolution'])
          original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert("RGB")
          original_img.save(fpath)
          if page.web: os.remove(fname)
          #shutil.move(fname, fpath)
          add_file(fpath)
    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)
    def pick_path(e):
        file_picker.pick_files(allow_multiple=True, allowed_extensions=["png", "PNG", "jpg", "jpeg"], dialog_title="Pick Image File to Enlarge")
    def upload_files(e):
        uf = []
        if file_picker.result != None and file_picker.result.files != None:
            for f in file_picker.result.files:
              if page.web:
                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))
              else:
                on_upload_progress(FilePickerUploadEvent(f.path, 1, ""))
            file_picker.upload(uf)
    page.overlay.append(file_picker)
    def add_image(e):
        save_dir = os.path.join(root_dir, 'my_model')
        if not os.path.exists(save_dir):
          os.mkdir(save_dir)
        if image_path.value.startswith('http'):
          import requests
          from io import BytesIO
          response = requests.get(image_path.value)
          fpath = os.path.join(save_dir, image_path.value.rpartition(slash)[2])
          model_image = PILImage.open(BytesIO(response.content)).convert("RGB")
          width, height = model_image.size
          width, height = scale_dimensions(width, height, LoRA_dreambooth_prefs['resolution'])
          model_image = model_image.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert("RGB")
          model_image.save(fpath)
          add_file(fpath)
        elif os.path.isfile(image_path.value):
          fpath = os.path.join(save_dir, image_path.value.rpartition(slash)[2])
          original_img = PILImage.open(image_path.value)
          width, height = original_img.size
          width, height = scale_dimensions(width, height, LoRA_dreambooth_prefs['resolution'])
          original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert("RGB")
          original_img.save(fpath)
          #shutil.copy(image_path.value, fpath)
          add_file(fpath)
        elif os.path.isdir(image_path.value):
          for f in os.listdir(image_path.value):
            file_path = os.path.join(image_path.value, f)
            if os.path.isdir(file_path): continue
            if f.lower().endswith(('.png', '.jpg', '.jpeg')):
              fpath = os.path.join(save_dir, f)
              original_img = PILImage.open(file_path)
              width, height = original_img.size
              width, height = scale_dimensions(width, height, LoRA_dreambooth_prefs['resolution'])
              original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert("RGB")
              original_img.save(fpath)
              #shutil.copy(file_path, fpath)
              add_file(fpath)
        else:
          if bool(image_path.value):
            alert_msg(page, "Couldn't find a valid File, Path or URL...")
          else:
            pick_path(e)
          return
        image_path.value = ""
        image_path.update()
    def load_images():
        if os.path.exists(save_dir):
          for f in os.listdir(save_dir):
            existing = os.path.join(save_dir, f)
            if os.path.isdir(existing): continue
            if f.lower().endswith(('.png', '.jpg', '.jpeg')):
              add_file(existing, update=False)
    def toggle_save(e):
        changed(e, 'save_model')
        where_to_save_model.visible = LoRA_dreambooth_prefs['save_model']
        where_to_save_model.update()
        readme_description.visible = LoRA_dreambooth_prefs['save_model']
        readme_description.update()
    instance_prompt = Container(content=Tooltip(message="The prompt with identifier specifying the instance", content=TextField(label="Instance Prompt Token Text", value=LoRA_dreambooth_prefs['instance_prompt'], on_change=lambda e:changed(e,'instance_prompt'))), col={'md':9})
    name_of_your_model = TextField(label="Name of your Model", value=LoRA_dreambooth_prefs['name_of_your_model'], on_change=lambda e:changed(e,'name_of_your_model'), col={'md':3})
    class_prompt = TextField(label="Class Prompt", value=LoRA_dreambooth_prefs['class_prompt'], on_change=lambda e:changed(e,'class_prompt'))
    lr_scheduler = Dropdown(label="Learning Rate Scheduler", width=250, options=[dropdown.Option("constant"), dropdown.Option("constant_with_warmup"), dropdown.Option("linear"), dropdown.Option("cosine"), dropdown.Option("cosine_with_restarts"), dropdown.Option("polynomial")], value=LoRA_dreambooth_prefs['lr_scheduler'], on_change=lambda e: changed(e, 'lr_scheduler'))
    prior_preservation = Checkbox(label="Prior Preservation", tooltip="If you'd like class of the model (e.g.: toy, dog, painting) is guaranteed to be preserved. This increases the quality and helps with generalization at the cost of training time", value=LoRA_dreambooth_prefs['prior_preservation'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'prior_preservation'))
    gradient_checkpointing = Checkbox(label="Gradient Checkpointing   ", tooltip="Whether or not to use gradient checkpointing to save memory at the expense of slower backward pass.", value=LoRA_dreambooth_prefs['gradient_checkpointing'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'gradient_checkpointing'))
    num_class_images = Tooltip(message="Minimal class images for prior preservation loss. If there are not enough images already present in class_data_dir, additional images will be sampled with class_prompt.", content=TextField(label="Number of Class Images", value=LoRA_dreambooth_prefs['num_class_images'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'num_class_images', ptype='int'), width = 160))
    sample_batch_size = Tooltip(message="Batch size (per device) for sampling images.", content=TextField(label="Sample Batch Size", value=LoRA_dreambooth_prefs['sample_batch_size'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'sample_batch_size', ptype='int'), width = 160))
    train_batch_size = Tooltip(message="Batch size (per device) for the training dataloader.", content=TextField(label="Train Batch Size", value=LoRA_dreambooth_prefs['train_batch_size'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'train_batch_size', ptype='int'), width = 160))
    prior_loss_weight = Tooltip(message="The weight of prior preservation loss.", content=TextField(label="Prior Loss Weight", value=LoRA_dreambooth_prefs['prior_loss_weight'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'prior_loss_weight', ptype='float'), width = 160))
    max_train_steps = Tooltip(message="Total number of training steps to perform.  If provided, overrides num_train_epochs.", content=TextField(label="Max Training Steps", value=LoRA_dreambooth_prefs['max_train_steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'max_train_steps', ptype='int'), width = 160))
    gradient_accumulation_steps = Tooltip(message="Number of updates steps to accumulate before performing a backward/update pass.", content=TextField(label="Gradient Accumulation Steps", value=LoRA_dreambooth_prefs['gradient_accumulation_steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'gradient_accumulation_steps', ptype='int'), width = 160))
    learning_rate = Tooltip(message="Initial learning rate (after the potential warmup period) to use.", content=TextField(label="Learning Rate", value=LoRA_dreambooth_prefs['learning_rate'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'learning_rate', ptype='float'), width = 160))
    lr_warmup_steps = Tooltip(message="Number of steps for the warmup in the lr scheduler.", content=TextField(label="LR Warmup Steps", value=LoRA_dreambooth_prefs['lr_warmup_steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'lr_warmup_steps', ptype='int'), width = 160))
    lr_num_cycles = Tooltip(message="Number of hard resets of the lr in cosine_with_restarts scheduler.", content=TextField(label="LR Number of Cycles", value=LoRA_dreambooth_prefs['lr_num_cycles'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'lr_num_cycles', ptype='int'), width = 160))
    lr_power = Tooltip(message="Power factor of the polynomial scheduler.", content=TextField(label="LR Power", value=LoRA_dreambooth_prefs['lr_power'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'lr_power', ptype='int'), width = 160))
    seed = Tooltip(message="0 or -1 for Random. Pick any number.", content=TextField(label="Seed", value=LoRA_dreambooth_prefs['seed'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'seed', ptype='int'), width = 160))
    #save_model = Checkbox(label="Save Model to HuggingFace   ", tooltip="", value=LoRA_dreambooth_prefs['save_model'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'save_model'))
    save_model = Switcher(label="Save Model to HuggingFace", value=LoRA_dreambooth_prefs['save_model'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_save, tooltip="Requires WRITE access on API Key to Upload Checkpoint")
    where_to_save_model = Dropdown(label="Where to Save Model", width=250, options=[dropdown.Option("Public HuggingFace"), dropdown.Option("Private HuggingFace")], value=LoRA_dreambooth_prefs['where_to_save_model'], on_change=lambda e: changed(e, 'where_to_save_model'))
    #class_data_dir = TextField(label="Prior Preservation Class Folder", value=LoRA_dreambooth_prefs['class_data_dir'], on_change=lambda e:changed(e,'class_data_dir'))
    readme_description = TextField(label="Extra README Description", value=LoRA_dreambooth_prefs['readme_description'], on_change=lambda e:changed(e,'readme_description'))
    max_row = SliderRow(label="Max Resolution Size", min=256, max=1024, divisions=6, multiple=64, suffix="px", pref=LoRA_dreambooth_prefs, key='resolution')
    image_path = TextField(label="Image File or Folder Path or URL to Train", value=LoRA_dreambooth_prefs['image_path'], on_change=lambda e:changed(e,'image_path'), suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_path), expand=1)
    add_image_button = ElevatedButton(content=Text("Add File or Folder"), on_click=add_image)
    page.lora_dreambooth_file_list = Column([], tight=True, spacing=0)
    load_images()
    where_to_save_model.visible = LoRA_dreambooth_prefs['save_model']
    readme_description.visible = LoRA_dreambooth_prefs['save_model']
    #lambda_entropy = TextField(label="Lambda Entropy", value=dreamfusLoRA_dreambooth_prefsion_prefs['lambda_entropy'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'lambda_entropy', ptype='float'), width = 160)
    #max_steps = TextField(label="Max Steps", value=LoRA_dreambooth_prefs['max_steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'max_steps', ptype='int'), width = 160)
    page.LoRA_dreambooth_output = Column([])
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.LoRA_dreambooth_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🌇  Training with Low-Rank Adaptation of Large Language Models (LoRA DreamBooth)", "Provide a collection of images to train. Adds on to the currently loaded Model Checkpoint...", actions=[IconButton(icon=icons.HELP, tooltip="Help with LoRA DreamBooth Settings", on_click=lora_dreambooth_help)]),
        ResponsiveRow([instance_prompt, name_of_your_model]),
        Row([num_class_images, sample_batch_size, train_batch_size, prior_loss_weight]),
        Row([prior_preservation, gradient_checkpointing, lr_scheduler]),
        Row([learning_rate, lr_warmup_steps, lr_num_cycles, lr_power]),
        Row([max_train_steps, gradient_accumulation_steps, seed]),
        Row([save_model, where_to_save_model]),
        readme_description,
        #Row([class_data_dir]),
        max_row,
        Row([image_path, add_image_button]),
        page.lora_dreambooth_file_list,
        Row([ElevatedButton(content=Text("🌄  Run LoRA DreamBooth", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_LoRA_dreambooth(page))]),
        page.LoRA_dreambooth_output,
        clear_button,
      ]
    ))], scroll=ScrollMode.AUTO)
    return c


def buildLoRA(page):
    global prefs, LoRA_prefs
    def changed(e, pref=None, ptype="str"):
        if pref is not None:
          try:
            if ptype == "int":
              LoRA_prefs[pref] = int(e.control.value)
            elif ptype == "float":
              LoRA_prefs[pref] = float(e.control.value)
            else:
              LoRA_prefs[pref] = e.control.value
          except Exception:
            alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
            pass
    def add_to_LoRA_output(o):
        page.LoRA_output.controls.append(o)
        page.LoRA_output.update()
    def clear_output(e):
        if prefs['enable_sounds']: page.snd_delete.play()
        page.LoRA_output.controls = []
        page.LoRA_output.update()
        clear_button.visible = False
        clear_button.update()
    def lora_help(e):
        def close_lora_dlg(e):
          nonlocal lora_help_dlg
          lora_help_dlg.open = False
          page.update()
        lora_help_dlg = AlertDialog(title=Text("💁   Help with LoRA DreamBooth"), content=Column([
            Text("First thing is to collect all your own images that you want to teach it to dream.  Feed it at least 5 square pictures of the object or style to learn, and it'll save your Custom Model Checkpoint."),
            Markdown("""Low-Rank Adaption of Large Language Models was first introduced by Microsoft in [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685) by *Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen*
In a nutshell, LoRA allows to adapt pretrained models by adding pairs of rank-decomposition matrices to existing weights and **only** training those newly added weights. This has a couple of advantages:
- Previous pretrained weights are kept frozen so that the model is not prone to [catastrophic forgetting](https://www.pnas.org/doi/10.1073/pnas.1611835114)
- Rank-decomposition matrices have significantly fewer parameters than the original model, which means that trained LoRA weights are easily portable.
- LoRA attention layers allow to control to which extent the model is adapted torwards new training images via a `scale` parameter.""", on_tap_link=lambda e: e.page.launch_url(e.data)),
            Text("Fine-tune your perameters, but be aware that the training process takes a long time to run, so careful with the settings if you don't have the patience or processor. Dream at your own risk."),
          ], scroll=ScrollMode.AUTO), actions=[TextButton(emojize(':sun_with_face:') + "  Neato... ", on_click=close_lora_dlg)], actions_alignment=MainAxisAlignment.END)
        page.dialog = lora_help_dlg
        lora_help_dlg.open = True
        page.update()
    def delete_image(e):
        f = e.control.data
        if os.path.isfile(f):
          os.remove(f)
          for i, fl in enumerate(page.lora_file_list.controls):
            if fl.title.value == f:
              del page.lora_file_list.controls[i]
              page.lora_file_list.update()
              continue
    def delete_all_images(e):
        for fl in page.lora_file_list.controls:
          f = fl.title.value
          if os.path.isfile(f):
            os.remove(f)
        page.lora_file_list.controls.clear()
        page.lora_file_list.update()
    def image_details(e):
        img = e.control.data
        #TODO: Get file size & resolution
        alert_msg(e.page, "Image Details", content=Column([Text(img), Img(src=img, gapless_playback=True)]), sound=False)
    def add_file(fpath, update=True):
        page.lora_file_list.controls.append(ListTile(title=Text(fpath), dense=True, trailing=PopupMenuButton(icon=icons.MORE_VERT,
          items=[#TODO: View Image
              PopupMenuItem(icon=icons.INFO, text="Image Details", on_click=image_details, data=fpath),
              PopupMenuItem(icon=icons.DELETE, text="Delete Image", on_click=delete_image, data=fpath),
              PopupMenuItem(icon=icons.DELETE_SWEEP, text="Delete All", on_click=delete_all_images, data=fpath),
          ]), subtitle=TextField(label="Caption Image Description", height=55, filled=True, content_padding=padding.only(top=12, left=12)), data=fpath, on_click=image_details))
        if update: page.lora_file_list.update()
    def file_picker_result(e: FilePickerResultEvent):
        if e.files != None:
          upload_files(e)
    save_dir = os.path.join(root_dir, 'my_model')
    def on_upload_progress(e: FilePickerUploadEvent):
        if e.progress == 1:
          if not os.path.exists(save_dir):
            os.mkdir(save_dir)
          if not slash in e.file_name:
            fname = os.path.join(root_dir, e.file_name)
            fpath = os.path.join(save_dir, e.file_name)
          else:
            fname = e.file_name
            fpath = os.path.join(save_dir, e.file_name.rpartition(slash)[2])
          original_img = PILImage.open(fname)
          width, height = original_img.size
          width, height = scale_dimensions(width, height, LoRA_prefs['resolution'])
          original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert("RGB")
          original_img.save(fpath)
          if page.web: os.remove(fname)
          #shutil.move(fname, fpath)
          add_file(fpath)
    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)
    def pick_path(e):
        file_picker.pick_files(allow_multiple=True, allowed_extensions=["png", "PNG", "jpg", "jpeg"], dialog_title="Pick Image File to Enlarge")
    def upload_files(e):
        uf = []
        if file_picker.result != None and file_picker.result.files != None:
            for f in file_picker.result.files:
              if page.web:
                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))
              else:
                on_upload_progress(FilePickerUploadEvent(f.path, 1, ""))
            file_picker.upload(uf)
    page.overlay.append(file_picker)
    def add_image(e):
        save_dir = os.path.join(root_dir, 'my_model')
        if not os.path.exists(save_dir):
          os.mkdir(save_dir)
        if image_path.value.startswith('http'):
          import requests
          from io import BytesIO
          response = requests.get(image_path.value)
          fpath = os.path.join(save_dir, image_path.value.rpartition(slash)[2])
          model_image = PILImage.open(BytesIO(response.content)).convert("RGB")
          width, height = model_image.size
          width, height = scale_dimensions(width, height, LoRA_prefs['resolution'])
          model_image = model_image.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert("RGB")
          model_image.save(fpath)
          add_file(fpath)
        elif os.path.isfile(image_path.value):
          fpath = os.path.join(save_dir, image_path.value.rpartition(slash)[2])
          original_img = PILImage.open(image_path.value)
          width, height = original_img.size
          width, height = scale_dimensions(width, height, LoRA_prefs['resolution'])
          original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert("RGB")
          original_img.save(fpath)
          #shutil.copy(image_path.value, fpath)
          add_file(fpath)
        elif os.path.isdir(image_path.value):
          for f in os.listdir(image_path.value):
            file_path = os.path.join(image_path.value, f)
            if os.path.isdir(file_path): continue
            if f.lower().endswith(('.png', '.jpg', '.jpeg')):
              fpath = os.path.join(save_dir, f)
              original_img = PILImage.open(file_path)
              width, height = original_img.size
              width, height = scale_dimensions(width, height, LoRA_prefs['resolution'])
              original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert("RGB")
              original_img.save(fpath)
              #shutil.copy(file_path, fpath)
              add_file(fpath)
        else:
          if bool(image_path.value):
            alert_msg(page, "Couldn't find a valid File, Path or URL...")
          else:
            pick_path(e)
          return
        image_path.value = ""
        image_path.update()
    def load_images():
        if os.path.exists(save_dir):
          for f in os.listdir(save_dir):
            existing = os.path.join(save_dir, f)
            if os.path.isdir(existing): continue
            if f.lower().endswith(('.png', '.jpg', '.jpeg')):
              add_file(existing, update=False)
    def toggle_save(e):
        changed(e, 'save_model')
        where_to_save_model.visible = LoRA_prefs['save_model']
        where_to_save_model.update()
        readme_description.visible = LoRA_prefs['save_model']
        readme_description.update()
    validation_prompt = Container(content=Tooltip(message="A prompt that is sampled during training for inference.", content=TextField(label="Validation Prompt Text", value=LoRA_prefs['validation_prompt'], on_change=lambda e:changed(e,'validation_prompt'))), col={'md':9})
    name_of_your_model = TextField(label="Name of your Model", value=LoRA_prefs['name_of_your_model'], on_change=lambda e:changed(e,'name_of_your_model'), col={'md':3})
    #class_prompt = TextField(label="Class Prompt", value=LoRA_prefs['class_prompt'], on_change=lambda e:changed(e,'class_prompt'))
    #'num_validation_images': 4, #Number of images that should be generated during validation with `validation_prompt`.
    #'validation_epochs': 1, #Run fine-tuning validation every X epochs. The validation process consists of running the prompt
    num_validation_images = Tooltip(message="Number of images that should be generated during validation with `validation_prompt`", content=TextField(label="# of Validation Images", value=LoRA_prefs['num_validation_images'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'num_validation_images', ptype='int'), width = 160))
    validation_epochs = Tooltip(message="Run fine-tuning validation every X epochs. The validation process consists of running the prompt", content=TextField(label="Validation Epochs", value=LoRA_prefs['validation_epochs'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'validation_epochs', ptype='int'), width = 160))
    lr_scheduler = Dropdown(label="Learning Rate Scheduler", width=250, options=[dropdown.Option("constant"), dropdown.Option("constant_with_warmup"), dropdown.Option("linear"), dropdown.Option("cosine"), dropdown.Option("cosine_with_restarts"), dropdown.Option("polynomial")], value=LoRA_prefs['lr_scheduler'], on_change=lambda e: changed(e, 'lr_scheduler'))
    prior_preservation = Checkbox(label="Prior Preservation", tooltip="If you'd like class of the model (e.g.: toy, dog, painting) is guaranteed to be preserved. This increases the quality and helps with generalization at the cost of training time", value=LoRA_prefs['prior_preservation'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'prior_preservation'))
    gradient_checkpointing = Checkbox(label="Gradient Checkpointing   ", tooltip="Whether or not to use gradient checkpointing to save memory at the expense of slower backward pass.", value=LoRA_prefs['gradient_checkpointing'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'gradient_checkpointing'))
    #num_class_images = Tooltip(message="Minimal class images for prior preservation loss. If there are not enough images already present in class_data_dir, additional images will be sampled with class_prompt.", content=TextField(label="Number of Class Images", value=LoRA_prefs['num_class_images'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'num_class_images', ptype='int'), width = 160))
    #sample_batch_size = Tooltip(message="Batch size (per device) for sampling images.", content=TextField(label="Sample Batch Size", value=LoRA_prefs['sample_batch_size'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'sample_batch_size', ptype='int'), width = 160))
    train_batch_size = Tooltip(message="Batch size (per device) for the training dataloader.", content=TextField(label="Train Batch Size", value=LoRA_prefs['train_batch_size'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'train_batch_size', ptype='int'), width = 160))
    #prior_loss_weight = Tooltip(message="The weight of prior preservation loss.", content=TextField(label="Prior Loss Weight", value=LoRA_prefs['prior_loss_weight'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'prior_loss_weight', ptype='float'), width = 160))
    max_train_steps = Tooltip(message="Total number of training steps to perform.  If provided, overrides num_train_epochs.", content=TextField(label="Max Training Steps", value=LoRA_prefs['max_train_steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'max_train_steps', ptype='int'), width = 160))
    gradient_accumulation_steps = Tooltip(message="Number of updates steps to accumulate before performing a backward/update pass.", content=TextField(label="Gradient Accumulation Steps", value=LoRA_prefs['gradient_accumulation_steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'gradient_accumulation_steps', ptype='int'), width = 160))
    learning_rate = Tooltip(message="Initial learning rate (after the potential warmup period) to use.", content=TextField(label="Learning Rate", value=LoRA_prefs['learning_rate'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'learning_rate', ptype='float'), width = 160))
    lr_warmup_steps = Tooltip(message="Number of steps for the warmup in the lr scheduler.", content=TextField(label="LR Warmup Steps", value=LoRA_prefs['lr_warmup_steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'lr_warmup_steps', ptype='int'), width = 160))
    #lr_num_cycles = Tooltip(message="Number of hard resets of the lr in cosine_with_restarts scheduler.", content=TextField(label="LR Number of Cycles", value=LoRA_prefs['lr_num_cycles'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'lr_num_cycles', ptype='int'), width = 160))
    #lr_power = Tooltip(message="Power factor of the polynomial scheduler.", content=TextField(label="LR Power", value=LoRA_prefs['lr_power'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'lr_power', ptype='int'), width = 160))
    seed = Tooltip(message="0 or -1 for Random. Pick any number.", content=TextField(label="Seed", value=LoRA_prefs['seed'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'seed', ptype='int'), width = 160))
    save_model = Checkbox(label="Save Model to HuggingFace   ", tooltip="", value=LoRA_prefs['save_model'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'save_model'))
    save_model = Tooltip(message="Requires WRITE access on API Key to Upload Checkpoint", content=Switcher(label="Save Model to HuggingFace    ", value=LoRA_prefs['save_model'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_save))
    where_to_save_model = Dropdown(label="Where to Save Model", width=250, options=[dropdown.Option("Public HuggingFace"), dropdown.Option("Private HuggingFace")], value=LoRA_prefs['where_to_save_model'], on_change=lambda e: changed(e, 'where_to_save_model'))
    #class_data_dir = TextField(label="Prior Preservation Class Folder", value=LoRA_prefs['class_data_dir'], on_change=lambda e:changed(e,'class_data_dir'))
    readme_description = TextField(label="Extra README Description", value=LoRA_prefs['readme_description'], on_change=lambda e:changed(e,'readme_description'))
    max_row = SliderRow(label="Max Resolution Size", min=256, max=1024, divisions=6, multiple=64, suffix="px", pref=LoRA_prefs, key='resolution')
    image_path = TextField(label="Image File or Folder Path or URL to Train", value=LoRA_prefs['image_path'], on_change=lambda e:changed(e,'image_path'), suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_path), expand=1)
    add_image_button = ElevatedButton(content=Text("Add File or Folder"), on_click=add_image)
    page.lora_file_list = Column([], tight=True, spacing=0)
    load_images()
    where_to_save_model.visible = LoRA_prefs['save_model']
    readme_description.visible = LoRA_prefs['save_model']
    #lambda_entropy = TextField(label="Lambda Entropy", value=dreamfusLoRA_prefsion_prefs['lambda_entropy'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'lambda_entropy', ptype='float'), width = 160)
    #max_steps = TextField(label="Max Steps", value=LoRA_prefs['max_steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'max_steps', ptype='int'), width = 160)
    page.LoRA_output = Column([])
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.LoRA_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🌫️  Training text-to-image Low-Rank Adaptation of Large Language Models (LoRA)", "Provide a collection of images to train. Smaller sized. Adds on to the currently loaded Model Checkpoint...", actions=[IconButton(icon=icons.HELP, tooltip="Help with LoRA DreamBooth Settings", on_click=lora_help)]),
        ResponsiveRow([validation_prompt, name_of_your_model]),
        Row([num_validation_images, validation_epochs, train_batch_size]),
        Row([prior_preservation, gradient_checkpointing]),
        Row([learning_rate, lr_warmup_steps, lr_scheduler]),
        Row([max_train_steps, gradient_accumulation_steps, seed]),
        Row([save_model, where_to_save_model]),
        readme_description,
        #Row([class_data_dir]),
        max_row,
        Row([image_path, add_image_button]),
        page.lora_file_list,
        Row([ElevatedButton(content=Text("🏄  Run LoRA Training", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_LoRA(page))]),
        page.LoRA_output,
        clear_button,
      ]
    ))], scroll=ScrollMode.AUTO)
    return c

converter_prefs = {
    'from_format': 'ckpt',
    'to_format': 'pytorch',
    'model_path': '',
    'model_name': '',
    'base_model': '',
    'model_type': 'SD v1.x text2image',
    'scheduler_type': 'pndm',
    'half_percision': True,
    'save_model': False,
    'where_to_save_model': "Public HuggingFace",
    'readme_description': '',
    'load_custom_model': True,
}

def buildConverter(page):
    global prefs, converter_prefs
    def changed(e, pref=None, ptype="str"):
        if pref is not None:
          try:
            if ptype == "int":
              converter_prefs[pref] = int(e.control.value)
            elif ptype == "float":
              converter_prefs[pref] = float(e.control.value)
            else:
              converter_prefs[pref] = e.control.value
          except Exception:
            alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
            pass
    def add_to_converter_output(o):
        page.converter_output.controls.append(o)
        page.converter_output.update()
    def clear_output(e):
        if prefs['enable_sounds']: page.snd_delete.play()
        page.converter_output.controls = []
        page.converter_output.update()
        clear_button.visible = False
        clear_button.update()
    def converter_help(e):
        def close_converter_dlg(e):
          nonlocal converter_help_dlg
          converter_help_dlg.open = False
          page.update()
        converter_help_dlg = AlertDialog(title=Text("💁   Help with Converters"), content=Column([
            Text("Because there have been so many competing formats for Stable Diffusion models, we here have standardized with HuggingFace Diffusers, which is great but doesn't support all the Checkpoint Model types that are out there in the wild.  This should allow you to take other peoples custom trained model files and convert it to the better Diffusers PyTorch format, and then it'll save your Custom Model Checkpoint to HuggingFace (free) to reuse and/or share."),
          ], scroll=ScrollMode.AUTO), actions=[TextButton("🎈  Handy... ", on_click=close_converter_dlg)], actions_alignment=MainAxisAlignment.END)
        page.dialog = converter_help_dlg
        converter_help_dlg.open = True
        page.update()
    def toggle_save(e):
        changed(e, 'save_model')
        where_to_save_model.visible = converter_prefs['save_model']
        where_to_save_model.update()
        readme_description.visible = converter_prefs['save_model']
        readme_description.update()
    def change_from_format(e):
        changed(e, 'from_format')
        base_model_row.visible = converter_prefs['from_format'] == "lora_safetensors"
        base_model_row.update()

    from_format = Dropdown(label="From Format", width=250, options=[dropdown.Option("ckpt"), dropdown.Option("safetensors"), dropdown.Option("lora_safetensors"), dropdown.Option("controlnet"), dropdown.Option("KerasCV")], value=converter_prefs['from_format'], on_change=change_from_format, col={'lg':6})
    to_format = Dropdown(label="To Format", width=250, options=[dropdown.Option("pytorch"), dropdown.Option("safetensors"), dropdown.Option("dance_diffusion")], value=converter_prefs['to_format'], on_change=lambda e: changed(e, 'to_format'), col={'lg':6})
    #instance_prompt = Container(content=Tooltip(message="The prompt with identifier specifying the instance", content=TextField(label="Instance Prompt Token Text", value=converter_prefs['instance_prompt'], on_change=lambda e:changed(e,'instance_prompt'))), col={'md':9})
    from_model_path = TextField(label="Model Path to HuggingFace or .ckpt or .safetensors file", value=converter_prefs['model_path'], on_change=lambda e:changed(e,'model_path'), col={'md':6})
    from_model_name = TextField(label="Name of your Model", value=converter_prefs['model_name'], on_change=lambda e:changed(e,'model_name'), col={'md':6})
    model_type = Dropdown(label="Model Type", width=250, options=[dropdown.Option("SD v1.x text2image"), dropdown.Option("SD v2.x text2image")], value=converter_prefs['model_type'], on_change=lambda e: changed(e, 'model_type'), col={'lg':6})
    base_model = TextField(label="Base Model Path to HuggingFace Diffusers", value=converter_prefs['base_model'], on_change=lambda e:changed(e,'base_model'), col={'md':6})
    base_model_row = ResponsiveRow([base_model])
    base_model_row.visible = converter_prefs['from_format'] == "lora_safetensors"
    #sd_version = Dropdown(label="Stable Diffusion Version", width=250, options=[dropdown.Option("text2image")], value=converter_prefs['model_type'], on_change=lambda e: changed(e, 'model_type'), col={'lg':6})
    #class_prompt = TextField(label="Class Prompt", value=converter_prefs['class_prompt'], on_change=lambda e:changed(e,'class_prompt'))
    scheduler_type = Dropdown(label="Original Scheduler Mode", hint_text="Hopefuly you know what Scheduler/Sampler they used in training", width=200,
            options=[
                dropdown.Option("pndm"),
                dropdown.Option("lms"),
                dropdown.Option("ddim"),
                dropdown.Option("euler"),
                dropdown.Option("euler-ancestral"),
                dropdown.Option("dpm"),
            ], value=converter_prefs['scheduler_type'], autofocus=False, on_change=lambda e:changed(e, 'scheduler_type'), col={'lg':6},
        )
    #save_model = Checkbox(label="Save Model to HuggingFace   ", tooltip="", value=converter_prefs['save_model'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'save_model'))
    save_model = Tooltip(message="Requires WRITE access on API Key to Upload Checkpoint", content=Switcher(label="Save Model to HuggingFace    ", value=converter_prefs['save_model'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_save))
    half_percision = Tooltip(message="Save weights in half precision.", content=Switcher(label="Save Half Percision Float16    ", value=converter_prefs['half_percision'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e: changed(e, 'half_percision')))
    where_to_save_model = Dropdown(label="Where to Save Model", width=250, options=[dropdown.Option("Public HuggingFace"), dropdown.Option("Private HuggingFace")], value=converter_prefs['where_to_save_model'], on_change=lambda e: changed(e, 'where_to_save_model'))
    #class_data_dir = TextField(label="Prior Preservation Class Folder", value=converter_prefs['class_data_dir'], on_change=lambda e:changed(e,'class_data_dir'))
    readme_description = TextField(label="Extra README Description", value=converter_prefs['readme_description'], on_change=lambda e:changed(e,'readme_description'))
    load_custom_model = Checkbox(label="Load Custom Model", tooltip="After conversion is done, will put it in your Custom Model setting, ready to test out", value=converter_prefs['load_custom_model'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'load_custom_model'))
    #resolution = Slider(min=256, max=1024, divisions=6, label="{value}px", value=float(converter_prefs['resolution']), expand=True, on_change=lambda e:changed(e,'resolution', ptype='int'))
    #max_row = Row([Text("Max Resolution Size: "), resolution])
    #image_path = TextField(label="Image File or Folder Path or URL to Train", value=converter_prefs['image_path'], on_change=lambda e:changed(e,'image_path'), suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_path), expand=1)
    #add_image_button = ElevatedButton(content=Text("Add File or Folder"), on_click=add_image)
    #page.converter_file_list = Column([], tight=True, spacing=0)
    #load_images()
    where_to_save_model.visible = converter_prefs['save_model']
    readme_description.visible = converter_prefs['save_model']
    #lambda_entropy = TextField(label="Lambda Entropy", value=dreamfusconverter_prefsion_prefs['lambda_entropy'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'lambda_entropy', ptype='float'), width = 160)
    #max_steps = TextField(label="Max Steps", value=converter_prefs['max_steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'max_steps', ptype='int'), width = 160)
    page.converter_output = Column([])
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.converter_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🔀  Model Converter Tool", "Lets you Convert Format of Model Checkpoints to work with Diffusers...", actions=[IconButton(icon=icons.HELP, tooltip="Help with Model Converters Settings", on_click=converter_help)]),
        ResponsiveRow([from_format, to_format]),
        ResponsiveRow([from_model_path, from_model_name]),
        base_model_row,
        ResponsiveRow([model_type, scheduler_type]),
        half_percision,
        Row([save_model, where_to_save_model]),
        readme_description,
        load_custom_model,
        #max_row,
        #Row([image_path, add_image_button]),
        #page.converter_file_list,
        Row([ElevatedButton(content=Text("〽️  Run Model Converter", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_converter(page))]),
        page.converter_output,
        clear_button,
      ]
    ))], scroll=ScrollMode.AUTO)
    return c

checkpoint_merger_prefs = {
    'pretrained_model': '',
    'selected_model': 'Stable Diffusion v1.5',
    'pretrained_models': [], #A list of valid pretrained model names in the HuggingFace hub or paths to locally stored models in the HuggingFace format.
    'alpha': 0.5, #The interpolation parameter. Ranges from 0 to 1.  It affects the ratio in which the checkpoints are merged. A 0.8 alpha would mean that the first model checkpoints would affect the final result far less than an alpha of 0.2
    'interp': 'weighted_sum', #The interpolation method to use for the merging. Supports "sigmoid", "inv_sigmoid", "add_difference" and None. Passing None uses the default interpolation which is weighted sum interpolation. For merging three checkpoints, only "add_difference" is supported.
    'force': False, #Whether to ignore mismatch in model_config.json for the current models. Defaults to False.
    'validation_prompt': '',
    'name_of_your_model': '',
    'save_model': True,
    'where_to_save_model': 'Public HuggingFace',
    'readme_description': '',
}

def buildCheckpointMerger(page):
    global prefs, checkpoint_merger_prefs
    def changed(e, pref=None, ptype="str"):
        if pref is not None:
          try:
            if ptype == "int":
              checkpoint_merger_prefs[pref] = int(e.control.value)
            elif ptype == "float":
              checkpoint_merger_prefs[pref] = float(e.control.value)
            else:
              checkpoint_merger_prefs[pref] = e.control.value
          except Exception:
            alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
            pass
    def checkpoint_merger_help(e):
        def close_checkpoint_merger_dlg(e):
          nonlocal checkpoint_merger_help_dlg
          checkpoint_merger_help_dlg.open = False
          page.update()
        checkpoint_merger_help_dlg = AlertDialog(title=Text("💁   Help with Checkpoint Merger"), content=Column([
            Text("Provide a list of valid pretrained model names in the HuggingFace hub or paths to locally stored models in the HuggingFace format.  Merges the Checkpoint Weights into a new model that you can save for free to HuggingFace to reuse."),
          ], scroll=ScrollMode.AUTO), actions=[TextButton("🍻  Sure thing... ", on_click=close_checkpoint_merger_dlg)], actions_alignment=MainAxisAlignment.END)
        page.dialog = checkpoint_merger_help_dlg
        checkpoint_merger_help_dlg.open = True
        page.update()
    def toggle_save(e):
        changed(e, 'save_model')
        where_to_save_model.visible = checkpoint_merger_prefs['save_model']
        where_to_save_model.update()
        readme_description.visible = checkpoint_merger_prefs['save_model']
        readme_description.update()
    def remove_model(e):
        f = e.control.data
        for i, fl in enumerate(page.checkpoint_merger_file_list.controls):
            if fl.title.value == f:
                del page.checkpoint_merger_file_list.controls[i]
                page.checkpoint_merger_file_list.update()
                continue
    def remove_all_models(e):
        checkpoint_merger_prefs['pretrained_models'].clear()
        page.checkpoint_merger_file_list.controls.clear()
        page.checkpoint_merger_file_list.update()
    def move_down(e):
        idx = checkpoint_merger_prefs['pretrained_models'].index(e.control.data)
        if idx < (len(checkpoint_merger_prefs['pretrained_models']) - 1):
          d = checkpoint_merger_prefs['pretrained_models'].pop(idx)
          checkpoint_merger_prefs['pretrained_models'].insert(idx+1, d)
          dr = page.checkpoint_merger_file_list.controls.pop(idx)
          page.checkpoint_merger_file_list.controls.insert(idx+1, dr)
          page.checkpoint_merger_file_list.update()
    def move_up(e):
        idx = checkpoint_merger_prefs['pretrained_models'].index(e.control.data)
        if idx > 0:
          d = checkpoint_merger_prefs['pretrained_models'].pop(idx)
          checkpoint_merger_prefs['pretrained_models'].insert(idx-1, d)
          dr = page.checkpoint_merger_file_list.controls.pop(idx)
          page.checkpoint_merger_file_list.controls.insert(idx-1, dr)
          page.checkpoint_merger_file_list.update()
    def add_selected_model(e):
        name = checkpoint_merger_prefs['selected_model']
        m = {'name':''}
        if name == "Stable Diffusion v2.1 x768":
            m = {'name':'Stable Diffusion v2.1 x768', 'path':'stabilityai/stable-diffusion-2-1'}
        elif name == "Stable Diffusion v2.1 x512":
            m = {'name':'Stable Diffusion v2.1 x512', 'path':'stabilityai/stable-diffusion-2-1-base'}
        elif name == "Stable Diffusion v2.0":
            m = {'name':'Stable Diffusion v2.0', 'path':'stabilityai/stable-diffusion-2'}
        elif name == "Stable Diffusion v2.0 x768":
            m = {'name':'Stable Diffusion v2.0 x768', 'path':'stabilityai/stable-diffusion-2'}
        elif name == "Stable Diffusion v2.0 x512":
            m = {'name':'Stable Diffusion v2.0 x512', 'path':'stabilityai/stable-diffusion-2-base'}
        elif name == "Stable Diffusion v1.5":
            m = {'name':'Stable Diffusion v1.5', 'path':'runwayml/stable-diffusion-v1-5'}
        elif name == "Stable Diffusion v1.4":
            m = {'name':'Stable Diffusion v1.4', 'path':'CompVis/stable-diffusion-v1-4'}
        else:
            m = get_finetuned_model(name)
            if not bool(m['name']):
                m = get_dreambooth_model(name)
        if bool(m['path']):
            add_model(m['path'])
    def add_custom_model(e):
        mpath = checkpoint_merger_prefs['pretrained_model']
        add_model(mpath)
    def add_model(mpath, update=True):
        if mpath in checkpoint_merger_prefs['pretrained_models']:
            alert_msg(page, "That model path is already in your list...")
            return
        page.checkpoint_merger_file_list.controls.append(ListTile(title=Text(mpath), dense=True, trailing=PopupMenuButton(icon=icons.MORE_VERT,
          items=[#TODO: View Image
              PopupMenuItem(icon=icons.DELETE, text="Remove Model", on_click=remove_model, data=mpath),
              PopupMenuItem(icon=icons.DELETE_SWEEP, text="Remove All", on_click=remove_all_models, data=mpath),
              PopupMenuItem(icon=icons.ARROW_UPWARD, text="Move Up", on_click=move_up, data=mpath),
              PopupMenuItem(icon=icons.ARROW_DOWNWARD, text="Move Down", on_click=move_down, data=mpath),
          ]), data=mpath))
        checkpoint_merger_prefs['pretrained_models'].append(mpath)
        if update: page.checkpoint_merger_file_list.update()
    validation_prompt = Container(content=Tooltip(message="Optional prompt to test after the merger is finished.", content=TextField(label="Validation Test Prompt", value=checkpoint_merger_prefs['validation_prompt'], on_change=lambda e:changed(e,'validation_prompt'))), col={'md':9})
    name_of_your_model = TextField(label="Name of New Model", value=checkpoint_merger_prefs['name_of_your_model'], on_change=lambda e:changed(e,'name_of_your_model'), col={'md':3})
    force = Checkbox(label="Force if Mismatch", tooltip="Whether to ignore mismatch in model_config.json for the current models.", value=checkpoint_merger_prefs['force'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'force'))
    alpha_row = SliderRow(label="Alpha Interpolation", min=0.0, max=1.0, divisions=20, round=2, pref=checkpoint_merger_prefs, key='alpha', tooltip="The interpolation parameter. Ranges from 0 to 1.  It affects the ratio in which the checkpoints are merged. A 0.8 alpha would mean that the first model checkpoints would affect the final result far less than an alpha of 0.2")
    interp = Dropdown(label="Interpolation Method", width=250, options=[dropdown.Option("weighted_sum"), dropdown.Option("sigmoid"), dropdown.Option("inv_sigmoid"), dropdown.Option("add_difference")], value=checkpoint_merger_prefs['interp'], on_change=lambda e: changed(e, 'interp'))
    #The interpolation method to use for the merging. Supports "sigmoid", "inv_sigmoid", "add_difference" and None. For merging three checkpoints, only "add_difference" is supported.
    model_ckpt = Dropdown(label="Model Checkpoint", options=[
        dropdown.Option("Stable Diffusion v2.1 x768"), dropdown.Option("Stable Diffusion v2.1 x512"),
        dropdown.Option("Stable Diffusion v2.0 x768"), dropdown.Option("Stable Diffusion v2.0 x512"), dropdown.Option("Stable Diffusion v1.5"), dropdown.Option("Stable Diffusion v1.4")], value=checkpoint_merger_prefs['selected_model'], on_change=lambda e: changed(e, 'selected_model'))
    for mod in finetuned_models:
        model_ckpt.options.append(dropdown.Option(mod["name"]))
    for db in dreambooth_models:
        model_ckpt.options.append(dropdown.Option(db["name"]))
    add_selected_model_button = ElevatedButton(content=Text("Add Selected Model"), on_click=add_selected_model)
    pretrained_model = TextField(label="HuggingFace Path or Local Path to Merge", value=checkpoint_merger_prefs['pretrained_model'], on_change=lambda e:changed(e,'pretrained_model'), expand=1)
    add_model_button = ElevatedButton(content=Text("Add Model Path"), on_click=add_custom_model)
    save_model = Tooltip(message="Requires WRITE access on API Key to Upload Checkpoint", content=Switcher(label="Save Model to HuggingFace    ", value=checkpoint_merger_prefs['save_model'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_save))
    where_to_save_model = Dropdown(label="Where to Save Model", width=250, options=[dropdown.Option("Public HuggingFace"), dropdown.Option("Private HuggingFace")], value=checkpoint_merger_prefs['where_to_save_model'], on_change=lambda e: changed(e, 'where_to_save_model'))
    readme_description = TextField(label="Extra README Description", value=checkpoint_merger_prefs['readme_description'], on_change=lambda e:changed(e,'readme_description'))

    page.checkpoint_merger_file_list = Column([], tight=True, spacing=0)
    page.checkpoint_merger_output = Column([])
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("👥  Checkpoint Merger Tool", "Combine together two or more custom models to create a mixture of weights...", actions=[IconButton(icon=icons.HELP, tooltip="Help with Checkpoint Merger Settings", on_click=checkpoint_merger_help)]),
        Row([model_ckpt, add_selected_model_button]),
        Row([pretrained_model, add_model_button]),
        page.checkpoint_merger_file_list,
        Divider(thickness=3, height=6),
        Row([interp, force]),
        alpha_row,
        ResponsiveRow([name_of_your_model, validation_prompt]),
        Row([save_model, where_to_save_model]),
        readme_description,
        Row([ElevatedButton(content=Text("🤗  Run Checkpoint Merger", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_checkpoint_merger(page))]),
        page.checkpoint_merger_output,
        #clear_button,
      ]
    ))], scroll=ScrollMode.AUTO)
    return c


tortoise_prefs = {
    'text': '',
    'preset': 'standard', #"ultra_fast", "fast", "standard", "high_quality"
    'voice': [],
    'voices': ['angie', 'applejack', 'daniel', 'deniro', 'emma', 'freeman', 'geralt', 'halle', 'jlaw', 'lj', 'mol', 'myself', 'pat', 'pat2', 'rainbow', 'snakes', 'tim_reynolds', 'tom', 'train_atkins', 'train_daws', 'train_dotrice', 'train_dreams', 'train_empire', 'train_grace', 'train_kennard', 'train_lescault', 'train_mouse', 'weaver', 'william'],
    'train_custom': False,
    'custom_voice_name': '',
    'custom_wavs': [],
    'wav_path': '',
    'batch_folder_name': '',
    'file_prefix': 'tts-',
}

def buildTortoiseTTS(page):
    global prefs, tortoise_prefs
    def changed(e, pref=None, ptype="str"):
        if pref is not None:
          try:
            if ptype == "int":
              tortoise_prefs[pref] = int(e.control.value)
            elif ptype == "float":
              tortoise_prefs[pref] = float(e.control.value)
            else:
              tortoise_prefs[pref] = e.control.value
          except Exception:
            alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
            pass
    def add_to_tortoise_output(o):
        page.tortoise_output.controls.append(o)
        page.tortoise_output.update()
    def clear_output(e):
        if prefs['enable_sounds']: page.snd_delete.play()
        page.tortoise_output.controls = []
        page.tortoise_output.update()
        clear_button.visible = False
        clear_button.update()
    def tortoise_help(e):
        def close_tortoise_dlg(e):
          nonlocal tortoise_help_dlg
          tortoise_help_dlg.open = False
          page.update()
        tortoise_help_dlg = AlertDialog(title=Text("💁   Help with Tortoise-TTS"), content=Column([
            Text("Tortoise was specifically trained to be a multi-speaker model. It accomplishes this by consulting reference clips. These reference clips are recordings of a speaker that you provide to guide speech generation. These clips are used to determine many properties of the output, such as the pitch and tone of the voice, speaking speed, and even speaking defects like a lisp or stuttering. The reference clip is also used to determine non-voice related aspects of the audio output like volume, background noise, recording quality and reverb."),
            Text("This comes with several pre-packaged voices. Voices prepended with 'train_' came from the training set and perform far better than the others. If your goal is high quality speech, we recommend you pick one of them. If you want to see what Tortoise can do for zero-shot mimicing, take a look at the others."),
            Text("To add new voices to Tortoise, you will need to do the following: Gather audio clips of your speaker(s). Good sources are YouTube interviews (you can use youtube-dl to fetch the audio), audiobooks or podcasts. Guidelines for good clips are in the next section. Cut your clips into ~10 second segments. You want at least 3 clips. More is better, but I only experimented with up to 5 in my testing. Save the clips as a WAV file with floating point format and a 22,050 sample rate."),
            Text("You can do prompt engineering with Tortoise to get the performance you want. For example, you can evoke emotion by including things like [I am really sad], before your text. It redacts the phrase in brackets, but keeps the context of the meaning to the voice reading. Experiment with grammar and spelling, and use an audio editor to perfect the vocals later."),
          ], scroll=ScrollMode.AUTO), actions=[TextButton("👄  What to say... ", on_click=close_tortoise_dlg)], actions_alignment=MainAxisAlignment.END)
        page.dialog = tortoise_help_dlg
        tortoise_help_dlg.open = True
        page.update()
    def delete_audio(e):
        f = e.control.data
        if os.path.isfile(f):
          os.remove(f)
          for i, fl in enumerate(page.tortoise_file_list.controls):
            if fl.title.value == f:
              del page.tortoise_file_list.controls[i]
              page.tortoise_file_list.update()
              if f in tortoise_prefs['custom_wavs']:
                tortoise_prefs['custom_wavs'].remove(f)
              continue
    def delete_all_audios(e):
        for fl in page.tortoise_file_list.controls:
          f = fl.title.value
          if os.path.isfile(f):
            os.remove(f)
        page.tortoise_file_list.controls.clear()
        page.tortoise_file_list.update()
        tortoise_prefs['custom_wavs'].clear()
    def add_file(fpath, update=True):
        page.tortoise_file_list.controls.append(ListTile(title=Text(fpath), dense=True, trailing=PopupMenuButton(icon=icons.MORE_VERT,
          items=[#TODO: View Image
              PopupMenuItem(icon=icons.DELETE, text="Delete Audio", on_click=delete_audio, data=fpath),
              PopupMenuItem(icon=icons.DELETE_SWEEP, text="Delete All", on_click=delete_all_audios, data=fpath),
          ])))
        tortoise_prefs['custom_wavs'].append(fpath)
        if update: page.tortoise_file_list.update()
    def file_picker_result(e: FilePickerResultEvent):
        if e.files != None:
          upload_files(e)
    save_dir = os.path.join(root_dir, 'tortoise-audio')
    def on_upload_progress(e: FilePickerUploadEvent):
        if e.progress == 1:
          if not os.path.exists(save_dir):
            os.mkdir(save_dir)
          if not slash in e.file_name:
            fname = os.path.join(root_dir, e.file_name)
            fpath = os.path.join(save_dir, e.file_name)
            shutil.move(fname, fpath)
          else:
            fname = e.file_name
            fpath = os.path.join(save_dir, e.file_name.rpartition(slash)[2])
            shutil.copy(fname, fpath)
          add_file(fpath)
    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)
    def pick_path(e):
        file_picker.pick_files(allow_multiple=True, allowed_extensions=["wav", "WAV", "mp3", "MP3"], dialog_title="Pick Voice WAV or MP3 Files to Train")
    def upload_files(e):
        uf = []
        if file_picker.result != None and file_picker.result.files != None:
            for f in file_picker.result.files:
              if page.web:
                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))
              else:
                on_upload_progress(FilePickerUploadEvent(f.path, 1, ""))
            file_picker.upload(uf)
    page.overlay.append(file_picker)
    def add_wav(e):
        if not os.path.exists(save_dir):
            os.mkdir(save_dir)
        if wav_path.value.startswith('http'):
            import requests
            from io import BytesIO
            #response = requests.get(wav_path.value)
            fpath = download_file(wav_path.value)
            #fpath = os.path.join(save_dir, wav_path.value.rpartition(slash)[2])
            add_file(fpath)
        elif os.path.isfile(wav_path.value):
          fpath = os.path.join(save_dir, wav_path.value.rpartition(slash)[2])
          shutil.copy(wav_path.value, fpath)
          add_file(fpath)
        elif os.path.isdir(wav_path.value):
          for f in os.listdir(wav_path.value):
            file_path = os.path.join(wav_path.value, f)
            if os.path.isdir(file_path): continue
            if f.lower().endswith(('.wav', '.WAV', '.mp3', '.MP3')):
              fpath = os.path.join(save_dir, f)
              shutil.copy(file_path, fpath)
              add_file(fpath)
        else:
          if bool(wav_path.value):
            alert_msg(page, "Couldn't find a valid File, Path or URL...")
          else:
            pick_path(e)
          return
        wav_path.value = ""
        wav_path.update()
    def load_wavs():
        if os.path.exists(save_dir):
          for f in os.listdir(save_dir):
            existing = os.path.join(save_dir, f)
            if os.path.isdir(existing): continue
            if f.lower().endswith(('.wav', '.WAV', '.mp3', '.MP3')):
              add_file(existing, update=False)
    def toggle_custom(e):
        changed(e, 'train_custom')
        custom_box.height = None if tortoise_prefs['train_custom'] else 0
        custom_box.update()
        custom_voice_name.visible = tortoise_prefs['train_custom']
        custom_voice_name.update()
    text = TextField(label="Text to Read", value=tortoise_prefs['text'], multiline=True, min_lines=1, max_lines=8, on_change=lambda e:changed(e,'text'))
    preset = Dropdown(label="Quality Preset", width=250, options=[dropdown.Option("ultra_fast"), dropdown.Option("fast"), dropdown.Option("standard"), dropdown.Option("high_quality")], value=tortoise_prefs['preset'], on_change=lambda e: changed(e, 'preset'))
    batch_folder_name = TextField(label="Batch Folder Name", value=tortoise_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    file_prefix = TextField(label="Filename Prefix", value=tortoise_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))
    page.tortoise_voices = ResponsiveRow(controls=[])
    for v in tortoise_prefs['voices']:
      page.tortoise_voices.controls.append(Checkbox(label=v, fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, col={'xs':12, 'sm':6, 'md':3, 'lg':3, 'xl': 2}))
    if len(prefs['tortoise_custom_voices']) > 0:
      for custom in prefs['tortoise_custom_voices']:
        page.tortoise_voices.controls.append(Checkbox(label=custom['name'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, col={'xs':12, 'sm':6, 'md':3, 'lg':3, 'xl': 2}))
    custom_voice_name = TextField(label="Custom Voice Name", value=tortoise_prefs['custom_voice_name'], on_change=lambda e:changed(e,'custom_voice_name'))
    train_custom = Switcher(label="Train Custom Voice  ", value=tortoise_prefs['train_custom'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=toggle_custom)
    wav_path = TextField(label="Audio Files or Folder Path or URL to Train", value=tortoise_prefs['wav_path'], on_change=lambda e:changed(e,'wav_path'), suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_path), expand=1)
    add_wav_button = ElevatedButton(content=Text("Add Audio Files"), on_click=add_wav)
    page.tortoise_file_list = Column([], tight=True, spacing=0)
    custom_box = Container(Column([Text("Provide 3 or more ~10 second clips of voice as mp3 or wav files with 22050 sample rate:"),
        Row([wav_path, add_wav_button]),
        page.tortoise_file_list,]), animate_size=animation.Animation(1000, AnimationCurve.EASE_IN_CIRC), clip_behavior=ClipBehavior.HARD_EDGE)
    custom_box.height = None if tortoise_prefs['train_custom'] else 0
    custom_voice_name.visible = tortoise_prefs['train_custom']
    load_wavs()
    #seed = TextField(label="Seed", value=tortoise_prefs['seed'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'seed', ptype='int'), width = 160)
    #lambda_entropy = TextField(label="Lambda Entropy", value=dreamfustortoise_prefsion_prefs['lambda_entropy'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'lambda_entropy', ptype='float'), width = 160)
    #max_steps = TextField(label="Max Steps", value=tortoise_prefs['max_steps'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'max_steps', ptype='int'), width = 160)
    page.tortoise_output = Column([])
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.tortoise_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🐢  Tortoise Text-to-Speech Voice Modeling", "Reads your text in a realistic AI voice, train your own to mimic vocal performances...", actions=[IconButton(icon=icons.HELP, tooltip="Help with Tortoise-TTS Settings", on_click=tortoise_help)]),
        text,
        preset,
        Row([batch_folder_name, file_prefix]),
        Row([Text("Select one or more voices:", weight=FontWeight.BOLD), Text("(none for random or custom)")]),
        page.tortoise_voices,
        Row([train_custom, custom_voice_name], vertical_alignment=CrossAxisAlignment.START),
        #Row([output_dir]),
        custom_box,
        ElevatedButton(content=Text("🗣️  Run Tortoise-TTS", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_tortoise_tts(page)),
        page.tortoise_output,
        clear_button,
      ]
    ))], scroll=ScrollMode.AUTO)
    return c

audioLDM_prefs = {
    'text': '',
    'duration': 5.0,
    'guidance_scale': 2.5,
    'n_candidates': 3,#This number control the number of candidates (e.g., generate three audios and choose the best to show you). A Larger value usually lead to better quality with heavier computation
    'seed': 0,
    'wav_path': '',
    'batch_folder_name': '',
    'file_prefix': 'ldm-',
}

def buildAudioLDM(page):
    global prefs, audioLDM_prefs
    def changed(e, pref=None, ptype="str"):
        if pref is not None:
          try:
            if ptype == "int":
              audioLDM_prefs[pref] = int(e.control.value)
            elif ptype == "float":
              audioLDM_prefs[pref] = float(e.control.value)
            else:
              audioLDM_prefs[pref] = e.control.value
          except Exception:
            alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
            pass
    def add_to_audioLDM_output(o):
        page.audioLDM_output.controls.append(o)
        page.audioLDM_output.update()
    def clear_output(e):
        if prefs['enable_sounds']: page.snd_delete.play()
        page.audioLDM_output.controls = []
        page.audioLDM_output.update()
        clear_button.visible = False
        clear_button.update()
    def audioLDM_help(e):
        def close_audioLDM_dlg(e):
          nonlocal audioLDM_help_dlg
          audioLDM_help_dlg.open = False
          page.update()
        audioLDM_help_dlg = AlertDialog(title=Text("💁   Help with Audio-LDM"), content=Column([
            Text("AudioLDM is a TTA system that is built on a latent space to learn the continuous audio representations from contrastive language-audio pretraining (CLAP) latents. The pretrained CLAP models enable us to train LDMs with audio embedding while providing text embedding as a condition during sampling. By learning the latent representations of audio signals and their compositions without modeling the cross-modal relationship, AudioLDM is advantageous in both generation quality and computational efficiency. Trained on AudioCaps with a single GPU, AudioLDM achieves state-of-the-art TTA performance measured by both objective and subjective metrics (e.g., frechet distance). Moreover, AudioLDM is the first TTA system that enables various text-guided audio manipulations (e.g., style transfer) in a zero-shot fashion."),
            Markdown("They built the model with data from [AudioSet](http://research.google.com/audioset/), [Freesound](https://freesound.org/) and [BBC Sound Effect library](https://sound-effects.bbcrewind.co.uk/). We share this demo based on the [UK copyright exception](https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/375954/Research.pdf) of data for academic research.", on_tap_link=lambda e: e.page.launch_url(e.data)),
          ], scroll=ScrollMode.AUTO), actions=[TextButton("🔔  Good to hear... ", on_click=close_audioLDM_dlg)], actions_alignment=MainAxisAlignment.END)
        page.dialog = audioLDM_help_dlg
        audioLDM_help_dlg.open = True
        page.update()
    duration_row = SliderRow(label="Duration", min=1, max=20, divisions=38, round=1, suffix="s", pref=audioLDM_prefs, key='duration')
    guidance = SliderRow(label="Guidance Scale", min=0, max=5, divisions=10, round=1, pref=audioLDM_prefs, key='guidance_scale', tooltip="Large => better quality and relavancy to text; Small => better diversity")
    text = TextField(label="Text Prompt to Auditorialize", value=audioLDM_prefs['text'], multiline=True, min_lines=1, max_lines=8, on_change=lambda e:changed(e,'text'))
    batch_folder_name = TextField(label="Batch Folder Name", value=audioLDM_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    file_prefix = TextField(label="Filename Prefix", value=audioLDM_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))
    n_candidates = Tooltip(message="Automatic quality control. Generates candidates and choose the best. Larger value usually lead to better quality with heavier computation.", content=NumberPicker(label="Number of Candidates:   ", min=1, max=5, value=audioLDM_prefs['n_candidates'], on_change=lambda e: changed(e, 'n_candidates')))
    seed = TextField(label="Seed", value=audioLDM_prefs['seed'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'seed', ptype='int'), width = 120)
    page.audioLDM_output = Column([])
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.audioLDM_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🦻  Audio LDM Modeling", "Text-to-Audio Generation with Latent Diffusion Model...", actions=[IconButton(icon=icons.HELP, tooltip="Help with Audio LDM-TTS Settings", on_click=audioLDM_help)]),
        text,
        duration_row,
        guidance,
        Row([n_candidates, seed]),
        Row([batch_folder_name, file_prefix]),
        ElevatedButton(content=Text("👏  Run AudioLDM", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_audio_ldm(page)),
        page.audioLDM_output,
        clear_button,
        #AudioPlayer(audio_file=os.path.join(assets, "snd-drop.mp3"), display="tester", page=page)
      ]
    ))], scroll=ScrollMode.AUTO)
    return c

audioLDM2_prefs = {
    'text': '',
    'transcription': '',
    'model_name': 'audioldm2-full',
    'duration': 10.0,
    'guidance_scale': 3.5,
    'n_candidates': 3,#This number control the number of candidates (e.g., generate three audios and choose the best to show you). A Larger value usually lead to better quality with heavier computation
    'seed': 0,
    'wav_path': '',
    'batch_size': 1,
    'batch_folder_name': '',
    'file_prefix': 'ldm2-',
}

def buildAudioLDM2(page):
    global prefs, audioLDM2_prefs
    def changed(e, pref=None, ptype="str"):
        if pref is not None:
          try:
            if ptype == "int":
              audioLDM2_prefs[pref] = int(e.control.value)
            elif ptype == "float":
              audioLDM2_prefs[pref] = float(e.control.value)
            else:
              audioLDM2_prefs[pref] = e.control.value
          except Exception:
            alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
            pass
    def add_to_audioLDM2_output(o):
        page.audioLDM2_output.controls.append(o)
        page.audioLDM2_output.update()
    def clear_output(e):
        if prefs['enable_sounds']: page.snd_delete.play()
        page.audioLDM2_output.controls = []
        page.audioLDM2_output.update()
        clear_button.visible = False
        clear_button.update()
    def audioLDM2_help(e):
        def close_audioLDM2_dlg(e):
          nonlocal audioLDM2_help_dlg
          audioLDM2_help_dlg.open = False
          page.update()
        audioLDM2_help_dlg = AlertDialog(title=Text("💁   Help with Audio-LDM 2"), content=Column([
            Text("AudioLDM-2 is a novel and versatile audio generation model that capable of performing conditional audio, music, and intelligible speech generation. The proposed method is based on a universal representation of audio, which enables large-scale self-supervised pretraining of the core latent diffusion model without audio annotation and helps to combine the advantages of both the auto-regressive and the latent diffusion model. AudioLDM 2 achieves state-of-the-art performance in text-to-audio and text-to-music generation, while also delivering competitive results in text-to-speech generation, comparable to the current SoTA."),
            Text("Although audio generation shares commonalities across different types of audio, such as speech, music, and sound effects, designing models for each type requires careful consideration of specific objectives and biases that can significantly differ from those of other types. To bring us closer to a unified perspective of audio generation, this paper proposes a framework that utilizes the same learning method for speech, music, and sound effect generation. Our framework introduces a general representation of audio, called 'language of audio' (LOA). Any audio can be translated into LOA based on AudioMAE, a self-supervised pre-trained representation learning model. In the generation process, we translate any modalities into LOA by using a GPT-2 model, and we perform self-supervised audio generation learning with a latent diffusion model conditioned on LOA. The proposed framework naturally brings advantages such as in-context learning abilities and reusable self-supervised pretrained AudioMAE and latent diffusion models. Experiments on the major benchmarks of text-to-audio, text-to-music, and text-to-speech demonstrate new state-of-the-art or competitive performance to previous approaches."),
            Markdown("[Project Page](https://audioldm.github.io/audioldm2/) | [Paper](https://arxiv.org/abs/2308.05734) | [GitHub Code](https://github.com/haoheliu/audioldm2)", on_tap_link=lambda e: e.page.launch_url(e.data)),
          ], scroll=ScrollMode.AUTO), actions=[TextButton("🔔  Good to hear... ", on_click=close_audioLDM2_dlg)], actions_alignment=MainAxisAlignment.END)
        page.dialog = audioLDM2_help_dlg
        audioLDM2_help_dlg.open = True
        page.update()
    def change_model(e):
        changed(e, 'model_name')
        if 'speech' in audioLDM2_prefs['model_name']:
            transcription.visible = True
        else:
            transcription.visible = False
        transcription.update()
    model_name = Dropdown(label="Audio-LDM2 Model", width=350, options=[dropdown.Option("audioldm2-full"), dropdown.Option("audioldm2-full-large-1150k"), dropdown.Option("audioldm2-music-665k"), dropdown.Option("audioldm2-speech-gigaspeech"), dropdown.Option("audioldm2-speech-ljspeech")], value=audioLDM2_prefs['model_name'], on_change=change_model)
    duration_row = SliderRow(label="Duration", min=1, max=20, divisions=38, round=1, suffix="s", pref=audioLDM2_prefs, key='duration')
    guidance = SliderRow(label="Guidance Scale", min=0, max=5, divisions=10, round=1, pref=audioLDM2_prefs, key='guidance_scale', tooltip="Large => better quality and relavancy to text; Small => better diversity")
    text = TextField(label="Text Prompt to Auditorialize", value=audioLDM2_prefs['text'], multiline=True, min_lines=1, max_lines=8, on_change=lambda e:changed(e,'text'))
    transcription = TextField(label="Text Transcript to Speak", value=audioLDM2_prefs['transcription'], multiline=True, min_lines=1, max_lines=8, visible=False, on_change=lambda e:changed(e,'transcription'))
    batch_folder_name = TextField(label="Batch Folder Name", value=audioLDM2_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    file_prefix = TextField(label="Filename Prefix", value=audioLDM2_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))
    n_candidates = Tooltip(message="Automatic quality control. Generates candidates and choose the best. Larger value usually lead to better quality with heavier computation.", content=NumberPicker(label="Number of Candidates:   ", min=1, max=5, value=audioLDM2_prefs['n_candidates'], on_change=lambda e: changed(e, 'n_candidates')))
    batch_size = NumberPicker(label="Batch Size:   ", min=1, max=5, value=audioLDM2_prefs['batch_size'], on_change=lambda e: changed(e, 'batch_size'))
    seed = TextField(label="Seed", value=audioLDM2_prefs['seed'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'seed', ptype='int'), width = 120)
    page.audioLDM2_output = Column([])
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.audioLDM2_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("📢  Audio LDM-2 Modeling", "Holistic Audio Generation with Self-supervised Pretraining...", actions=[IconButton(icon=icons.HELP, tooltip="Help with Audio LDM-TTS Settings", on_click=audioLDM2_help)]),
        text,
        transcription,
        model_name,
        duration_row,
        guidance,
        Row([n_candidates, seed]),#, batch_size
        Row([batch_folder_name, file_prefix]),
        ElevatedButton(content=Text("🎙  Run AudioLDM 2", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_audio_ldm2(page)),
        page.audioLDM2_output,
        clear_button,
        #AudioPlayer(audio_file=os.path.join(assets, "snd-drop.mp3"), display="tester", page=page)
      ]
    ))], scroll=ScrollMode.AUTO)
    return c

bark_prefs = {
    'text': '',
    'text_temp': 0.7,
    'waveform_temp': 0.7,
    'acoustic_prompt': 'Unconditional',
    'n_iterations': 1,
    'seed': 0,
    'use_bettertransformer': False,
    'batch_folder_name': '',
    'file_prefix': 'bark-',
}

def buildBark(page):
    global prefs, bark_prefs
    def changed(e, pref=None, ptype="str"):
        if pref is not None:
          try:
            if ptype == "int":
              bark_prefs[pref] = int(e.control.value)
            elif ptype == "float":
              bark_prefs[pref] = float(e.control.value)
            else:
              bark_prefs[pref] = e.control.value
          except Exception:
            alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
            pass
    def add_to_bark_output(o):
        page.bark_output.controls.append(o)
        page.bark_output.update()
    def clear_output(e):
        if prefs['enable_sounds']: page.snd_delete.play()
        page.bark_output.controls = []
        page.bark_output.update()
        clear_button.visible = False
        clear_button.update()
    def bark_help(e):
        def close_bark_dlg(e):
          nonlocal bark_help_dlg
          bark_help_dlg.open = False
          page.update()
        bark_help_dlg = AlertDialog(title=Text("💁   Help with Bark AI"), content=Column([
            Text("Bark is a transformer-based text-to-audio model created by Suno. Bark can generate highly realistic, multilingual speech as well as other audio - including music, background noise and simple sound effects. The model can also produce nonverbal communications like laughing, sighing and crying. To support the research community, we are providing access to pretrained model checkpoints ready for inference."),
            Text("Below is a list of some known non-speech sounds, but we are finding more every day. Please let us know if you find patterns that work particularly well on Discord!"),
            Text("[laughter], [laughs], [sighs], [music], [gasps], [clears throat], — or … for hesitations, ♪ for song lyrics, capitalization for emphasis of a word, MAN/WOMAN: for bias towards speaker"),
            Markdown("Checkout their [GitHub Project](https://github.com/suno-ai/bark), [HuggingFace Space](https://huggingface.co/spaces/suno/bark) and [Suno Discord](https://discord.com/invite/J2B2vsjKuE).", on_tap_link=lambda e: e.page.launch_url(e.data)),
          ], scroll=ScrollMode.AUTO), actions=[TextButton("🦊  Woof... ", on_click=close_bark_dlg)], actions_alignment=MainAxisAlignment.END)
        page.dialog = bark_help_dlg
        bark_help_dlg.open = True
        page.update()
    #duration_row = SliderRow(label="Duration", min=1, max=20, divisions=38, round=1, suffix="s", pref=bark_prefs, key='duration')
    text_temp = SliderRow(label="Text Temperature", min=0, max=1, divisions=20, round=2, pref=bark_prefs, key='text_temp', tooltip="1.0 more diverse, 0.0 more conservative")
    waveform_temp = SliderRow(label="Wave Temperature", min=0, max=1, divisions=20, round=2, pref=bark_prefs, key='waveform_temp', tooltip="1.0 more diverse, 0.0 more conservative")
    text = TextField(label="Text Prompt to Vocalize", value=bark_prefs['text'], multiline=True, min_lines=1, max_lines=8, on_change=lambda e:changed(e,'text'))
    acoustic_prompt = Dropdown(label="Acoustic Prompt", width=250, options=[dropdown.Option("Unconditional"), dropdown.Option("Announcer")], value=bark_prefs['acoustic_prompt'], on_change=lambda e: changed(e, 'acoustic_prompt'))
    langs = ["en", "de", "es", "fr", "hi", "it", "ja", "ko", "pl", "pt", "ru", "tr", "zh"]
    for lang in langs:
        for n in range(10):
            #label = f"Speaker {n} ({lang})"
            acoustic_prompt.options.append(dropdown.Option(f"{lang}_speaker_{n}"))
    batch_folder_name = TextField(label="Batch Folder Name", value=bark_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    file_prefix = TextField(label="Filename Prefix", value=bark_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))
    n_iterations = Tooltip(message="Make multiple for quality control. Generates candidates and choose the best.", content=NumberPicker(label="Number of Iterations:   ", min=1, max=10, value=bark_prefs['n_iterations'], on_change=lambda e: changed(e, 'n_iterations')))
    seed = TextField(label="Seed", value=bark_prefs['seed'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'seed', ptype='int'), width = 120)
    use_bettertransformer = Switcher(label="Use Optimum BetterTransformer", value=bark_prefs['use_bettertransformer'], active_color=colors.PRIMARY_CONTAINER, active_track_color=colors.PRIMARY, on_change=lambda e:changed(e,'use_bettertransformer'), tooltip="Enables HuggingFace Optimum Transformers to go 25% faster.")
    page.bark_output = Column([])
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.bark_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🐶  Bark AI", "Text-to-Audio Generation for Multilingual Speech, Music and Sound Effects...", actions=[IconButton(icon=icons.HELP, tooltip="Help with Audio LDM-TTS Settings", on_click=bark_help)]),
        text,
        text_temp,
        waveform_temp,
        Row([acoustic_prompt, n_iterations]),
        #Row([n_iterations, seed]),
        Row([batch_folder_name, file_prefix]),
        use_bettertransformer,
        ElevatedButton(content=Text("🐕  Run Bark", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_bark(page)),
        page.bark_output,
        clear_button,
      ]
    ))], scroll=ScrollMode.AUTO)
    return c


riffusion_prefs = {
    'prompt': '',
    'negative_prompt': '',
    'audio_file': '',
    'duration': 5.0,
    'steps': 50,
    'strength': 0.5,
    'guidance_scale': 7.0,
    'batch_size': 1,
    'max_size': 768,
    'seed': 0,
    'wav_path': '',
    'batch_folder_name': '',
    'file_prefix': 'riff-',
    'loaded_pipe': '',
}

def buildRiffusion(page):
    global prefs, riffusion_prefs
    def changed(e, pref=None, ptype="str"):
        if pref is not None:
          try:
            if ptype == "int":
              riffusion_prefs[pref] = int(e.control.value)
            elif ptype == "float":
              riffusion_prefs[pref] = float(e.control.value)
            else:
              riffusion_prefs[pref] = e.control.value
          except Exception:
            alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
            pass
    def add_to_riffusion_output(o):
        page.riffusion_output.controls.append(o)
        page.riffusion_output.update()
    def clear_output(e):
        if prefs['enable_sounds']: page.snd_delete.play()
        page.riffusion_output.controls = []
        page.riffusion_output.update()
        clear_button.visible = False
        clear_button.update()
    def riffusion_help(e):
        def close_riffusion_dlg(e):
          nonlocal riffusion_help_dlg
          riffusion_help_dlg.open = False
          page.update()
        riffusion_help_dlg = AlertDialog(title=Text("💁   Help with Riffusion"), content=Column([
            Text("This is the v1.5 stable diffusion model with no modifications, just fine-tuned on images of spectrograms paired with text. Audio processing happens downstream of the model. It can generate infinite variations of a prompt by varying the seed. All the same web UIs and techniques like img2img, inpainting, negative prompts, and interpolation work out of the box."),
            Markdown("[Project Page](https://www.riffusion.com/about), [Codebase](https://github.com/riffusion/riffusion), [Discord](https://discord.gg/yu6SRwvX4v)", on_tap_link=lambda e: e.page.launch_url(e.data)),
          ], scroll=ScrollMode.AUTO), actions=[TextButton("🎺  Let's Jam... ", on_click=close_riffusion_dlg)], actions_alignment=MainAxisAlignment.END)
        page.dialog = riffusion_help_dlg
        riffusion_help_dlg.open = True
        page.update()
    def file_picker_result(e: FilePickerResultEvent):
        if e.files != None:
            upload_files(e)
    def on_upload_progress(e: FilePickerUploadEvent):
        if e.progress == 1:
            if not slash in e.file_name:
              fname = os.path.join(root_dir, e.file_name)
              riffusion_prefs['file_name'] = e.file_name.rpartition('.')[0]
            else:
              fname = e.file_name
              riffusion_prefs['file_name'] = e.file_name.rpartition(slash)[2].rpartition('.')[0]
            audio_file.value = fname
            audio_file.update()
            riffusion_prefs['audio_file'] = fname
            page.update()
    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)
    def upload_files(e):
        uf = []
        if file_picker.result != None and file_picker.result.files != None:
            for f in file_picker.result.files:
              if page.web:
                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))
              else:
                on_upload_progress(FilePickerUploadEvent(f.path, 1, ""))
            file_picker.upload(uf)
    page.overlay.append(file_picker)
    def pick_audio(e):
        file_picker.pick_files(allow_multiple=False, allowed_extensions=["mp3", "wav"], dialog_title="Pick Init Audio File")
    audio_file = TextField(label="Input Audio File (optional)", value=riffusion_prefs['audio_file'], on_change=lambda e:changed(e,'audio_file'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_audio))

    def change_duration(e):
        changed(e, 'duration', ptype="float")
        duration_value.value = f" {riffusion_prefs['duration']}s"
        duration_value.update()
    duration = Slider(min=1, max=20, divisions=38, label="{value}s", value=float(riffusion_prefs['duration']), expand=True, on_change=change_duration)
    duration_value = Text(f" {float(riffusion_prefs['duration'])}s", weight=FontWeight.BOLD)
    duration_row = Row([Text("Duration: "), duration_value, duration])
    steps_row = SliderRow(label="Number of Steps", min=1, max=100, divisions=99, pref=riffusion_prefs, key='steps', tooltip="The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.")
    guidance = SliderRow(label="Guidance Scale", min=0, max=10, divisions=20, round=1, pref=riffusion_prefs, key='guidance_scale', tooltip="Large => better quality and relavancy to text; Small => better diversity")
    prompt = TextField(label="Musical Text Prompt", value=riffusion_prefs['prompt'], multiline=True, min_lines=1, max_lines=8, on_change=lambda e:changed(e,'prompt'), col={'md':9})
    negative_prompt = TextField(label="Negative Prompt", value=riffusion_prefs['negative_prompt'], multiline=True, min_lines=1, max_lines=8, on_change=lambda e:changed(e,'negative_prompt'), col={'md':3})
    max_size_row = SliderRow(label="Max Size", min=256, max=1024, divisions=12, multiple=32, suffix="px", pref=riffusion_prefs, key='max_size')
    batch_folder_name = TextField(label="Batch Folder Name", value=riffusion_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    file_prefix = TextField(label="Filename Prefix", value=riffusion_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))
    batch_size = NumberPicker(label="Batch Size:   ", min=1, max=5, value=riffusion_prefs['batch_size'], on_change=lambda e: changed(e, 'batch_size'))
    seed = TextField(label="Seed", value=riffusion_prefs['seed'], keyboard_type=KeyboardType.NUMBER, on_change=lambda e: changed(e, 'seed', ptype='int'), width = 120)
    page.riffusion_output = Column([])
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.riffusion_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("💽  Riffusion Spectrogram Sound Modeling", "Stable Diffusion for real-time music generation...", actions=[IconButton(icon=icons.HELP, tooltip="Help with Audio LDM-TTS Settings", on_click=riffusion_help)]),
        ResponsiveRow([prompt, negative_prompt]),
        #audio_file,
        #duration_row,
        guidance,
        steps_row,
        max_size_row,
        Row([batch_size, seed]),
        Row([batch_folder_name, file_prefix]),
        ElevatedButton(content=Text("👨‍🎤️  Run Riffusion", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_riffusion(page)),
        page.riffusion_output,
        clear_button,
      ]
    ))], scroll=ScrollMode.AUTO)
    return c

mubert_prefs = {
    'prompt': '',
    'duration': 30,
    'is_loop': False,
    'email': '',
    'tags': '',
    'tag_list': '',
    'batch_folder_name': '',
    'file_prefix': 'mu-'
}

def buildMubert(page):
    global mubert_prefs
    def changed(e, pref=None, ptype="str"):
        if pref is not None:
          try:
            if ptype == "int":
              mubert_prefs[pref] = int(e.control.value)
            elif ptype == "float":
              mubert_prefs[pref] = float(e.control.value)
            else:
              mubert_prefs[pref] = e.control.value
          except Exception:
            alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
            pass
    def add_to_mubert_output(o):
        page.mubert_output.controls.append(o)
        page.mubert_output.update()
    def clear_output(e):
        if prefs['enable_sounds']: page.snd_delete.play()
        page.mubert_output.controls = []
        page.mubert_output.update()
        clear_button.visible = False
        clear_button.update()
    def mubert_help(e):
        def close_mubert_dlg(e):
          nonlocal mubert_help_dlg
          mubert_help_dlg.open = False
          page.update()
        mubert_help_dlg = AlertDialog(title=Text("💁   Help with Mubert"), content=Column([
            Text("Mubert AI is a cutting-edge tool that allows you to create realistic and infinite music by learning from a large dataset of existing music. The result is a high-quality and original music stream that sounds like it was composed by a professional musician."),
            Text("This uses the Mubert Developer API which is actually quite expensive to use, so don't be surprised if your generation fails because the API Key has used it's monthy quota. Sorry, it's not free/opensource..."),
          ], scroll=ScrollMode.AUTO), actions=[TextButton("🥁  Gimme a Beat... ", on_click=close_mubert_dlg)], actions_alignment=MainAxisAlignment.END)
        page.dialog = mubert_help_dlg
        mubert_help_dlg.open = True
        page.update()
    prompt = TextField(label="Prompt to generate a track (genre, theme, etc.)", value=mubert_prefs['prompt'], multiline=True, min_lines=1, max_lines=8, on_change=lambda e:changed(e,'prompt'))
    duration_row = SliderRow(label="Duration", min=1, max=250, divisions=249, suffix="s", pref=mubert_prefs, key='duration')
    is_loop = Checkbox(label="Is Audio Loop   ", value=mubert_prefs['is_loop'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'is_loop'))
    email = TextField(label="Email Address (for API use)", keyboard_type=KeyboardType.EMAIL, value=mubert_prefs['email'], on_change=lambda e:changed(e,'email'))
    batch_folder_name = TextField(label="Batch Folder Name", value=mubert_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    file_prefix = TextField(label="Filename Prefix", value=mubert_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))
    #page.mubert_songs = ResponsiveRow(controls=[])
    #for v in mubert_prefs['tag_list']:
    #  page.mubert_songs.controls.append(Checkbox(label=v, fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, col={'xs':12, 'sm':6, 'md':3, 'lg':3, 'xl': 2}))
    page.mubert_output = Column([])
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.mubert_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🎼  Mubert Music Generator", "AI music is generated by Mubert API. Pretty good grooves...", actions=[IconButton(icon=icons.HELP, tooltip="Help with Mubert Settings", on_click=mubert_help)]),
        prompt,
        duration_row,
        is_loop,
        email,
        Row([batch_folder_name, file_prefix]),
        #Row([Text("Select one or more tags:", weight=FontWeight.BOLD), Text("(none for random or custom)")]),
        #page.mubert_songs,
        ElevatedButton(content=Text("🎻  Run Mubert Music", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_mubert(page)),
        page.mubert_output,
        clear_button,
      ]
    ))], scroll=ScrollMode.AUTO)
    return c

whisper_prefs = {
    'audio_file': '',
    'model_size': 'base',
    'trim_audio': True,
    'AI_engine': 'ChatGPT-3.5 Turbo',
    'AI_temperature': 0.7,
    'simple_transcribe': True,
    'detect_language': False,
    'reformat': False,
    'rewrite': False,
    'summarize': False,
    'describe': False,
    'article': False,
    'keypoints': False,
    'keywords': False,
    'translate': False,
    'to_language': '',
}
whisper_requests = {
        'reformat': 'Format the following text to have correct punctuations, grammar and sentence structure.',
        'rewrite': 'Rewrite and edit the following text to be more proper.',
        'summarize': 'Write a summary of the following text to reflect the most important information.',
        'describe': 'Write a description of the following text to explain the subject and discussed topics.',
        'article': 'Write an article based on the following text, using the interesting points to discuss.',
        'keypoints': 'Outline the key points of the following text to summarize the useful information contained.',
        'keywords': 'List the relevant keywords and phrases of the following text for SEO search engine optimization.',
        'translate': 'Translate the following text into ',
    }

def buildWhisper(page):
    global prefs, whisper_prefs, whisper_requests
    def changed(e, pref=None, ptype="str"):
        if pref is not None:
          try:
            if ptype == "int":
              whisper_prefs[pref] = int(e.control.value)
            elif ptype == "float":
              whisper_prefs[pref] = float(e.control.value)
            else:
              whisper_prefs[pref] = e.control.value
          except Exception:
            alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
            pass
    def clear_output(e):
        if prefs['enable_sounds']: page.snd_delete.play()
        page.whisper_output.controls = []
        page.whisper_output.update()
        clear_button.visible = False
        clear_button.update()
    def whisper_help(e):
        def close_whisper_dlg(e):
          nonlocal whisper_help_dlg
          whisper_help_dlg.open = False
          page.update()
        whisper_help_dlg = AlertDialog(title=Text("💁   Help with Whisper-AI"), content=Column([
            Text("Whisper is an automatic speech recognition (ASR) system trained on 680,000 hours of multilingual and multitask supervised data collected from the web. We show that the use of such a large and diverse dataset leads to improved robustness to accents, background noise and technical language. Moreover, it enables transcription in multiple languages, as well as translation from those languages into English. We are open-sourcing models and inference code to serve as a foundation for building useful applications and for further research on robust speech processing."),
            Text("The Whisper architecture is a simple end-to-end approach, implemented as an encoder-decoder Transformer. Input audio is split into 30-second chunks, converted into a log-Mel spectrogram, and then passed into an encoder. A decoder is trained to predict the corresponding text caption, intermixed with special tokens that direct the single model to perform tasks such as language identification, phrase-level timestamps, multilingual speech transcription, and to-English speech translation."),
            Markdown("[Project Page](https://openai.com/research/whisper) | [GitHub Code](https://github.com/openai/whisper)", on_tap_link=lambda e: e.page.launch_url(e.data)),
          ], scroll=ScrollMode.AUTO), actions=[TextButton("🙉  Understood... ", on_click=close_whisper_dlg)], actions_alignment=MainAxisAlignment.END)
        page.dialog = whisper_help_dlg
        whisper_help_dlg.open = True
        page.update()
    def file_picker_result(e: FilePickerResultEvent):
        if e.files != None:
          upload_files(e)
    def on_upload_progress(e: FilePickerUploadEvent):
        if e.progress == 1:
            if not slash in e.file_name:
              fname = os.path.join(root_dir, e.file_name)
              whisper_prefs['file_name'] = e.file_name.rpartition('.')[0]
            else:
              fname = e.file_name
              fpath = os.path.join(root_dir, e.file_name.rpartition(slash)[2])
              whisper_prefs['file_name'] = e.file_name.rparition(slash)[2].rpartition('.')[0]
            audio_file.value = fname
            audio_file.update()
            whisper_prefs['audio_file'] = fname
            page.update()
    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)
    def upload_files(e):
        uf = []
        if file_picker.result != None and file_picker.result.files != None:
            for f in file_picker.result.files:
              if page.web:
                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))
              else:
                on_upload_progress(FilePickerUploadEvent(f.path, 1, ""))
            file_picker.upload(uf)
    page.overlay.append(file_picker)
    def pick_audio(e):
        file_picker.pick_files(allow_multiple=False, allowed_extensions=["mp3", "wav"], dialog_title="Pick Init Audio File")
    audio_file = TextField(label="Input Audio File (MP3, URL or YouTube URL)", value=whisper_prefs['audio_file'], on_change=lambda e:changed(e,'audio_file'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_audio))
    model_size = Dropdown(label="Whisper Model Size", width=200, options=[dropdown.Option("tiny"), dropdown.Option("base"), dropdown.Option("small"), dropdown.Option("medium"), dropdown.Option("large")], value=whisper_prefs['model_size'], on_change=lambda e: changed(e, 'model_size'))
    trim_audio = Checkbox(label="Trim Audio to 30s", value=whisper_prefs['trim_audio'], tooltip="Prefers a short audio chunk", fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'trim_audio'))
    AI_engine = Dropdown(label="AI Engine", width=200, options=[dropdown.Option("OpenAI GPT-3"), dropdown.Option("ChatGPT-3.5 Turbo")], value=whisper_prefs['AI_engine'], on_change=lambda e: changed(e, 'AI_engine'))
    AI_temperature = SliderRow(label="AI Temperature", min=0, max=1, divisions=10, round=1, expand=True, pref=whisper_prefs, key="AI_temperature")
    reformat = Checkbox(label="Reformat grammar and structure of transcript", value=whisper_prefs['reformat'], tooltip=whisper_requests['reformat'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, col={'md':6, 'lg':4, 'xl':3}, on_change=lambda e:changed(e,'reformat'))
    rewrite = Checkbox(label="Rewrite and edit content of transcript", value=whisper_prefs['rewrite'], tooltip=whisper_requests['rewrite'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, col={'md':6, 'lg':4, 'xl':3}, on_change=lambda e:changed(e,'rewrite'))
    summarize = Checkbox(label="Summarize the information of transcript", value=whisper_prefs['summarize'], tooltip=whisper_requests['summarize'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, col={'md':6, 'lg':4, 'xl':3}, on_change=lambda e:changed(e,'summarize'))
    describe = Checkbox(label="Describe the subject of transcript", value=whisper_prefs['describe'], tooltip=whisper_requests['describe'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, col={'md':6, 'lg':4, 'xl':3}, on_change=lambda e:changed(e,'describe'))
    article = Checkbox(label="Write article about content of transcript", value=whisper_prefs['article'], tooltip=whisper_requests['article'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, col={'md':6, 'lg':4, 'xl':3}, on_change=lambda e:changed(e,'article'))
    keypoints = Checkbox(label="Outline key points of transcript", value=whisper_prefs['keypoints'], tooltip=whisper_requests['keypoints'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, col={'md':6, 'lg':4, 'xl':3}, on_change=lambda e:changed(e,'keypoints'))
    keywords = Checkbox(label="List SEO Keywords in transcript", value=whisper_prefs['keywords'], tooltip=whisper_requests['keywords'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, col={'md':6, 'lg':4, 'xl':3}, on_change=lambda e:changed(e,'keywords'))
    translate = Checkbox(label="Translate transcript", value=whisper_prefs['translate'], tooltip=whisper_requests['translate'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'translate'))
    to_language = TextField(label="to Language", value=whisper_prefs['to_language'], width=120, on_change=lambda e:changed(e,'to_language'))
    translate_to = Container(Row([translate, to_language], vertical_alignment=CrossAxisAlignment.START), col={'md':6, 'lg':4, 'xl':3})
    page.whisper_output = Column([])
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.whisper_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🧏  OpenAI Whisper-AI Speech-To-Text", "Generate Text Transcriptions from Speech Recordings, then optionally process text with GPT...", actions=[IconButton(icon=icons.HELP, tooltip="Help with Audio Diffusion-TTS Settings", on_click=whisper_help)]),
        audio_file,
        Row([model_size, trim_audio]),
        Row([AI_engine, AI_temperature]),
        ResponsiveRow([reformat, rewrite, summarize, describe, article, keypoints, keywords, translate_to]),
        ElevatedButton(content=Text("👂  Run Whisper", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_whisper(page)),
        page.whisper_output,
        clear_button,
      ]
    ))], scroll=ScrollMode.AUTO)
    return c

voice_fixer_prefs = {
    'audio_file': '',
    'mode': 0,
    'mode_0': True,
    'mode_1': False,
    'mode_2': False,
    'audio_name': '',
    'wav_path': '',
    'batch_folder_name': '',
    'file_prefix': 'voicefixer-',
}

def buildVoiceFixer(page):
    global prefs, voice_fixer_prefs, voice_fixer_requests
    def changed(e, pref=None, ptype="str"):
        if pref is not None:
          try:
            if ptype == "int":
              voice_fixer_prefs[pref] = int(e.control.value)
            elif ptype == "float":
              voice_fixer_prefs[pref] = float(e.control.value)
            else:
              voice_fixer_prefs[pref] = e.control.value
          except Exception:
            alert_msg(page, "Error updating field. Make sure your Numbers are numbers...")
            pass
    def clear_output(e):
        if prefs['enable_sounds']: page.snd_delete.play()
        page.voice_fixer_output.controls = []
        page.voice_fixer_output.update()
        clear_button.visible = False
        clear_button.update()
    def voice_fixer_help(e):
        def close_voice_fixer_dlg(e):
          nonlocal voice_fixer_help_dlg
          voice_fixer_help_dlg.open = False
          page.update()
        voice_fixer_help_dlg = AlertDialog(title=Text("💁   Help with VoiceFixer"), content=Column([
            Text("Voicefixer aims to restore human speech regardless how serious it's degraded. It can handle noise, reveberation, low resolution (2kHz~44.1kHz) and clipping (0.1-1.0 threshold) effect within one model.  This package provides a pretrained Voicefixer, which is build based on neural vocoder and pretrained 44.1k universal speaker-independent neural vocoder."),
            Markdown("[Arxiv Paper](https://arxiv.org/pdf/2109.13731.pdf) | [GitHub Code](https://github.com/haoheliu/voicefixer) | [Demo Page](https://haoheliu.github.io/demopage-voicefixer/)", on_tap_link=lambda e: e.page.launch_url(e.data)),
          ], scroll=ScrollMode.AUTO), actions=[TextButton("🦜  Loud & clear... ", on_click=close_voice_fixer_dlg)], actions_alignment=MainAxisAlignment.END)
        page.dialog = voice_fixer_help_dlg
        voice_fixer_help_dlg.open = True
        page.update()
    def file_picker_result(e: FilePickerResultEvent):
        if e.files != None:
          upload_files(e)
    def on_upload_progress(e: FilePickerUploadEvent):
        if e.progress == 1:
            if not slash in e.file_name:
              fname = os.path.join(root_dir, e.file_name)
              voice_fixer_prefs['file_name'] = e.file_name.rpartition('.')[0]
            else:
              fname = e.file_name
              fpath = os.path.join(root_dir, e.file_name.rpartition(slash)[2])
              voice_fixer_prefs['file_name'] = e.file_name.rparition(slash)[2].rpartition('.')[0]
            audio_file.value = fname
            audio_file.update()
            voice_fixer_prefs['audio_file'] = fname
            page.update()
    file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)
    def upload_files(e):
        uf = []
        if file_picker.result != None and file_picker.result.files != None:
            for f in file_picker.result.files:
              if page.web:
                uf.append(FilePickerUploadFile(f.name, upload_url=page.get_upload_url(f.name, 600)))
              else:
                on_upload_progress(FilePickerUploadEvent(f.path, 1, ""))
            file_picker.upload(uf)
    page.overlay.append(file_picker)
    def pick_audio(e):
        file_picker.pick_files(allow_multiple=False, allowed_extensions=["wav", "mp3", "flac"], dialog_title="Pick Init Audio File")
    audio_file = TextField(label="Input Audio File (WAV, MP3, URL or YouTube URL)", value=voice_fixer_prefs['audio_file'], on_change=lambda e:changed(e,'audio_file'), height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_audio))
    #model_size = Dropdown(label="VoiceFixer Model Size", width=200, options=[dropdown.Option("tiny"), dropdown.Option("base"), dropdown.Option("small"), dropdown.Option("medium"), dropdown.Option("large")], value=voice_fixer_prefs['model_size'], on_change=lambda e: changed(e, 'model_size'))
    #trim_audio = Checkbox(label="Trim Audio to 30s", value=voice_fixer_prefs['trim_audio'], tooltip="Prefers a short audio chunk", fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'trim_audio'))
    mode_0 = Checkbox(label="Mode 0", value=voice_fixer_prefs['mode_0'], tooltip="", fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'mode_0'))
    mode_1 = Checkbox(label="Mode 1", value=voice_fixer_prefs['mode_1'], tooltip="", fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'mode_1'))
    mode_2 = Checkbox(label="Mode 2", value=voice_fixer_prefs['mode_2'], tooltip="", fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, on_change=lambda e:changed(e,'mode_2'))
    audio_name = TextField(label="Audio File Name", value=voice_fixer_prefs['audio_name'], on_change=lambda e:changed(e,'audio_name'))
    batch_folder_name = TextField(label="Batch Folder Name", value=voice_fixer_prefs['batch_folder_name'], on_change=lambda e:changed(e,'batch_folder_name'))
    file_prefix = TextField(label="Filename Prefix", value=voice_fixer_prefs['file_prefix'], width=120, on_change=lambda e:changed(e,'file_prefix'))

    page.voice_fixer_output = Column([])
    clear_button = Row([ElevatedButton(content=Text("❌   Clear Output"), on_click=clear_output)], alignment=MainAxisAlignment.END)
    clear_button.visible = len(page.voice_fixer_output.controls) > 0
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("💬  Voice Fixer - Speech Restoration with Neural Vocoder", "Cleans up bad vocals and fixes the unwanted noise...", actions=[IconButton(icon=icons.HELP, tooltip="Help with Audio Diffusion-TTS Settings", on_click=voice_fixer_help)]),
        audio_file,
        Row([mode_0, mode_1, mode_2]),
        #Row([model_size, trim_audio]),
        Row([audio_name, batch_folder_name, file_prefix]),
        ElevatedButton(content=Text("🗣  Run VoiceFixer", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=lambda _: run_voice_fixer(page)),
        page.voice_fixer_output,
        clear_button,
      ]
    ))], scroll=ScrollMode.AUTO)
    return c


def buildCustomModelManager(page):
    global prefs
    def title_header(title, type):
        return Row([Text(title, style=TextThemeStyle.BODY_LARGE),
                    ft.FilledButton(f"Add {type} Model", on_click=lambda e: add_model(type))], alignment=MainAxisAlignment.SPACE_BETWEEN)
    def model_tile(name, path, token, type):
        return ListTile(title=Row([Text(name, weight=FontWeight.BOLD), Text(path), Text(token)], alignment=MainAxisAlignment.SPACE_BETWEEN), data=type, dense=True, trailing=PopupMenuButton(icon=icons.MORE_VERT,
          items=[PopupMenuItem(icon=icons.EDIT, text="Edit Custom Model", on_click=lambda e: edit_model(e, name), data=type),
                 PopupMenuItem(icon=icons.DELETE, text="Delete Custom Model", on_click=lambda e: del_model(e, name), data=type)]), on_click=lambda e: edit_model(e, name))
    def load_customs():
        #custom_models.controls.append(title_header("Stable Diffusion Finetuned Models", type="Finetuned"))
        for mod in prefs['custom_models']:
            token = mod['prefix'] if 'prefix' in mod else ""
            custom_models.controls.append(model_tile(mod['name'], mod['path'], token, "Finetuned"))
            #page.custom_models.controls.append(ListTile(title=Row([Text(f".{slash}{dir.rpartition(slash)[2]}", weight=FontWeight.BOLD), Text("")], alignment=MainAxisAlignment.SPACE_BETWEEN), data=dir, dense=True, trailing=PopupMenuButton(icon=icons.MORE_VERT,
            #  items=[PopupMenuItem(icon=icons.DELETE, text="Delete Custom Model", on_click=del_model, data=dir)])))
        #custom_models.update()
        for mod in prefs['custom_LoRA_models']:
            token = mod['prefix'] if 'prefix' in mod else ""
            custom_LoRA_models.controls.append(model_tile(mod['name'], mod['path'], token, "LoRA"))
        #custom_LoRA_models.update()
        for mod in prefs['custom_dance_diffusion_models']:
            token = mod['prefix'] if 'prefix' in mod else ""
            custom_dance_diffusion_models.controls.append(model_tile(mod['name'], mod['path'], token, "DanceDiffusion"))
        #custom_dance_diffusion_models.update()
        for mod in prefs['tortoise_custom_voices']:
            token = mod['prefix'] if 'prefix' in mod else ""
            tortoise_custom_voices.controls.append(model_tile(mod['name'], mod['folder'], token, "Tortoise"))
        #tortoise_custom_voices.update()
    def model_list(type):
        if type == "Finetuned":
            return prefs["custom_models"]
        elif type == "LoRA":
            return prefs["custom_LoRA_models"]
        elif type == "DanceDiffusion":
            return prefs["custom_dance_diffusion_models"]
        elif type == "Tortoise":
            return prefs["tortoise_custom_voices"]
    def edit_model(e, name):
        #name = e.control.title.controls[0].value
        #path = e.control.title.controls[1].value
        type = e.control.data
        mod = None
        for sub in model_list(type):
            if sub['name'] == name:
                mod = sub
                break
        if mod is None: return
        path = mod['path' if type != "Tortoise" else 'folder']
        #print(str(mod))
        def close_dlg(e):
            dlg_edit.open = False
            page.update()
        def save_model(e):
            if type == "Finetuned":
                for m in custom_models.controls:
                    if m.title.controls[0].value == name:
                        m.title.controls[0].value = model_name.value
                        m.title.controls[1].value = model_path.value
                        m.update()
            elif type == "LoRA":
                for m in custom_LoRA_models.controls:
                    if m.title.controls[0].value == name:
                        m.title.controls[0].value = model_name.value
                        m.title.controls[1].value = model_path.value
                        m.update()
            elif type == "DanceDiffusion":
                for m in custom_dance_diffusion_models.controls:
                    if m.title.controls[0].value == name:
                        m.title.controls[0].value = model_name.value
                        m.title.controls[1].value = model_path.value
                        m.update()
            elif type == "Tortoise":
                for m in tortoise_custom_voices.controls:
                    if m.title.controls[0].value == name:
                        m.title.controls[0].value = model_name.value
                        m.title.controls[1].value = model_path.value
                        m.update()
            mod['name'] = model_name.value
            mod['path' if type != "Tortoise" else 'folder'] = model_path.value
            dlg_edit.open = False
            e.control.update()
            page.update()
        model_name = TextField(label="Custom Model Name", value=name)
        model_path = TextField(label="Model Path", value=path)
        dlg_edit = AlertDialog(modal=False, title=Text(f"🧳 Edit {type} Model Info"), content=Container(Column([model_name, model_path], alignment=MainAxisAlignment.START, tight=True, scroll=ScrollMode.AUTO)), actions=[TextButton(content=Text("Cancel", size=18), on_click=close_dlg), ElevatedButton(content=Text(value=emojize(":floppy_disk:") + "  Save Model ", size=19, weight=FontWeight.BOLD), on_click=save_model)], actions_alignment=MainAxisAlignment.END)
        page.dialog = dlg_edit
        dlg_edit.open = True
        page.update()
    def add_model(type):
        mod_list = model_list(type)
        def close_dlg(e):
            dlg_edit.open = False
            page.update()
        def save_model(e):
            mod = {'name': model_name.value, 'path': model_path.value}
            mod_list.append(mod)
            if type == "Finetuned":
                custom_models.controls.append(model_tile(mod['name'], mod['path'], "", "Finetuned"))
                custom_models.update()
            elif type == "LoRA":
                custom_LoRA_models.controls.append(model_tile(mod['name'], mod['path'], "", "LoRA"))
                custom_LoRA_models.update()
            elif type == "DanceDiffusion":
                custom_dance_diffusion_models.controls.append(model_tile(mod['name'], mod['path'], "", "DanceDiffusion"))
                custom_dance_diffusion_models.update()
            elif type == "Tortoise":
                tortoise_custom_voices.controls.append(model_tile(mod['name'], mod['folder'], "", "Tortoise"))
                tortoise_custom_voices.update()
            dlg_edit.open = False
            e.control.update()
            page.update()
        model_name = TextField(label="Custom Model Name")
        model_path = TextField(label="Model Path")
        dlg_edit = AlertDialog(modal=False, title=Text(f"🧳 Add Custom {type} Model"), content=Container(Column([model_name, model_path], alignment=MainAxisAlignment.START, tight=True, scroll=ScrollMode.AUTO)), actions=[TextButton(content=Text("Cancel", size=18), on_click=close_dlg), ElevatedButton(content=Text(value=emojize(":floppy_disk:") + "  Save Model ", size=19, weight=FontWeight.BOLD), on_click=save_model)], actions_alignment=MainAxisAlignment.END)
        page.dialog = dlg_edit
        dlg_edit.open = True
        page.update()
    def del_model(e, name):
        type = e.control.data
        mod_list = model_list(type)
        for i, sub in enumerate(mod_list):
            if sub['name'] == name:
                mod = sub
                del mod_list[i]
                break
        if type == "Finetuned":
            for i, l in enumerate(custom_models.controls):
                if l.title.controls[0].value == name:
                    del custom_models.controls[i]
                    custom_models.update()
                    break
        elif type == "LoRA":
            for i, l in enumerate(custom_LoRA_models.controls):
                if l.title.controls[0].value == name:
                    del custom_LoRA_models.controls[i]
                    custom_LoRA_models.update()
                    break
        elif type == "DanceDiffusion":
            for i, l in enumerate(custom_dance_diffusion_models.controls):
                if l.title.controls[0].value == name:
                    del custom_dance_diffusion_models.controls[i]
                    custom_dance_diffusion_models.update()
                    break
        elif type == "Tortoise":
            for i, l in enumerate(tortoise_custom_voices.controls):
                if l.title.controls[0].value == name:
                    del tortoise_custom_voices.controls[i]
                    tortoise_custom_voices.update()
                    break
        if prefs['enable_sounds']: e.page.snd_delete.play()
    def update_list(e):
        page.finetuned_model.options.clear()
        for cust in model_list("Finetuned"):
            page.finetuned_model.options.append(dropdown.Option(cust["name"]))
        for mod in finetuned_models:
            page.finetuned_model.options.append(dropdown.Option(mod["name"]))
        try: page.finetuned_model.update()
        except: pass
        page.LoRA_model.options.clear()
        for cust in model_list("LoRA"):
            page.LoRA_model.options.append(dropdown.Option(cust["name"]))
        for mod in LoRA_models:
            page.LoRA_model.options.append(dropdown.Option(mod["name"]))
        page.LoRA_model.options.append(dropdown.Option("Custom LoRA Path"))
        try: page.LoRA_model.update()
        except: pass
        page.community_dance_diffusion_model.options.clear()
        for cust in model_list("DanceDiffusion"):
            page.community_dance_diffusion_model.options.append(dropdown.Option(cust["name"]))
        for mod in community_dance_diffusion_models:
            page.community_dance_diffusion_model.options.append(dropdown.Option(mod["name"]))
        try: page.community_dance_diffusion_model.update()
        except: pass
        page.tortoise_voices.controls.clear()
        for v in tortoise_prefs['voices']:
            page.tortoise_voices.controls.append(Checkbox(label=v, fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, col={'xs':12, 'sm':6, 'md':3, 'lg':3, 'xl': 2}))
        if len(prefs['tortoise_custom_voices']) > 0:
            for custom in prefs['tortoise_custom_voices']:
                page.tortoise_voices.controls.append(Checkbox(label=custom['name'], fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, col={'xs':12, 'sm':6, 'md':3, 'lg':3, 'xl': 2}))
        try: page.tortoise_voices.update()
        except: pass
        save_settings_file(e.page)
        if prefs['enable_sounds']: page.snd_drop.play()
    custom_models = Column([], spacing=0)
    custom_LoRA_models = Column([], spacing=0)
    custom_dance_diffusion_models = Column([], spacing=0)
    tortoise_custom_voices = Column([], spacing=0)
    load_customs()
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🧧   Manage your Saved Custom Models", "Add or Edit your favorite models from HuggingFace, URL or Local Path"),
        title_header("Custom Finetuned Models", "Finetuned"),
        custom_models,
        title_header("Custom LoRA Models", "LoRA"),
        custom_LoRA_models,
        title_header("Custom Tortoise Voice Models", "Tortoise"),
        tortoise_custom_voices,
        title_header("Custom Dance Diffusion Models", "DanceDiffusion"),
        custom_dance_diffusion_models,
        ElevatedButton(content=Text("🛄  Update Custom Dropdowns", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=update_list),
      ]
    ))], scroll=ScrollMode.AUTO)
    return c

def get_directory_size(directory):
    total = 0
    for entry in os.scandir(directory):
        if entry.is_file():
            total += entry.stat().st_size
        elif entry.is_dir():
            try:
                total += get_directory_size(entry.path)
            except FileNotFoundError:
                pass
    return total
def convert_bytes(num):
    step_unit = 1000.0 #1024 bad the size
    for x in ['bytes', 'KB', 'MB', 'GB', 'TB']:
        if num < step_unit:
            return "%3.1f %s" % (num, x)
        num /= step_unit

def buildCachedModelManager(page):
    global prefs
    def scan_cache(e):
      if not bool(prefs['cache_dir']):
        alert_msg(page, "You haven't set a Cache Directory in your Settings...")
        return
      elif not os.path.isdir(prefs['cache_dir']):
        alert_msg(page, "The Cache Directory in your Settings can't be found...")
        return
      if len(page.cached_folders.controls) > 1:
        page.cached_folders.controls.clear()
        page.cached_folders.update()
      page.cached_folders.controls.append(Installing(f"Scanning {prefs['cache_dir']}"))
      dirs = [f.path for f in os.scandir(prefs['cache_dir']) if f.is_dir()]
      del page.cached_folders.controls[-1]
      page.cached_folders.update()
      for dir in dirs:
        page.cached_folders.controls.append(ListTile(title=Row([Text(f".{slash}{dir.rpartition(slash)[2]}", weight=FontWeight.BOLD), Text("")], alignment=MainAxisAlignment.SPACE_BETWEEN), data=dir, dense=True, trailing=PopupMenuButton(icon=icons.MORE_VERT,
          items=[PopupMenuItem(icon=icons.DELETE, text="Delete Model Directory", on_click=del_dir, data=dir)])))
        page.cached_folders.update()
      for l in page.cached_folders.controls:
        size = convert_bytes(get_directory_size(l.data))
        l.title.controls[1].value = size
        l.title.controls[1].update()
        page.cached_folders.update()
    def del_dir(e):
      dir = e.control.data
      shutil.rmtree(dir, ignore_errors=True)
      for i, l in enumerate(page.cached_folders.controls):
        if l.data == dir:
          del page.cached_folders.controls[i]
          page.cached_folders.update()
          break
      if prefs['enable_sounds']: e.page.snd_delete.play()
    page.cached_folders = Column([])
    c = Column([Container(
      padding=padding.only(18, 14, 20, 10),
      content=Column([
        Header("🗂️   Manage your Cache Directory Saved Models", "If you're cacheing your model files, it can fill up your drive space quickly, so you can trim the fat as needed... Redownloads when used."),
        page.cached_folders,
        ElevatedButton(content=Text("🔍  Scan Cache Dirctory", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45, on_click=scan_cache),
      ]
    ))], scroll=ScrollMode.AUTO)
    return c

if 'pipe' in locals():
    clear_pipes()
use_custom_scheduler = False
retry_attempts_if_NSFW = 3
unet = None
pipe = None
pipe_img2img = None
pipe_SDXL = None
pipe_SDXL_refiner = None
compel_base = None
compel_refiner = None
pipe_interpolation = None
pipe_clip_guided = None
pipe_conceptualizer = None
pipe_repaint = None
pipe_imagic = None
pipe_composable = None
pipe_safe = None
pipe_versatile = None
pipe_versatile_text2img = None
pipe_versatile_variation = None
pipe_versatile_dualguided = None
pipe_upscale = None
pipe_depth = None
pipe_image_variation = None
pipe_semantic = None
pipe_DiffEdit = None
pipe_EDICT = None
text_encoder_EDICT = None
pipe_unCLIP = None
pipe_unCLIP_image_variation = None
pipe_unCLIP_interpolation = None
pipe_unCLIP_image_interpolation = None
pipe_magic_mix = None
pipe_paint_by_example = None
pipe_instruct_pix2pix = None
pipe_alt_diffusion = None
pipe_alt_diffusion_img2img = None
pipe_SAG = None
pipe_attend_and_excite = None
pipe_panorama = None
pipe_DiT = None
pipe_dance = None
pipe_kandinsky = None
pipe_kandinsky_prior = None
pipe_tortoise_tts = None
pipe_audio_ldm = None
pipe_audio_ldm2 = None
pipe_riffusion = None
pipe_audio_diffusion = None
pipe_music_gen = None
pipe_voice_fixer = None
pipe_text_to_video = None
pipe_text_to_video_zero = None
pipe_video_to_video = None
pipe_deepfloyd = None
pipe_deepfloyd2 = None
pipe_deepfloyd3 = None
pipe_gpt2 = None
pipe_distil_gpt2 = None
pipe_background_remover = None
pipe_stable_lm = None
tokenizer_stable_lm = None
depth_estimator = None
pipe_reference = None
pipe_controlnet_qr = None
pipe_controlnet_segment = None
pipe_kandinsky_controlnet_prior = None
pipe_controlnet = None
controlnet = None
controlnet_models = {"Canny Map Edge":None, "Scribble":None, "OpenPose":None, "Depth":None, "HED":None, "M-LSD":None, "Normal Map":None, "Segmented":None, "LineArt":None, "Shuffle":None, "Instruct Pix2Pix":None}
controlnet_xl_models = {"Canny Map Edge":None, "OpenPose":None, "Depth":None, "Softedge":None, "Segmented":None, "LineArt":None, "Shuffle":None, "Instruct Pix2Pix":None}
stability_api = None
safety = {'safety_checker':None, 'requires_safety_checker':False, 'feature_extractor':None} if prefs['disable_nsfw_filter'] else {}
model_path = "CompVis/stable-diffusion-v1-4"
inpaint_model = "stabilityai/stable-diffusion-2-inpainting"
#"runwayml/stable-diffusion-inpainting"
scheduler = None
scheduler_clip = None
if is_Colab:
  from google.colab import output
  output.enable_custom_widget_manager()


def get_model(name):
  #dropdown.Option("Stable Diffusion v1.5"), dropdown.Option("Stable Diffusion v1.4", dropdown.Option("Community Finetuned Model", dropdown.Option("DreamBooth Library Model"), dropdown.Option("Custom Model Path")
  if name == "Stable Diffusion v2.1 x768":
    return {'name':'Stable Diffusion v2.1 x768', 'path':'stabilityai/stable-diffusion-2-1', 'prefix':'', 'revision': 'fp16'}
  elif name == "Stable Diffusion v2.1 x512":
    return {'name':'Stable Diffusion v2.1 x512', 'path':'stabilityai/stable-diffusion-2-1-base', 'prefix':''}
  elif name == "Stable Diffusion v2.0":
    return {'name':'Stable Diffusion v2.0', 'path':'stabilityai/stable-diffusion-2', 'prefix':'', 'revision': 'fp16'}
  elif name == "Stable Diffusion v2.0 x768":
    return {'name':'Stable Diffusion v2.0 x768', 'path':'stabilityai/stable-diffusion-2', 'prefix':'', 'revision': 'fp16'}
  elif name == "Stable Diffusion v2.0 x512":
    return {'name':'Stable Diffusion v2.0 x512', 'path':'stabilityai/stable-diffusion-2-base', 'prefix':'', 'revision': 'fp16'}
  elif name == "Stable Diffusion v1.5":
    return {'name':'Stable Diffusion v1.5', 'path':'runwayml/stable-diffusion-v1-5', 'prefix':'', 'revision': 'fp16'}
  elif name == "Stable Diffusion v1.4":
    return {'name':'Stable Diffusion v1.4', 'path':'CompVis/stable-diffusion-v1-4', 'prefix':'', 'revision': 'fp16'}
  elif name == "Community Finetuned Model":
    return get_finetuned_model(prefs['finetuned_model'])
  elif name == "DreamBooth Library Model":
    return get_dreambooth_model(prefs['dreambooth_model'])
  elif name == "Custom Model Path":
    return {'name':'Custom Model', 'path':prefs['custom_model'], 'prefix':''}
  else:
    return {'name':'', 'path':'', 'prefix':''}

def get_finetuned_model(name):
  for mod in finetuned_models:
      if mod['name'] == name:
        return mod
  for mod in prefs['custom_models']:
      if mod['name'] == name:
        return mod
  return {'name':'', 'path':'', 'prefix':''}
def get_dreambooth_model(name):
  for mod in dreambooth_models:
      if mod['name'] == name:
        return {'name':mod['name'], 'path':f'sd-dreambooth-library/{mod["name"]}', 'prefix':mod['token']}
  return {'name':'', 'path':'', 'prefix':''}
def get_LoRA_model(name):
  if name == "Custom LoRA Path":
      return {'name':"Custom LoRA Model", 'path':prefs['custom_LoRA_model'], 'weight_name':None}
  for mod in LoRA_models:
      if mod['name'] == name:
        return {'name':mod['name'], 'path':mod['path'], 'weight_name':None if 'weight_name' in mod else mod['weight_name']}
  if len(prefs['custom_LoRA_models']) > 0:
    for mod in prefs['custom_LoRA_models']:
      if mod['name'] == name:
        return {'name':mod['name'], 'path':mod['path'], 'weight_name':None if 'weight_name' in mod else mod['weight_name']}
  return {'name':'', 'path':''}

def get_diffusers(page):
    global scheduler, model_path, prefs, status
    torch_installed = False
    try:
        import torch
        torch_installed = True
    except:
        pass
    if torch_installed:
        if version.parse(torch.__version__) < version.parse("2.0.0"):
            torch_installed = False
    if not torch_installed:
        page.console_msg("Upgrading Torch 2.0 Packages...")
        run_process("pip uninstall --yes torch torchaudio torchvision torchtext torchdata", page=page)
        if is_Colab:
            run_process("pip install torch torchaudio torchvision torchtext torchdata", page=page)
        else: #TODO: Check OS and run platform specific
            run_process("pip install torch torchvision torchaudio torchtext torchdata --index-url https://download.pytorch.org/whl/cu118", page=page)
    if prefs['memory_optimization'] == 'Xformers Mem Efficient Attention':
        try:
            import xformers
            if force_updates: raise ModuleNotFoundError("Forcing update")
        except ModuleNotFoundError:
            page.console_msg("Installing FaceBook's Xformers Memory Efficient Package...")
            run_process("pip install --pre -U triton", page=page)
            run_process("pip install -U xformers==0.0.18", page=page)
            import xformers
            page.console_msg("Installing Hugging Face Diffusers Pipeline...")
            pass
        #run_process("pip install pyre-extensions==0.0.23", page=page)
        #run_process("pip install -i https://test.pypi.org/simple/ formers==0.0.15.dev376", page=page)
        #run_process("pip install -q https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/T4/xformers-0.0.13.dev0-py3-none-any.whl", page=page)
        #run_process("pip install https://github.com/metrolobo/xformers_wheels/releases/download/1d31a3ac/xformers-0.0.14.dev0-cp37-cp37m-linux_x86_64.whl", page=page)
        #if install_xformers(page):
        status['installed_xformers'] = True
    run_sp("pip install invisible-watermark", realtime=False) #pip install --no-deps invisible-watermark>=0.2.0
    '''try:
        import accelerate
    except ModuleNotFoundError:
        page.console_msg("Installing Hugging Face Accelerate Package...")
        run_process("pip install --upgrade accelerate -q", page=page) #~=0.18
        #run_process("pip install --upgrade git+https://github.com/huggingface/accelerate.git -q", page=page)
        #run_sp("python -c from accelerate.utils import write_basic_config; write_basic_config(mixed_precision='fp16')")
        from accelerate.utils import write_basic_config
        #write_basic_config(mixed_precision='fp16')
        pass'''
    page.console_msg("Installing Hugging Face Diffusers Pipeline...")
    try:
        import transformers
        #print(f"transformers=={transformers.__version__}")
        if transformers.__version__ == "4.21.3": #Workaround because CLIP-Interrogator required other version
          run_process("pip uninstall -y git+https://github.com/pharmapsychotic/BLIP.git@lib#egg=blip", realtime=False)
          run_process("pip uninstall -y clip-interrogator", realtime=False)
          run_process("pip uninstall -y transformers", realtime=False)
          #run_process("pip uninstall -q transformers==4.21.3", page=page, realtime=False)
        if transformers.__version__ == "4.23.1": # Kandinsky conflict
          run_process("pip uninstall -y transformers", realtime=False)
    except ModuleNotFoundError:
        pass
    #run_process("pip install -q huggingface_hub", page=page)
    '''try:
      from huggingface_hub import notebook_login, HfApi, HfFolder, login
      from diffusers import StableDiffusionPipeline, logging
      import transformers
    except ModuleNotFoundError as e:#ModuleNotFoundError as e:'''
    try:
        import diffusers
        if force_updates: raise ModuleNotFoundError("Forcing update")
    except ModuleNotFoundError:
        #run_process("pip install --upgrade git+https://github.com/Skquark/diffusers.git", page=page)
        run_process("pip install --upgrade git+https://github.com/Skquark/diffusers.git@main#egg=diffusers[torch]", page=page)
        pass
    try:
        import transformers
        if force_updates: raise ModuleNotFoundError("Forcing update")
    except ModuleNotFoundError:
        #run_process("pip install -qq --upgrade git+https://github.com/huggingface/transformers", page=page)
        run_process("pip install --upgrade transformers~=4.28", page=page)
        pass
    try:
        import scipy
    except ModuleNotFoundError:
        run_process("pip install -upgrade scipy", page=page)
        pass
    try:
        import ftfy
    except ModuleNotFoundError:
        run_process("pip install --upgrade ftfy", page=page)
        pass
    try:
        import safetensors
    except ModuleNotFoundError:
        run_process("pip install --upgrade safetensors~=0.3", page=page)
        import safetensors
        from safetensors import safe_open
        pass
    try:
        import ipywidgets
    except ModuleNotFoundError:
        run_process('pip install -qq "ipywidgets>=7,<8"', page=page)
        pass
    run_process("git config --global credential.helper store", page=page)

    from huggingface_hub import notebook_login, HfApi, HfFolder, login
    #from diffusers import StableDiffusionPipeline, logging
    from diffusers import logging
    logging.set_verbosity_error()
    if not os.path.exists(HfFolder.path_token):
        #from huggingface_hub.commands.user import _login
        #_login(HfApi(), token=prefs['HuggingFace_api_key'])
        try:
          login(token=prefs['HuggingFace_api_key'], add_to_git_credential=True)
        except Exception:
          alert_msg(page, "ERROR Logging into HuggingFace... Check your API Key or Internet conenction.")
          return
    # TODO: Get Username to prefs
    api = HfApi()
    prefs['HuggingFace_username'] = api.whoami()["name"]
    #if prefs['model_ckpt'] == "Stable Diffusion v1.5": model_path =  "runwayml/stable-diffusion-v1-5"
    #elif prefs['model_ckpt'] == "Stable Diffusion v1.4": model_path =  "CompVis/stable-diffusion-v1-4"
    model = get_model(prefs['model_ckpt'])
    model_path = model['path']
    #try:
    #  scheduler = model_scheduler(model_path)
    #except Exception as e:
    #  alert_msg(page, f"ERROR: {prefs['scheduler_mode']} Scheduler couldn't load for {model_path}", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip())]))
    #  pass
    status['finetuned_model'] = False if model['name'].startswith("Stable") else True


def model_scheduler(model, big3=False):
    scheduler_mode = prefs['scheduler_mode']
    if scheduler_mode == "LMS Discrete":
      from diffusers import LMSDiscreteScheduler
      s = LMSDiscreteScheduler.from_pretrained(model, subfolder="scheduler")
    elif scheduler_mode == "PNDM":
      from diffusers import PNDMScheduler
      s = PNDMScheduler.from_pretrained(model, subfolder="scheduler")
    elif scheduler_mode == "DDIM":
      from diffusers import DDIMScheduler
      s = DDIMScheduler.from_pretrained(model, subfolder="scheduler")
    elif big3:
      from diffusers import DDIMScheduler
      s = DDIMScheduler.from_pretrained(model, subfolder="scheduler")
    elif scheduler_mode == "DPM Solver":
      from diffusers import DPMSolverMultistepScheduler #"hf-internal-testing/tiny-stable-diffusion-torch"
      s = DPMSolverMultistepScheduler.from_pretrained(model, subfolder="scheduler")
    elif scheduler_mode == "DPM Solver Singlestep":
      from diffusers import DPMSolverSinglestepScheduler
      s = DPMSolverSinglestepScheduler.from_pretrained(model, subfolder="scheduler")
    elif scheduler_mode == "DPM Solver Inverse":
      from diffusers import DPMSolverMultistepInverseScheduler
      s = DPMSolverMultistepInverseScheduler.from_pretrained(model, subfolder="scheduler")
    elif scheduler_mode == "K-Euler Discrete":
      from diffusers import EulerDiscreteScheduler
      s = EulerDiscreteScheduler.from_pretrained(model, subfolder="scheduler")
    elif scheduler_mode == "K-Euler Ancestral":
      from diffusers import EulerAncestralDiscreteScheduler
      s = EulerAncestralDiscreteScheduler.from_pretrained(model, subfolder="scheduler")
    elif scheduler_mode == "Karras-LMS":
      from diffusers import LMSDiscreteScheduler
      s = LMSDiscreteScheduler.from_pretrained(model, subfolder="scheduler")
      scheduler_config = s.get_scheduler_config()
      s = LMSDiscreteScheduler(**scheduler_config, use_karras_sigmas=True)
    elif scheduler_mode == "DPM Stochastic":
      from diffusers import DPMSolverSDEScheduler
      s = DPMSolverSDEScheduler.from_pretrained(model, subfolder="scheduler")
    elif scheduler_mode == "SDE-DPM Solver++":
      from diffusers import DPMSolverMultistepScheduler
      s = DPMSolverMultistepScheduler.from_pretrained(model, subfolder="scheduler")
      s.config.algorithm_type = 'sde-dpmsolver++'
    elif scheduler_mode == "DPM Solver++":
      from diffusers import DPMSolverMultistepScheduler
      s = DPMSolverMultistepScheduler.from_pretrained(model, subfolder="scheduler",
        beta_start=0.00085,
        beta_end=0.012,
        beta_schedule="scaled_linear",
        num_train_timesteps=1000,
        trained_betas=None,
        #predict_epsilon=True,
        prediction_type="v_prediction" if model.startswith('stabilityai') else "epsilon",
        thresholding=False,
        algorithm_type="dpmsolver++",
        solver_type="midpoint",
        solver_order=2,
        #denoise_final=True,
        lower_order_final=True,
      )
    elif scheduler_mode == "Heun Discrete":
      from diffusers import HeunDiscreteScheduler
      s = HeunDiscreteScheduler.from_pretrained(model, subfolder="scheduler")
    elif scheduler_mode == "Karras Heun Discrete":
      from diffusers import HeunDiscreteScheduler
      s = HeunDiscreteScheduler.from_pretrained(model, subfolder="scheduler", use_karras_sigmas=True)
    elif scheduler_mode == "K-DPM2 Ancestral":
      from diffusers import KDPM2AncestralDiscreteScheduler
      s = KDPM2AncestralDiscreteScheduler.from_pretrained(model, subfolder="scheduler")
    elif scheduler_mode == "K-DPM2 Discrete":
      from diffusers import KDPM2DiscreteScheduler
      s = KDPM2DiscreteScheduler.from_pretrained(model, subfolder="scheduler")
    elif scheduler_mode == "IPNDM":
      from diffusers import IPNDMScheduler
      s = IPNDMScheduler.from_pretrained(model, subfolder="scheduler")
    elif scheduler_mode == "DEIS Multistep":
      from diffusers import DEISMultistepScheduler
      s = DEISMultistepScheduler.from_pretrained(model, subfolder="scheduler")
    elif scheduler_mode == "UniPC Multistep":
      from diffusers import UniPCMultistepScheduler
      s = UniPCMultistepScheduler.from_pretrained(model, subfolder="scheduler")
    elif scheduler_mode == "Score-SDE-Vp":
      from diffusers import ScoreSdeVpScheduler
      s = ScoreSdeVpScheduler() #(num_train_timesteps=2000, beta_min=0.1, beta_max=20, sampling_eps=1e-3, tensor_format="np")
      use_custom_scheduler = True
    elif scheduler_mode == "Score-SDE-Ve":
      from diffusers import ScoreSdeVeScheduler
      s = ScoreSdeVeScheduler() #(num_train_timesteps=2000, snr=0.15, sigma_min=0.01, sigma_max=1348, sampling_eps=1e-5, correct_steps=1, tensor_format="pt"
      use_custom_scheduler = True
    elif scheduler_mode == "DDPM":
      from diffusers import DDPMScheduler
      s = DDPMScheduler(num_train_timesteps=1000, beta_start=0.0001, beta_end=0.02, beta_schedule="linear", trained_betas=None, variance_type="fixed_small", clip_sample=True, tensor_format="pt")
      use_custom_scheduler = True
    elif scheduler_mode == "Karras-Ve":
      from diffusers import KarrasVeScheduler
      s = KarrasVeScheduler() #(sigma_min=0.02, sigma_max=100, s_noise=1.007, s_churn=80, s_min=0.05, s_max=50, tensor_format="pt")
      use_custom_scheduler = True
    elif scheduler_mode == "LMS": #no more
      from diffusers import LMSScheduler
      s = LMSScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule="scaled_linear")
      #(num_train_timesteps=1000, beta_start=0.0001, beta_end=0.02, beta_schedule="linear", trained_betas=None, timestep_values=None, tensor_format="pt")
      use_custom_scheduler = True
    #print(f"Loaded Schedueler {scheduler_mode} {type(scheduler)}")
    else:
      print(f"Unknown scheduler request {scheduler_mode} - Using LMS Discrete")
      from diffusers import LMSDiscreteScheduler
      s = LMSDiscreteScheduler.from_pretrained(model, subfolder="scheduler")
    return s

def pipeline_scheduler(p, big3=False, from_scheduler = True):
    scheduler_mode = prefs['scheduler_mode']
    if scheduler_mode == "LMS Discrete":
      from diffusers import LMSDiscreteScheduler
      s = LMSDiscreteScheduler.from_config(p.scheduler.config if from_scheduler else p.config)
    elif scheduler_mode == "PNDM":
      from diffusers import PNDMScheduler
      s = PNDMScheduler.from_config(p.scheduler.config if from_scheduler else p.config)
    elif scheduler_mode == "DDIM":
      from diffusers import DDIMScheduler
      s = DDIMScheduler.from_config(p.scheduler.config if from_scheduler else p.config)
    elif big3:
      from diffusers import DDIMScheduler
      s = DDIMScheduler.from_config(p.scheduler.config if from_scheduler else p.config)
    elif scheduler_mode == "DPM Solver":
      from diffusers import DPMSolverMultistepScheduler #"hf-internal-testing/tiny-stable-diffusion-torch"
      s = DPMSolverMultistepScheduler.from_config(p.scheduler.config if from_scheduler else p.config)
    elif scheduler_mode == "DPM Solver Singlestep":
      from diffusers import DPMSolverSinglestepScheduler
      s = DPMSolverSinglestepScheduler.from_config(p.scheduler.config if from_scheduler else p.config)
    elif scheduler_mode == "DPM Solver Inverse":
      from diffusers import DPMSolverMultistepInverseScheduler
      s = DPMSolverMultistepInverseScheduler.from_config(p.scheduler.config if from_scheduler else p.config)
    elif scheduler_mode == "DPM Stochastic":
      from diffusers import DPMSolverSDEScheduler
      s = DPMSolverSDEScheduler.from_config(p.scheduler.config if from_scheduler else p.config)
    elif scheduler_mode == "SDE-DPM Solver++":
      from diffusers import DPMSolverMultistepScheduler #"hf-internal-testing/tiny-stable-diffusion-torch"
      s = DPMSolverMultistepScheduler.from_config(p.scheduler.config if from_scheduler else p.config)
      s.config.algorithm_type = 'sde-dpmsolver++'
    elif scheduler_mode == "K-Euler Discrete":
      from diffusers import EulerDiscreteScheduler
      s = EulerDiscreteScheduler.from_config(p.scheduler.config if from_scheduler else p.config)
    elif scheduler_mode == "K-Euler Ancestral":
      from diffusers import EulerAncestralDiscreteScheduler
      s = EulerAncestralDiscreteScheduler.from_config(p.scheduler.config if from_scheduler else p.config)
    elif scheduler_mode == "Karras-LMS":
      from diffusers import LMSDiscreteScheduler
      s = LMSDiscreteScheduler.from_config(p.scheduler.config if from_scheduler else p.config)
      scheduler_config = s.get_scheduler_config()
      s = LMSDiscreteScheduler(**scheduler_config, use_karras_sigmas=True)
    elif scheduler_mode == "DPM Solver++":
      from diffusers import DPMSolverMultistepScheduler
      try:
        p_model = p.model
      except Exception:
        p_model = ""
        pass
      s = DPMSolverMultistepScheduler.from_config(p.scheduler.config if from_scheduler else p.config,
        beta_start=0.00085,
        beta_end=0.012,
        beta_schedule="scaled_linear",
        num_train_timesteps=1000,
        trained_betas=None,
        #predict_epsilon=True,
        prediction_type="v_prediction" if p_model.startswith('stabilityai') else "epsilon",
        thresholding=False,
        algorithm_type="dpmsolver++",
        solver_type="midpoint",
        solver_order=2,
        #denoise_final=True,
        lower_order_final=True,
      )
    elif scheduler_mode == "Heun Discrete":
      from diffusers import HeunDiscreteScheduler
      s = HeunDiscreteScheduler.from_config(p.scheduler.config if from_scheduler else p.config)
    elif scheduler_mode == "Karras Heun Discrete":
      from diffusers import HeunDiscreteScheduler
      s = HeunDiscreteScheduler.from_config(p.scheduler.config if from_scheduler else p.config, use_karras_sigmas=True)
    elif scheduler_mode == "K-DPM2 Ancestral":
      from diffusers import KDPM2AncestralDiscreteScheduler
      s = KDPM2AncestralDiscreteScheduler.from_config(p.scheduler.config if from_scheduler else p.config)
    elif scheduler_mode == "K-DPM2 Discrete":
      from diffusers import KDPM2DiscreteScheduler
      s = KDPM2DiscreteScheduler.from_config(p.scheduler.config if from_scheduler else p.config)
    elif scheduler_mode == "IPNDM":
      from diffusers import IPNDMScheduler
      s = IPNDMScheduler.from_config(p.scheduler.config if from_scheduler else p.config)
    elif scheduler_mode == "DEIS Multistep":
      from diffusers import DEISMultistepScheduler
      s = DEISMultistepScheduler.from_config(p.scheduler.config if from_scheduler else p.config)
    elif scheduler_mode == "UniPC Multistep":
      from diffusers import UniPCMultistepScheduler
      s = UniPCMultistepScheduler.from_config(p.scheduler.config if from_scheduler else p.config)
    elif scheduler_mode == "Score-SDE-Vp":
      from diffusers import ScoreSdeVpScheduler
      s = ScoreSdeVpScheduler() #(num_train_timesteps=2000, beta_min=0.1, beta_max=20, sampling_eps=1e-3, tensor_format="np")
      use_custom_scheduler = True
    elif scheduler_mode == "Score-SDE-Ve":
      from diffusers import ScoreSdeVeScheduler
      s = ScoreSdeVeScheduler() #(num_train_timesteps=2000, snr=0.15, sigma_min=0.01, sigma_max=1348, sampling_eps=1e-5, correct_steps=1, tensor_format="pt"
      use_custom_scheduler = True
    elif scheduler_mode == "DDPM":
      from diffusers import DDPMScheduler
      s = DDPMScheduler(num_train_timesteps=1000, beta_start=0.0001, beta_end=0.02, beta_schedule="linear", trained_betas=None, variance_type="fixed_small", clip_sample=True, tensor_format="pt")
      use_custom_scheduler = True
    elif scheduler_mode == "Karras-Ve":
      from diffusers import KarrasVeScheduler
      s = KarrasVeScheduler() #(sigma_min=0.02, sigma_max=100, s_noise=1.007, s_churn=80, s_min=0.05, s_max=50, tensor_format="pt")
      use_custom_scheduler = True
    elif scheduler_mode == "LMS": #no more
      from diffusers import LMSScheduler
      s = LMSScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule="scaled_linear")
      #(num_train_timesteps=1000, beta_start=0.0001, beta_end=0.02, beta_schedule="linear", trained_betas=None, timestep_values=None, tensor_format="pt")
      use_custom_scheduler = True
    #print(f"Loaded Schedueler {scheduler_mode} {type(scheduler)}")
    else:
      print(f"Unknown scheduler request {scheduler_mode} - Using LMS Discrete")
      from diffusers import LMSDiscreteScheduler
      s = LMSDiscreteScheduler.from_config(p.scheduler.config)
    p.scheduler = s
    return p
#if is_Colab:
#    os.remove("/usr/local/lib/python3.8/dist-packages/torch/lib/libcudnn.so.8")
#    download_file("https://github.com/Skquark/diffusers/blob/main/utils/libcudnn.so.8?raw=true", to="/usr/local/lib/python3.8/dist-packages/torch/lib/")
torch_device = "cuda"
try:
    from packaging import version
    import torch
    if version.parse(torch.__version__) < version.parse("2.0.1") and torch.cuda.is_available():
      raise ModuleNotFoundError("")
except ModuleNotFoundError:
    #page.console_msg("Installing PyTorch with CUDA 1.17")
    print("Installing PyTorch 2.0.1 with CUDA 1.18")
    run_sp("pip install -U --force-reinstall torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118", realtime=False)
    #pip install --pre torch torchvision torchaudio --force-reinstall --index-url https://download.pytorch.org/whl/nightly/cu118
    #run_sp("pip install --pre torch torchvision --index-url https://download.pytorch.org/whl/nightly/cu117", realtime=False)
    try:
      import torch
    except ModuleNotFoundError:
      print("Failed Installing Torch Packages...")
      run_sp("pip install -q torch")
      import torch
      pass
    pass
finally:
    torch_device = "cuda" if torch.cuda.is_available() else "cpu"
    if torch_device == "cpu": print("WARNING: CUDA is only available with CPU, so GPU tasks are limited. Can use Stability-API & OpenAI, but not Diffusers...")

try:
    import psutil
except ModuleNotFoundError:
    run_sp("pip install -U psutil", realtime=False)
    import psutil
    pass
import gc
#from torch.amp.autocast_mode import autocast
from random import random
import time

status['cpu_memory'] = psutil.virtual_memory().total / (1024 * 1024 * 1024)
try:
    status['gpu_memory'] = torch.cuda.get_device_properties(0).total_memory / (1024 * 1024 * 1024)
except Exception:
    status['gpu_memory'] = "N/A"
    pass
pb = ProgressBar(width=420, bar_height=8)
total_steps = args['steps']
def callback_fn(step: int, timestep: int, latents: torch.FloatTensor) -> None:
    callback_fn.has_been_called = True
    global total_steps, pb
    if total_steps is None: total_steps = timestep
    if total_steps == 0: total_steps = len(latents)
    multiplier = 1
    if prefs['scheduler_mode'].startswith("Heun") or prefs['scheduler_mode'].startswith("K-DPM"):
      multiplier = 2
    percent = (step +1)/ (total_steps * multiplier)
    pb.value = percent
    pb.tooltip = f"[{step +1} / {total_steps * multiplier}] (Timestep: {timestep})"
    #print(f"step: {step}, total: {total_steps}, latent: {len(latents)}")
    #if step == 0:
        #latents = latents.detach().cpu().numpy()
        #assert latents.shape == (1, 4, 64, 64)
        #latents_slice = latents[0, -3:, -3:, -1]
        #expected_slice = np.array([1.8285, 1.2857, -0.1024, 1.2406, -2.3068, 1.0747, -0.0818, -0.6520, -2.9506])
        #assert np.abs(latents_slice.flatten() - expected_slice).max() < 1e-3
    pb.update()

def optimize_pipe(p, vae=False, unet=False, no_cpu=False, vae_tiling=False, to_gpu=True, tome=True, torch_compile=True, model_offload=False):
    global prefs, status
    if model_offload:
      p.enable_model_cpu_offload()
    if prefs['memory_optimization'] == 'Attention Slicing':
      #if not model['name'].startswith('Stable Diffusion v2'): #TEMP hack until it updates my git with fix
      if prefs['sequential_cpu_offload'] and not no_cpu:
        p.enable_attention_slicing(1)
      else:
        p.enable_attention_slicing()
    elif prefs['memory_optimization'] == 'Xformers Mem Efficient Attention' and status['installed_xformers']:
      #p.set_use_memory_efficient_attention_xformers(True)
      p.enable_xformers_memory_efficient_attention()
    elif prefs['memory_optimization'] == 'Xformers Mem Efficient Attention':
      p.enable_attention_slicing()
    if prefs['vae_slicing'] and vae:
      p.enable_vae_slicing()
    if prefs['vae_tiling'] and vae_tiling:
      p.enable_vae_tiling()
    if unet:
      p.unet = torch.compile(p.unet)
    if prefs['use_LoRA_model']:
      lora = get_LoRA_model(prefs['LoRA_model'])
      if bool(lora['weight_name']):
        p.load_lora_weights(lora['path'], weight_name=lora['weight_name'])
      else:
        p.load_lora_weights(lora['path'])
      #TODO: , weight_name=lora_filename
      #p.unet.load_attn_procs(lora['path'])
    if prefs['sequential_cpu_offload'] and not no_cpu:
      p.enable_sequential_cpu_offload()
    else:
      if to_gpu and not (prefs['enable_torch_compile'] and torch_compile) and not model_offload:
        p = p.to(torch_device)
    if prefs['enable_torch_compile'] and torch_compile:
      p.unet.to(memory_format=torch.channels_last)
      p.unet = torch.compile(p.unet, mode="reduce-overhead", fullgraph=True)
    if prefs['enable_tome'] and tome:
      try:
        import tomesd
      except Exception:
        run_sp("pip install tomesd", realtime=False)
        import tomesd
        pass
      tomesd.apply_patch(p, ratio=prefs['tome_ratio'])
    status['loaded_scheduler'] = prefs['scheduler_mode']
    status['loaded_model'] = get_model(prefs['model_ckpt'])['path']
    return p

def optimize_SDXL(p, vae=False, no_cpu=False, vae_tiling=True, torch_compile=True, model_offload=False):
    global prefs, status
    low_ram = int(status['cpu_memory']) <= 12
    to_gpu = True
    if model_offload and not low_ram:
      p.enable_model_cpu_offload()
      to_gpu = False
    elif prefs['sequential_cpu_offload'] and not no_cpu:
      p.enable_sequential_cpu_offload()
      to_gpu = False
    if prefs['memory_optimization'] == 'Xformers Mem Efficient Attention' and status['installed_xformers']:
      #p.set_use_memory_efficient_attention_xformers(True)
      p.enable_xformers_memory_efficient_attention()
    if prefs['vae_slicing'] and vae:
      p.enable_vae_slicing()
    if prefs['vae_tiling'] and vae_tiling:
      p.enable_vae_tiling()
    if prefs['use_LoRA_model']:
      lora = get_LoRA_model(prefs['LoRA_model'])
      if bool(lora['weight_name']):
        p.load_lora_weights(lora['path'], weight_name=lora['weight_name'])
      else:
        p.load_lora_weights(lora['path'])
      #p.unet.load_attn_procs(lora['path'])
    #if to_gpu and not (prefs['enable_torch_compile'] and torch_compile) and not model_offload:
    if prefs['enable_torch_compile'] and torch_compile:
      #p.unet.to(memory_format=torch.channels_last)
      p.unet = torch.compile(p.unet, mode="reduce-overhead", fullgraph=True)
    if to_gpu:
      p = p.to(torch_device)
    p = pipeline_scheduler(p)
    status['loaded_scheduler'] = prefs['scheduler_mode']
    status['loaded_model'] = "stabilityai/stable-diffusion-xl-base-1.0" #get_model(prefs['model_ckpt'])['path']
    p.set_progress_bar_config(disable=True)
    return p

def install_xformers(page):
    ''' No longer needed, they finally updated to make it easier'''
    run_process("pip install -U --pre triton", page=page)
    from subprocess import getoutput

    s = getoutput('nvidia-smi')
    if 'T4' in s:
      gpu = 'T4'
    elif 'P100' in s:
      gpu = 'P100'
    elif 'V100' in s:
      gpu = 'V100'
    elif 'A100' in s:
      gpu = 'A100'
    if not (gpu=='T4'or gpu=='P100'or gpu=='V100'or gpu=='A100'):
      alert_msg(page, "Xformers Error: It seems that your GPU is not supported at the moment")
      return False
    print(f"Installing Xformers for {gpu}")
    if (gpu=='T4'):
      run_process("pip install -q https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/T4/xformers-0.0.13.dev0-py3-none-any.whl", page=page, show=True)
    elif (gpu=='P100'):
      run_process("pip install -q https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/P100/xformers-0.0.13.dev0-py3-none-any.whl", page=page)
    elif (gpu=='V100'):
      run_process("pip install -q https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/V100/xformers-0.0.13.dev0-py3-none-any.whl", page=page)
    elif (gpu=='A100'):
      run_process("pip install -q https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/A100/xformers-0.0.13.dev0-py3-none-any.whl", page=page)
    return True

def get_text2image(page):
    os.chdir(root_dir)
    global pipe, unet, scheduler, prefs, model
    def open_url(e):
      page.launch_url(e.data)
    '''if pipe is not None:
        #print("Clearing the ol' pipe first...")
        del pipe
        flush()
        pipe = None'''
    try:
      if use_custom_scheduler: # Not really using anymore, maybe later
        from transformers import CLIPTextModel, CLIPTokenizer
        from diffusers import AutoencoderKL, UNet2DConditionModel
        # 1. Load the autoencoder model which will be used to decode the latents into image space.
        vae = AutoencoderKL.from_pretrained(model_path, subfolder="vae", use_auth_token=True)
        # 2. Load the tokenizer and text encoder to tokenize and encode the text.
        tokenizer = CLIPTokenizer.from_pretrained("openai/clip-vit-large-patch14")
        text_encoder = CLIPTextModel.from_pretrained("openai/clip-vit-large-patch14")
        if prefs['higher_vram_mode']:
          unet = UNet2DConditionModel.from_pretrained(model_path, subfolder="unet", use_auth_token=True, device_map="auto")
        else:
          unet = UNet2DConditionModel.from_pretrained(model_path, revision="fp16", torch_dtype=torch.float16, subfolder="unet", use_auth_token=True, device_map="auto")
        vae = vae.to(torch_device)
        text_encoder = text_encoder.to(torch_device)
        #if enable_attention_slicing:
        #  unet.enable_attention_slicing() #slice_size
        unet = unet.to(torch_device)
      else:
        #if status['finetuned_model']: pipe = get_txt2img_pipe()
        #else:
        pipe = get_lpw_pipe()
    except EnvironmentError as e:
      model = get_model(prefs['model_ckpt'])
      model_url = f"https://huggingface.co/{model['path']}"
      alert_msg(page, f'ERROR: Looks like you need to accept the HuggingFace {model["name"]} Model Cards to use Checkpoint',
                content=Column([Markdown(f'[{model_url}]({model_url})', on_tap_link=open_url), Text(str(e)), Text(str(traceback.format_exc()).strip(), selectable=True)]))

# I thought it's what I wanted, but current implementation does same as mine but doesn't clear memory between
def get_mega_pipe():
  global pipe, scheduler, model_path, prefs
  from diffusers import DiffusionPipeline
  from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker

  if prefs['higher_vram_mode']:
    pipe = DiffusionPipeline.from_pretrained(model_path, custom_pipeline="stable_diffusion_mega", safety_checker=None if prefs['disable_nsfw_filter'] else StableDiffusionSafetyChecker.from_pretrained("CompVis/stable-diffusion-safety-checker"))
    #pipe = StableDiffusionPipeline.from_pretrained(model_path, scheduler=scheduler, safety_checker=None if prefs['disable_nsfw_filter'] else StableDiffusionSafetyChecker.from_pretrained("CompVis/stable-diffusion-safety-checker"))
  else:
    pipe = DiffusionPipeline.from_pretrained(model_path, custom_pipeline="stable_diffusion_mega", revision="fp16", torch_dtype=torch.float16, safety_checker=None if prefs['disable_nsfw_filter'] else StableDiffusionSafetyChecker.from_pretrained("CompVis/stable-diffusion-safety-checker"))
    #pipe = StableDiffusionPipeline.from_pretrained(model_path, scheduler=scheduler, revision="fp16", torch_dtype=torch.float16, safety_checker=None if prefs['disable_nsfw_filter'] else StableDiffusionSafetyChecker.from_pretrained("CompVis/stable-diffusion-safety-checker"))
  #pipe = pipe.to(torch_device)
  pipe = pipeline_scheduler(pipe)
  pipe = optimize_pipe(pipe)
  pipe.set_progress_bar_config(disable=True)
  return pipe

def get_lpw_pipe():
  global pipe, scheduler, model_path, prefs
  from diffusers import DiffusionPipeline
  from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker
  from diffusers import AutoencoderKL, UNet2DConditionModel
  model = get_model(prefs['model_ckpt'])
  os.chdir(root_dir)
  #if not os.path.isfile(os.path.join(root_dir, 'lpw_stable_diffusion.py')):
  #  run_sp("wget -q --show-progress --no-cache --backups=1 https://raw.githubusercontent.com/Skquark/diffusers/main/examples/community/lpw_stable_diffusion.py")
  #from lpw_stable_diffusion import StableDiffusionLongPromptWeightingPipeline
  if pipe is not None:
    if model['path'] != status['loaded_model']:
      #clear_txt2img_pipe()
      clear_pipes()
    elif prefs['scheduler_mode'] != status['loaded_scheduler']:
      pipe = pipeline_scheduler(pipe)
      return pipe
    else:
      return pipe
  if 'revision' in model:
    pipe = DiffusionPipeline.from_pretrained(model_path, custom_pipeline="AlanB/lpw_stable_diffusion_update", cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, revision=model['revision'], torch_dtype=torch.float16, **safety)
  else:
    if 'vae' in model:
      from diffusers import AutoencoderKL, UNet2DConditionModel
      vae = AutoencoderKL.from_pretrained(model_path, subfolder="vae", torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32)
      unet = UNet2DConditionModel.from_pretrained(model_path, subfolder="unet", torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32)
      pipe = DiffusionPipeline.from_pretrained(model_path, custom_pipeline="AlanB/lpw_stable_diffusion_update", vae=vae, unet=unet, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32, **safety)
    else:
      if 'from_ckpt' in model:
        pipe = DiffusionPipeline.from_single_file(model_path, custom_pipeline="AlanB/lpw_stable_diffusion_update", cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32, **safety)
      else:
        pipe = DiffusionPipeline.from_pretrained(model_path, custom_pipeline="AlanB/lpw_stable_diffusion_update", cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32, **safety)
    #pipe = DiffusionPipeline.from_pretrained(model_path, community="lpw_stable_diffusion", scheduler=scheduler, revision="fp16", torch_dtype=torch.float16, safety_checker=None if prefs['disable_nsfw_filter'] else StableDiffusionSafetyChecker.from_pretrained("CompVis/stable-diffusion-safety-checker"))
  #if prefs['enable_attention_slicing']: pipe.enable_attention_slicing()
  #pipe = pipe.to(torch_device)
  pipe = pipeline_scheduler(pipe)
  pipe = optimize_pipe(pipe, vae=True)
  pipe.set_progress_bar_config(disable=True)
  return pipe

def get_txt2img_pipe():
  global pipe, scheduler, model_path, prefs, status
  from diffusers import StableDiffusionPipeline
  from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker
  #from diffusers import AutoencoderKL, UNet2DConditionModel
  #if status['finetuned_model']:
  #  vae = AutoencoderKL.from_pretrained(model_path, subfolder="vae", torch_dtype=torch.float16)
  #  unet = UNet2DConditionModel.from_pretrained(model_path, subfolder="unet", torch_dtype=torch.float16)
  #pipe = optimize_pipe(pipe, vae=True)
  #pipe.set_progress_bar_config(disable=True)
  #pipe = pipe.to(torch_device)
  pipe = get_lpw_pipe()
  return pipe

def get_compel_pipe():
  global pipe, scheduler, model_path, prefs
  from diffusers import DiffusionPipeline
  from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker
  from diffusers import AutoencoderKL, UNet2DConditionModel
  model = get_model(prefs['model_ckpt'])
  os.chdir(root_dir)
  #if not os.path.isfile(os.path.join(root_dir, 'lpw_stable_diffusion.py')):
  #  run_sp("wget -q --show-progress --no-cache --backups=1 https://raw.githubusercontent.com/Skquark/diffusers/main/examples/community/lpw_stable_diffusion.py")
  #from lpw_stable_diffusion import StableDiffusionLongPromptWeightingPipeline
  if pipe is not None:
    if model['path'] != status['loaded_model']:
      #clear_txt2img_pipe()
      clear_pipes()
    elif prefs['scheduler_mode'] != status['loaded_scheduler']:
      pipe = pipeline_scheduler(pipe)
      return pipe
    else:
      return pipe
  if 'revision' in model:
    pipe = DiffusionPipeline.from_pretrained(model_path, custom_pipeline="AlanB/lpw_stable_diffusion_update", cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, revision=model['revision'], torch_dtype=torch.float16, **safety)
  else:
    if 'vae' in model:
      from diffusers import AutoencoderKL, UNet2DConditionModel
      vae = AutoencoderKL.from_pretrained(model_path, subfolder="vae", torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32)
      unet = UNet2DConditionModel.from_pretrained(model_path, subfolder="unet", torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32)
      pipe = DiffusionPipeline.from_pretrained(model_path, custom_pipeline="AlanB/lpw_stable_diffusion_update", vae=vae, unet=unet, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32, **safety)
    else:
      if 'from_ckpt' in model:
        pipe = DiffusionPipeline.from_single_file(model_path, custom_pipeline="AlanB/lpw_stable_diffusion_update", cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32, **safety)
      else:
        pipe = DiffusionPipeline.from_pretrained(model_path, custom_pipeline="AlanB/lpw_stable_diffusion_update", cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32, **safety)
    #pipe = DiffusionPipeline.from_pretrained(model_path, community="lpw_stable_diffusion", scheduler=scheduler, revision="fp16", torch_dtype=torch.float16, safety_checker=None if prefs['disable_nsfw_filter'] else StableDiffusionSafetyChecker.from_pretrained("CompVis/stable-diffusion-safety-checker"))
  #if prefs['enable_attention_slicing']: pipe.enable_attention_slicing()
  #pipe = pipe.to(torch_device)
  pipe = pipeline_scheduler(pipe)
  pipe = optimize_pipe(pipe, vae=True)
  pipe.set_progress_bar_config(disable=True)
  return pipe

def get_unet_pipe():
  global unet, scheduler, model_path, prefs
  from transformers import CLIPTextModel, CLIPTokenizer
  from diffusers import AutoencoderKL, UNet2DConditionModel
  from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker
  # 1. Load the autoencoder model which will be used to decode the latents into image space.
  vae = AutoencoderKL.from_pretrained(model_path, subfolder="vae")
  # 2. Load the tokenizer and text encoder to tokenize and encode the text.
  tokenizer = CLIPTokenizer.from_pretrained("openai/clip-vit-large-patch14")
  text_encoder = CLIPTextModel.from_pretrained("openai/clip-vit-large-patch14")
  if prefs['higher_vram_mode']:
    unet = UNet2DConditionModel.from_pretrained(model_path, subfolder="unet", **safety, device_map="auto")
  else:
    unet = UNet2DConditionModel.from_pretrained(model_path, revision="fp16", torch_dtype=torch.float16, subfolder="unet", **safety, device_map="auto")
  vae = vae.to(torch_device)
  text_encoder = text_encoder.to(torch_device)
  #if enable_attention_slicing:
  #  unet.enable_attention_slicing() #slice_size
  unet = unet.to(torch_device)
  return unet

def get_interpolation(page):
    from diffusers import DDIMScheduler, PNDMScheduler, LMSDiscreteScheduler
    import torch, gc
    global pipe_interpolation
    torch_device = "cuda" if torch.cuda.is_available() else "cpu"
    if pipe_interpolation is not None:
      #print("Clearing the ol' pipe first...")
      del pipe_interpolation
      flush()
      pipe_interpolation = None

    pipe_interpolation = get_interpolation_pipe()
    run_process("pip install watchdog -q", page=page, realtime=False)
    status['loaded_interpolation'] = True

def get_interpolation_pipe():
    global pipe_interpolation, scheduler, model_path, prefs
    from diffusers import StableDiffusionPipeline
    from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker
    os.chdir(root_dir)
    if not os.path.isfile(os.path.join(root_dir, 'clip_guided_stable_diffusion.py')):
      run_sp("wget -q --show-progress --no-cache --backups=1 https://raw.githubusercontent.com/Skquark/diffusers/main/examples/community/interpolate_stable_diffusion.py")
    from interpolate_stable_diffusion import StableDiffusionWalkPipeline
    model = get_model(prefs['model_ckpt'])
    if pipe_interpolation is not None:
      if model['path'] != status['loaded_model']:
        clear_interpolation_pipe()
      elif prefs['scheduler_mode'] != status['loaded_scheduler']:
        pipe_interpolation = pipeline_scheduler(pipe_interpolation)
        return pipe_interpolation
      else:
        return pipe_interpolation
    if 'revision' in model:
      pipe_interpolation = StableDiffusionWalkPipeline.from_pretrained(model_path, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, revision=model['revision'], torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32, **safety)
    else:
      pipe_interpolation = StableDiffusionWalkPipeline.from_pretrained(model_path, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32, **safety)
    #pipe = StableDiffusionPipeline.from_pretrained(model_path, scheduler=scheduler, revision="fp16", torch_dtype=torch.float16, safety_checker=None if prefs['disable_nsfw_filter'] else StableDiffusionSafetyChecker.from_pretrained("CompVis/stable-diffusion-safety-checker"))
    #pipe_interpolation = pipe_interpolation.to(torch_device)
    pipe_interpolation = pipeline_scheduler(pipe_interpolation)
    pipe_interpolation = optimize_pipe(pipe_interpolation)
    pipe_interpolation.set_progress_bar_config(disable=True)
    return pipe_interpolation

def get_image2image(page):
    from diffusers import StableDiffusionInpaintPipeline, DDIMScheduler, PNDMScheduler, LMSDiscreteScheduler
    import torch, gc
    global pipe_img2img
    def open_url(e):
      page.launch_url(e.data)
    torch_device = "cuda" if torch.cuda.is_available() else "cpu"
    if pipe_img2img is not None:
      if model['path'] != status['loaded_model']:
        clear_img2img_pipe()
      elif prefs['scheduler_mode'] != status['loaded_scheduler']:
        pipe_img2img = pipeline_scheduler(pipe_img2img)
        return pipe_img2img
      else:
        return pipe_img2img
    try:
      pipe_img2img = get_img2img_pipe()
    except EnvironmentError:
      model_url = f"https://huggingface.co/{inpaint_model}"
      alert_msg(page, f'ERROR: Looks like you need to accept the HuggingFace Inpainting Model Card to use Checkpoint',
                content=Markdown(f'[{model_url}]({model_url})', on_tap_link=open_url))
    loaded_img2img = True

def get_img2img_pipe():
  global pipe_img2img, scheduler, model_path, inpaint_model, prefs, callback_fn
  from diffusers import DiffusionPipeline
  from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker
  if pipe_img2img is not None:
      if prefs['scheduler_mode'] != status['loaded_scheduler']:
        pipe_img2img = pipeline_scheduler(pipe_img2img)
        return pipe_img2img
      else:
        return pipe_img2img
  if prefs['higher_vram_mode']:
    pipe_img2img = DiffusionPipeline.from_pretrained(
        inpaint_model,
        custom_pipeline="img2img_inpainting",
        #scheduler=model_scheduler(inpaint_model),
        cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,
        **safety
    )
  else:
      pipe_img2img = DiffusionPipeline.from_pretrained(
      inpaint_model,
      custom_pipeline="img2img_inpainting",
      #scheduler=model_scheduler(inpaint_model),
      cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,
      revision="fp16",
      torch_dtype=torch.float16,
      **safety)
  #pipe_img2img.to(torch_device)
  #if prefs['enable_attention_slicing']: pipe_img2img.enable_attention_slicing() #slice_size
  pipe_img2img = pipeline_scheduler(pipe_img2img)
  pipe_img2img = optimize_pipe(pipe_img2img, vae=True)
  pipe_img2img.set_progress_bar_config(disable=True)
  #def dummy(images, **kwargs): return images, False
  #pipe_img2img.safety_checker = dummy
  return pipe_img2img

def get_imagic(page):
    global pipe_imagic
    pipe_imagic = get_imagic_pipe()

def get_imagic_pipe():
  global pipe_imagic, scheduler, model_path, prefs
  from diffusers import DiffusionPipeline#, DDIMScheduler
  from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker
  #ddim = DDIMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule="scaled_linear", clip_sample=False, set_alpha_to_one=False)
  #if prefs['higher_vram_mode']:
  if pipe_imagic is not None:
      if model_path != status['loaded_model']:
        clear_imagic_pipe()
      elif prefs['scheduler_mode'] != status['loaded_scheduler']:
        pipe_imagic = pipeline_scheduler(pipe_imagic)
        return pipe_imagic
      else:
        return pipe_imagic
  if True:
    pipe_imagic = DiffusionPipeline.from_pretrained(model_path, custom_pipeline="AlanB/imagic_stable_diffusion_mod", use_auth_token=True, **safety)
  else:
    pipe_imagic = DiffusionPipeline.from_pretrained(model_path, custom_pipeline="AlanB/imagic_stable_diffusion_mod", revision="fp16", torch_dtype=torch.float16, **safety)
  #pipe_imagic = pipe_imagic.to(torch_device)
  def dummy(images, **kwargs):
    return images, False
  if prefs['disable_nsfw_filter']:
    pipe_imagic.safety_checker = dummy
  pipe_imagic = pipeline_scheduler(pipe_imagic, big3=True)
  pipe_imagic = optimize_pipe(pipe_imagic)
  #pipe_imagic.set_progress_bar_config(disable=True)
  return pipe_imagic

def get_composable(page):
    global pipe_composable
    pipe_composable = get_composable_pipe()

def get_composable_pipe():
  global pipe_composable, scheduler, model_path, prefs
  from diffusers import DiffusionPipeline
  from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker
  if pipe_composable is not None:
      if model_path != status['loaded_model']:
        clear_composable_pipe()
      elif prefs['scheduler_mode'] != status['loaded_scheduler']:
        pipe_composable = pipeline_scheduler(pipe_composable, big3=True)
        return pipe_composable
      else:
        return pipe_composable
  #if prefs['higher_vram_mode']:
  if True:
    pipe_composable = DiffusionPipeline.from_pretrained(model_path, custom_pipeline="AlanB/composable_stable_diffusion_mod", use_auth_token=True, feature_extractor=None, safety_checker=None)
  else:
    pipe_composable = DiffusionPipeline.from_pretrained(model_path, custom_pipeline="AlanB/composable_stable_diffusion_mod", revision="fp16", torch_dtype=torch.float16, feature_extractor=None, safety_checker=None)
  #pipe_composable = pipe_composable.to(torch_device)
  def dummy(images, **kwargs):
    return images, False
  if prefs['disable_nsfw_filter']:
    pipe_composable.safety_checker = dummy
  pipe_composable = pipeline_scheduler(pipe_composable, big3=True)
  pipe_composable = optimize_pipe(pipe_composable)
  #pipe_composable.set_progress_bar_config(disable=True)
  return pipe_composable

def get_SDXL(page):
    global pipe_SDXL
    def open_url(e):
      page.launch_url(e.data)
    try:
      pipe_SDXL = get_SDXL_pipe()
      return True
    except Exception as er:
      model_url = f"https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0"
      alert_msg(page, f'ERROR: Looks like you need to accept the HuggingFace Stable Diffusion XL Model Card to use Checkpoint',
                content=Markdown(f'[{model_url}]({model_url})<br>{er}', on_tap_link=open_url))
      return False

def get_SDXL_pipe(task="text2image"):
  global pipe_SDXL, pipe_SDXL_refiner, prefs, status, compel_base, compel_refiner
  from diffusers import StableDiffusionXLPipeline, StableDiffusionXLImg2ImgPipeline, AutoencoderKL # , AutoencoderTiny
  from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker
  try:
      from imwatermark import WatermarkEncoder
  except ModuleNotFoundError:
      run_sp("pip install --no-deps invisible-watermark>=0.2.0", realtime=False)
      pass
  if prefs['SDXL_compel']:
      try:
          from compel import Compel
      except ModuleNotFoundError:
          run_sp("pip install --upgrade compel", realtime=False)
          pass
  model_id = "stabilityai/stable-diffusion-xl-base-1.0"
  refiner_id = "stabilityai/stable-diffusion-xl-refiner-1.0"
  if task != status['loaded_SDXL']:
      clear_pipes()
  watermark = False
  low_ram = int(status['cpu_memory']) <= 12
  #vae = AutoencoderTiny.from_pretrained("madebyollin/taesdxl", torch_dtype=torch.float16)
  vae = AutoencoderKL.from_pretrained("madebyollin/sdxl-vae-fp16-fix" if not prefs['higher_vram_mode'] else "stabilityai/sdxl-vae", torch_dtype=torch.float16, force_upcast=False)
  if task == "text2image":
      status['loaded_SDXL'] = task
      if pipe_SDXL is not None:
          if prefs['scheduler_mode'] != status['loaded_scheduler']:
            pipe_SDXL = pipeline_scheduler(pipe_SDXL)
            pipe_SDXL_refiner = pipeline_scheduler(pipe_SDXL_refiner)
          return pipe_SDXL
      pipe_SDXL = StableDiffusionXLPipeline.from_pretrained(
          model_id,
          variant="fp16",
          torch_dtype=torch.float16,# if not prefs['higher_vram_mode'] else torch.float32,
          vae=vae,
          use_safetensors=True,
          add_watermarker=watermark,
          cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,
          **safety,
      )
      pipe_SDXL = optimize_SDXL(pipe_SDXL)
      pipe_SDXL_refiner = StableDiffusionXLImg2ImgPipeline.from_pretrained(refiner_id, torch_dtype=torch.float16, use_safetensors=True, variant="fp16",
          text_encoder_2=pipe_SDXL.text_encoder_2,
          vae=pipe_SDXL.vae,
          add_watermarker=watermark,
          cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,
          **safety,
      )
      pipe_SDXL_refiner = optimize_SDXL(pipe_SDXL_refiner)

  elif task == "image2image":
      status['loaded_SDXL'] = task
      if pipe_SDXL is not None:
          if prefs['scheduler_mode'] != status['loaded_scheduler']:
            pipe_SDXL = pipeline_scheduler(pipe_SDXL)
            pipe_SDXL_refiner = pipeline_scheduler(pipe_SDXL_refiner)
            return pipe_SDXL
          else:
            return pipe_SDXL
      pipe_SDXL = StableDiffusionXLImg2ImgPipeline.from_pretrained(
          model_id, torch_dtype=torch.float16, variant="fp16", use_safetensors=True,
          vae=vae,
          add_watermarker=watermark,
          cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,
          **safety,
      )
      pipe_SDXL = optimize_SDXL(pipe_SDXL)
      pipe_SDXL_refiner = StableDiffusionXLImg2ImgPipeline.from_pretrained(
          refiner_id, torch_dtype=torch.float16, variant="fp16", use_safetensors=True,
          vae=pipe_SDXL.vae,
          add_watermarker=watermark,
          cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,
          **safety,
      )
      pipe_SDXL_refiner = optimize_SDXL(pipe_SDXL_refiner)

  elif task == "inpainting":
      status['loaded_SDXL'] = task
      from diffusers import StableDiffusionXLInpaintPipeline
      if pipe_SDXL is not None:
          if prefs['scheduler_mode'] != status['loaded_scheduler']:
            pipe_SDXL = pipeline_scheduler(pipe_SDXL)
            pipe_SDXL_refiner = pipeline_scheduler(pipe_SDXL_refiner)
            return pipe_SDXL
          else:
            return pipe_SDXL
      pipe_SDXL = StableDiffusionXLInpaintPipeline.from_pretrained(
          model_id, torch_dtype=torch.float16, variant="fp16", use_safetensors=True,
          vae=vae,
          add_watermarker=watermark,
          cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,
          **safety,
      )
      pipe_SDXL = optimize_SDXL(pipe_SDXL)

      pipe_SDXL_refiner = StableDiffusionXLInpaintPipeline.from_pretrained(
          refiner_id,
          text_encoder_2=pipe_SDXL.text_encoder_2,
          vae=pipe_SDXL.vae,
          torch_dtype=torch.float16,
          use_safetensors=True,
          variant="fp16",
          add_watermarker=watermark,
          cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,
          **safety,
      )
      pipe_SDXL_refiner = optimize_SDXL(pipe_SDXL_refiner)
  if prefs['SDXL_compel']:
      from compel import Compel, ReturnedEmbeddingsType
      compel_base = Compel(tokenizer=[pipe_SDXL.tokenizer, pipe_SDXL.tokenizer_2], text_encoder=[pipe_SDXL.text_encoder, pipe_SDXL.text_encoder_2], returned_embeddings_type=ReturnedEmbeddingsType.PENULTIMATE_HIDDEN_STATES_NON_NORMALIZED, requires_pooled=[False, True])
      compel_refiner = Compel(tokenizer=[pipe_SDXL_refiner.tokenizer_2], text_encoder=[pipe_SDXL_refiner.text_encoder_2], returned_embeddings_type=ReturnedEmbeddingsType.PENULTIMATE_HIDDEN_STATES_NON_NORMALIZED, requires_pooled=[True])
      #compel_refiner = Compel(tokenizer=[pipe_SDXL_refiner.tokenizer, pipe_SDXL_refiner.tokenizer_2] , text_encoder=[pipe_SDXL_refiner.text_encoder, pipe_SDXL_refiner.text_encoder_2], returned_embeddings_type=ReturnedEmbeddingsType.PENULTIMATE_HIDDEN_STATES_NON_NORMALIZED, requires_pooled=[False, True])
      #compel_refiner = Compel(tokenizer=pipe_SDXL_refiner.tokenizer_2, text_encoder=pipe_SDXL_refiner.text_encoder_2, returned_embeddings_type=ReturnedEmbeddingsType.PENULTIMATE_HIDDEN_STATES_NON_NORMALIZED, requires_pooled=[False, True])
      #truncate_long_prompts=True,
  return pipe_SDXL

def get_versatile(page):
    import torch, gc
    global pipe_versatile_text2img
    def open_url(e):
      page.launch_url(e.data)
    try:
      pipe_versatile_text2img = get_versatile_text2img_pipe()
    except Exception as er:
      model_url = f"https://huggingface.co/shi-labs/versatile-diffusion"
      alert_msg(page, f'ERROR: Looks like you need to accept the HuggingFace Versatile Diffusion Model Card to use Checkpoint',
                content=Markdown(f'[{model_url}]({model_url})<br>{er}', on_tap_link=open_url))

def get_versatile_pipe(): # Mega was taking up too much vram and crashing the system
  global pipe_versatile, scheduler, model_path, prefs
  from diffusers import VersatileDiffusionPipeline
  from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker
  model_id = "shi-labs/versatile-diffusion"
  if pipe_composable is not None:
      if prefs['scheduler_mode'] != status['loaded_scheduler']:
        pipe_composable = pipeline_scheduler(pipe_composable)
        return pipe_composable
      else:
        return pipe_composable
  pipe_versatile = VersatileDiffusionPipeline.from_pretrained(
      model_id,
      #scheduler=model_scheduler(model_id),
      cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,
      #revision="fp16",
      torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32,
      **safety
  )
  #pipe_versatile.to(torch_device)
  pipe_versatile = pipeline_scheduler(pipe_versatile)
  pipe_versatile = optimize_pipe(pipe_versatile)
  pipe_versatile.set_progress_bar_config(disable=True)
  return pipe_versatile

def get_versatile_text2img_pipe():
  global pipe_versatile_text2img, scheduler, model_path, prefs
  from diffusers import VersatileDiffusionTextToImagePipeline
  from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker
  model_id = "shi-labs/versatile-diffusion"
  if pipe_versatile_text2img is not None:
      if prefs['scheduler_mode'] != status['loaded_scheduler']:
        pipe_versatile_text2img = pipeline_scheduler(pipe_versatile_text2img)
        return pipe_versatile_text2img
      else:
        return pipe_versatile_text2img
  pipe_versatile_text2img = VersatileDiffusionTextToImagePipeline.from_pretrained(
      model_id,
      #scheduler=model_scheduler(model_id),
      cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,
      #revision="fp16",
      torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32,
      **safety
  )
  #pipe_versatile_text2img.to(torch_device)
  pipe_versatile_text2img = pipeline_scheduler(pipe_versatile_text2img)
  pipe_versatile_text2img = optimize_pipe(pipe_versatile_text2img)
  pipe_versatile_text2img.set_progress_bar_config(disable=True)
  return pipe_versatile_text2img

def get_versatile_variation_pipe():
  global pipe_versatile_variation, scheduler, model_path, prefs
  from diffusers import VersatileDiffusionImageVariationPipeline
  from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker
  model_id = "shi-labs/versatile-diffusion"
  if pipe_versatile_variation is not None:
      if prefs['scheduler_mode'] != status['loaded_scheduler']:
        pipe_versatile_variation = pipeline_scheduler(pipe_versatile_variation)
        return pipe_versatile_variation
      else:
        return pipe_versatile_variation
  if prefs['higher_vram_mode']:
    pipe_versatile_variation = VersatileDiffusionImageVariationPipeline.from_pretrained(
        model_id,
        #scheduler=model_scheduler(model_id),
        cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,
        **safety
    )
  else:
    pipe_versatile_variation = VersatileDiffusionImageVariationPipeline.from_pretrained(
        model_id,
        #scheduler=model_scheduler(model_id),
        cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,
        #revision="fp16",
        torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32,
        **safety
    )
  #pipe_versatile_variation.to(torch_device)
  pipe_versatile_variation = pipeline_scheduler(pipe_versatile_variation)
  pipe_versatile_variation = optimize_pipe(pipe_versatile_variation)
  pipe_versatile_variation.set_progress_bar_config(disable=True)
  return pipe_versatile_variation

def get_versatile_dualguided_pipe():
  global pipe_versatile_dualguided, scheduler, model_path, prefs
  from diffusers import VersatileDiffusionDualGuidedPipeline
  from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker
  model_id = "shi-labs/versatile-diffusion"
  if pipe_versatile_dualguided is not None:
      if prefs['scheduler_mode'] != status['loaded_scheduler']:
        pipe_versatile_dualguided = pipeline_scheduler(pipe_versatile_dualguided)
        return pipe_versatile_dualguided
      else:
        return pipe_versatile_dualguided
  if prefs['higher_vram_mode']:
    pipe_versatile_dualguided = VersatileDiffusionDualGuidedPipeline.from_pretrained(
        model_id,
        #scheduler=model_scheduler(model_id),
        cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,
        **safety
    )
  else:
    pipe_versatile_dualguided = VersatileDiffusionDualGuidedPipeline.from_pretrained(
        model_id,
        #scheduler=model_scheduler(model_id),
        cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,
        #revision="fp16",
        torch_dtype=torch.float16,
        **safety
    )
  #pipe_versatile_dualguided.to(torch_device)
  pipe_versatile_dualguided = pipeline_scheduler(pipe_versatile_dualguided)
  pipe_versatile_dualguided = optimize_pipe(pipe_versatile_dualguided)
  pipe_versatile_dualguided.set_progress_bar_config(disable=True)
  return pipe_versatile_dualguided

def get_safe(page):
    import torch, gc
    global pipe_safe
    def open_url(e):
      page.launch_url(e.data)
    try:
      pipe_safe = get_safe_pipe()
    except Exception as er:
      model_url = f"https://huggingface.co/AIML-TUDA/stable-diffusion-safe"
      alert_msg(page, f'ERROR: Looks like you need to accept the HuggingFace Safe Model Card to use Checkpoint. Reinstall after accepting TOS.',
                content=Markdown(f'[{model_url}]({model_url})<br>{er}', on_tap_link=open_url))

def get_safe_pipe():
  global pipe_safe, scheduler, model_path, prefs, callback_fn
  from diffusers import StableDiffusionPipelineSafe
  from diffusers.pipelines.stable_diffusion_safe import StableDiffusionPipelineSafe
  #from diffusers.pipelines.safety_checker import SafeStableDiffusionPipelineSafe
  #from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker
  model_id = "AIML-TUDA/stable-diffusion-safe"
  if pipe_safe is not None:
      if prefs['scheduler_mode'] != status['loaded_scheduler']:
        pipe_safe = pipeline_scheduler(pipe_safe)
        return pipe_safe
      else:
        return pipe_safe
  #if prefs['higher_vram_mode']:
  if True:
    pipe_safe = StableDiffusionPipelineSafe.from_pretrained(
        model_id,
        #scheduler=model_scheduler(model_id),
        cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,
        **safety,
    )
  else:
      pipe_safe = StableDiffusionPipelineSafe.from_pretrained(
        model_id,
        #scheduler=model_scheduler(model_id),
        cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,
        revision="fp16",
        torch_dtype=torch.float16,
        **safety
      )
  #pipe_safe.to(torch_device)
  pipe_safe = pipeline_scheduler(pipe_safe)
  pipe_safe = optimize_pipe(pipe_safe)
  pipe_safe.set_progress_bar_config(disable=True)
  return pipe_safe

def get_SAG(page):
  global pipe_SAG
  #clear_SAG_pipe()
  pipe_SAG = get_SAG_pipe()

def get_SAG_pipe():
  global pipe_SAG, scheduler, model_path, prefs
  from diffusers import StableDiffusionSAGPipeline
  from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker
  if pipe_SAG is not None:
      if model_path != status['loaded_model']:
        clear_SAG_pipe()
      elif prefs['scheduler_mode'] != status['loaded_scheduler']:
        pipe_SAG = pipeline_scheduler(pipe_SAG, big3=True)
        return pipe_SAG
      else:
        return pipe_SAG
  if prefs['higher_vram_mode']:
    pipe_SAG = StableDiffusionSAGPipeline.from_pretrained(
        model_path,
        #scheduler=model_scheduler(model_path, big3=True),
        cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,
        **safety
    )
  else:
      pipe_SAG = StableDiffusionSAGPipeline.from_pretrained(
      model_path,
      #scheduler=model_scheduler(model_path, big3=True),
      cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,
      torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32,
      **safety)
  pipe_SAG = pipeline_scheduler(pipe_SAG, big3=True)
  pipe_SAG = optimize_pipe(pipe_SAG, vae=False)
  pipe_SAG.set_progress_bar_config(disable=True)
  return pipe_SAG

def get_attend_and_excite(page):
  global pipe_attend_and_excite
  #clear_attend_and_excite_pipe()
  pipe_attend_and_excite = get_attend_and_excite_pipe()

def get_attend_and_excite_pipe():
  global pipe_attend_and_excite, scheduler, model_path, prefs
  from diffusers import StableDiffusionAttendAndExcitePipeline
  from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker
  if pipe_attend_and_excite is not None:
      if model_path != status['loaded_model']:
        clear_attend_and_excite_pipe()
      elif prefs['scheduler_mode'] != status['loaded_scheduler']:
        pipe_attend_and_excite = pipeline_scheduler(pipe_attend_and_excite)
        return pipe_attend_and_excite
      else:
        return pipe_attend_and_excite
  if prefs['higher_vram_mode']:
    pipe_attend_and_excite = StableDiffusionAttendAndExcitePipeline.from_pretrained(
        model_path,
        #scheduler=model_scheduler(model_path),
        cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,
        **safety
    )
  else:
      pipe_attend_and_excite = StableDiffusionAttendAndExcitePipeline.from_pretrained(
      model_path,
      #scheduler=model_scheduler(model_path),
      cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,
      torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32,
      **safety)
  pipe_attend_and_excite = pipeline_scheduler(pipe_attend_and_excite)
  pipe_attend_and_excite = optimize_pipe(pipe_attend_and_excite, vae=True)
  pipe_attend_and_excite.set_progress_bar_config(disable=True)
  return pipe_attend_and_excite

def get_panorama(page):
  global pipe_panorama
  #clear_panorama_pipe()
  pipe_panorama = get_panorama_pipe()

def get_panorama_pipe():
  global pipe_panorama, scheduler, model_path, prefs
  from diffusers import StableDiffusionPanoramaPipeline
  from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker
  if pipe_panorama is not None:
      if model_path != status['loaded_model']:
        clear_panorama_pipe()
      elif prefs['scheduler_mode'] != status['loaded_scheduler']:
        pipe_panorama = pipeline_scheduler(pipe_panorama)
        return pipe_panorama
      else:
        return pipe_panorama
  if prefs['higher_vram_mode']:
    pipe_panorama = StableDiffusionPanoramaPipeline.from_pretrained(
        model_path,
        #scheduler=model_scheduler(model_path, big3=True),
        cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,
        **safety
    )
  else:
      pipe_panorama = StableDiffusionPanoramaPipeline.from_pretrained(
      model_path,
      #scheduler=model_scheduler(model_path, big3=True),
      cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,
      torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32,
      **safety)
  pipe_panorama = pipeline_scheduler(pipe_panorama)
  pipe_panorama = optimize_pipe(pipe_panorama, vae=True)
  pipe_panorama.set_progress_bar_config(disable=True)
  return pipe_panorama

def get_upscale(page):
    import torch, gc
    global pipe_upscale
    def open_url(e):
      page.launch_url(e.data)
    if pipe_upscale is None:
      try:
        pipe_upscale = get_upscale_pipe()
      except Exception as er:
        model_url = f"https://huggingface.co/{model_path}"
        alert_msg(page, f'ERROR: Looks like you need to accept the HuggingFace Upscale Model Card to use Checkpoint',
                  content=Markdown(f'[{model_url}]({model_url})<br>{er}', on_tap_link=open_url))

def get_upscale_pipe():
  global pipe_upscale, scheduler, prefs
  from diffusers import StableDiffusionUpscalePipeline
  model_id = "stabilityai/stable-diffusion-x4-upscaler"
  if pipe_upscale is not None:
      if prefs['scheduler_mode'] != status['loaded_scheduler']:
        pipe_upscale = pipeline_scheduler(pipe_upscale, big3=True)
        return pipe_upscale
      else:
        return pipe_upscale
  if prefs['higher_vram_mode']:
    pipe_upscale = StableDiffusionUpscalePipeline.from_pretrained(
        model_id,
        #scheduler=model_scheduler(model_id, big3=True),
        cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,
        #safety_checker=None if prefs['disable_nsfw_filter'] else StableDiffusionSafetyChecker.from_pretrained("CompVis/stable-diffusion-safety-checker"),
    )
  else:
    pipe_upscale = StableDiffusionUpscalePipeline.from_pretrained(
      model_id,
      #scheduler=model_scheduler(model_id, big3=True),
      cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,
      revision="fp16",
      torch_dtype=torch.float16,
      #safety_checker=None if prefs['disable_nsfw_filter'] else StableDiffusionSafetyChecker.from_pretrained("CompVis/stable-diffusion-safety-checker")
    )
  #pipe_upscale.to(torch_device)
  pipe_upscale = pipeline_scheduler(pipe_upscale, big3=True)
  pipe_upscale = optimize_pipe(pipe_upscale)
  pipe_upscale.set_progress_bar_config(disable=True)
  return pipe_upscale

def get_clip(page):
    global pipe_clip_guided, model_path
    pipe_clip_guided = get_clip_guided_pipe()

def get_clip_guided_pipe():
    global pipe_clip_guided, scheduler_clip, prefs, model_path
    from diffusers import DiffusionPipeline
    from diffusers import LMSDiscreteScheduler, PNDMScheduler, StableDiffusionPipeline
    from transformers import CLIPModel, CLIPFeatureExtractor #, CLIPGuidedStableDiffusion
    if pipe_clip_guided is not None:
      if model_path != status['loaded_model']:
        clear_clip_guided_pipe()
      elif prefs['scheduler_mode'] != status['loaded_scheduler']:
        pipe_clip_guided = pipeline_scheduler(pipe_clip_guided, big3=True)
        return pipe_clip_guided
      else:
        return pipe_clip_guided
    #if isinstance(scheduler, LMSDiscreteScheduler) or isinstance(scheduler, PNDMScheduler):
    #  scheduler_clip = scheduler
    #else:
    #  scheduler_clip = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule="scaled_linear")
    model = get_model(prefs['model_ckpt'])

    clip_model = CLIPModel.from_pretrained(prefs['clip_model_id'], torch_dtype=torch.float16)
    feature_extractor = CLIPFeatureExtractor.from_pretrained(prefs['clip_model_id'])

    if 'revision' in model:
      pipe_clip_guided = DiffusionPipeline.from_pretrained(
              model_path,
              custom_pipeline="AlanB/clip_guided_stable_diffusion_mod",
              clip_model=clip_model,
              feature_extractor=feature_extractor,
              #scheduler=model_scheduler(model_path, big3=True),
              cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,
              safety_checker=None,
              torch_dtype=torch.float16,
              revision=model['revision'],
              #device_map="auto",
          )
    else:
      pipe_clip_guided = DiffusionPipeline.from_pretrained(model_path, custom_pipeline="AlanB/clip_guided_stable_diffusion_mod", clip_model=clip_model, feature_extractor=feature_extractor, safety_checker=None, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, torch_dtype=torch.float16)
    #pipe_clip_guided = pipe_clip_guided.to(torch_device)
    '''
    pipe_clip_guided = CLIPGuidedStableDiffusion(
        unet=pipeline.unet,
        vae=pipeline.vae,
        tokenizer=pipeline.tokenizer,
        text_encoder=pipeline.text_encoder,
        scheduler=scheduler_clip,
        clip_model=clip_model,
        feature_extractor=feature_extractor,
    )'''
    pipe_clip_guided = pipeline_scheduler(pipe_clip_guided, big3=True)
    pipe_clip_guided = optimize_pipe(pipe_clip_guided)
    return pipe_clip_guided

def get_clip_guided_img2img_pipe():
    global pipe_clip_guided, scheduler_clip, prefs, model_path
    from diffusers import DiffusionPipeline
    from diffusers import LMSDiscreteScheduler, PNDMScheduler, StableDiffusionPipeline
    from transformers import CLIPModel, CLIPFeatureExtractor #, CLIPGuidedStableDiffusion
    if pipe_clip_guided is not None:
      if model_path != status['loaded_model']:
        clear_clip_guided_pipe()
      elif prefs['scheduler_mode'] != status['loaded_scheduler']:
        pipe_clip_guided = pipeline_scheduler(pipe_clip_guided, big3=True)
        return pipe_clip_guided
      else:
        return pipe_clip_guided
    #if isinstance(scheduler, LMSDiscreteScheduler) or isinstance(scheduler, PNDMScheduler):
    #  scheduler_clip = scheduler
    #else:
    #  scheduler_clip = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule="scaled_linear")
    model = get_model(prefs['model_ckpt'])

    clip_model = CLIPModel.from_pretrained(prefs['clip_model_id'], torch_dtype=torch.float16)
    feature_extractor = CLIPFeatureExtractor.from_pretrained(prefs['clip_model_id'])

    if 'revision' in model:
      pipe_clip_guided = DiffusionPipeline.from_pretrained(
              model_path,
              custom_pipeline="AlanB/clip_guided_stable_diffusion_mod",
              clip_model=clip_model,
              feature_extractor=feature_extractor,
              #scheduler=model_scheduler(model_path, big3=True),
              cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,
              safety_checker=None,
              torch_dtype=torch.float16,
              revision=model['revision'],
              #device_map="auto",
          )
    else:
      pipe_clip_guided = DiffusionPipeline.from_pretrained(model_path, custom_pipeline="AlanB/clip_guided_stable_diffusion_mod", clip_model=clip_model, feature_extractor=feature_extractor, safety_checker=None, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, torch_dtype=torch.float16)
    #pipe_clip_guided = pipe_clip_guided.to(torch_device)
    '''
    pipe_clip_guided = CLIPGuidedStableDiffusion(
        unet=pipeline.unet,
        vae=pipeline.vae,
        tokenizer=pipeline.tokenizer,
        text_encoder=pipeline.text_encoder,
        scheduler=scheduler_clip,
        clip_model=clip_model,
        feature_extractor=feature_extractor,
    )'''
    pipe_clip_guided = pipeline_scheduler(pipe_clip_guided, big3=True)
    pipe_clip_guided = optimize_pipe(pipe_clip_guided)
    return pipe_clip_guided

def get_repaint(page):
    global pipe_repaint
    pipe_repaint = get_repaint_pipe()

def get_repaint_pipe():
    global pipe_repaint
    from diffusers import UNet2DModel, RePaintScheduler, RePaintPipeline
    #model = get_model(prefs['model_ckpt'])
    #model_path = model['path']
    model_id = "google/ddpm-ema-celebahq-256"
    unet = UNet2DModel.from_pretrained(model_id)
    repaint_scheduler = RePaintScheduler.from_pretrained(model_id)
    pipe_repaint = RePaintPipeline(unet=unet, scheduler=repaint_scheduler).to(torch_device)
    return pipe_repaint

def get_depth2img(page):
  global pipe_depth
  pipe_depth = get_depth_pipe()

def get_depth_pipe():
  global pipe_depth, prefs
  from diffusers import StableDiffusionDepth2ImgPipeline
  model_id = "stabilityai/stable-diffusion-2-depth"
  if pipe_depth is not None:
      if prefs['scheduler_mode'] != status['loaded_scheduler']:
        pipe_depth = pipeline_scheduler(pipe_depth)
        return pipe_depth
      else:
        return pipe_depth
  if prefs['higher_vram_mode']:
    pipe_depth = StableDiffusionDepth2ImgPipeline.from_pretrained(
        model_id,
        #scheduler=model_scheduler(model_id),
        cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,
    )
  else:
    pipe_depth = StableDiffusionDepth2ImgPipeline.from_pretrained(
        model_id,
        #scheduler=model_scheduler(model_id),
        cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,
        revision="fp16",
        torch_dtype=torch.float16,
    )
  #pipe_depth.to(torch_device)
  pipe_depth = pipeline_scheduler(pipe_depth)
  pipe_depth = optimize_pipe(pipe_depth)
  pipe_depth.set_progress_bar_config(disable=True)
  return pipe_depth

def get_alt_diffusion(page):
    global pipe_alt_diffusion
    run_process("pip install -q sentencepiece", page=page)
    pipe_alt_diffusion = get_alt_diffusion_pipe()

def get_alt_diffusion_pipe():
    global pipe_alt_diffusion
    from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker
    from diffusers import AltDiffusionPipeline, StableDiffusionPipeline
    #from diffusers.pipelines.alt_diffusion.modeling_roberta_series import (RobertaSeriesConfig, RobertaSeriesModelWithTransformation)
    model_id = "BAAI/AltDiffusion-m9"
    if pipe_alt_diffusion is not None:
      if prefs['scheduler_mode'] != status['loaded_scheduler']:
        pipe_alt_diffusion = pipeline_scheduler(pipe_alt_diffusion)
        return pipe_alt_diffusion
      else:
        return pipe_alt_diffusion
    if prefs['higher_vram_mode']:
      pipe_alt_diffusion = StableDiffusionPipeline.from_pretrained(
          model_id,
          #scheduler=model_scheduler(model_id),
          cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,
          **safety
      )
    else:
      pipe_alt_diffusion = StableDiffusionPipeline.from_pretrained(
          model_id,
          #scheduler=model_scheduler(model_id),
          cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,
          torch_dtype=torch.float16,
          **safety
      )
    #pipe_alt_diffusion.to(torch_device)
    pipe_alt_diffusion = pipeline_scheduler(pipe_alt_diffusion)
    pipe_alt_diffusion = optimize_pipe(pipe_alt_diffusion)
    pipe_alt_diffusion.set_progress_bar_config(disable=True)
    return pipe_alt_diffusion

def get_alt_diffusion_img2img(page):
    global pipe_alt_diffusion_img2img
    pipe_alt_diffusion_img2img = get_alt_diffusion_img2img_pipe()

def get_alt_diffusion_img2img_pipe():
    global pipe_alt_diffusion_img2img
    from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker
    from diffusers import AltDiffusionImg2ImgPipeline, StableDiffusionImg2ImgPipeline
    model_id = "BAAI/AltDiffusion-m9"
    if pipe_alt_diffusion_img2img is not None:
      if prefs['scheduler_mode'] != status['loaded_scheduler']:
        pipe_alt_diffusion_img2img = pipeline_scheduler(pipe_alt_diffusion_img2img)
        return pipe_alt_diffusion_img2img
      else:
        return pipe_alt_diffusion_img2img
    if prefs['higher_vram_mode']:
      pipe_alt_diffusion_img2img = StableDiffusionImg2ImgPipeline.from_pretrained(
          model_id,
          #scheduler=model_scheduler(model_id),
          cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,
          **safety
      )
    else:
      pipe_alt_diffusion_img2img = StableDiffusionImg2ImgPipeline.from_pretrained(
          model_id,
          #scheduler=model_scheduler(model_id),
          cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,
          torch_dtype=torch.float16,
          **safety
      )
    #pipe_alt_diffusion_img2img.to(torch_device)
    pipe_alt_diffusion_img2im = pipeline_scheduler(pipe_alt_diffusion_img2im)
    pipe_alt_diffusion_img2img = optimize_pipe(pipe_alt_diffusion_img2img)
    pipe_alt_diffusion_img2img.set_progress_bar_config(disable=True)
    return pipe_alt_diffusion_img2img

def postprocess_latent(pipe, latent):
    vae_output = pipe.vae.decode(
        latent.images / pipe.vae.config.scaling_factor, return_dict=False
    )[0].detach()
    return pipe.image_processor.postprocess(vae_output, output_type="pil")[0]

SD_sampler = None
def get_stability(page):
    global prefs, SD_sampler#, stability_api
    '''try:
      from stability_sdk import client
      import stability_sdk.interfaces.gooseai.generation.generation_pb2 as generation
    except ImportError as e:
      run_process("pip install stability-sdk -q", page=page)
      from stability_sdk import client
      import stability_sdk.interfaces.gooseai.generation.generation_pb2 as generation
      pass
    stability_api = client.StabilityInference(
        key=prefs['Stability_api_key'],
        verbose=True,
        engine=prefs['model_checkpoint']# if prefs['model_checkpoint'] == "stable-diffusion-v1-5" else "stable-diffusion-v1",
    )
    SD_sampler = client.get_sampler_from_str(prefs['generation_sampler'].lower())'''
    # New way, other is obsolete
    import requests
    api_host = os.getenv('API_HOST', 'https://api.stability.ai')
    stability_url = f"{api_host}/v1/engines/list" #user/account"
    response = requests.get(stability_url, headers={"Authorization": prefs['Stability_api_key']})
    if response.status_code != 200:
      alert_msg(page, "ERROR with Stability-ai: " + str(response.text))
      return
    payload = response.json()
    print(str(payload))
    status['installed_stability'] = True

'''
def update_stability():
    global SD_sampler, stability_api
    from stability_sdk import client
    stability_api = client.StabilityInference(
        key=prefs['Stability_api_key'],
        verbose=True,
        engine=prefs['model_checkpoint']
    )
    SD_sampler = client.get_sampler_from_str(prefs['generation_sampler'].lower())
'''
def get_AIHorde(page):
    global prefs, status
    import requests
    api_host = os.getenv('API_HOST', 'https://stablehorde.net/api/')
    horde_url = f"{api_host}/v2/find_user" #user/account"
    response = requests.get(horde_url, headers={"apikey": prefs['AIHorde_api_key'], 'accept': 'application/json'})
    if response.status_code != 200:
      alert_msg(page, "ERROR {response.status_code} with AIHorde Authentication", content=Text(str(response.text)))
      return
    payload = response.json()
    print(str(payload))
    AI_Horde = os.path.join(dist_dir, "AI-Horde-CLI")
    if not os.path.exists(AI_Horde) or force_updates:
      run_sp("git clone https://github.com/db0/AI-Horde-CLI.git", cwd=dist_dir, realtime=False)
      run_sp("pip install -r cli_requirements.txt --user", cwd=AI_Horde, realtime=False)
    try:
      import yaml
    except ModuleNotFoundError:
      run_sp("pip install pyyaml", realtime=False)
      pass
    status['installed_AIHorde'] = True

def get_ESRGAN(page):
    os.chdir(dist_dir)
    run_process(f"git clone https://github.com/xinntao/Real-ESRGAN.git -q", page=page, cwd=dist_dir)
    os.chdir(os.path.join(dist_dir, 'Real-ESRGAN'))
    run_process("pip install basicsr --quiet", page=page, cwd=os.path.join(dist_dir, 'Real-ESRGAN'))
    run_process("pip install facexlib --quiet", page=page, cwd=os.path.join(dist_dir, 'Real-ESRGAN'))
    run_process("pip install gfpgan --quiet", page=page, cwd=os.path.join(dist_dir, 'Real-ESRGAN'))
    run_process(f"pip install -r requirements.txt --quiet", page=page, realtime=False, cwd=os.path.join(dist_dir, 'Real-ESRGAN'))
    run_process(f"python setup.py develop --quiet", page=page, realtime=False, cwd=os.path.join(dist_dir, 'Real-ESRGAN'))
    run_process(f"wget https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.5.0/realesr-general-x4v3.pth -P experiments/pretrained_models --quiet", page=page, cwd=os.path.join(dist_dir, 'Real-ESRGAN'))
    #run_process(f"wget https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/RealESRGAN_x4plus.pth -P experiments/pretrained_models --quiet", page=page, cwd=os.path.join(dist_dir, 'Real-ESRGAN'))
    os.chdir(root_dir)


concepts = [{'name': 'cat-toy', 'token': 'cat-toy'}, {'name': 'madhubani-art', 'token': 'madhubani-art'}, {'name': 'birb-style', 'token': 'birb-style'}, {'name': 'indian-watercolor-portraits', 'token': 'watercolor-portrait'}, {'name': 'xyz', 'token': 'xyz'}, {'name': 'poolrooms', 'token': 'poolrooms'}, {'name': 'cheburashka', 'token': 'cheburashka'}, {'name': 'hours-style', 'token': 'hours'}, {'name': 'turtlepics', 'token': 'henry-leonardi'}, {'name': 'karl-s-lzx-1', 'token': 'lzx'}, {'name': 'canary-cap', 'token': 'canary-cap'}, {'name': 'ti-junglepunk-v0', 'token': 'jungle-punk'}, {'name': 'mafalda-character', 'token': 'mafalda-quino'}, {'name': 'magic-pengel', 'token': 'magic-pengel'}, {'name': 'schloss-mosigkau', 'token': 'ralph'}, {'name': 'cubex', 'token': 'cube'}, {'name': 'covid-19-rapid-test', 'token': 'covid-test'}, {'name': 'character-pingu', 'token': 'character-pingu'}, {'name': '2814-roth', 'token': '2814Roth'}, {'name': 'vkuoo1', 'token': 'style-vkuoo1'}, {'name': 'ina-art', 'token': ''}, {'name': 'monte-novo', 'token': 'monte novo cutting board'}, {'name': 'interchanges', 'token': 'xchg'}, {'name': 'walter-wick-photography', 'token': 'walter-wick'}, {'name': 'arcane-style-jv', 'token': 'arcane-style-jv'}, {'name': 'w3u', 'token': 'w3u'}, {'name': 'smiling-friend-style', 'token': 'smilingfriends-cartoon'}, {'name': 'dr-livesey', 'token': 'dr-livesey'}, {'name': 'monster-girl', 'token': 'monster-girl'}, {'name': 'abstract-concepts', 'token': 'art-style'}, {'name': 'reeducation-camp', 'token': 'reeducation-camp'}, {'name': 'miko-3-robot', 'token': 'miko-3'}, {'name': 'party-girl', 'token': 'party-girl'}, {'name': 'dicoo', 'token': 'Dicoo'}, {'name': 'kuvshinov', 'token': 'kuvshinov'}, {'name': 'mass', 'token': 'mass'}, {'name': 'ldr', 'token': 'ldr'}, {'name': 'hub-city', 'token': 'HubCity'}, {'name': 'masyunya', 'token': 'masyunya'}, {'name': 'david-moreno-architecture', 'token': 'dm-arch'}, {'name': 'lolo', 'token': 'lolo'}, {'name': 'apulian-rooster-v0-1', 'token': 'apulian-rooster-v0.1'}, {'name': 'fractal', 'token': 'fractal'}, {'name': 'nebula', 'token': 'nebula'}, {'name': 'ldrs', 'token': 'ldrs'}, {'name': 'art-brut', 'token': 'art-brut'}, {'name': 'malika-favre-art-style', 'token': 'malika-favre'}, {'name': 'line-art', 'token': 'line-art'}, {'name': 'shrunken-head', 'token': 'shrunken-head'}, {'name': 'bonzi-monkey', 'token': 'bonzi'}, {'name': 'herge-style', 'token': 'herge'}, {'name': 'johnny-silverhand', 'token': 'johnny-silverhand'}, {'name': 'linnopoke', 'token': 'linnopoke'}, {'name': 'koko-dog', 'token': 'koko-dog'}, {'name': 'stuffed-penguin-toy', 'token': 'pengu-toy'}, {'name': 'monster-toy', 'token': 'monster-toy'}, {'name': 'dong-ho', 'token': 'dong-ho'}, {'name': 'orangejacket', 'token': 'orangejacket'}, {'name': 'fergal-cat', 'token': 'fergal-cat'}, {'name': 'summie-style', 'token': 'summie-style'}, {'name': 'chonkfrog', 'token': 'chonkfrog'}, {'name': 'alberto-mielgo', 'token': 'street'}, {'name': 'lucky-luke', 'token': 'lucky-luke'}, {'name': 'zdenek-art', 'token': 'zdenek-artwork'}, {'name': 'star-tours-posters', 'token': 'star-tours'}, {'name': 'huang-guang-jian', 'token': 'huang-guang-jian'}, {'name': 'painting', 'token': 'will'}, {'name': 'line-style', 'token': 'line-style'}, {'name': 'venice', 'token': 'venice'}, {'name': 'russian', 'token': 'Russian'}, {'name': 'tony-diterlizzi-s-planescape-art', 'token': 'tony-diterlizzi-planescape'}, {'name': 'moeb-style', 'token': 'moe-bius'}, {'name': 'amine', 'token': 'ayna'}, {'name': 'kojima-ayami', 'token': 'KOJIMA'}, {'name': 'dong-ho2', 'token': 'dong-ho-2'}, {'name': 'ruan-jia', 'token': 'ruan-jia'}, {'name': 'purplefishli', 'token': 'purplefishli'}, {'name': 'cry-baby-style', 'token': 'cry-baby'}, {'name': 'between2-mt-fade', 'token': 'b2MTfade'}, {'name': 'mtl-longsky', 'token': 'mtl-longsky'}, {'name': 'scrap-style', 'token': 'style-chewie'}, {'name': 'tela-lenca', 'token': 'tela-lenca'}, {'name': 'zillertal-can', 'token': 'zillertal-ipa'}, {'name': 'shu-doll', 'token': 'shu-doll'}, {'name': 'eastward', 'token': 'eastward'}, {'name': 'chuck-walton', 'token': 'Chuck_Walton'}, {'name': 'chucky', 'token': 'merc'}, {'name': 'smw-map', 'token': 'smw-map'}, {'name': 'erwin-olaf-style', 'token': 'erwin-olaf'}, {'name': 'maurice-quentin-de-la-tour-style', 'token': 'maurice'}, {'name': 'dan-seagrave-art-style', 'token': 'dan-seagrave'}, {'name': 'drive-scorpion-jacket', 'token': 'drive-scorpion-jacket'}, {'name': 'dark-penguin-pinguinanimations', 'token': 'darkpenguin-robot'}, {'name': 'rd-paintings', 'token': 'rd-painting'}, {'name': 'borderlands', 'token': 'borderlands'}, {'name': 'depthmap', 'token': 'depthmap'}, {'name': 'lego-astronaut', 'token': 'lego-astronaut'}, {'name': 'transmutation-circles', 'token': 'tcircle'}, {'name': 'mycat', 'token': 'mycat'}, {'name': 'ilya-shkipin', 'token': 'ilya-shkipin-style'}, {'name': 'moxxi', 'token': 'moxxi'}, {'name': 'riker-doll', 'token': 'rikerdoll'}, {'name': 'apex-wingman', 'token': 'wingman-apex'}, {'name': 'naf', 'token': 'nal'}, {'name': 'handstand', 'token': 'handstand'}, {'name': 'vb-mox', 'token': 'vb-mox'}, {'name': 'pixel-toy', 'token': 'pixel-toy'}, {'name': 'olli-olli', 'token': 'olli-olli'}, {'name': 'floral', 'token': 'ntry not foun'}, {'name': 'minecraft-concept-art', 'token': 'concept'}, {'name': 'yb-anime', 'token': 'anime-character'}, {'name': 'ditko', 'token': 'cat-toy'}, {'name': 'disquieting-muses', 'token': 'muses'}, {'name': 'ned-flanders', 'token': 'flanders'}, {'name': 'fluid-acrylic-jellyfish-creatures-style-of-carl-ingram-art', 'token': 'jelly-core'}, {'name': 'ic0n', 'token': 'ic0n'}, {'name': 'pyramidheadcosplay', 'token': 'Cos-Pyramid'}, {'name': 'phc', 'token': 'Cos-Pyramid'}, {'name': 'og-mox-style', 'token': 'og-mox-style'}, {'name': 'klance', 'token': 'klance'}, {'name': 'john-blanche', 'token': 'john-blanche'}, {'name': 'cowboy', 'token': 'cowboyStyle'}, {'name': 'darkpenguinanimatronic', 'token': 'penguin-robot'}, {'name': 'doener-red-line-art', 'token': 'dnr'}, {'name': 'style-of-marc-allante', 'token': 'Marc_Allante'}, {'name': 'crybaby-style-2-0', 'token': 'crybaby2'}, {'name': 'werebloops', 'token': 'werebloops'}, {'name': 'xbh', 'token': 'xbh'}, {'name': 'unfinished-building', 'token': 'unfinished-building'}, {'name': 'teelip-ir-landscape', 'token': 'teelip-ir-landscape'}, {'name': 'road-to-ruin', 'token': 'RtoR'}, {'name': 'piotr-jablonski', 'token': 'piotr-jablonski'}, {'name': 'jamiels', 'token': 'jamiels'}, {'name': 'tomcat', 'token': 'tom-cat'}, {'name': 'meyoco', 'token': 'meyoco'}, {'name': 'nixeu', 'token': 'nixeu'}, {'name': 'tnj', 'token': 'tnj'}, {'name': 'cute-bear', 'token': 'cute-bear'}, {'name': 'leica', 'token': 'leica'}, {'name': 'anime-boy', 'token': 'myAItestShota'}, {'name': 'garfield-pizza-plush', 'token': 'garfield-plushy'}, {'name': 'design', 'token': 'design'}, {'name': 'mikako-method', 'token': 'm-m'}, {'name': 'cornell-box', 'token': 'cornell-box'}, {'name': 'sculptural-style', 'token': 'diaosu'}, {'name': 'aavegotchi', 'token': 'aave-gotchi'}, {'name': 'swamp-choe-2', 'token': 'cat-toy'}, {'name': 'super-nintendo-cartridge', 'token': 'snesfita-object'}, {'name': 'garfield-pizza-plush-v2', 'token': 'garfield-plushy'}, {'name': 'rickyart', 'token': 'RickyArt'}, {'name': 'eye-of-agamotto', 'token': 'eye-aga'}, {'name': 'freddy-fazbear', 'token': 'freddy-fazbear'}, {'name': 'glass-pipe', 'token': 'glass-sherlock'}, {'name': 'black-waifu', 'token': 'black-waifu'}, {'name': 'roy-lichtenstein', 'token': 'roy-lichtenstein'}, {'name': 'ugly-sonic', 'token': 'ugly-sonic'}, {'name': 'glow-forest', 'token': 'dark-forest'}, {'name': 'painted-student', 'token': 'painted_student'}, {'name': 'salmonid', 'token': 'salmonid'}, {'name': 'huayecai820-greyscale', 'token': 'huayecaigreyscale-style'}, {'name': 'arthur1', 'token': 'arthur1'}, {'name': 'huckleberry', 'token': 'huckleberry'}, {'name': 'collage3', 'token': 'Collage3'}, {'name': 'spritual-monsters', 'token': 'spritual-monsters'}, {'name': 'baldi', 'token': 'baldi'}, {'name': 'tcirle', 'token': 'tcircle'}, {'name': 'pantone-milk', 'token': 'pantone-milk'}, {'name': 'retropixelart-pinguin', 'token': 'retropixelart-style'}, {'name': 'doose-s-realistic-art-style', 'token': 'doose-realistic'}, {'name': 'grit-toy', 'token': 'grit-toy'}, {'name': 'pink-beast-pastelae-style', 'token': 'pinkbeast'}, {'name': 'mikako-methodi2i', 'token': 'm-mi2i'}, {'name': 'aj-fosik', 'token': 'AJ-Fosik'}, {'name': 'collage-cutouts', 'token': 'collage-cutouts'}, {'name': 'cute-cat', 'token': 'cute-bear'}, {'name': 'kaleido', 'token': 'kaleido'}, {'name': 'xatu', 'token': 'xatu-pokemon'}, {'name': 'a-female-hero-from-the-legend-of-mir', 'token': ' <female-hero> from The Legend of Mi'}, {'name': 'cologne', 'token': 'cologne-dom'}, {'name': 'wlop-style', 'token': 'wlop-style'}, {'name': 'larrette', 'token': 'larrette'}, {'name': 'bert-muppet', 'token': 'bert-muppet'}, {'name': 'my-hero-academia-style', 'token': 'MHA style'}, {'name': 'vcr-classique', 'token': 'vcr_c'}, {'name': 'xatu2', 'token': 'xatu-test'}, {'name': 'tela-lenca2', 'token': 'tela-lenca'}, {'name': 'dragonborn', 'token': 'dragonborn'}, {'name': 'mate', 'token': 'mate'}, {'name': 'alien-avatar', 'token': 'alien-avatar'}, {'name': 'pastelartstyle', 'token': 'Arzy'}, {'name': 'kings-quest-agd', 'token': 'ings-quest-ag'}, {'name': 'doge-pound', 'token': 'doge-pound'}, {'name': 'type', 'token': 'typeface'}, {'name': 'fileteado-porteno', 'token': 'fileteado-porteno'}, {'name': 'bullvbear', 'token': 'bullVBear'}, {'name': 'freefonix-style', 'token': 'Freefonix'}, {'name': 'garcon-the-cat', 'token': 'garcon-the-cat'}, {'name': 'better-collage3', 'token': 'C3'}, {'name': 'metagabe', 'token': 'metagabe'}, {'name': 'ggplot2', 'token': 'ggplot2'}, {'name': 'yoshi', 'token': 'yoshi'}, {'name': 'illustration-style', 'token': 'illustration-style'}, {'name': 'centaur', 'token': 'centaur'}, {'name': 'zoroark', 'token': 'zoroark'}, {'name': 'bad_Hub_Hugh', 'token': 'HubHugh'}, {'name': 'irasutoya', 'token': 'irasutoya'}, {'name': 'liquid-light', 'token': 'lls'}, {'name': 'zaneypixelz', 'token': 'zaneypixelz'}, {'name': 'tubby', 'token': 'tubby'}, {'name': 'atm-ant', 'token': 'atm-ant'}, {'name': 'fang-yuan-001', 'token': 'fang-yuan'}, {'name': 'dullboy-caricature', 'token': 'dullboy-cari'}, {'name': 'bada-club', 'token': 'bada-club'}, {'name': 'zaney', 'token': 'zaney'}, {'name': 'a-tale-of-two-empires', 'token': 'two-empires'}, {'name': 'dabotap', 'token': 'dabotap'}, {'name': 'harley-quinn', 'token': 'harley-quinn'}, {'name': 'vespertine', 'token': 'Vesp'}, {'name': 'ricar', 'token': 'ricard'}, {'name': 'conner-fawcett-style', 'token': 'badbucket'}, {'name': 'ingmar-bergman', 'token': 'ingmar-bergman'}, {'name': 'poutine-dish', 'token': 'poutine-qc'}, {'name': 'shev-linocut', 'token': 'shev-linocut'}, {'name': 'grifter', 'token': 'grifter'}, {'name': 'dog', 'token': 'Winston'}, {'name': 'tangles', 'token': 'cora-tangle'}, {'name': 'lost-rapper', 'token': 'lost-rapper'}, {'name': 'eddie', 'token': 'ddi'}, {'name': 'thunderdome-covers', 'token': 'thunderdome'}, {'name': 'she-mask', 'token': 'she-mask'}, {'name': 'chillpill', 'token': 'Chillpill'}, {'name': 'robertnava', 'token': 'robert-nava'}, {'name': 'looney-anime', 'token': 'looney-anime'}, {'name': 'axe-tattoo', 'token': 'axe-tattoo'}, {'name': 'fireworks-over-water', 'token': 'firework'}, {'name': 'collage14', 'token': 'C14'}, {'name': 'green-tent', 'token': 'green-tent'}, {'name': 'dtv-pkmn', 'token': 'dtv-pkm2'}, {'name': 'crinos-form-garou', 'token': 'crinos'}, {'name': '8bit', 'token': '8bit'}, {'name': 'tubby-cats', 'token': 'tubby'}, {'name': 'travis-bedel', 'token': 'bedelgeuse2'}, {'name': 'uma', 'token': 'uma'}, {'name': 'ie-gravestone', 'token': 'internet-explorer-gravestone'}, {'name': 'colossus', 'token': 'colossus'}, {'name': 'uma-style-classic', 'token': 'uma'}, {'name': 'collage3-hubcity', 'token': 'C3Hub'}, {'name': 'goku', 'token': 'goku'}, {'name': 'galaxy-explorer', 'token': 'galaxy-explorer'}, {'name': 'rl-pkmn-test', 'token': 'rl-pkmn'}, {'name': 'naval-portrait', 'token': 'naval-portrait'}, {'name': 'daycare-attendant-sun-fnaf', 'token': 'biblic-sun-fnaf'}, {'name': 'reksio-dog', 'token': 'reksio-dog'}, {'name': 'breakcore', 'token': 'reakcor'}, {'name': 'junji-ito-artstyle', 'token': 'junji-ito-style'}, {'name': 'gram-tops', 'token': 'gram-tops'}, {'name': 'henjo-techno-show', 'token': 'HENJOTECHNOSHOW'}, {'name': 'trash-polka-artstyle', 'token': 'trash-polka-style'}, {'name': 'faraon-love-shady', 'token': ''}, {'name': 'trigger-studio', 'token': 'Trigger Studio'}, {'name': 'tb303', 'token': '"tb303'}, {'name': 'neon-pastel', 'token': 'neon-pastel'}, {'name': 'fursona', 'token': 'fursona-2'}, {'name': 'sterling-archer', 'token': 'archer-style'}, {'name': 'captain-haddock', 'token': 'captain-haddock'}, {'name': 'my-mug', 'token': 'my-mug'}, {'name': 'joe-whiteford-art-style', 'token': 'joe-whiteford-artstyle'}, {'name': 'on-kawara', 'token': 'on-kawara'}, {'name': 'hours-sentry-fade', 'token': 'Hours_Sentry'}, {'name': 'rektguy', 'token': 'rektguy'}, {'name': 'dyoudim-style', 'token': 'DyoudiM-style'}, {'name': 'kaneoya-sachiko', 'token': 'Kaneoya'}, {'name': 'retro-girl', 'token': 'retro-girl'}, {'name': 'buddha-statue', 'token': 'buddha-statue'}, {'name': 'hitokomoru-style-nao', 'token': 'hitokomoru-style'}, {'name': 'plant-style', 'token': 'plant'}, {'name': 'cham', 'token': 'cham'}, {'name': 'mayor-richard-irvin', 'token': 'Richard_Irvin'}, {'name': 'sd-concepts-library-uma-meme', 'token': 'uma-object-full'}, {'name': 'uma-meme', 'token': 'uma-object-full'}, {'name': 'thunderdome-cover', 'token': 'thunderdome-cover'}, {'name': 'sem-mac2n', 'token': 'SEM_Mac2N'}, {'name': 'hoi4', 'token': 'hoi4'}, {'name': 'hd-emoji', 'token': 'HDemoji-object'}, {'name': 'lumio', 'token': 'lumio'}, {'name': 't-skrang', 'token': 'tskrang'}, {'name': 'agm-style-nao', 'token': 'agm-style'}, {'name': 'uma-meme-style', 'token': 'uma-meme-style'}, {'name': 'retro-mecha-rangers', 'token': 'aesthetic'}, {'name': 'babushork', 'token': 'babushork'}, {'name': 'qpt-atrium', 'token': 'QPT_ATRIUM'}, {'name': 'sushi-pixel', 'token': 'sushi-pixel'}, {'name': 'osrsmini2', 'token': ''}, {'name': 'ttte', 'token': 'ttte-2'}, {'name': 'atm-ant-2', 'token': 'atm-ant'}, {'name': 'dan-mumford', 'token': 'dan-mumford'}, {'name': 'renalla', 'token': 'enall'}, {'name': 'cow-uwu', 'token': 'cow-uwu'}, {'name': 'one-line-drawing', 'token': 'lineart'}, {'name': 'inuyama-muneto-style-nao', 'token': 'inuyama-muneto-style'}, {'name': 'altvent', 'token': 'AltVent'}, {'name': 'accurate-angel', 'token': 'accurate-angel'}, {'name': 'mtg-card', 'token': 'mtg-card'}, {'name': 'ddattender', 'token': 'ddattender'}, {'name': 'thalasin', 'token': 'thalasin-plus'}, {'name': 'moebius', 'token': 'moebius'}, {'name': 'liqwid-aquafarmer', 'token': 'aquafarmer'}, {'name': 'onepunchman', 'token': 'OnePunch'}, {'name': 'kawaii-colors', 'token': 'kawaii-colors-style'}, {'name': 'naruto', 'token': 'Naruto'}, {'name': 'backrooms', 'token': 'Backrooms'}, {'name': 'a-hat-kid', 'token': 'hatintime-kid'}, {'name': 'furrpopasthetic', 'token': 'furpop'}, {'name': 'RINGAO', 'token': ''}, {'name': 'csgo-awp-texture-map', 'token': 'csgo_awp_texture'}, {'name': 'luinv2', 'token': 'luin-waifu'}, {'name': 'hydrasuit', 'token': 'hydrasuit'}, {'name': 'milady', 'token': 'milady'}, {'name': 'ganyu-genshin-impact', 'token': 'ganyu'}, {'name': 'wayne-reynolds-character', 'token': 'warcharport'}, {'name': 'david-firth-artstyle', 'token': 'david-firth-artstyle'}, {'name': 'seraphimmoonshadow-art', 'token': 'seraphimmoonshadow-art'}, {'name': 'osrstiny', 'token': 'osrstiny'}, {'name': 'lugal-ki-en', 'token': 'lugal-ki-en'}, {'name': 'seamless-ground', 'token': 'seamless-ground'}, {'name': 'sewerslvt', 'token': 'ewerslv'}, {'name': 'diaosu-toy', 'token': 'diaosu-toy'}, {'name': 'sakimi-style', 'token': 'sakimi'}, {'name': 'rj-palmer', 'token': 'rj-palmer'}, {'name': 'harmless-ai-house-style-1', 'token': 'bee-style'}, {'name': 'harmless-ai-1', 'token': 'bee-style'}, {'name': 'yerba-mate', 'token': 'yerba-mate'}, {'name': 'bella-goth', 'token': 'bella-goth'}, {'name': 'bobs-burgers', 'token': 'bobs-burgers'}, {'name': 'jamie-hewlett-style', 'token': 'hewlett'}, {'name': 'belen', 'token': 'belen'}, {'name': 'shvoren-style', 'token': 'shvoren-style'}, {'name': 'gymnastics-leotard-v2', 'token': 'gymnastics-leotard2'}, {'name': 'rd-chaos', 'token': 'rd-chaos'}, {'name': 'armor-concept', 'token': 'armor-concept'}, {'name': 'ouroboros', 'token': 'ouroboros'}, {'name': 'm-geo', 'token': 'm-geo'}, {'name': 'Akitsuki', 'token': ''}, {'name': 'uzumaki', 'token': 'NARUTO'}, {'name': 'sorami-style', 'token': 'sorami-style'}, {'name': 'lxj-o4', 'token': 'csp'}, {'name': 'she-hulk-law-art', 'token': 'shehulk-style'}, {'name': 'led-toy', 'token': 'led-toy'}, {'name': 'durer-style', 'token': 'drr-style'}, {'name': 'hiten-style-nao', 'token': 'hiten-style-nao'}, {'name': 'mechasoulall', 'token': 'mechasoulall'}, {'name': 'wish-artist-stile', 'token': 'wish-style'}, {'name': 'max-foley', 'token': 'max-foley'}, {'name': 'loab-style', 'token': 'loab-style'}, {'name': '3d-female-cyborgs', 'token': 'A female cyborg'}, {'name': 'r-crumb-style', 'token': 'rcrumb'}, {'name': 'paul-noir', 'token': 'paul-noir'}, {'name': 'cgdonny1', 'token': 'donny1'}, {'name': 'valorantstyle', 'token': 'valorant'}, {'name': 'loab-character', 'token': 'loab-character'}, {'name': 'Atako', 'token': ''}, {'name': 'threestooges', 'token': 'threestooges'}, {'name': 'dsmuses', 'token': 'DSmuses'}, {'name': 'fish', 'token': 'fish'}, {'name': 'glass-prism-cube', 'token': 'glass-prism-cube'}, {'name': 'elegant-flower', 'token': 'elegant-flower'}, {'name': 'hanfu-anime-style', 'token': 'hanfu-anime-style'}, {'name': 'green-blue-shanshui', 'token': 'green-blue shanshui'}, {'name': 'lizardman', 'token': 'laceholderTokenLizardma'}, {'name': 'rail-scene', 'token': 'rail-pov'}, {'name': 'lula-13', 'token': 'lula-13'}, {'name': 'laala-character', 'token': 'laala'}, {'name': 'margo', 'token': 'dog-margo'}, {'name': 'carrascharacter', 'token': 'Carras'}, {'name': 'vietstoneking', 'token': 'vietstoneking'}, {'name': 'rhizomuse-machine-bionic-sculpture', 'token': ''}, {'name': 'rcrumb-portraits-style', 'token': 'rcrumb-portraits'}, {'name': 'mu-sadr', 'token': '783463b'}, {'name': 'bozo-22', 'token': 'bozo-22'}, {'name': 'skyfalls', 'token': 'SkyFalls'}, {'name': 'zk', 'token': ''}, {'name': 'tudisco', 'token': 'cat-toy'}, {'name': 'kogecha', 'token': 'kogecha'}, {'name': 'ori-toor', 'token': 'ori-toor'}, {'name': 'isabell-schulte-pviii-style', 'token': 'isabell-schulte-p8-style'}, {'name': 'rilakkuma', 'token': 'rilakkuma'}, {'name': 'indiana', 'token': 'indiana'}, {'name': 'black-and-white-design', 'token': 'PM_style'}, {'name': 'isabell-schulte-pviii-1024px-1500-steps-style', 'token': 'isabell-schulte-p8-style-1024p-1500s'}, {'name': 'fold-structure', 'token': 'fold-geo'}, {'name': 'brunnya', 'token': 'Brunnya'}, {'name': 'jos-de-kat', 'token': 'kat-jos'}, {'name': 'singsing-doll', 'token': 'singsing'}, {'name': 'singsing', 'token': 'singsing'}, {'name': 'isabell-schulte-pviii-12tiles-3000steps-style', 'token': 'isabell-schulte-p8-style-12tiles-3000s'}, {'name': 'f-22', 'token': 'f-22'}, {'name': 'jin-kisaragi', 'token': 'jin-kisaragi'}, {'name': 'depthmap-style', 'token': 'depthmap'}, {'name': 'crested-gecko', 'token': 'crested-gecko'}, {'name': 'grisstyle', 'token': 'gris'}, {'name': 'ikea-fabler', 'token': 'ikea-fabler'}, {'name': 'joe-mad', 'token': 'joe-mad'}, {'name': 'boissonnard', 'token': 'boissonnard'}, {'name': 'overprettified', 'token': 'overprettified'}, {'name': 'all-rings-albuns', 'token': 'rings-all-albuns'}, {'name': 'shiny-polyman', 'token': 'shiny-polyman'}, {'name': 'scarlet-witch', 'token': 'sw-mom'}, {'name': 'wojaks-now', 'token': 'red-wojak'}, {'name': 'carasibana', 'token': 'carasibana'}, {'name': 'towerplace', 'token': 'TowerPlace'}, {'name': 'cumbia-peruana', 'token': 'cumbia-peru'}, {'name': 'bloo', 'token': 'owl-guy'}, {'name': 'dog-django', 'token': 'dog-django'}, {'name': 'facadeplace', 'token': 'FacadePlace'}, {'name': 'blue-zombie', 'token': 'blue-zombie'}, {'name': 'blue-zombiee', 'token': 'blue-zombie'}, {'name': 'jinjoon-lee-they', 'token': 'jinjoon_lee_they'}, {'name': 'ralph-mcquarrie', 'token': 'ralph-mcquarrie'}, {'name': 'hiyuki-chan', 'token': 'hiyuki-chan'}, {'name': 'isabell-schulte-pviii-4tiles-6000steps', 'token': 'isabell-schulte-p8-style-4tiles-6000s'}, {'name': 'liliana', 'token': 'liliana'}, {'name': 'morino-hon-style', 'token': 'morino-hon'}, {'name': 'artist-yukiko-kanagai', 'token': 'Yukiko Kanagai '}, {'name': 'wheatland', 'token': ''}, {'name': 'm-geoo', 'token': 'm-geo'}, {'name': 'wheatland-arknight', 'token': 'golden-wheats-fields'}, {'name': 'mokoko', 'token': 'mokoko'}, {'name': '001glitch-core', 'token': '01glitch_cor'}, {'name': 'stardew-valley-pixel-art', 'token': 'pixelart-stardew'}, {'name': 'isabell-schulte-pviii-4tiles-500steps', 'token': 'isabell-schulte-p8-style-4tiles-500s'}, {'name': 'anime-girl', 'token': 'anime-girl'}, {'name': 'heather', 'token': 'eather'}, {'name': 'rail-scene-style', 'token': 'rail-pov'}, {'name': 'quiesel', 'token': 'quiesel'}, {'name': 'matthew-stone', 'token': 'atthew-ston'}, {'name': 'dreamcore', 'token': 'dreamcore'}, {'name': 'pokemon-conquest-sprites', 'token': 'poke-conquest'}, {'name': 'tili-concept', 'token': 'tili'}, {'name': 'nouns-glasses', 'token': 'nouns glasses'}, {'name': 'shigure-ui-style', 'token': 'shigure-ui'}, {'name': 'pen-ink-portraits-bennorthen', 'token': 'ink-portrait-by-BenNorthern'}, {'name': 'nikodim', 'token': 'nikodim'}, {'name': 'ori', 'token': 'Ori'}, {'name': 'anya-forger', 'token': 'anya-forger'}, {'name': 'lavko', 'token': 'lavko'}, {'name': 'fasina', 'token': 'Fasina'}, {'name': 'uma-clean-object', 'token': 'uma-clean-object'}, {'name': 'wojaks-now-now-now', 'token': 'red-wojak'}, {'name': 'memnarch-mtg', 'token': 'mtg-memnarch'}, {'name': 'tonal1', 'token': 'Tonal'}, {'name': 'tesla-bot', 'token': 'tesla-bot'}, {'name': 'red-glasses', 'token': 'red-glasses'}, {'name': 'csgo-awp-object', 'token': 'csgo_awp'}, {'name': 'stretch-re1-robot', 'token': 'stretch'}, {'name': 'isabell-schulte-pv-pvii-3000steps', 'token': 'isabell-schulte-p5-p7-style-3000s'}, {'name': 'insidewhale', 'token': 'InsideWhale'}, {'name': 'noggles', 'token': 'noggles'}, {'name': 'isometric-tile-test', 'token': 'iso-tile'}, {'name': 'bamse-og-kylling', 'token': 'bamse-kylling'}, {'name': 'marbling-art', 'token': 'marbling-art'}, {'name': 'joemad', 'token': 'joemad'}, {'name': 'bamse', 'token': 'bamse'}, {'name': 'dq10-anrushia', 'token': 'anrushia'}, {'name': 'test', 'token': 'AIO'}, {'name': 'naoki-saito', 'token': 'naoki_saito'}, {'name': 'raichu', 'token': 'raichu'}, {'name': 'child-zombie', 'token': 'child-zombie'}, {'name': 'yf21', 'token': 'YF21'}, {'name': 'titan-robot', 'token': 'titan'}, {'name': 'cyberpunk-lucy', 'token': 'cyberpunk-lucy'}, {'name': 'giygas', 'token': 'giygas'}, {'name': 'david-martinez-cyberpunk', 'token': 'david-martinez-cyberpunk'}, {'name': 'phan-s-collage', 'token': 'pcollage'}, {'name': 'jojo-bizzare-adventure-manga-lineart', 'token': 'JoJo_lineart'}, {'name': 'homestuck-sprite', 'token': 'homestuck-sprite'}, {'name': 'kogatan-shiny', 'token': 'ogata'}, {'name': 'moo-moo', 'token': 'moomoo'}, {'name': 'detectivedinosaur1', 'token': 'dd1'}, {'name': 'arcane-face', 'token': 'arcane-face'}, {'name': 'sherhook-painting', 'token': 'sherhook'}, {'name': 'isabell-schulte-pviii-1-image-style', 'token': 'isabell-schulte-p8-1-style'}, {'name': 'dicoo2', 'token': 'dicoo'}, {'name': 'hrgiger-drmacabre', 'token': 'barba'}, {'name': 'babau', 'token': 'babau'}, {'name': 'darkplane', 'token': 'DarkPlane'}, {'name': 'wildkat', 'token': 'wildkat'}, {'name': 'half-life-2-dog', 'token': 'hl-dog'}, {'name': 'outfit-items', 'token': 'outfit-items'}, {'name': 'midjourney-style', 'token': 'midjourney-style'}, {'name': 'puerquis-toy', 'token': 'puerquis'}, {'name': 'maus', 'token': 'Maus'}, {'name': 'jetsetdreamcastcovers', 'token': 'jet'}, {'name': 'karan-gloomy', 'token': 'karan'}, {'name': 'yoji-shinkawa-style', 'token': 'yoji-shinkawa'}, {'name': 'million-live-akane-15k', 'token': 'akane'}, {'name': 'million-live-akane-3k', 'token': 'akane'}, {'name': 'sherhook-painting-v2', 'token': 'sherhook'}, {'name': 'gba-pokemon-sprites', 'token': 'GBA-Poke-Sprites'}, {'name': 'gim', 'token': 'grimes-album-style'}, {'name': 'char-con', 'token': 'char-con'}, {'name': 'bluebey', 'token': 'bluebey'}, {'name': 'homestuck-troll', 'token': 'homestuck-troll'}, {'name': 'million-live-akane-shifuku-3k', 'token': 'akane'}, {'name': 'thegeneral', 'token': 'bobknight'}, {'name': 'million-live-spade-q-object-3k', 'token': 'spade_q'}, {'name': 'million-live-spade-q-style-3k', 'token': 'spade_q'}, {'name': 'ibere-thenorio', 'token': 'ibere-thenorio'}, {'name': 'yinit', 'token': 'init-dropca'}, {'name': 'bee', 'token': 'b-e-e'}, {'name': 'pixel-mania', 'token': 'pixel-mania'}, {'name': 'sunfish', 'token': 'SunFish'}, {'name': 'test2', 'token': 'AIOCARD'}, {'name': 'pool-test', 'token': 'pool_test'}, {'name': 'mokoko-seed', 'token': 'mokoko-seed'}, {'name': 'isabell-schulte-pviii-4-tiles-1-lr-3000-steps-style', 'token': 'isabell-schulte-p8-4tiles-1lr-300s-style'}, {'name': 'ghostproject-men', 'token': 'ghostsproject-style'}, {'name': 'phan', 'token': 'phan'}, {'name': 'chen-1', 'token': 'chen-1'}, {'name': 'bluebey-2', 'token': 'bluebey'}, {'name': 'waterfallshadow', 'token': 'WaterfallShadow'}, {'name': 'chop', 'token': 'Le Petit Prince'}, {'name': 'sintez-ico', 'token': 'sintez-ico'}, {'name': 'carlitos-el-mago', 'token': 'carloscarbonell'}, {'name': 'david-martinez-edgerunners', 'token': 'david-martinez-edgerunners'}, {'name': 'isabell-schulte-pviii-4-tiles-3-lr-5000-steps-style', 'token': 'isabell-schulte-p8-4tiles-3lr-5000s-style'}, {'name': 'guttestreker', 'token': 'guttestreker'}, {'name': 'ransom', 'token': 'ransom'}, {'name': 'museum-by-coop-himmelblau', 'token': 'coop himmelblau museum'}, {'name': 'coop-himmelblau', 'token': 'coop himmelblau'}, {'name': 'yesdelete', 'token': 'yesdelete'}, {'name': 'conway-pirate', 'token': 'conway'}, {'name': 'ilo-kunst', 'token': 'ilo-kunst'}, {'name': 'yilanov2', 'token': 'yilanov'}, {'name': 'dr-strange', 'token': 'dr-strange'}, {'name': 'hubris-oshri', 'token': 'Hubris'}, {'name': 'osaka-jyo', 'token': 'osaka-jyo'}, {'name': 'paolo-bonolis', 'token': 'paolo-bonolis'}, {'name': 'repeat', 'token': 'repeat'}, {'name': 'geggin', 'token': 'geggin'}, {'name': 'lex', 'token': 'lex'}, {'name': 'osaka-jyo2', 'token': 'osaka-jyo2'}, {'name': 'owl-house', 'token': 'owl-house'}, {'name': 'nazuna', 'token': 'nazuna'}, {'name': 'thorneworks', 'token': 'Thorneworks'}, {'name': 'kysa-v-style', 'token': 'kysa-v-style'}, {'name': 'senneca', 'token': 'Senneca'}, {'name': 'zero-suit-samus', 'token': 'zero-suit-samus'}, {'name': 'kanv1', 'token': 'KAN'}, {'name': 'dlooak', 'token': 'dlooak'}, {'name': 'wire-angels', 'token': 'wire-angels'}, {'name': 'mizkif', 'token': 'mizkif'}, {'name': 'brittney-williams-art', 'token': 'Brittney_Williams'}, {'name': 'wheelchair', 'token': 'wheelchair'}, {'name': 'yuji-himukai-style', 'token': 'Yuji Himukai-Style'}, {'name': 'cindlop', 'token': 'cindlop'}, {'name': 'sas-style', 'token': 'smooth-aesthetic-style'}, {'name': 'remert', 'token': 'Remert'}, {'name': 'alex-portugal', 'token': 'alejandro-portugal'}, {'name': 'explosions-cat', 'token': 'explosions-cat'}, {'name': 'onzpo', 'token': 'onzpo'}, {'name': 'eru-chitanda-casual', 'token': 'c-eru-chitanda'}, {'name': 'poring-ragnarok-online', 'token': 'poring-ro'}, {'name': 'cg-bearded-man', 'token': 'LH-Keeper'}, {'name': 'ba-shiroko', 'token': 'shiroko'}, {'name': 'at-wolf-boy-object', 'token': 'AT-Wolf-Boy-Object'}, {'name': 'fairytale', 'token': 'fAIrytale'}, {'name': 'kira-sensei', 'token': 'kira-sensei'}, {'name': 'kawaii-girl-plus-style', 'token': 'kawaii_girl'}, {'name': 'kawaii-girl-plus-object', 'token': 'kawaii_girl'}, {'name': 'boris-anderson', 'token': 'boris-anderson'}, {'name': 'medazzaland', 'token': 'edazzalan'}, {'name': 'duranduran', 'token': 'uranDura'}, {'name': 'crbart', 'token': 'crbart'}, {'name': 'happy-person12345', 'token': 'Happy-Person12345'}, {'name': 'fzk', 'token': 'fzk'}, {'name': 'rishusei-style', 'token': 'crishusei-style'}, {'name': 'felps', 'token': 'Felps'}, {'name': 'plen-ki-mun', 'token': 'plen-ki-mun'}, {'name': 'babs-bunny', 'token': 'babs_bunny'}, {'name': 'james-web-space-telescope', 'token': 'James-Web-Telescope'}, {'name': 'blue-haired-boy', 'token': 'Blue-Haired-Boy'}, {'name': '80s-anime-ai', 'token': '80s-anime-AI'}, {'name': 'spider-gwen', 'token': 'spider-gwen'}, {'name': 'takuji-kawano', 'token': 'takuji-kawano'}, {'name': 'fractal-temple-style', 'token': 'fractal-temple'}, {'name': 'sanguo-guanyu', 'token': 'sanguo-guanyu'}, {'name': 's1m-naoto-ohshima', 'token': 's1m-naoto-ohshima'}, {'name': 'kawaii-girl-plus-style-v1-1', 'token': 'kawaii'}, {'name': 'nathan-wyatt', 'token': 'Nathan-Wyatt'}, {'name': 'kasumin', 'token': 'kasumin'}, {'name': 'happy-person12345-assets', 'token': 'Happy-Person12345-assets'}, {'name': 'oleg-kuvaev', 'token': 'oleg-kuvaev'}, {'name': 'kanovt', 'token': 'anov'}, {'name': 'lphr-style', 'token': 'lphr-style'}, {'name': 'concept-art', 'token': 'concept-art'}, {'name': 'trust-support', 'token': 'trust'}, {'name': 'altyn-helmet', 'token': 'Altyn'}, {'name': '80s-anime-ai-being', 'token': 'anime-AI-being'}, {'name': 'baluchitherian', 'token': 'baluchiter'}, {'name': 'pineda-david', 'token': 'pineda-david'}, {'name': 'ohisashiburi-style', 'token': 'ohishashiburi-style'}, {'name': 'crb-portraits', 'token': 'crbportrait'}, {'name': 'i-love-chaos', 'token': 'chaos'}, {'name': 'alex-thumbnail-object-2000-steps', 'token': 'alex'}, {'name': '852style-girl', 'token': '852style-girl'}, {'name': 'nomad', 'token': 'nomad'}, {'name': 'new-priests', 'token': 'new-priest'}, {'name': 'liminalspaces', 'token': 'liminal image'}, {'name': 'aadhav-face', 'token': 'aadhav-face'}, {'name': 'jang-sung-rak-style', 'token': 'Jang-Sung-Rak-style'}, {'name': 'mattvidpro', 'token': 'mattvidpro'}, {'name': 'chungus-poodl-pet', 'token': 'poodl-chungus-big'}, {'name': 'liminal-spaces-2-0', 'token': 'iminal imag'}, {'name': 'crb-surrealz', 'token': 'crbsurreal'}, {'name': 'final-fantasy-logo', 'token': 'final-fantasy-logo'}, {'name': 'canadian-goose', 'token': 'canadian-goose'}, {'name': 'scratch-project', 'token': 'scratch-project'}, {'name': 'lazytown-stephanie', 'token': 'azytown-stephani'}, {'name': 'female-kpop-singer', 'token': 'female-kpop-star'}, {'name': 'aleyna-tilki', 'token': 'aleyna-tilki'}, {'name': 'other-mother', 'token': 'ther-mothe'}, {'name': 'beldam', 'token': 'elda'}, {'name': 'button-eyes', 'token': 'utton-eye'}, {'name': 'alisa', 'token': 'alisa-selezneva'}, {'name': 'im-poppy', 'token': 'm-popp'}, {'name': 'fractal-flame', 'token': 'fractal-flame'}, {'name': 'Exodus-Styling', 'token': 'Exouds-Style'}, {'name': '8sconception', 'token': '80s-car'}, {'name': 'christo-person', 'token': 'christo'}, {'name': 'slm', 'token': 'c-w388'}, {'name': 'meze-audio-elite-headphones', 'token': 'meze-elite'}, {'name': 'fox-purple', 'token': 'foxi-purple'}, {'name': 'roblox-avatar', 'token': 'roblox-avatar'}, {'name': 'toy-bonnie-plush', 'token': 'toy-bonnie-plush'}, {'name': 'alf', 'token': 'alf'}, {'name': 'wojak', 'token': 'oja'}, {'name': 'animalve3-1500seq', 'token': 'diodio'}, {'name': 'muxoyara', 'token': 'muxoyara'}, {'name': 'selezneva-alisa', 'token': 'selezneva-alisa'}, {'name': 'ayush-spider-spr', 'token': 'spr-mn'}, {'name': 'natasha-johnston', 'token': 'natasha-johnston'}, {'name': 'nard-style', 'token': 'nard'}, {'name': 'kirby', 'token': 'kirby'}, {'name': 'el-salvador-style-style', 'token': 'el-salvador-style'}, {'name': 'rahkshi-bionicle', 'token': 'rahkshi-bionicle'}, {'name': 'masyanya', 'token': 'masyanya'}, {'name': 'command-and-conquer-remastered-cameos', 'token': 'command_and_conquer_remastered_cameos'}, {'name': 'lucario', 'token': 'lucario'}, {'name': 'bruma', 'token': 'Bruma-the-cat'}, {'name': 'nissa-revane', 'token': 'nissa-revane'}, {'name': 'tamiyo', 'token': 'tamiyo'}, {'name': 'pascalsibertin', 'token': 'pascalsibertin'}, {'name': 'chandra-nalaar', 'token': 'chandra-nalaar'}, {'name': 'sam-yang', 'token': 'sam-yang'}, {'name': 'kiora', 'token': 'kiora'}, {'name': 'wedding', 'token': 'wedding1'}, {'name': 'arwijn', 'token': 'rwij'}, {'name': 'gba-fe-class-cards', 'token': 'lasscar'}, {'name': 'painted-by-silver-of-999', 'token': 'cat-toy'}, {'name': 'painted-by-silver-of-999-2', 'token': 'girl-painted-by-silver-of-999'}, {'name': 'toyota-sera', 'token': 'toyota-sera'}, {'name': 'vraska', 'token': 'vraska'}, {'name': 'mystical-nature', 'token': ''}, {'name': 'cartoona-animals', 'token': 'cartoona-animals'}, {'name': 'amogus', 'token': 'amogus'}, {'name': 'kinda-sus', 'token': 'amogus'}, {'name': 'xuna', 'token': 'Xuna'}, {'name': 'pion-by-august-semionov', 'token': 'pion'}, {'name': 'rikiart', 'token': 'rick-art'}, {'name': 'jacqueline-the-unicorn', 'token': 'jacqueline'}, {'name': 'flaticon-lineal-color', 'token': 'flaticon-lineal-color'}, {'name': 'test-epson', 'token': 'epson-branch'}, {'name': 'orientalist-art', 'token': 'orientalist-art'}, {'name': 'ki', 'token': 'ki-mars'}, {'name': 'fnf-boyfriend', 'token': 'fnf-boyfriend'}, {'name': 'phoenix-01', 'token': 'phoenix-style'}, {'name': 'society-finch', 'token': 'society-finch'}, {'name': 'rikiboy-art', 'token': 'Rikiboy-Art'}, {'name': 'flatic', 'token': 'flat-ct'}, {'name': 'logo-with-face-on-shield', 'token': 'logo-huizhang'}, {'name': 'elspeth-tirel', 'token': 'elspeth-tirel'}, {'name': 'zero', 'token': 'zero'}, {'name': 'willy-hd', 'token': 'willy_character'}, {'name': 'kaya-ghost-assasin', 'token': 'kaya-ghost-assasin'}, {'name': 'starhavenmachinegods', 'token': 'StarhavenMachineGods'}, {'name': 'namine-ritsu', 'token': 'namine-ritsu'}, {'name': 'mildemelwe-style', 'token': 'mildemelwe'}, {'name': 'nahiri', 'token': 'nahiri'}, {'name': 'ghost-style', 'token': 'ghost'}, {'name': 'arq-render', 'token': 'arq-style'}, {'name': 'saheeli-rai', 'token': 'saheeli-rai'}, {'name': 'youpi2', 'token': 'youpi'}, {'name': 'youtooz-candy', 'token': 'youtooz-candy'}, {'name': 'beholder', 'token': 'beholder'}, {'name': 'progress-chip', 'token': 'progress-chip'}, {'name': 'lofa', 'token': 'lofa'}, {'name': 'huatli', 'token': 'huatli'}, {'name': 'vivien-reid', 'token': 'vivien-reid'}, {'name': 'wedding-HandPainted', 'token': ''}, {'name': 'sims-2-portrait', 'token': 'sims2-portrait'}, {'name': 'flag-ussr', 'token': 'flag-ussr'}, {'name': 'cortana', 'token': 'cortana'}, {'name': 'azura-from-vibrant-venture', 'token': 'azura'}, {'name': 'liliana-vess', 'token': 'liliana-vess'}, {'name': 'dreamy-painting', 'token': 'dreamy-painting'}, {'name': 'munch-leaks-style', 'token': 'munch-leaks-style'}, {'name': 'gta5-artwork', 'token': 'gta5-artwork'}, {'name': 'xioboma', 'token': 'xi-obama'}, {'name': 'ashiok', 'token': 'ashiok'}, {'name': 'Aflac-duck', 'token': 'aflac duck'}, {'name': 'toho-pixel', 'token': 'toho-pixel'}, {'name': 'alicebeta', 'token': 'Alice-style'}, {'name': 'cute-game-style', 'token': 'cute-game-style'}, {'name': 'a-yakimova', 'token': 'a-yakimova'}, {'name': 'anime-background-style', 'token': 'anime-background-style'}, {'name': 'uliana-kudinova', 'token': 'liana-kudinov'}, {'name': 'msg', 'token': 'MSG69'}, {'name': 'gio', 'token': 'gio-single'}, {'name': 'smooth-pencils', 'token': ''}, {'name': 'pintu', 'token': 'pintu-dog'}, {'name': 'marty6', 'token': 'marty6'}, {'name': 'marty', 'token': 'marty'}, {'name': 'xi', 'token': 'JinpingXi'}, {'name': 'captainkirb', 'token': 'captainkirb'}, {'name': 'urivoldemort', 'token': 'uriboldemort'}, {'name': 'anime-background-style-v2', 'token': 'anime-background-style-v2'}, {'name': 'hk-peach', 'token': 'hk-peach'}, {'name': 'hk-goldbuddha', 'token': 'hk-goldbuddha'}, {'name': 'edgerunners-style', 'token': 'edgerunners-style-av'}, {'name': 'warhammer-40k-drawing-style', 'token': 'warhammer40k-drawing-style'}, {'name': 'hk-opencamera', 'token': 'hk-opencamera'}, {'name': 'hk-breakfast', 'token': 'hk-breakfast'}, {'name': 'iridescent-illustration-style', 'token': 'iridescent-illustration-style'}, {'name': 'edgerunners-style-v2', 'token': 'edgerunners-style-av-v2'}, {'name': 'leif-jones', 'token': 'leif-jones'}, {'name': 'hk-buses', 'token': 'hk-buses'}, {'name': 'hk-goldenlantern', 'token': 'hk-goldenlantern'}, {'name': 'hk-hkisland', 'token': 'hk-hkisland'}, {'name': 'hk-leaves', 'token': ''}, {'name': 'hk-oldcamera', 'token': 'hk-oldcamera'}, {'name': 'frank-frazetta', 'token': 'rank franzett'}, {'name': 'obama-based-on-xi', 'token': 'obama> <JinpingXi'}, {'name': 'hk-vintage', 'token': ''}, {'name': 'degods', 'token': 'degods'}, {'name': 'dishonored-portrait-styles', 'token': 'portrait-style-dishonored'}, {'name': 'manga-style', 'token': 'manga'}, {'name': 'degodsheavy', 'token': 'degods-heavy'}, {'name': 'teferi', 'token': 'teferi'}, {'name': 'car-toy-rk', 'token': 'car-toy'}, {'name': 'anders-zorn', 'token': 'anders-zorn'}, {'name': 'rayne-weynolds', 'token': 'rayne-weynolds'}, {'name': 'hk-bamboo', 'token': 'hk-bamboo'}, {'name': 'hk-betweenislands', 'token': 'hk-betweenislands'}, {'name': 'hk-bicycle', 'token': 'hk-bicycle'}, {'name': 'hk-blackandwhite', 'token': 'hk-blackandwhite'}, {'name': 'pjablonski-style', 'token': 'pjablonski-style'}, {'name': 'hk-market', 'token': 'hk-market'}, {'name': 'hk-phonevax', 'token': 'hk-phonevax'}, {'name': 'hk-clouds', 'token': 'hk-cloud'}, {'name': 'hk-streetpeople', 'token': 'hk-streetpeople'}, {'name': 'iridescent-photo-style', 'token': 'iridescent-photo-style'}, {'name': 'color-page', 'token': 'coloring-page'}, {'name': 'hoi4-leaders', 'token': 'HOI4-Leader'}, {'name': 'franz-unterberger', 'token': 'franz-unterberger'}, {'name': 'angus-mcbride-style', 'token': 'angus-mcbride-style'}, {'name': 'happy-chaos', 'token': 'happychaos'}, {'name': 'gt-color-paint-2', 'token': 'my-color-paint-GT'}, {'name': 'smurf-style', 'token': 'smurfy'}, {'name': 'coraline', 'token': 'coraline'}, {'name': 'terraria-style', 'token': 'terr-sty'}, {'name': 'ettblackteapot', 'token': 'my-teapot'}, {'name': 'gibasachan-v0.1', 'token': 'gibasachan'}, {'name': 'kodakvision500t', 'token': 'kodakvision_500T'}, {'name': 'obama-based-on-xi', 'token': 'obama'}, {'name': 'obama-self-2', 'token': 'Obama'}, {'name': 'bob-dobbs', 'token': 'bob'}, {'name': 'ahx-model-1', 'token': 'ivan-stripes'}, {'name': 'ahx-model-2', 'token': 'artist'}, {'name': 'beetlejuice-cartoon-style', 'token': 'beetlejuice-cartoon'}, {'name': 'pokemon-modern-artwork', 'token': 'pkmn-modern'}, {'name': 'pokemon-classic-artwork', 'token': 'pkmn-classic'}, {'name': 'pokemon-gens-1-to-8', 'token': 'pkmn-galar'}, {'name': 'pokemon-rgby-sprite', 'token': 'pkmn-rgby'}, {'name': 'max-twain', 'token': 'max-twain'}, {'name': 'ihylc', 'token': 'ihylc'}, {'name': 'test-man', 'token': 'Test-man'}, {'name': 'tron-style', 'token': 'tron-style>'}, {'name': 'dulls', 'token': 'dulls-avatar'}, {'name': 'vie-proceres', 'token': 'vie-proceres'}, {'name': 'dovin-baan', 'token': 'dovin-baan'}, {'name': 'polki-jewellery', 'token': 'ccess to model sd-concepts-library/polki-jewellery is restricted and you are not in the authorized list. Visit https://huggingface.co/sd-concepts-library/polki-jewellery to ask for access'}, {'name': 'dog2', 'token': 'ccess to model sd-concepts-library/dog2 is restricted and you are not in the authorized list. Visit https://huggingface.co/sd-concepts-library/dog2 to ask for access'}, {'name': 'caitlin-fairchild-character-gen13-comics', 'token': 'Caitlin-Fairchild'}, {'name': 'ugly-sonic', 'token': 'ugly-sonic'}, {'name': 'utopia-beer-mat', 'token': 'utopia-beer-mat'}, {'name': 'old-brno', 'token': 'old-brno'}, {'name': 'moka-pot', 'token': 'moka-pot'}, {'name': 'brno-trenck', 'token': 'brno-trenck'}, {'name': 'brno-tram', 'token': 'brno-tram'}, {'name': 'brno-obasa', 'token': 'brno-obasa'}, {'name': 'brno-night', 'token': 'brno-night'}, {'name': 'brno-dorm', 'token': 'brno-dorm'}, {'name': 'brno-busstop', 'token': 'brno-busstop'}, {'name': 'twitch-league-of-legends', 'token': 'twitch-lol'}, {'name': 'fp-shop2', 'token': 'fp-shop2'}, {'name': 'fp-shop1', 'token': 'fp-shop1'}, {'name': 'fp-content-b', 'token': 'fp-content-b'}, {'name': 'fp-content-a', 'token': 'fp-content-a'}, {'name': 'fp-city', 'token': 'fp-city'}, {'name': 'brno-city-results', 'token': 'brno-city-results'}, {'name': 'brno-city', 'token': 'brno-city'}, {'name': 'brno-chair-results', 'token': 'brno-chair-results'}, {'name': 'brno-chair', 'token': 'brno-chair'}, {'name': 'manga-char-nov-23', 'token': 'char-nov23'}, {'name': 'manga-nov-23', 'token': 'manga-characters-nov23'}, {'name': 'yellow-cockatiel-parrot', 'token': 'rosa-popugai'}, {'name': 'dreams', 'token': 'meeg'}, {'name': 'alberto-montt', 'token': 'AlbertoMontt'}, {'name': 'tooth-wu', 'token': 'tooth-wu'}, {'name': 'filename-2', 'token': 'filename2'}, {'name': 'iridescent-photo-style', 'token': 'iridescent-photo-style'}, {'name': 'bored-ape-textual-inversion', 'token': 'bored_ape'}, {'name': 'ghibli-face', 'token': 'ghibli-face'}, {'name': 'yoshimurachi', 'token': 'yoshi-san'}, {'name': 'jm-bergling-monogram', 'token': 'JM-Bergling-monogram'}, {'name': '4tnght', 'token': '4tNGHT'}, {'name': 'dancing-cactus', 'token': 'dancing-cactus'}, {'name': 'yolandi-visser', 'token': 'olandi-visse'}, {'name': 'zizigooloo', 'token': 'zizigooloo'}, {'name': 'princess-knight-art', 'token': 'princess-knight'}, {'name': 'belle-delphine', 'token': 'elle-delphin'}, {'name': 'cancer_style', 'token': 'cancer_style'}, {'name': 'trypophobia', 'token': 'rypophobi'}, {'name': 'incendegris-grey', 'token': 'incendegris-grey'}, {'name': 'fairy-tale-painting-style', 'token': 'fairy-tale-painting-style'}, {'name': 'arcimboldo-style', 'token': 'arcimboldo-style'}, {'name': 'xidiversity', 'token': 'JinpingXi'}, {'name': 'obama-based-on-xi', 'token': 'obama'}, {'name': 'zero-bottle', 'token': 'zero-bottle'}, {'name': 'victor-narm', 'token': 'victor-narm'}, {'name': 'supitcha-mask', 'token': 'supitcha-mask'}, {'name': 'smarties', 'token': 'smarties'}, {'name': 'rico-face', 'token': 'rico-face'}, {'name': 'rex-deno', 'token': 'rex-deno'}, {'name': 'abby-face', 'token': 'abby-face'}, {'name': 'nic-papercuts', 'token': 'nic-papercuts'}]


def get_concept(name):
  for con in concepts:
      if con['name'] == name:
        return con
  return {'name':'', 'token':''}

def get_conceptualizer(page):
    from huggingface_hub import hf_hub_download
    from diffusers import StableDiffusionPipeline
    from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker
    from transformers import CLIPFeatureExtractor, CLIPTextModel, CLIPTokenizer
    global pipe_conceptualizer
    repo_id_embeds = f"sd-concepts-library/{prefs['concepts_model']}"
    embeds_url = "" #Add the URL or path to a learned_embeds.bin file in case you have one
    placeholder_token_string = "" #Add what is the token string in case you are uploading your own embed

    downloaded_embedding_folder = os.path.join(root_dir, "downloaded_embedding")
    if not os.path.exists(downloaded_embedding_folder):
      os.mkdir(downloaded_embedding_folder)
    try:
      if(not embeds_url):
        embeds_path = hf_hub_download(repo_id=repo_id_embeds, filename="learned_embeds.bin")
        token_path = hf_hub_download(repo_id=repo_id_embeds, filename="token_identifier.txt")
        shutil.copy(embeds_path, downloaded_embedding_folder)
        shutil.copy(token_path, downloaded_embedding_folder)
        with open(f'{downloaded_embedding_folder}/token_identifier.txt', 'r') as file:
          placeholder_token_string = file.read()
      else:
        run_sp(f"wget -q -O {downloaded_embedding_folder}/learned_embeds.bin {embeds_url}")
        #!wget -q -O $downloaded_embedding_folder/learned_embeds.bin $embeds_url
    except Exception as e:
      alert_msg(page, f"Error getting concept. May need to accept model at https://huggingface.co/sd-concepts-library/{prefs['concepts_model']}", content=Text(e))
      return
    learned_embeds_path = f"{downloaded_embedding_folder}/learned_embeds.bin"
    tokenizer = CLIPTokenizer.from_pretrained(model_path, subfolder="tokenizer")
    text_encoder = CLIPTextModel.from_pretrained(model_path, subfolder="text_encoder", torch_dtype=torch.float16)
    def load_learned_embed_in_clip(learned_embeds_path, text_encoder, tokenizer, token=None):
      loaded_learned_embeds = torch.load(learned_embeds_path, map_location="cpu")
      trained_token = list(loaded_learned_embeds.keys())[0]
      embeds = loaded_learned_embeds[trained_token]
      dtype = text_encoder.get_input_embeddings().weight.dtype
      embeds.to(dtype)
      token = token if token is not None else trained_token
      num_added_tokens = tokenizer.add_tokens(token)
      if num_added_tokens == 0:
        alert_msg(page, f"The tokenizer already contains the token {token}. Please pass a different `token` that is not already in the tokenizer.")
        return
      text_encoder.resize_token_embeddings(len(tokenizer))
      token_id = tokenizer.convert_tokens_to_ids(token)
      text_encoder.get_input_embeddings().weight.data[token_id] = embeds
    try:
      load_learned_embed_in_clip(learned_embeds_path, text_encoder, tokenizer)
    except Exception as e:
      alert_msg(page, f"Error Loading Concept", content=Text(e))
      return
    pipe_conceptualizer = StableDiffusionPipeline.from_pretrained(
        model_path,
        revision="fp16",
        torch_dtype=torch.float16,
        text_encoder=text_encoder,
        tokenizer=tokenizer,
        cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,
        **safety,
    )
    pipe_conceptualizer = pipeline_scheduler(pipe_conceptualizer)
    pipe_conceptualizer = optimize_pipe(pipe_conceptualizer)
    pipe_conceptualizer.set_progress_bar_config(disable=True)
    #pipe_conceptualizer = pipe_conceptualizer.to(torch_device)
    return pipe_conceptualizer

def flush():
    gc.collect()
    torch.cuda.empty_cache()
    #torch.cuda.clear_autocast_cache()

def clear_img2img_pipe():
  global pipe_img2img
  if pipe_img2img is not None:
    #print("Clearing out img2img pipeline for more VRAM")
    del pipe_img2img
    flush()
    pipe_img2img = None
def clear_txt2img_pipe():
  global pipe
  if pipe is not None:
    #print("Clearing out text2img pipeline for more VRAM")
    del pipe
    flush()
    pipe = None
def clear_unet_pipe():
  global unet
  if unet is not None:
    #print("Clearing out unet custom pipeline for more VRAM")
    del unet
    flush()
    unet = None
def clear_SDXL_pipe():
  global pipe_SDXL, pipe_SDXL_refiner, compel_base, compel_refiner
  if prefs['SDXL_compel']:
    if compel_base is not None:
      del compel_base, compel_refiner
      compel_base = None
      compel_refiner = None
  if pipe_SDXL is not None:
    del pipe_SDXL
    flush()
    pipe_SDXL = None
  if pipe_SDXL_refiner is not None:
    del pipe_SDXL_refiner
    flush()
    pipe_SDXL_refiner = None

def clear_clip_guided_pipe():
  global pipe_clip_guided
  if pipe_clip_guided is not None:
    #print("Clearing out CLIP Guided pipeline for more VRAM")
    del pipe_clip_guided
    flush()
    pipe_clip_guided = None
def clear_conceptualizer_pipe():
  global pipe_conceptualizer
  if pipe_conceptualizer is not None:
    #print("Clearing out CLIP Guided pipeline for more VRAM")
    del pipe_conceptualizer
    flush()
    pipe_conceptualizer = None
def clear_repaint_pipe():
  global pipe_repaint
  if pipe_repaint is not None:
    del pipe_repaint
    flush()
    pipe_repaint = None
def clear_imagic_pipe():
  global pipe_imagic
  if pipe_imagic is not None:
    del pipe_imagic
    flush()
    pipe_imagic = None
def clear_composable_pipe():
  global pipe_composable
  if pipe_composable is not None:
    del pipe_composable
    flush()
    pipe_composable = None
def clear_versatile_pipe():
  global pipe_versatile
  if pipe_versatile is not None:
    del pipe_versatile
    flush()
    pipe_versatile = None
def clear_versatile_text2img_pipe():
  global pipe_versatile_text2img
  if pipe_versatile_text2img is not None:
    del pipe_versatile_text2img
    flush()
    pipe_versatile_text2img = None
def clear_versatile_variation_pipe():
  global pipe_versatile_variation
  if pipe_versatile_variation is not None:
    del pipe_versatile_variation
    flush()
    pipe_versatile_variation = None
def clear_versatile_dualguided_pipe():
  global pipe_versatile_dualguided
  if pipe_versatile_dualguided is not None:
    del pipe_versatile_dualguided
    flush()
    pipe_versatile_dualguided = None
def clear_depth_pipe():
  global pipe_depth
  if pipe_depth is not None:
    del pipe_depth
    flush()
    pipe_depth = None
def clear_interpolation_pipe():
  global pipe_interpolation
  if pipe_interpolation is not None:
    del pipe_interpolation
    flush()
    pipe_interpolation = None
def clear_safe_pipe():
  global pipe_safe
  if pipe_safe is not None:
    del pipe_safe
    flush()
    pipe_safe = None
def clear_upscale_pipe():
  global pipe_upscale
  if pipe_upscale is not None:
    del pipe_upscale
    flush()
    pipe_upscale = None
def clear_image_variation_pipe():
  global pipe_image_variation
  if pipe_image_variation is not None:
    del pipe_image_variation
    flush()
    pipe_image_variation = None
def clear_semantic_pipe():
  global pipe_semantic
  if pipe_semantic is not None:
    del pipe_semantic
    flush()
    pipe_semantic = None
def clear_EDICT_pipe():
  global pipe_EDICT, text_encoder_EDICT
  if pipe_EDICT is not None:
    del pipe_EDICT
    del text_encoder_EDICT
    flush()
    pipe_EDICT = None
    text_encoder_EDICT = None
def clear_DiffEdit_pipe():
  global pipe_DiffEdit
  if pipe_DiffEdit is not None:
    del pipe_DiffEdit
    flush()
    pipe_DiffEdit = None
def clear_unCLIP_pipe():
  global pipe_unCLIP
  if pipe_unCLIP is not None:
    del pipe_unCLIP
    flush()
    pipe_unCLIP = None
def clear_unCLIP_image_variation_pipe():
  global pipe_unCLIP_image_variation
  if pipe_unCLIP_image_variation is not None:
    del pipe_unCLIP_image_variation
    flush()
    pipe_unCLIP_image_variation = None
def clear_unCLIP_interpolation_pipe():
  global pipe_unCLIP_interpolation
  if pipe_unCLIP_interpolation is not None:
    del pipe_unCLIP_interpolation
    flush()
    pipe_unCLIP_interpolation = None
def clear_unCLIP_image_interpolation_pipe():
  global pipe_unCLIP_image_interpolation
  if pipe_unCLIP_image_interpolation is not None:
    del pipe_unCLIP_image_interpolation
    flush()
    pipe_unCLIP_image_interpolation = None
def clear_magic_mix_pipe():
  global pipe_magic_mix
  if pipe_magic_mix is not None:
    del pipe_magic_mix
    flush()
    pipe_magic_mix = None
def clear_paint_by_example_pipe():
  global pipe_paint_by_example
  if pipe_paint_by_example is not None:
    del pipe_paint_by_example
    flush()
    pipe_paint_by_example = None
def clear_instruct_pix2pix_pipe():
  global pipe_instruct_pix2pix
  if pipe_instruct_pix2pix is not None:
    del pipe_instruct_pix2pix
    flush()
    pipe_instruct_pix2pix = None
def clear_alt_diffusion_pipe():
  global pipe_alt_diffusion
  if pipe_alt_diffusion is not None:
    del pipe_alt_diffusion
    flush()
    pipe_alt_diffusion = None
def clear_alt_diffusion_img2img_pipe():
  global pipe_alt_diffusion_img2img
  if pipe_alt_diffusion_img2img is not None:
    del pipe_alt_diffusion_img2img
    flush()
    pipe_alt_diffusion_img2img = None
def clear_SAG_pipe():
  global pipe_SAG
  if pipe_SAG is not None:
    del pipe_SAG
    flush()
    pipe_SAG = None
def clear_attend_and_excite_pipe():
  global pipe_attend_and_excite
  if pipe_attend_and_excite is not None:
    del pipe_attend_and_excite
    flush()
    pipe_attend_and_excite = None
def clear_panorama_pipe():
  global pipe_panorama
  if pipe_panorama is not None:
    del pipe_panorama
    flush()
    pipe_panorama = None
def clear_dance_pipe():
  global pipe_dance
  if pipe_dance is not None:
    del pipe_dance
    flush()
    pipe_dance = None
def clear_audio_diffusion_pipe():
  global pipe_audio_diffusion
  if pipe_audio_diffusion is not None:
    del pipe_audio_diffusion
    flush()
    pipe_audio_diffusion = None
def clear_music_gen_pipe():
  global pipe_music_gen
  if pipe_music_gen is not None:
    del pipe_music_gen
    flush()
    pipe_music_gen = None
def clear_riffusion_pipe():
  global pipe_riffusion
  if pipe_riffusion is not None:
    del pipe_riffusion
    flush()
    pipe_riffusion = None
def clear_voice_fixer_pipe():
  global pipe_voice_fixer
  if pipe_voice_fixer is not None:
    del pipe_voice_fixer
    flush()
    pipe_voice_fixer = None
def clear_text_to_video_pipe():
  global pipe_text_to_video
  if pipe_text_to_video is not None:
    del pipe_text_to_video
    flush()
    pipe_text_to_video = None
def clear_text_to_video_zero_pipe():
  global pipe_text_to_video_zero
  if pipe_text_to_video_zero is not None:
    del pipe_text_to_video_zero
    flush()
    torch.cuda.ipc_collect()
    torch.cuda.reset_peak_memory_stats()
    pipe_text_to_video_zero = None
def clear_video_to_video_pipe():
  global pipe_video_to_video
  if pipe_video_to_video is not None:
    del pipe_video_to_video
    flush()
    pipe_video_to_video = None
def clear_deepfloyd_pipe():
  global pipe_deepfloyd, pipe_deepfloyd2, pipe_deepfloyd3
  if pipe_deepfloyd is not None:
    del pipe_deepfloyd, pipe_deepfloyd2, pipe_deepfloyd3
    flush()
    pipe_deepfloyd = None
    pipe_deepfloyd2 = None
    pipe_deepfloyd3 = None
def clear_reference_pipe():
  global pipe_reference
  if pipe_reference is not None:
    del pipe_reference
    flush()
    pipe_reference = None
def clear_controlnet_qr_pipe():
  global pipe_controlnet_qr, pipe_controlnet
  if pipe_controlnet_qr is not None:
    del pipe_controlnet_qr
    flush()
    pipe_controlnet_qr = None
  if pipe_controlnet is not None:
    del pipe_controlnet
    flush()
    pipe_controlnet = None
def clear_controlnet_segment_pipe():
  global pipe_controlnet_segment, pipe_controlnet
  if pipe_controlnet_segment is not None:
    del pipe_controlnet_segment
    flush()
    pipe_controlnet_segment = None
  if pipe_controlnet is not None:
    del pipe_controlnet
    flush()
    pipe_controlnet = None
def clear_DiT_pipe():
  global pipe_DiT
  if pipe_DiT is not None:
    del pipe_DiT
    flush()
    pipe_DiT = None
def clear_kandinsky_pipe(all=True):
  global pipe_kandinsky, pipe_kandinsky_prior, pipe_kandinsky_controlnet_prior, loaded_kandinsky_task, depth_estimator
  if pipe_kandinsky is not None:
    del pipe_kandinsky
    if all:
      del pipe_kandinsky_prior
      del pipe_kandinsky_controlnet_prior
      del depth_estimator
    flush()
    pipe_kandinsky = None
    if all:
      pipe_kandinsky_prior = None
      pipe_kandinsky_controlnet_prior = None
      depth_estimator = None
    loaded_kandinsky_task = ""
def clear_tortoise_tts_pipe():
  global pipe_tortoise_tts
  if pipe_tortoise_tts is not None:
    del pipe_tortoise_tts
    flush()
    pipe_tortoise_tts = None
def clear_audio_ldm_pipe():
  global pipe_audio_ldm
  if pipe_audio_ldm is not None:
    del pipe_audio_ldm
    flush()
    pipe_audio_ldm = None
def clear_audio_ldm2_pipe():
  global pipe_audio_ldm2
  if pipe_audio_ldm2 is not None:
    del pipe_audio_ldm2
    flush()
    pipe_audio_ldm2 = None
def clear_gpt2_pipe():
  global pipe_gpt2
  if pipe_gpt2 is not None:
    del pipe_gpt2
    flush()
    pipe_gpt2 = None
def clear_distil_gpt2_pipe():
  global pipe_distil_gpt2
  if pipe_distil_gpt2 is not None:
    del pipe_distil_gpt2
    flush()
    pipe_distil_gpt2 = None
def clear_background_remover_pipe():
  global pipe_background_remover
  if pipe_background_remover is not None:
    del pipe_background_remover
    flush()
    pipe_background_remover = None
def clear_controlnet_pipe():
  global pipe_controlnet, controlnet, controlnet_models, controlnet_xl_models, status
  if pipe_controlnet is not None:
    del pipe_controlnet
    del controlnet
    for k, v in controlnet_models.items():
      if v != None:
        del v
        controlnet_models[k] = None
    for k, v in controlnet_xl_models.items():
      if v != None:
        del v
        controlnet_xl_models[k] = None
    flush()
    pipe_controlnet = None
    controlnet = None
    status['loaded_controlnet'] = None
def clear_stable_lm_pipe():
  global pipe_stable_lm, tokenizer_stable_lm
  if pipe_stable_lm is not None:
    del pipe_stable_lm
    del tokenizer_stable_lm
    flush()
    pipe_stable_lm = None
    tokenizer_stable_lm = None

def clear_pipes(allbut=None):
    if torch_device == "cpu": return
    but = [] if allbut == None else [allbut] if type(allbut) is str else allbut
    if not 'txt2img' in but: clear_txt2img_pipe()
    if not 'img2img' in but: clear_img2img_pipe()
    if not 'SDXL' in but: clear_SDXL_pipe()
    if not 'unet' in but: clear_unet_pipe()
    if not 'clip_guided' in but: clear_clip_guided_pipe()
    if not 'conceptualizer' in but: clear_conceptualizer_pipe()
    if not 'repaint' in but: clear_repaint_pipe()
    if not 'imagic' in but: clear_imagic_pipe()
    if not 'composable': clear_composable_pipe()
    if not 'versatile_text2img' in but: clear_versatile_text2img_pipe()
    if not 'versatile_variation' in but: clear_versatile_variation_pipe()
    if not 'versatile_dualguided' in but: clear_versatile_dualguided_pipe()
    if not 'depth' in but: clear_depth_pipe()
    if not 'interpolation' in but: clear_interpolation_pipe()
    if not 'safe' in but: clear_safe_pipe()
    if not 'upscale' in but: clear_upscale_pipe()
    if not 'unCLIP' in but: clear_unCLIP_pipe()
    if not 'EDICT' in but: clear_EDICT_pipe()
    if not 'DiffEdit' in but: clear_DiffEdit_pipe()
    if not 'unCLIP_image_variation' in but: clear_unCLIP_image_variation_pipe()
    if not 'unCLIP_interpolation' in but: clear_unCLIP_interpolation_pipe()
    if not 'image_variation' in but: clear_image_variation_pipe()
    if not 'semantic' in but: clear_semantic_pipe()
    if not 'magic_mix' in but: clear_magic_mix_pipe()
    if not 'alt_diffusion' in but: clear_alt_diffusion_pipe()
    if not 'alt_diffusion_img2img' in but: clear_alt_diffusion_img2img_pipe()
    if not 'paint_by_example' in but: clear_paint_by_example_pipe()
    if not 'instruct_pix2pix' in but: clear_instruct_pix2pix_pipe()
    if not 'SAG' in but: clear_SAG_pipe()
    if not 'attend_and_excite' in but: clear_attend_and_excite_pipe()
    if not 'deepfloyd' in but: clear_deepfloyd_pipe()
    if not 'reference' in but: clear_reference_pipe()
    if not 'controlnet_qr' in but: clear_controlnet_qr_pipe()
    if not 'controlnet_segment' in but: clear_controlnet_segment_pipe()
    if not 'DiT' in but: clear_DiT_pipe()
    if not 'controlnet' in but: clear_controlnet_pipe()
    if not 'panorama' in but: clear_panorama_pipe()
    if not 'kandinsky' in but: clear_kandinsky_pipe(all=not ('kandinsky_prior' in but or 'kandinsky_controlnet_prior' in but))
    if not 'dance' in but: clear_dance_pipe()
    if not 'riffusion' in but: clear_riffusion_pipe()
    if not 'audio_diffusion' in but: clear_audio_diffusion_pipe()
    if not 'music_gen' in but: clear_music_gen_pipe()
    if not 'voice_fixer' in but: clear_voice_fixer_pipe()
    if not 'text_to_video' in but: clear_text_to_video_pipe()
    if not 'text_to_video_zero' in but: clear_text_to_video_zero_pipe()
    if not 'video_to_video' in but: clear_video_to_video_pipe()
    if not 'tortoise_tts' in but: clear_tortoise_tts_pipe()
    if not 'audio_ldm' in but: clear_audio_ldm_pipe()
    if not 'audio_ldm2' in but: clear_audio_ldm2_pipe()
    if not 'gpt2' in but: clear_gpt2_pipe()
    if not 'distil_gpt2' in but: clear_distil_gpt2_pipe()
    if not 'background_remover' in but: clear_background_remover_pipe()
    if not 'controlnet' in but: clear_controlnet_pipe()
    if not 'stable_lm' in but: clear_stable_lm_pipe()
    try:
        torch.cuda.ipc_collect()
        torch.cuda.reset_peak_memory_stats()
    except Exception:
        pass
import base64
def get_base64(image_path):
    with open(image_path, "rb") as img_file:
        base64_string = base64.b64encode(img_file.read()).decode('utf-8')
        return base64_string
def pil_to_base64(image):
    image_stream = io.BytesIO()
    image.save(image_stream, format='PNG')
    image_bytes = image_stream.getvalue()
    base64_string = base64.b64encode(image_bytes).decode('utf-8')
    return base64_string

def available_file(folder, name, idx, ext='png', no_num=False):
  available = False
  while not available:
    # Todo, check if using PyDrive2
    if no_num:
      if not os.path.isfile(os.path.join(folder, f'{name}.{ext}')):
        return os.path.join(folder, f'{name}.{ext}')
    if os.path.isfile(os.path.join(folder, f'{name}-{idx}.{ext}')):
      idx += 1
    else: available = True
  return os.path.join(folder, f'{name}-{idx}.{ext}')

def available_folder(folder, name, idx):
  available = False
  while not available:
    if os.path.isdir(os.path.join(folder, f'{name}-{idx}')):
      idx += 1
    else: available = True
  return os.path.join(folder, f'{name}-{idx}')

#import asyncio
#async
def start_diffusion(page):
  global pipe, unet, pipe_img2img, pipe_clip_guided, pipe_interpolation, pipe_conceptualizer, pipe_imagic, pipe_depth, pipe_composable, pipe_versatile_text2img, pipe_versatile_variation, pipe_versatile_dualguided, pipe_SAG, pipe_attend_and_excite, pipe_alt_diffusion, pipe_alt_diffusion_img2img, pipe_panorama, pipe_safe, pipe_upscale, pipe_SDXL, pipe_SDXL_refiner
  global SD_sampler, stability_api, total_steps, pb, prefs, args, total_steps, compel_base, compel_refiner
  def prt(line, update=True):
    if type(line) == str:
      line = Text(line)
    try:
      page.imageColumn.controls.append(line)
      if update:
        page.imageColumn.update()
    except Exception:
      clear_image_output()
      pass
    if update:
      page.Images.update()
  def clear_last(update=True):
    del page.imageColumn.controls[-1]
    if update:
      page.imageColumn.update()
      page.Images.update()
  abort_run = False
  def abort_diffusion(e):
    nonlocal abort_run
    abort_run = True
    page.snd_error.play()
    page.snd_delete.play()
  def callback_cancel(cancel) -> None:
    callback_cancel.has_been_called = True
    if abort_run:
      return True
  def download_image(e):
    if is_Colab:
      print(f"{type(e.control.data)} {e.control.data}")
      from google.colab import files
      if os.path.isfile(e.control.data):
        files.download(e.control.data)
      else:
        time.sleep(5)
        files.download(e.control.data)
  def clear_image_output():
    for co in reversed(page.imageColumn.controls):
      del co
    page.imageColumn.controls.clear()
    try:
      page.imageColumn.update()
    except Exception as e:
      try:
        page.imageColumn = Column([], auto_scroll=True, scroll=ScrollMode.AUTO)
      except Exception as er:
        alert_msg(page, f"ERROR: Problem Clearing Image Output List. May need to stop script and restart app to recover, sorry...", content=Text(f'{e}\n{er}'))
        page.Images = buildImages(page)
        pass
      page.update()
      pass
# Why getting Exception: control with ID '_3607' not found when re-running after error
  #page.Images.content.controls = []
  if int(status['cpu_memory']) <= 8 and prefs['use_SDXL'] and status['installed_SDXL']: # and prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']
      alert_msg(page, f"Sorry, you only have {int(status['cpu_memory'])}GB RAM which is not quite enough to run SD XL. Either Change runtime type to High-RAM mode and restart or use other pipelines.")
      return
  clear_image_output()
  #pb = ProgressBar(bar_height=8)
  pb.width=(page.width if page.web else page.window_width) - 50
  #prt(Row([Text("▶️   Running Stable Diffusion on Batch Prompts List", style=TextThemeStyle.TITLE_LARGE, color=colors.SECONDARY, weight=FontWeight.BOLD), IconButton(icon=icons.CANCEL, tooltip="Abort Current Diffusion Run", on_click=abort_diffusion)], alignment=MainAxisAlignment.SPACE_BETWEEN))
  prt(Header("▶️   Running Stable Diffusion on Batch Prompts List", actions=[IconButton(icon=icons.CANCEL, tooltip="Abort Current Diffusion Run", on_click=abort_diffusion)]))
  import string, shutil, random, gc, io, json
  from collections import ChainMap
  from PIL.PngImagePlugin import PngInfo
  from contextlib import contextmanager, nullcontext
  import copy

  if status['installed_diffusers']:
    from diffusers import StableDiffusionPipeline
  os.chdir(stable_dir)
  generator = None
  clear_repaint_pipe()
  output_files = []
  pipe_used = ""
  retry_attempts_if_NSFW = prefs['retry_attempts']
  #if (prefs['use_Stability_api'] and status['installed_stability']) or bool(not status['installed_diffusers'] and status['installed_stability']):
  #  update_stability()
  last_seed = args['seed']
  if args['seed'] < 1 or args['seed'] is None:
    rand_seed = random.randint(0,2147483647)
    if (not (prefs['use_Stability_api'] or (not status['installed_diffusers'] and status['installed_stability']))) and (not (prefs['use_AIHorde_api'] or (not status['installed_diffusers'] and status['installed_AIHorde']))):
      if use_custom_scheduler:
        generator = torch.manual_seed(rand_seed)
      else:
        generator = torch.Generator("cuda").manual_seed(rand_seed)
    last_seed = rand_seed
  else:
    if not (prefs['use_Stability_api'] or (not status['installed_diffusers'] and status['installed_stability'])) and (not (prefs['use_AIHorde_api'] or (not status['installed_diffusers'] and status['installed_AIHorde']))):
      if use_custom_scheduler:
        generator = torch.manual_seed(args['seed'])
      else:
        generator = torch.Generator("cuda").manual_seed(args['seed'])
  strikes = 0
  p_idx = 0
  if prefs['centipede_prompts_as_init_images']:
    os.makedirs(os.path.join(root_dir, 'init_images'), exist_ok=True)
  last_image = None
  updated_prompts = []
  model = get_model(prefs['model_ckpt'])
      
  if not (prefs["use_interpolation"] and status['installed_interpolation']):
    for p in prompts:
      pr = None
      arg = {}
      if type(p) == list or type(p) == str:
        pr = p
        arg = args.copy()
      elif isinstance(p, Dream):
        pr = p.prompt
        arg = merge_dict(args, p.arg)
      else: prt(f'Unknown item in list of type {type(p)}')
      #print(str(arg))
      arg['width'] = int(arg['width'])
      arg['height'] = int(arg['height'])
      arg['seed'] = int(arg['seed'])
      arg['guidance_scale'] = float(arg['guidance_scale'])
      arg['batch_size'] = int(arg['batch_size'])
      arg['n_iterations'] = int(arg['n_iterations'])
      arg['steps'] = int(arg['steps'])
      arg['eta'] = float(arg['eta'])
      arg['init_image_strength'] = float(arg['init_image_strength'])
      p.arg = arg
      iterations = arg['n_iterations']
      updated_prompts.append(p)
      if iterations > 1:
        #print(f"Iterating {iterations} times - {pr}")
        for d in range(iterations - 1):
          new_dream = None
          if isinstance(p, Dream):
            new_dream = copy.copy(p)
            new_dream.prompt = pr[0] if type(pr) == list else pr
            new_arg = new_dream.arg.copy()
            new_arg['seed'] = random.randint(0,2147483647)
            new_arg['n_iterations'] = 1
            new_dream.arg = new_arg
            #new_dream.arg['seed'] = random.randint(0,4294967295)
          else:
            new_dream = Dream(p, seed=random.randint(0,2147483647), n_iterations=1)
          new_dream.arg['n_iterations'] = 1
          #prompts.insert(p_idx+1, new_dream)
          updated_prompts.append(new_dream)

    if bool(model['prefix']):
      if model['prefix'][-1] != ' ':
        model['prefix'] = model['prefix'] + ' '
    for p in updated_prompts:
      pr = ""
      images = None
      usable_image = True
      arg = {}
      if type(p) == list or type(p) == str:
        if (status['installed_stability'] and prefs['use_Stability_api']) or (status['installed_AIHorde'] and prefs['use_AIHorde_api']):
          pr = p
        else:
          pr = model['prefix'] + p
        arg = args.copy()
      elif isinstance(p, Dream):
        if (status['installed_stability'] and prefs['use_Stability_api']) or (status['installed_AIHorde'] and prefs['use_AIHorde_api']):
          pr = model['prefix'] + p.prompt
        else:
          pr = p.prompt
        arg = merge_dict(args, p.arg)
      else: prt(f"Unknown object {type(p)} in the prompt list")
      if arg['batch_size'] > 1:
        pr = [pr] * arg['batch_size']
        #if bool(arg['negative_prompt']):
        arg['negative_prompt'] = [arg['negative_prompt']] * arg['batch_size']
      if last_seed != arg['seed']:
        if arg['seed'] < 1 or arg['seed'] is None:
          rand_seed = random.randint(0,2147483647)
          if (not (prefs['use_Stability_api'] or (not status['installed_diffusers'] and status['installed_stability']))) and (not (prefs['use_AIHorde_api'] or (not status['installed_diffusers'] and status['installed_AIHorde']))):
            if use_custom_scheduler:
              generator = torch.manual_seed(rand_seed)
            else:
              generator = torch.Generator("cuda").manual_seed(rand_seed)
          arg['seed'] = rand_seed
        else:
          if (not(prefs['use_Stability_api'] or (not status['installed_diffusers'] and status['installed_stability']))) and (not (prefs['use_AIHorde_api'] or (not status['installed_diffusers'] and status['installed_AIHorde']))):
            if use_custom_scheduler:
              generator = torch.manual_seed(arg['seed'])
            else:
              generator = torch.Generator("cuda").manual_seed(arg['seed'])
        last_seed = arg['seed']
      if prefs['centipede_prompts_as_init_images'] and last_image is not None:
        arg['init_image'] = last_image
      p_count = f'[{p_idx + 1} of {len(updated_prompts)}]  '
      #if p_idx % 30 == 0 and p_idx > 1:
      #  clear_output()
      #  print(f"{Color.BEIGE2}Cleared console display due to memory limit in console logging.  Images still saving.{Color.END}")
      #prt(Divider(height=6, thickness=2), update=False)
      prt(Row([Text(p_count), Text(pr[0] if type(pr) == list else pr, expand=True, weight=FontWeight.BOLD), Text(f'seed: {arg["seed"]}     ')]))
      time.sleep(0.1)
      page.auto_scrolling(False)
      #prt(p_count + ('─' * 90))
      #prt(f'{pr[0] if type(pr) == list else pr} - seed:{arg["seed"]}')
      total_steps = arg['steps']
      #if prefs['use_Stability_api'] or bool(arg['use_Stability'] or (not status['installed_diffusers'] and status['installed_stability'])):
      if status['installed_stability'] and (not status['installed_diffusers'] or prefs['use_Stability_api']) and not (status['installed_AIHorde'] and prefs['use_AIHorde_api']):
        #print('use_Stability_api')
        if not status['installed_stability']:
          alert_msg(page, f"ERROR: To use Stability-API, you must run the install it first and have proper API key")
          return
        else:
          prt('Stability API Diffusion ')# + ('─' * 100))
          #print(f'"{SD_prompt}", height={SD_height}, width={SD_width}, steps={SD_steps}, cfg_scale={SD_guidance_scale}, seed={SD_seed}, sampler={generation_sampler}')
          #strikes = 0
          images = []
          arg['width'] = multiple_of_64(arg['width'])
          arg['height'] = multiple_of_64(arg['height'])
          prt(pb)
          import stability_sdk.interfaces.gooseai.generation.generation_pb2 as generation
          answers = response = None

          import requests
          from io import BytesIO
          import base64
          api_host = os.getenv('API_HOST', 'https://api.stability.ai')
          engine_id = prefs['model_checkpoint']# if prefs['model_checkpoint'] == "stable-diffusion-v1-5" else "stable-diffusion-v1"
          url = f"{api_host}/v1/generation/{engine_id}/"#image-to-image"
          headers = {
              'Content-Type': 'application/json',
              'Accept': 'application/json',#'image/png',
              'Authorization': prefs['Stability_api_key'],
          }
          payload = {
              "cfg_scale": arg['guidance_scale'],
              "clip_guidance_preset": prefs['clip_guidance_preset'],
              "height": arg['height'],
              "width": arg['width'],
              "sampler": prefs['generation_sampler'],
              "samples": arg['batch_size'],
              "seed": arg['seed'],
              "steps": arg['steps'],
              "text_prompts": [
                  {
                      "text": pr[0] if type(pr) == list else pr,
                      "weight": 1
                  }
              ],
          }
          if bool(arg['negative_prompt']):
            payload['text_prompts'].append({"text": arg['negative_prompt'][0] if type(arg['negative_prompt']) == list else arg['negative_prompt'], "weight": -10})
          if bool(arg['mask_image']) or (bool(arg['init_image']) and arg['alpha_mask']):
            if not bool(arg['init_image']):
              clear_last()
              prt(f"ERROR: You have not selected an init_image to go with your image mask..")
              continue
            if arg['init_image'].startswith('http'):
              i_response = requests.get(arg['init_image'])
              init_img = PILImage.open(BytesIO(i_response.content)).convert("RGB")
            else:
              if os.path.isfile(arg['init_image']):
                init_img = PILImage.open(arg['init_image'])
              else:
                clear_last()
                prt(f"ERROR: Couldn't find your init_image {arg['init_image']}")
            init_img = init_img.resize((arg['width'], arg['height']))
            buff = BytesIO()
            init_img.save(buff, format="PNG")
            buff.seek(0)
            img_str = io.BufferedReader(buff).read()
            #init_image = preprocess(init_img)
            if not arg['alpha_mask']:
              if arg['mask_image'].startswith('http'):
                i_response = requests.get(arg['mask_image'])
                mask_img = PILImage.open(BytesIO(i_response.content)).convert("RGB")
              else:
                if os.path.isfile(arg['mask_image']):
                  mask_img = PILImage.open(arg['mask_image'])
                else:
                  clear_last()
                  prt(f"ERROR: Couldn't find your mask_image {arg['mask_image']}")
              mask = mask_img.resize((arg['width'], arg['height']))

              buff = BytesIO()
              mask.save(buff, format="PNG")
              buff.seek(0)
              mask_str = io.BufferedReader(buff).read()
            #payload["step_schedule_end"] = 0.01
            payload["step_schedule_start"] = 1# - arg['init_image_strength']
            files = {
                'init_image': img_str,#base64.b64encode(init_img.tobytes()).decode(),#open(init_img, 'rb'),
                #'mask_image': mask_str,
                'mask_source': "INIT_IMAGE_ALPHA" if arg['alpha_mask'] else "MASK_IMAGE_BLACK" if arg['invert_mask'] else "MASK_IMAGE_WHITE",
                'options': (None, json.dumps(payload)),
            }
            if not arg['alpha_mask']:
              files['mask_image'] = mask_str
            pipe_used = "Stability-API Inpainting"
            #engine_id = prefs['model_checkpoint'] if prefs['model_checkpoint'] == "stable-diffusion-v1-5" else "stable-diffusion-v1"
            response = requests.post(url+"image-to-image/masking", headers=headers, files=files)
            #answers = stability_api.generate(prompt=pr, height=arg['height'], width=arg['width'], mask_image=mask, init_image=init_img, start_schedule= 1 - arg['init_image_strength'], steps=arg['steps'], cfg_scale=arg['guidance_scale'], samples=arg['batch_size'], safety=not prefs["disable_nsfw_filter"], seed=arg['seed'], sampler=SD_sampler)
          elif bool(arg['init_image']):
            if arg['init_image'].startswith('http'):
              i_response = requests.get(arg['init_image'])
              init_img = PILImage.open(BytesIO(i_response.content)).convert("RGB")
            else:
              if os.path.isfile(arg['init_image']):
                init_img = PILImage.open(arg['init_image']).convert("RGB")
              else:
                clear_last()
                prt(f"ERROR: Couldn't find your init_image {arg['init_image']}")
            init_img = init_img.resize((arg['width'], arg['height']))

            buff = BytesIO()
            init_img.save(buff, format="PNG")
            buff.seek(0)
            img_str = io.BufferedReader(buff).read()
            #img_str = open(buff.read(), 'rb') #base64.b64encode(buff.getvalue())  init_img.tobytes("raw")
            payload["step_schedule_end"] = 0.01
            payload["step_schedule_start"] = 1 - arg['init_image_strength']
            files = {
                'init_image': img_str,#base64.b64encode(init_img.tobytes()).decode(),#open(init_img, 'rb'),
                'options': (None, json.dumps(payload)),
            }
            pipe_used = "Stability-API Image-to-Image"
            response = requests.post(url+"image-to-image", headers=headers, files=files)
            #answers = stability_api.generate(prompt=pr, height=arg['height'], width=arg['width'], init_image=init_img, start_schedule= 1 - arg['init_image_strength'], steps=arg['steps'], cfg_scale=arg['guidance_scale'], samples=arg['batch_size'], safety=not prefs["disable_nsfw_filter"], seed=arg['seed'], sampler=SD_sampler)
          else:
            pipe_used = "Stability-API Text-to-Image"
            response = requests.post(url+"text-to-image", headers=headers, json=payload)
            #answers = stability_api.generate(prompt=pr, height=arg['height'], width=arg['width'], steps=arg['steps'], cfg_scale=arg['guidance_scale'], seed=arg['seed'], samples=arg['batch_size'], safety=False, sampler=SD_sampler)
          clear_last(update=False)
          clear_last()
          if response != None:
            if response.status_code != 200:
              if response.status_code == 402:
                alert_msg(page, "Stability-API ERROR: Insufficient Credit Balance. Reload at DreamStudio.com...", content=Text(str(response.text)))
                return
              else:
                prt(Text(f"Stability-API ERROR {response.status_code}: " + str(response.text), selectable=True))
                print(payload)
                continue
            #with open(output_file, "wb") as f:
            #  f.write(response.content)
            artifacts = json.loads(response.content)
            for resp in artifacts['artifacts']:
              #print(f'{type(resp)} - {resp["seed"]}')
              if resp == None: continue
              images.append(PILImage.open(io.BytesIO(base64.b64decode(resp['base64']))))
            #print(f'{type(response.content)} {response.content}')
          if answers != None:
            for resp in answers:
              for artifact in resp.artifacts:
                print("Artifact reason: " + str(artifact.finish_reason))
                if artifact.finish_reason == generation.FILTER:
                  usable_image = False
                if artifact.finish_reason == generation.ARTIFACT_TEXT:
                  usable_image = False
                  prt(f"Couldn't process NSFW text in prompt.  Can't retry so change your request.")
                if artifact.type == generation.ARTIFACT_IMAGE:
                  images.append(PILImage.open(io.BytesIO(artifact.binary)))
      elif prefs['use_AIHorde_api'] and status['installed_AIHorde']:# or bool(prefs['use_AIHorde_api'] or (not status['installed_diffusers'] and status['installed_AIHorde'])):
        if not status['installed_AIHorde']:
          alert_msg(page, f"ERROR: To use AIHorde-API, you must run the install it first and have proper API key")
          return
        stats = Text("Stable Horde API Diffusion ")
        prt(stats)
        #prt('Stable Horde API Diffusion ')# + ('─' * 100))
        #print(f'"{SD_prompt}", height={SD_height}, width={SD_width}, steps={SD_steps}, cfg_scale={SD_guidance_scale}, seed={SD_seed}, sampler={generation_sampler}')
        #strikes = 0
        images = []
        arg['width'] = multiple_of_64(arg['width'])
        arg['height'] = multiple_of_64(arg['height'])
        prt(pb)
        answers = None
        response = None

        import requests
        from io import BytesIO
        import base64
        try:
            import cv2
        except ModuleNotFoundError:
            run_sp("pip install opencv-python", realtime=False)
            import cv2
            pass
        api_host = 'https://stablehorde.net/api'
        engine_id = prefs['AIHorde_model']
        api_check_url = f"{api_host}/v2/generate/check/"
        api_get_result_url = f"{api_host}/v2/generate/status/"
        url = f"{api_host}/v2/generate/async"
        headers = {
            #'Content-Type': 'application/json',
            #'Accept': 'application/json',
            'apikey': prefs['AIHorde_api_key'],
        }
        text_prompt = pr[0] if type(pr) == list else pr
        if bool(arg['negative_prompt']):
          text_prompt += "###" +arg['negative_prompt'][0] if type(arg['negative_prompt']) == list else arg['negative_prompt']
        payload = {
          "prompt": text_prompt,
          "nsfw": prefs["disable_nsfw_filter"],
          "models": [prefs["AIHorde_model"]]
        }
        params = {
          "cfg_scale": arg['guidance_scale'],
          "denoising_strength": arg['init_image_strength'],
          "width": arg['width'],
          "height": arg['height'],
          "sampler_name": prefs['AIHorde_sampler'],
          "n": arg['batch_size'],
          "seed": str(arg['seed']),
          "steps": arg['steps'],
        }
        if prefs['AIHorde_post_processing'] != "None":
          params['post_processing'] = [prefs['AIHorde_post_processing']]
        if bool(arg['mask_image']) or (bool(arg['init_image']) and arg['alpha_mask']):
          if not bool(arg['init_image']):
            clear_last()
            prt(f"ERROR: You have not selected an init_image to go with your image mask..")
            continue
          if arg['init_image'].startswith('http'):
            i_response = requests.get(arg['init_image'])
            init_img = PILImage.open(BytesIO(i_response.content)).convert("RGB")
          else:
            if os.path.isfile(arg['init_image']):
              init_img = PILImage.open(arg['init_image'])
            else:
              clear_last()
              prt(f"ERROR: Couldn't find your init_image {arg['init_image']}")
          init_img = init_img.resize((arg['width'], arg['height']))
          buff = BytesIO()
          init_img.save(buff, format="PNG")
          buff.seek(0)
          img_str = io.BufferedReader(buff).read()
          #init_image = preprocess(init_img)
          if not arg['alpha_mask']:
            if arg['mask_image'].startswith('http'):
              i_response = requests.get(arg['mask_image'])
              mask_img = PILImage.open(BytesIO(i_response.content)).convert("RGB")
            else:
              if os.path.isfile(arg['mask_image']):
                mask_img = PILImage.open(arg['mask_image'])
              else:
                clear_last()
                prt(f"ERROR: Couldn't find your mask_image {arg['mask_image']}")
            mask = mask_img.resize((arg['width'], arg['height']))

            buff = BytesIO()
            mask.save(buff, format="PNG")
            buff.seek(0)
            mask_str = io.BufferedReader(buff).read()
          payload['source_image'] = img_str
          if not arg['alpha_mask']:
            payload['source_mask'] = mask_str
          pipe_used = "Stable Horde-API Inpainting"
          payload['source_processing'] = "inpainting"
          #engine_id = prefs['model_checkpoint'] if prefs['model_checkpoint'] == "stable-diffusion-v1-5" else "stable-diffusion-v1"
          #response = requests.post(url+"image-to-image/masking", headers=headers, files=files)
          #answers = stability_api.generate(prompt=pr, height=arg['height'], width=arg['width'], mask_image=mask, init_image=init_img, start_schedule= 1 - arg['init_image_strength'], steps=arg['steps'], cfg_scale=arg['guidance_scale'], samples=arg['batch_size'], safety=not prefs["disable_nsfw_filter"], seed=arg['seed'], sampler=SD_sampler)
        elif bool(arg['init_image']):
          if arg['init_image'].startswith('http'):
            i_response = requests.get(arg['init_image'])
            init_img = PILImage.open(BytesIO(i_response.content)).convert("RGB")
          else:
            if os.path.isfile(arg['init_image']):
              init_img = PILImage.open(arg['init_image']).convert("RGB")
            else:
              clear_last()
              prt(f"ERROR: Couldn't find your init_image {arg['init_image']}")
          init_img = init_img.resize((arg['width'], arg['height']))

          buff = BytesIO()
          init_img.save(buff, format="PNG")
          buff.seek(0)
          img_str = io.BufferedReader(buff).read()
          #img_str = open(buff.read(), 'rb') #base64.b64encode(buff.getvalue())  init_img.tobytes("raw")
          payload['source_image'] = img_str
          pipe_used = "Stable Horde-API Image-to-Image"
          payload['source_processing'] = "img2img"
          #answers = stability_api.generate(prompt=pr, height=arg['height'], width=arg['width'], init_image=init_img, start_schedule= 1 - arg['init_image_strength'], steps=arg['steps'], cfg_scale=arg['guidance_scale'], samples=arg['batch_size'], safety=not prefs["disable_nsfw_filter"], seed=arg['seed'], sampler=SD_sampler)
        else:
          pipe_used = "Stable Horde-API Text-to-Image"
          #response = requests.post(url+"text-to-image", headers=headers, json=payload)
          #answers = stability_api.generate(prompt=pr, height=arg['height'], width=arg['width'], steps=arg['steps'], cfg_scale=arg['guidance_scale'], seed=arg['seed'], samples=arg['batch_size'], safety=False, sampler=SD_sampler)
        payload["params"] = params
        #print(params)
        response = requests.post(url, headers=headers, json=payload)

        if response != None:
          if response.status_code != 202:
            if response.status_code == 400:
              alert_msg(page, "Stable Horde-API ERROR: Validation Error...", content=Text(str(response.text)))
              return
            else:
              prt(Text(f"Stable Horde-API ERROR {response.status_code}: " + str(response.text), selectable=True))
              print(payload)
              continue
          #with open(output_file, "wb") as f:
          #  f.write(response.content)

        artifacts = json.loads(response.content)
        q_id = artifacts['id']
        #print(str(artifacts))
        #stats = Text("")
        #prt(stats)
        elapsed_seconds = 0
        try:
          while True:
            if abort_run: break
            check_response = requests.get(api_check_url + q_id)
            check = json.loads(check_response.content)
            try:
              div = check['wait_time'] + elapsed_seconds
              percentage = (1 - check['wait_time'] / div)
            except Exception:
              div = 0
              continue
              pass
            if div == 0: continue
            pb.value = percentage
            pb.update()
            status_txt = f"Stable Horde API Diffusion - Queued Position: {check['queue_position']} - Waiting: {check['waiting']} - Wait Time: {check['wait_time']}"
            stats.value = status_txt #str(check)
            stats.update()
            if bool(check['done']):
              break
            time.sleep(1)
            elapsed_seconds += 1
        except Exception as e:
          alert_msg(page, f"EXCEPTION ERROR: Unknown error processing image. Check parameters and try again. Restart app if persists.", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip(), selectable=True)]))
          return
        get_response = requests.get(api_get_result_url + q_id)
        final_results = json.loads(get_response.content)
        clear_last(update=False)
        clear_last()
        for gen in final_results["generations"]:
          #print(f'{type(resp)} - {resp["seed"]}')
          if gen["censored"]:
            prt(f"Couldn't process NSFW text in prompt.  Can't retry so change your request.")
            continue
          img_response = requests.get(gen['img'])
          webp_file = io.BytesIO(img_response.content)
          cv_img = cv2.imdecode(np.frombuffer(webp_file.read(), np.uint8), cv2.IMREAD_UNCHANGED)
          images.append(PILImage.fromarray(cv_img))
          #cv_img = cv2.imdecode(io.BytesIO(base64.b64decode(gen['img'])), cv2.IMREAD_COLOR)
          #cv_img = cv2.imdecode(np.frombuffer(base64.b64decode(gen['img']), dtype=np.uint8), cv2.IMREAD_UNCHANGED)
          #decoded_string = base64.b64decode(gen['img'])
          #nparr = np.frombuffer(decoded_string, np.uint8)
          #cv_img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
          #print(gen['img'])
          #print(str(type(gen['img'])))
          #img_bytes = io.BytesIO(decoded_string)
          #img = Image.open(io.BytesIO(img_response.content))
          #pil_image = PILImage.open(webp_file)
          #images.append(PILImage.open(io.BytesIO(img_response.content)))
          #images.append(PILImage.open(pil_image))
          #images.append(PILImage.open(img_response.content))
          #images.append(PILImage.open(io.BytesIO(base64.b64decode(gen['img']))).convert('RGB'))
          #print(f'{type(response.content)} {response.content}')
          '''if answers != None:
            for resp in answers:
              for artifact in resp.artifacts:
                #print("Artifact reason: " + str(artifact.finish_reason))
                if artifact.finish_reason == generation.FILTER:
                  usable_image = False
                if artifact.finish_reason == generation.ARTIFACT_TEXT:
                  usable_image = False
                  prt(f"Couldn't process NSFW text in prompt.  Can't retry so change your request.")
                if artifact.type == generation.ARTIFACT_IMAGE:
                  images.append(PILImage.open(io.BytesIO(artifact.binary)))'''
      else:
        #from torch.amp.autocast_mode import autocast
        #precision_scope = autocast if prefs['precision']=="autocast" else nullcontext
        try:
          if use_custom_scheduler and not bool(arg['init_image']) and not bool(arg['mask_image']) and not bool(arg['prompt2']):
            # Not implemented correctly anymore, old code but might reuse custom
            text_input = tokenizer(pr[0] if type(pr) == list else pr, padding="max_length", max_length=tokenizer.model_max_length, truncation=True, return_tensors="pt")
            with torch.no_grad():
              text_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]# We'll also get the unconditional text embeddings for classifier-free guidance, which are just the embeddings for the padding token (empty text). They need to have the same shape as the conditional text_embeddings (batch_size and seq_length)
            max_length = text_input.input_ids.shape[-1]
            uncond_input = tokenizer([""] * arg['batch_size'], padding="max_length", max_length=max_length, return_tensors="pt")
            with torch.no_grad():
              uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0]   #For classifier-free guidance, we need to do two forward passes. One with the conditioned input (`text_embeddings`), and another with the unconditional embeddings (`uncond_embeddings`). In practice, we can concatenate both into a single batch to avoid doing two forward passes.

            text_embeddings = torch.cat([uncond_embeddings, text_embeddings])#Generate the intial random noise.
            #if generator:
            #latents = torch.randn((arg['batch_size'], unet.in_channels, arg['height'], arg['width']), generator=generator)
            latents = torch.randn((arg['batch_size'], unet.in_channels, arg['height'] // 8,  arg['width'] // 8), generator=generator)
            #else:
            #  latents = torch.randn((batch_size, unet.in_channels, arg['height'] // 8, arg['width'] // 8))
            latents = latents.to(torch_device)
            latents.shape
            #Cool  64×64  is expected. The model will transform this latent representation (pure noise) into a 512 × 512 image later on.
            #Next, we initialize the scheduler with our chosen num_inference_steps. This will compute the sigmas and exact time step values to be used during the denoising process.
            scheduler.set_timesteps(arg['steps'])#The LMS Discrete scheduler needs to multiple the `latents` by its `sigma` values. Let's do this here
            if prefs['scheduler_mode'] == "LMS Discrete" or prefs['scheduler_mode'] == "Score-SDE-Vp":
              latents = latents * scheduler.sigmas[0]#We are ready to write the denoising loop.
            from tqdm.auto import tqdm
            clear_pipes("unet")
            if unet is None:
              unet = get_unet_pipe()
            #with precision_scope("cuda"):
            #with autocast("cuda"):
            for i, t in tqdm(enumerate(scheduler.timesteps)):
              # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.
              latent_model_input = torch.cat([latents] * 2)
              if prefs['scheduler_mode'] == "LMS Discrete" or prefs['scheduler_mode'] == "Score-SDE-Vp":
                sigma = scheduler.sigmas[i]
                latent_model_input = latent_model_input / ((sigma**2 + 1) ** 0.5)
              # predict the noise residual
              if prefs['scheduler_mode'] == "DDPM":
                #TODO: Work in progress, still not perfect
                noisy_sample = torch.randn(1, unet.config.in_channels, unet.config.sample_size, unet.config.sample_size)
                noisy_residual = unet(sample=noisy_sample, timestep=2)["sample"]
                less_noisy_sample = scheduler.step(model_output=noisy_residual, timestep=2, sample=noisy_sample)["prev_sample"]
                less_noisy_sample.shape
              with torch.no_grad():
                noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings).images
              # perform guidance
              noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)
              noise_pred = noise_pred_uncond + arg['guidance_scale'] * (noise_pred_text - noise_pred_uncond)
              # compute the previous noisy sample x_t -> x_t-1
              latents = scheduler.step(noise_pred, i, latents)["prev_sample"]#We now use the vae to decode the generated latents back into the image.
            # scale and decode the image latents with vae
            latents = 1 / 0.18215 * latents
            with torch.no_grad():
              image = vae.decode(latents)
            image = (image / 2 + 0.5).clip(0, 1)
            image = image.detach().cpu().permute(0, 2, 3, 1).numpy()
            uint8_images = (image * 255).round().astype("uint8")
            #for img in uint8_images: images.append(Image.fromarray(img))
            images = [PILImage.fromarray(img) for img in uint8_images]
          else:
            if bool(arg['use_clip_guided_model']) and status['installed_clip']:
              if bool(arg['init_image']) or bool(arg['mask_image']):
                #raise ValueError("Cannot use CLIP Guided Model with init or mask image yet.")
                alert_msg(page, "Cannot use CLIP Guided Model with init or mask image yet.")
                return
              clear_pipes("clip_guided")
              if pipe_clip_guided is None:
                prt(Installing("Initializing CLIP-Guided Pipeline..."))
                pipe_clip_guided = get_clip_guided_pipe()
                clear_last()
              clip_prompt = arg["clip_prompt"] if arg["clip_prompt"].strip() != "" else None
              if bool(arg["unfreeze_unet"]):
                pipe_clip_guided.unfreeze_unet()
              else:
                pipe_clip_guided.freeze_unet()
              if bool(arg["unfreeze_vae"]):
                pipe_clip_guided.unfreeze_vae()
              else:
                pipe_clip_guided.freeze_vae()
              # TODO: Figure out why it's broken with use_cutouts=False and doesn't generate, hacking it True for now
              arg["use_cutouts"] = True
              prt(pb)
              page.auto_scrolling(False)
              pipe_used = "CLIP Guided"
              images = pipe_clip_guided(pr, height=arg['height'], width=arg['width'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], clip_prompt=clip_prompt, clip_guidance_scale=arg["clip_guidance_scale"], num_cutouts=int(arg["num_cutouts"]) if arg["use_cutouts"] else None, use_cutouts=arg["use_cutouts"], generator=generator).images
              clear_last()
              page.auto_scrolling(True)
            elif bool(prefs['use_conceptualizer']) and status['installed_conceptualizer']:
              clear_pipes("conceptualizer")
              if pipe_conceptualizer is None:
                prt(Installing("Initializing Conceptualizer Pipeline..."))
                pipe_conceptualizer = get_conceptualizer(page)
                clear_last()
              total_steps = arg['steps']
              prt(pb)
              page.auto_scrolling(False)
              pipe_used = f"Conceptualizer {prefs['concepts_model']}"
              images = pipe_conceptualizer(prompt=pr, negative_prompt=arg['negative_prompt'], height=arg['height'], width=arg['width'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback=callback_fn, callback_steps=1).images
              clear_last()
              page.auto_scrolling(True)
            elif bool(arg['mask_image']) or (not bool(arg['mask_image']) and bool(arg['init_image']) and bool(arg['alpha_mask'])):
              if not bool(arg['init_image']):
                alert_msg(page, f"ERROR: You have not selected an init_image to go with your image mask..")
                return
              #if pipe_inpainting is None:
              #  pipe_inpainting = get_inpainting_pipe()
              if prefs['use_inpaint_model'] and status['installed_img2img']:
                clear_pipes("img2img")
                if pipe_img2img is None:
                  prt(Installing("Initializing Inpaint Pipeline..."))
                  pipe_img2img = get_img2img_pipe()
                  clear_last()
              elif prefs['use_SDXL'] and status['installed_SDXL']:
                if status['loaded_SDXL'] == "inpainting":
                  clear_pipes("SDXL")
                else:
                  clear_pipes()
                if pipe_SDXL is None:
                  prt(Installing("Initializing Stable Diffusion XL Inpainting Pipeline..."))
                  get_SDXL_pipe("inpainting")
                  clear_last()
              else:
                clear_pipes("txt2img")
                if pipe is None:
                  prt(Installing("Initializing Long Prompt Weighting Inpaint Pipeline..."))
                  pipe = get_txt2img_pipe()
                  clear_last()
              '''if pipe_img2img is None:
                try:
                  pipe_img2img = get_img2img_pipe()
                except NameError:
                  prt(f"{Color.RED}You must install the image2image Pipeline above.{Color.END}")
                finally:
                  raise NameError("You must install the image2image Pipeline above")'''
              import requests
              from io import BytesIO
              if arg['init_image'].startswith('http'):
                response = requests.get(arg['init_image'])
                init_img = PILImage.open(BytesIO(response.content))
              else:
                if os.path.isfile(arg['init_image']):
                  init_img = PILImage.open(arg['init_image'])
                else: prt(f"ERROR: Couldn't find your init_image {arg['init_image']}")
              if bool(arg['alpha_mask']):
                init_img = init_img.convert("RGBA")
              else:
                init_img = init_img.convert("RGB")
              init_img = init_img.resize((arg['width'], arg['height']))
              #init_image = preprocess(init_img)
              mask_img = None
              if not bool(arg['mask_image']) and bool(arg['alpha_mask']):
                mask_img = init_img.convert('RGBA')
                red, green, blue, alpha = PILImage.Image.split(init_img)
                mask_img = alpha.convert('L')
              else:
                if arg['mask_image'].startswith('http'):
                  response = requests.get(arg['mask_image'])
                  mask_img = PILImage.open(BytesIO(response.content))
                else:
                  if os.path.isfile(arg['mask_image']):
                    mask_img = PILImage.open(arg['mask_image'])
                  else: prt(f"ERROR: Couldn't find your mask_image {arg['mask_image']}")
              if arg['invert_mask'] and not arg['alpha_mask']:
                from PIL import ImageOps
                mask_img = ImageOps.invert(mask_img.convert('RGB'))
              mask_img = mask_img.convert("L")
              mask_img = mask_img.resize((arg['width'], arg['height']), resample=PILImage.Resampling.LANCZOS).convert("RGB")
              #mask = mask_img.resize((arg['width'], arg['height']))
              #mask = np.array(mask).astype(np.float32) / 255.0
              #mask = np.tile(mask,(4,1,1))
              #mask = mask[None].transpose(0, 1, 2, 3)
              #mask[np.where(mask != 0.0 )] = 1.0 #make sure mask is actually valid
              #mask_img = torch.from_numpy(mask)
              prt(pb)
              page.auto_scrolling(False)
              #with autocast("cuda"):
              if prefs['use_inpaint_model'] and status['installed_img2img']:
                pipe_used = "Diffusers Inpaint"
                images = pipe_img2img(prompt=pr, negative_prompt=arg['negative_prompt'], mask_image=mask_img, image=init_img, strength= 1 - arg['init_image_strength'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback=callback_fn, callback_steps=1).images
              elif prefs['use_SDXL'] and status['installed_SDXL']:
                pipe_used = "Stable Diffusion XL Inpainting"
                high_noise_frac = prefs['SDXL_high_noise_frac']
                total_steps = int(arg['steps'] * high_noise_frac)
                if prefs['SDXL_compel']:
                  prompt_embed, pooled = compel_base(pr)
                  negative_embed, negative_pooled = compel_base(arg['negative_prompt'])
                  #[prompt_embed, negative_embed] = compel_base.pad_conditioning_tensors_to_same_length([prompt_embed, negative_embed])
                  image = pipe_SDXL(prompt_embeds=prompt_embed, pooled_prompt_embeds=pooled, negative_prompt_embeds=negative_embed, negative_pooled_prompt_embeds=negative_pooled, image=init_img, mask_image=mask_img, output_type="latent", denoising_end=high_noise_frac, target_size=(arg['width'], arg['height']), num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback=callback_fn, callback_steps=1).images#[0]
                else:
                  image = pipe_SDXL(prompt=pr, negative_prompt=arg['negative_prompt'], image=init_img, mask_image=mask_img, output_type="latent", denoising_end=high_noise_frac, target_size=(arg['width'], arg['height']), num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback=callback_fn, callback_steps=1).images#[0]
                total_steps = int(arg['steps'] * (1 - high_noise_frac))
                if prefs['SDXL_compel']:
                  prompt_embed, pooled = compel_refiner(pr)
                  negative_embed, negative_pooled = compel_refiner(arg['negative_prompt'])
                  #[prompt_embed, negative_embed] = compel_refiner.pad_conditioning_tensors_to_same_length([prompt_embed, negative_embed])
                  images = pipe_SDXL_refiner(prompt_embeds=prompt_embed, pooled_prompt_embeds=pooled, negative_prompt_embeds=negative_embed, negative_pooled_prompt_embeds=negative_pooled, image=image, mask_image=mask_img, num_inference_steps=arg['steps'], denoising_start=high_noise_frac, guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback=callback_fn, callback_steps=1).images
                  del prompt_embed, negative_embed, pooled, negative_pooled
                else:
                  images = pipe_SDXL_refiner(prompt=pr, negative_prompt=arg['negative_prompt'], image=image, mask_image=mask_img, num_inference_steps=arg['steps'], denoising_start=high_noise_frac, guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback=callback_fn, callback_steps=1).images
                #images = pipe_SDXL_refiner(prompt=pr, negative_prompt=arg['negative_prompt'], image=init_img, mask_image=mask_img, strength=arg['init_image_strength'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback=callback_fn, callback_steps=1).images
                flush()
              else:
                pipe_used = "Long Prompt Weight Inpaint"
                images = pipe.inpaint(prompt=pr, negative_prompt=arg['negative_prompt'], mask_image=mask_img, image=init_img, strength= 1 - arg['init_image_strength'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback=callback_fn, callback_steps=1).images
              clear_last()
              page.auto_scrolling(True)
            elif bool(arg['init_image']):
              if not status['installed_txt2img'] and not (prefs['use_imagic'] and status['installed_imagic']) and not (prefs['use_depth2img'] and status['installed_depth2img']) and not (prefs['use_alt_diffusion'] and status['installed_alt_diffusion']) and not status['installed_SDXL']:
                alert_msg(page, f"CRITICAL ERROR: You have not installed a image2image pipeline yet.  Run in the Installer..")
                continue
              if prefs['use_versatile'] and status['installed_versatile']:
                if len(pr.strip()) > 2: # Find another way to know the difference
                  clear_pipes("versatile_dualguided")
                  if pipe_versatile_dualguided is None:
                    prt(Installing("Initializing Versatile Dual-Guided Pipeline..."))
                    pipe_versatile_dualguided = get_versatile_dualguided_pipe()
                    clear_last()
                else:
                  clear_pipes("versatile_variation")
                  if pipe_versatile_variation is None:
                    prt(Installing("Initializing Versatile Image Variation Pipeline..."))
                    pipe_versatile_variation = get_versatile_variation_pipe()
                    clear_last()
              elif prefs['use_SDXL'] and status['installed_SDXL']:
                if status['loaded_SDXL'] == "image2image":
                  clear_pipes("SDXL")
                else:
                  clear_pipes()
                if pipe_SDXL_refiner is None:
                  prt(Installing("Initializing Stable Diffusion XL Image2Image Pipeline..."))
                  get_SDXL_pipe("image2image")
                  clear_last()
              elif prefs['use_alt_diffusion'] and status['installed_alt_diffusion']:
                clear_pipes("alt_diffusion_img2img")
                if pipe_alt_diffusion_img2img is None:
                  prt(Installing("Initializing AltDiffusion Image2Image Pipeline..."))
                  pipe_alt_diffusion_img2img = get_alt_diffusion_img2img_pipe()
                  clear_last()
              elif prefs['use_depth2img'] and status['installed_depth2img']:
                clear_pipes("depth")
                if pipe_depth is None:
                  prt(Installing("Initializing SD2 Depth2Image Pipeline..."))
                  pipe_depth = get_depth_pipe()
                  clear_last()
              elif prefs['use_inpaint_model'] and status['installed_img2img']:
                clear_pipes("img2img")
                if pipe_img2img is None:
                  prt(Installing("Initializing Inpaint Pipeline..."))
                  pipe_img2img = get_img2img_pipe()
                  clear_last()
              elif prefs['use_imagic'] and status['installed_imagic']:
                clear_pipes("imagic")
                if pipe_imagic is None:
                  prt(Installing("Initializing iMagic Image2Image Pipeline..."))
                  pipe_imagic = get_imagic_pipe()
                  clear_last()
              else:
                clear_pipes("txt2img")
                if pipe is None:
                  prt(Installing("Initializing Long Prompt Weighting Image2Image Pipeline..."))
                  pipe = get_txt2img_pipe()
                  clear_last()
              '''if pipe_img2img is None:
                try:
                  pipe_img2img = get_img2img_pipe()
                except NameError:
                  prt(f"{Color.RED}You must install the image2image Pipeline above.{Color.END}")
                  raise NameError("You must install the image2image Pipeline above")'''
                #finally:
              import requests
              from io import BytesIO
              if arg['init_image'].startswith('http'):
                response = requests.get(arg['init_image'])
                init_img = PILImage.open(BytesIO(response.content)).convert("RGB")
              else:
                if os.path.isfile(arg['init_image']):
                  init_img = PILImage.open(arg['init_image']).convert("RGB")
                else:
                  alert_msg(page, f"ERROR: Couldn't find your init_image {arg['init_image']}")
              init_img = init_img.resize((arg['width'], arg['height']))
              #init_image = preprocess(init_img)
              #white_mask = PILImage.new("RGB", (arg['width'], arg['height']), (255, 255, 255))
              prt(pb)
              time.sleep(0.4)
              page.auto_scrolling(False)
              #with autocast("cuda"):
              #images = pipe_img2img(prompt=pr, negative_prompt=arg['negative_prompt'], init_image=init_img, mask_image=white_mask, strength= 1 - arg['init_image_strength'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback=callback_fn, callback_steps=1).images
              if prefs['use_versatile'] and status['installed_versatile']:
                if len(pr.strip()) > 2:
                  pipe_used = "Versatile Dual-Guided"
                  images = pipe_versatile_dualguided(prompt=pr, negative_prompt=arg['negative_prompt'], image=init_img, text_to_image_strength= arg['init_image_strength'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback=callback_fn, callback_steps=1).images
                else:
                  pipe_used = "Versatile Variation"
                  images = pipe_versatile_variation(negative_prompt=arg['negative_prompt'], image=init_img, num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback=callback_fn, callback_steps=1).images
              elif prefs['use_SDXL'] and status['installed_SDXL']:
                pipe_used = "Stable Diffusion XL Image-to-Image"
                high_noise_frac = prefs['SDXL_high_noise_frac']
                total_steps = int(arg['steps'] * high_noise_frac)
                if prefs['SDXL_compel']:
                  prompt_embed, pooled = compel_base(pr)
                  negative_embed, negative_pooled = compel_base(arg['negative_prompt'])
                  #[prompt_embed, negative_embed] = compel_base.pad_conditioning_tensors_to_same_length([prompt_embed, negative_embed])
                  image = pipe_SDXL(prompt_embeds=prompt_embed, pooled_prompt_embeds=pooled, negative_prompt_embeds=negative_embed, negative_pooled_prompt_embeds=negative_pooled, image=init_img, strength=arg['init_image_strength'], output_type="latent", denoising_end=high_noise_frac, target_size=(arg['width'], arg['height']), num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback=callback_fn, callback_steps=1).images#[0]
                else:
                  #image = pipe_SDXL(prompt=pr, negative_prompt=arg['negative_prompt'], image=init_img, strength=arg['init_image_strength'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback=callback_fn, callback_steps=1).images
                  image = pipe_SDXL(prompt=pr, negative_prompt=arg['negative_prompt'], image=init_img, strength=arg['init_image_strength'], output_type="latent", denoising_end=high_noise_frac, target_size=(arg['width'], arg['height']), num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback=callback_fn, callback_steps=1).images#[0]
                total_steps = int(arg['steps'] * (1 - high_noise_frac))
                if prefs['SDXL_compel']:
                  prompt_embed, pooled = compel_refiner(pr)
                  negative_embed, negative_pooled = compel_refiner(arg['negative_prompt'])
                  #[prompt_embed, negative_embed] = compel_refiner.pad_conditioning_tensors_to_same_length([prompt_embed, negative_embed])
                  images = pipe_SDXL_refiner(prompt_embeds=prompt_embed, pooled_prompt_embeds=pooled, negative_prompt_embeds=negative_embed, negative_pooled_prompt_embeds=negative_pooled, image=image, num_inference_steps=arg['steps'], denoising_start=high_noise_frac, guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback=callback_fn, callback_steps=1).images
                  del prompt_embed, negative_embed, pooled, negative_pooled
                else:
                  images = pipe_SDXL_refiner(prompt=pr, negative_prompt=arg['negative_prompt'], image=image, num_inference_steps=arg['steps'], denoising_start=high_noise_frac, guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback=callback_fn, callback_steps=1).images
                #images = pipe_SDXL(prompt=pr, negative_prompt=arg['negative_prompt'], image=init_img, strength=arg['init_image_strength'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback=callback_fn, callback_steps=1).images
                flush()
              elif prefs['use_alt_diffusion'] and status['installed_alt_diffusion']:
                pipe_used = "AltDiffusion Image-to-Image"
                with torch.autocast("cuda"):
                  images = pipe_alt_diffusion_img2img(prompt=pr, negative_prompt=arg['negative_prompt'], image=init_img, strength= 1 - arg['init_image_strength'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback=callback_fn, callback_steps=1).images
              elif prefs['use_depth2img'] and status['installed_depth2img']:
                pipe_used = "Depth-to-Image"
                images = pipe_depth(prompt=pr, negative_prompt=arg['negative_prompt'], image=init_img, strength=arg['init_image_strength'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback=callback_fn, callback_steps=1).images
              elif prefs['use_inpaint_model'] and status['installed_img2img']:
                pipe_used = "Diffusers Inpaint Image-to-Image"
                white_mask = PILImage.new("RGB", (arg['width'], arg['height']), (255, 255, 255))
                images = pipe_img2img(prompt=pr, negative_prompt=arg['negative_prompt'], image=init_img, mask_image=white_mask, strength= 1 - arg['init_image_strength'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback=callback_fn, callback_steps=1).images
              elif prefs['use_imagic'] and status['installed_imagic']:
                pipe_used = "iMagic Image-to-Image"
                #only one element tensors can be converted to Python scalars
                total_steps = None
                res = pipe_imagic.train(pr, init_img, num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback=callback_fn, callback_steps=1).images
                images = []
                # TODO: alpha= arguments to customize which to make
                total_steps = 0
                res = pipe_imagic(alpha=1, callback=callback_fn, callback_steps=1)
                images.append(res.images[0])
                res = pipe_imagic(alpha=1.5, callback=callback_fn, callback_steps=1)
                images.append(res.images[0])
                res = pipe_imagic(alpha=2, callback=callback_fn, callback_steps=1)
                images.append(res.images[0])
              else:
                pipe_used = "Long Prompt Weight Image-to-Image"
                images = pipe.img2img(prompt=pr, negative_prompt=arg['negative_prompt'], image=init_img, strength= 1 - arg['init_image_strength'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback=callback_fn, callback_steps=1).images
              clear_last()
              page.auto_scrolling(True)
            elif bool(arg['prompt2']):
              if pipe is None:
                pipe = get_txt2img_pipe()
              #with precision_scope("cuda"):
              #    with torch.no_grad():
              pipe_used = "LPW Tween Lerp"
              images_tween = pipe.lerp_between_prompts(pr, arg["prompt2"], length = arg['tweens'], save=False, height=arg['height'], width=arg['width'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator)
              #print(str(images_tween))
              images = images_tween['images']
              #images = pipe(pr, height=arg['height'], width=arg['width'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator)["sample"]
            else:
              if prefs['use_composable'] and status['installed_composable']:
                clear_pipes("composable")
                if pipe_composable is None:
                  prt(Installing("Initializing Composable Text2Image Pipeline..."))
                  pipe_composable = get_composable_pipe()
                  clear_last()
              elif prefs['use_SDXL'] and status['installed_SDXL']:
                if status['loaded_SDXL'] == "text2image":
                  clear_pipes("SDXL")
                else:
                  clear_pipes()
                if pipe_SDXL is None:
                  prt(Installing("Initializing Stable Diffusion XL Text2Image Pipeline..."))
                  get_SDXL_pipe("text2image")
                  clear_last()
              elif prefs['use_alt_diffusion'] and status['installed_alt_diffusion']:
                clear_pipes("alt_diffusion")
                if pipe_alt_diffusion is None:
                  prt(Installing("Initializing AltDiffusion Text2Image Pipeline..."))
                  pipe_alt_diffusion = get_alt_diffusion_pipe()
                  clear_last()
              elif prefs['use_SAG'] and status['installed_SAG']:
                clear_pipes("SAG")
                if pipe_SAG is None:
                  prt(Installing("Initializing Self-Attention Guidance Text2Image Pipeline..."))
                  pipe_SAG = get_SAG_pipe()
                  clear_last()
              elif prefs['use_attend_and_excite'] and status['installed_attend_and_excite']:
                clear_pipes("attend_and_excite")
                if pipe_attend_and_excite is None:
                  prt(Installing("Initializing Attend and Excite Text2Image Pipeline..."))
                  pipe_attend_and_excite = get_attend_and_excite_pipe()
                  clear_last()
              elif prefs['use_versatile'] and status['installed_versatile']:
                clear_pipes("versatile_text2img")
                if pipe_versatile_text2img is None:
                  prt(Installing("Initializing Versatile Text2Image Pipeline..."))
                  pipe_versatile_text2img = get_versatile_text2img_pipe()
                  clear_last()
              elif prefs['use_safe'] and status['installed_safe']:
                clear_pipes("safe")
                if pipe_safe is None:
                  prt(Installing("Initializing Safe Stable Diffusion Pipeline..."))
                  pipe_safe = get_safe_pipe()
                  clear_last()
              elif prefs['use_panorama'] and status['installed_panorama']:
                clear_pipes("panorama")
                if pipe_panorama is None:
                  prt(Installing("Initializing Panorama MultiDiffusion Pipeline..."))
                  pipe_panorama = get_panorama_pipe()
                  clear_last()
              elif pipe is None:
                clear_pipes("txt2img")
                prt(Installing("Initializing Long Prompt Weighting Text2Image Pipeline..."))
                pipe = get_txt2img_pipe()
                clear_last()
              '''with io.StringIO() as buf, redirect_stdout(buf):
                get_text2image(page)
                output = buf.getvalue()
                page.Images.content.controls.append(Text(output.strip())
                page.Images.content.update()
                page.Images.update()
                page.update()'''
              total_steps = arg['steps']
              prt(pb)
              page.auto_scrolling(False)
              if prefs['use_composable'] and status['installed_composable']:
                weights = arg['negative_prompt'] #" 1 | 1"  # Equal weight to each prompt. Can be negative
                if not bool(weights):
                  segments = len(pr.split('|'))
                  weights = '|'.join(['1' * segments])
                pipe_used = "Composable Text-to-Image"
                images = pipe_composable(pr, height=arg['height'], width=arg['width'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], weights=weights, generator=generator, callback=callback_fn, callback_steps=1).images
              elif prefs['use_SDXL'] and status['installed_SDXL']:
                pipe_used = "Stable Diffusion XL Text-to-Image"
                #if arg['batch_size'] > 1:
                #    neg_prompts = [arg['negative_prompt']] * arg['batch_size']
                high_noise_frac = prefs['SDXL_high_noise_frac']
                #TODO: Figure out batch num_images_per_prompt + option to not refine , image[None, :] , num_images_per_prompt=arg['batch_size']
                total_steps = int(arg['steps'] * high_noise_frac)
                if prefs['SDXL_compel']:
                  print(f"pr:{pr} - neg: {arg['negative_prompt']}")
                  prompt_embed, pooled = compel_base(pr)
                  negative_embed, negative_pooled = compel_base(arg['negative_prompt'])
                  #[prompt_embed, negative_embed] = compel_base.pad_conditioning_tensors_to_same_length([prompt_embed, negative_embed])
                  image = pipe_SDXL(prompt_embeds=prompt_embed, pooled_prompt_embeds=pooled, negative_prompt_embeds=negative_embed, negative_pooled_prompt_embeds=negative_pooled, output_type="latent", denoising_end=high_noise_frac, height=arg['height'], width=arg['width'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback=callback_fn, callback_steps=1).images#[0]
                  del pooled, negative_pooled
                else:
                  image = pipe_SDXL(prompt=pr, negative_prompt=arg['negative_prompt'], output_type="latent", denoising_end=high_noise_frac, height=arg['height'], width=arg['width'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback=callback_fn, callback_steps=1).images#[0]
                total_steps = int(arg['steps'] * (1 - high_noise_frac))
                if prefs['SDXL_compel']:
                  prompt_embed_refiner, pooled_refiner = compel_refiner(pr)
                  negative_embed_refiner, negative_pooled_refiner = compel_refiner(arg['negative_prompt'])
                  #[prompt_embed_refiner, negative_embed_refiner] = compel_refiner.pad_conditioning_tensors_to_same_length([prompt_embed_refiner, negative_embed_refiner])
                  images = pipe_SDXL_refiner(prompt_embeds=prompt_embed_refiner, pooled_prompt_embeds=pooled_refiner, negative_prompt_embeds=negative_embed_refiner, negative_pooled_prompt_embeds=negative_pooled_refiner, image=image, num_inference_steps=arg['steps'], denoising_start=high_noise_frac, guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback=callback_fn, callback_steps=1).images
                  #images = pipe_SDXL_refiner(prompt=pr, negative_prompt=arg['negative_prompt'], image=image, num_inference_steps=arg['steps'], denoising_start=high_noise_frac, guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback=callback_fn, callback_steps=1).images
                  del prompt_embed_refiner, negative_embed_refiner, pooled_refiner, negative_pooled_refiner
                else:
                  images = pipe_SDXL_refiner(prompt=pr, negative_prompt=arg['negative_prompt'], image=image, num_inference_steps=arg['steps'], denoising_start=high_noise_frac, guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback=callback_fn, callback_steps=1).images
                flush()
              elif prefs['use_alt_diffusion'] and status['installed_alt_diffusion']:
                pipe_used = "AltDiffusion Text-to-Image"
                with torch.autocast("cuda"):
                  images = pipe_alt_diffusion(prompt=pr, negative_prompt=arg['negative_prompt'], height=arg['height'], width=arg['width'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback=callback_fn, callback_steps=1).images
              elif prefs['use_SAG'] and status['installed_SAG']:
                pipe_used = "Self-Attention Guidance Text-to-Image"
                #size = pipe_SAG.unet.config.sample_size * pipe_SAG.vae_scale_factor
                #arg['width'] = size
                #arg['height'] = size
                with torch.autocast("cuda"): #, height=arg['height'], width=arg['width']
                  images = pipe_SAG(prompt=pr, negative_prompt=arg['negative_prompt'], sag_scale=prefs['sag_scale'], height=arg['height'], width=arg['width'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback=callback_fn, callback_steps=1).images
              elif prefs['use_attend_and_excite'] and status['installed_attend_and_excite']:
                pipe_used = "Attend and Excite Text-to-Image"
                size = pipe_attend_and_excite.unet.config.sample_size * pipe_attend_and_excite.vae_scale_factor
                arg['width'] = size
                arg['height'] = size
                token_indices = []
                words = []
                ptext = pr[0] if type(pr) == list else pr
                ntext = arg['negative_prompt'][0] if type(arg['negative_prompt']) == list else arg['negative_prompt']
                for ti, w in enumerate(ptext.split()):
                  if w[:1] == '+':
                    token_indices.append(ti + 1)
                    words.append(w[1:])
                    #print(f'indices: {ti + 1} - {w[1:]}')
                  else:
                    words.append(w)
                ptext = ' '.join(words)
                pr = [ptext * arg['batch_size']] if type(pr) == list else ptext
                print(f"token_indices: {token_indices} | ptext: {ptext}")
                if len(token_indices) < 1:
                  token_indices = pipe_attend_and_excite.get_indices(ptext)
                print(f"indices: {token_indices}")
                #, negative_prompt=arg['negative_prompt'], num_images_per_prompt=int(arg['batch_size'])
                #images = pipe_attend_and_excite(prompt=ptext, token_indices=token_indices, max_iter_to_alter=int(prefs['max_iter_to_alter']), height=arg['height'], width=arg['width'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback=callback_fn, callback_steps=1).images
                images = pipe_attend_and_excite(prompt=ptext, negative_prompt=ntext, token_indices=token_indices, max_iter_to_alter=int(prefs['max_iter_to_alter']), height=arg['height'], width=arg['width'], num_images_per_prompt=int(arg['batch_size']), num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback=callback_fn, callback_steps=1).images
              elif prefs['use_versatile'] and status['installed_versatile']:
                pipe_used = "Versatile Text-to-Image"
                images = pipe_versatile_text2img(prompt=pr, negative_prompt=arg['negative_prompt'], height=arg['height'], width=arg['width'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback=callback_fn, callback_steps=1).images
              elif prefs['use_panorama'] and status['installed_panorama']:
                pipe_used = "MultiDiffusion Panorama Text-to-Image"
                arg['width'] = prefs['panorama_width']
                arg['height'] = 512
                images = pipe_panorama(prompt=pr, negative_prompt=arg['negative_prompt'], height=arg['height'], width=arg['width'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback=callback_fn, callback_steps=1, circular_padding=prefs['panorama_circular_padding']).images
              elif prefs['use_safe'] and status['installed_safe']:
                from diffusers.pipelines.stable_diffusion_safe import SafetyConfig
                s = prefs['safety_config']
                safety = SafetyConfig.WEAK if s == 'Weak' else SafetyConfig.MEDIUM if s == 'Medium' else SafetyConfig.STRONG if s == 'Strong' else SafetyConfig.MAX if s == 'Max' else SafetyConfig.STRONG
                pipe_used = f"Safe Diffusion {safety}"
                images = pipe_safe(prompt=pr, negative_prompt=arg['negative_prompt'], height=arg['height'], width=arg['width'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback=callback_fn, callback_steps=1, **safety).images
              else:
                pipe_used = "Long Prompt Weight Text-to-Image"
                images = pipe.text2img(prompt=pr, negative_prompt=arg['negative_prompt'], height=arg['height'], width=arg['width'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], generator=generator, callback=callback_fn, callback_steps=1).images
              '''if prefs['precision'] == "autocast":
                with autocast("cuda"):
                  images = pipe(pr, height=arg['height'], width=arg['width'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], seed = arg['seed'], generator=generator, callback=callback_fn, callback_steps=1)["sample"]
              else:
                with precision_scope("cuda"):
                  with torch.no_grad():
                    images = pipe(pr, height=arg['height'], width=arg['width'], num_inference_steps=arg['steps'], guidance_scale=arg['guidance_scale'], eta=arg['eta'], seed = arg['seed'], generator=generator, callback=callback_fn, callback_steps=1)["sample"]'''
              clear_last()
              page.auto_scrolling(True)
        except RuntimeError as e:
          clear_last()
          if 'out of memory' in str(e):
            alert_msg(page, f"CRITICAL ERROR: GPU ran out of memory! Flushing memory to save session... Try reducing image size.", content=Text(str(e).strip()))
            clear_pipes()
            pass
          else:
            alert_msg(page, f"RUNTIME ERROR: Unknown error processing image. Check parameters and try again. Restart app if persists.", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip(), selectable=True)]))
            pass
        except Exception as e:
          alert_msg(page, f"EXCEPTION ERROR: Unknown error processing image. Check parameters and try again. Restart app if persists.", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip(), selectable=True)]))
          abort_run = True
          pass
        finally:
          gc.collect()
          torch.cuda.empty_cache()

      txt2img_output = stable_dir #f'{stable_dir}/stable-diffusion/outputs/txt2img-samples'
      batch_output = prefs['image_output']
      if bool(prefs['batch_folder_name']):
        txt2img_output = os.path.join(stable_dir, prefs['batch_folder_name'])
        batch_output = os.path.join(prefs['image_output'], prefs['batch_folder_name'])
      if not os.path.exists(txt2img_output):
        os.makedirs(txt2img_output)
      if save_to_GDrive or storage_type == "Colab Google Drive":
        if not os.path.exists(batch_output):
          os.makedirs(batch_output)
      elif storage_type == "PyDrive Google Drive": # TODO: I'm not getting the parent folder id right, their docs got confusing
        newFolder = gdrive.CreateFile({'title': prefs['batch_folder_name'], "parents": [{"kind": "drive#fileLink", "id": prefs['image_output']}],"mimeType": "application/vnd.google-apps.folder"})
        newFolder.Upload()
        batch_output = newFolder
      else:
        if not os.path.exists(batch_output):
          os.makedirs(batch_output)

      filename = format_filename(pr[0] if type(pr) == list else pr)
      if images is None:
        prt(f"ERROR: Problem generating images, check your settings and run above blocks again, or report the error to Skquark if it really seems broken.")
        images = []

      idx = num = 0
      for image in images:
        cur_seed = arg['seed']
        if idx > 0:
          cur_seed += idx
          i_count = f'  ({idx + 1} of {len(images)})  '
          prt(Row([Text(i_count), Text(pr[0] if type(pr) == list else pr, expand=True, weight=FontWeight.BOLD), Text(f'seed: {cur_seed}     ')]))
          #prt(f'{pr[0] if type(pr) == list else pr} - seed:{cur_seed}')
        seed_suffix = "" if not prefs['file_suffix_seed'] else f"-{cur_seed}"
        if prefs['use_imagic'] and status['installed_imagic'] and bool(arg['init_image'] and not bool(arg['mask_image'])):
          if idx == 0: seed_suffix += '-alpha_1'
          if idx == 1: seed_suffix += '-alpha_1_5'
          if idx == 2: seed_suffix += '-alpha_2'
        fname = f'{prefs["file_prefix"]}{filename}{seed_suffix}'
        image_path = available_file(txt2img_output, fname, idx)
        num = int(image_path.rpartition('-')[2].partition('.')[0])
        #image_path = os.path.join(txt2img_output, f'{fname}-{idx}.png')
        image.save(image_path)
        #print(f'size:{os.path.getsize(f"{fname}-{idx}.png")}')
        if os.path.getsize(image_path) < 2000 or not usable_image: #False: #not sum(image.convert("L").getextrema()) in (0, 2): #image.getbbox():#
          os.remove(os.path.join(txt2img_output, f'{fname}-{num}.png'))
          if strikes >= retry_attempts_if_NSFW:
            if retry_attempts_if_NSFW != 0: prt("Giving up on finding safe image...")
            strikes = 0
            continue
          else: strikes += 1
          new_dream = None
          if isinstance(p, Dream):
            new_dream = p
            new_dream.prompt = pr[0] if type(pr) == list else pr
            new_dream.arg['seed'] = random.randint(0,4294967295)
          else:
            new_dream = Dream(p, arg=dict(seed=random.randint(0,4294967295)))
          updated_prompts.insert(p_idx+1, new_dream)
          prt(f"Filtered NSFW image, retrying prompt with new seed. Attempt {strikes} of {retry_attempts_if_NSFW}...")
          continue
        else: strikes = 0
        #if not prefs['display_upscaled_image'] or not prefs['apply_ESRGAN_upscale']:
          #print(f"Image path:{image_path}")
          #time.sleep(0.4)
          #prt(Row([Img(src=image_path, width=arg['width'], height=arg['height'], fit=ImageFit.FILL, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
          #display(image)
        #if bool(batch_folder_name):
        #  fpath = os.path.join(txt2img_output, batch_folder_name, f'{fname}-{idx}.png')
        #fpath = os.path.join(txt2img_output, f'{fname}-{idx}.png')
        #fpath = available_file(txt2img_output, fname, idx)
        fpath = image_path
        if txt2img_output != batch_output:
          new_file = available_file(batch_output, fname, num)
        else:
          new_file = image_path
        #print(f'fpath: {fpath} - idx: {idx}')
        if prefs['centipede_prompts_as_init_images']:
          shutil.copy(fpath, os.path.join(root_dir, 'init_images'))
          last_image = os.path.join(root_dir, 'init_images', f'{fname}-{num}.png')
        page.auto_scrolling(True)
        #if (not prefs['display_upscaled_image'] or not prefs['apply_ESRGAN_upscale']) and prefs['apply_ESRGAN_upscale']:
        if not prefs['display_upscaled_image'] and prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
          #print(f"Image path:{image_path}")
          upscaled_path = new_file #os.path.join(batch_output if save_to_GDrive else txt2img_output, new_file)
          #time.sleep(0.2)
          #prt(Row([GestureDetector(content=Img(src_base64=get_base64(fpath), width=arg['width'], height=arg['height'], fit=ImageFit.FILL, gapless_playback=True), data=new_file, on_long_press_end=download_image, on_secondary_tap=download_image)], alignment=MainAxisAlignment.CENTER))
          #prt(Row([GestureDetector(content=Img(src=fpath, width=arg['width'], height=arg['height'], fit=ImageFit.FILL, gapless_playback=True), data=new_file, on_long_press_end=download_image, on_secondary_tap=download_image)], alignment=MainAxisAlignment.CENTER))
          prt(Row([ImageButton(src=fpath, data=new_file, width=arg['width'], height=arg['height'], subtitle=pr[0] if type(pr) == list else pr, center=True, page=page)], alignment=MainAxisAlignment.CENTER))
          #prt(ImageButton(src=fpath, width=arg['width'], height=arg['height'], data=new_file, subtitle=pr[0] if type(pr) == list else pr, center=True, page=page))
          #time.sleep(0.3)
          #display(image)
        if prefs['use_upscale'] and status['installed_upscale']:
          clear_pipes(['upscale'])
          if pipe_upscale == None:
            prt(Installing("Initializing Stable Diffusion 2 Upscale Pipeline..."))
            pipe_upscale = get_upscale_pipe()
            clear_last()
          prt(Row([Text("Upscaling 4X"), pb]))
          try:
            output = pipe_upscale(prompt=pr, image=image, guidance_scale=arg['guidance_scale'], generator=generator, noise_level=prefs['upscale_noise_level'], callback=callback_fn, callback_steps=1)
            output.images[0].save(fpath)
            #clear_upscale()
          except Exception as e:
            alert_msg(page, "Error Upscaling Image.  Most likely out of Memory... Reduce image size to less than 512px.", content=Text(e))
            pass
          clear_last()
          #clear_upscale_pipe()
        if prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
          w = int(arg['width'] * prefs["enlarge_scale"])
          h = int(arg['height'] * prefs["enlarge_scale"])
          prt(Row([Text(f'Enlarging {prefs["enlarge_scale"]}X to {w}x{h}')], alignment=MainAxisAlignment.CENTER))
          os.chdir(os.path.join(dist_dir, 'Real-ESRGAN'))
          upload_folder = 'upload'
          result_folder = 'results'
          if os.path.isdir(upload_folder):
              shutil.rmtree(upload_folder)
          if os.path.isdir(result_folder):
              shutil.rmtree(result_folder)
          os.mkdir(upload_folder)
          os.mkdir(result_folder)
          short_name = f'{fname[:80]}-{num}.png'
          dst_path = os.path.join(dist_dir, 'Real-ESRGAN', upload_folder, short_name)
          #print(f'Moving {fpath} to {dst_path}')
          #shutil.move(fpath, dst_path)
          shutil.copy(fpath, dst_path)
          faceenhance = ' --face_enhance' if prefs["face_enhance"] else ''
          #python inference_realesrgan.py -n RealESRGAN_x4plus -i upload --outscale {enlarge_scale}{faceenhance}
          run_sp(f'python inference_realesrgan.py -n realesr-general-x4v3 -i upload --outscale {prefs["enlarge_scale"]}{faceenhance}', cwd=os.path.join(dist_dir, 'Real-ESRGAN'), realtime=False)
          out_file = short_name.rpartition('.')[0] + '_out.png'
          #print(f'move {root_dir}Real-ESRGAN/{result_folder}/{out_file} to {fpath}')
          #shutil.move(f'{root_dir}Real-ESRGAN/{result_folder}/{out_file}', fpath)
          shutil.move(os.path.join(dist_dir, 'Real-ESRGAN', result_folder, out_file), fpath)
          # !python inference_realesrgan.py --model_path experiments/pretrained_models/RealESRGAN_x4plus.pth --input upload --netscale 4 --outscale 3.5 --half --face_enhance
          os.chdir(stable_dir)
          clear_last(update=False)

        config_json = arg.copy()
        del config_json['batch_size']
        del config_json['n_iterations']
        del config_json['precision']
        config_json['prompt'] = pr[0] if type(pr) == list else pr
        config_json['sampler'] = prefs['generation_sampler'] if prefs['use_Stability_api'] else prefs['scheduler_mode']
        if bool(prefs['meta_ArtistName']): config_json['artist'] = prefs['meta_ArtistName']
        if bool(prefs['meta_Copyright']): config_json['copyright'] = prefs['meta_Copyright']
        if prefs['use_Stability_api']: del config_json['eta']
        del config_json['use_Stability']
        if not bool(config_json['negative_prompt']): del config_json['negative_prompt']
        if not bool(config_json['prompt2']):
          del config_json['prompt2']
          del config_json['tweens']
        if not bool(config_json['init_image']):
          del config_json['init_image']
          del config_json['init_image_strength']
          del config_json['alpha_mask']
        if not bool(config_json['mask_image']):
          del config_json['mask_image']
          del config_json['invert_mask']
        if not bool(config_json['use_clip_guided_model']):
          del config_json["use_clip_guided_model"]
          del config_json["clip_prompt"]
          del config_json["clip_guidance_scale"]
          del config_json["num_cutouts"]
          del config_json["use_cutouts"]
          del config_json["unfreeze_unet"]
          del config_json["unfreeze_vae"]
        else:
          config_json["clip_model_id"] = prefs['clip_model_id']
        if prefs['apply_ESRGAN_upscale']:
          config_json['upscale'] = f"Upscaled {prefs['enlarge_scale']}x with ESRGAN" + (" with GFPGAN Face-Enhance" if prefs['face_enhance'] else "")
        sampler_str = prefs['generation_sampler'] if prefs['use_Stability_api'] else prefs['scheduler_mode']
        config_json['pipeline'] = pipe_used
        config_json['scheduler_mode'] = sampler_str
        config_json['model_path'] = model_path
        if prefs['use_SDXL'] and status['installed_SDXL']:
          config_json['model_path'] = "stabilityai/stable-diffusion-xl-base-1.0"
        if prefs['save_image_metadata']:
          img = PILImage.open(fpath)
          metadata = PngInfo()
          metadata.add_text("artist", prefs['meta_ArtistName'])
          metadata.add_text("copyright", prefs['meta_Copyright'])
          metadata.add_text("software", "Stable Diffusion Deluxe" + f", upscaled {prefs['enlarge_scale']}x with ESRGAN" if prefs['apply_ESRGAN_upscale'] else "")
          metadata.add_text("title", pr[0] if type(pr) == list else pr)
          if prefs['save_config_in_metadata']:
            config = f"prompt: {pr[0] if type(pr) == list else pr}, seed: {cur_seed}, steps: {arg['steps']}, CGS: {arg['guidance_scale']}, iterations: {arg['n_iterations']}" + f", eta: {arg['eta']}" if not prefs['use_Stability_api'] else ""
            config += f", sampler: {sampler_str}"
            if bool(arg['init_image']): config += f", init_image: {arg['init_image']}, init_image_strength: {arg['init_image_strength']}"
            metadata.add_text("config", config)
            #metadata.add_text("prompt", p)
            metadata.add_text("config_json", json.dumps(config_json, ensure_ascii=True, indent=4))
          img.save(fpath, pnginfo=metadata)

        #new_file = available_file(batch_output if save_to_GDrive else txt2img_output, fname, idx)
        #new_file = fname #.rpartition('.')[0] #f'{file_prefix}{filename}'
        #if os.path.isfile(os.path.join(batch_output if save_to_GDrive else txt2img_output, f'{new_file}-{idx}.png')):
        #  new_file += '-' + random.choice(string.ascii_letters) + random.choice(string.ascii_letters)
        #new_file += f'-{idx}.png'
        if save_to_GDrive:
          shutil.copy(fpath, new_file)#os.path.join(batch_output, new_file))
          #shutil.move(fpath, os.path.join(batch_output, new_file))
        elif storage_type == "PyDrive Google Drive":
          #batch_output
          out_file = gdrive.CreateFile({'title': new_file})
          out_file.SetContentFile(fpath)
          out_file.Upload()
        elif bool(prefs['image_output']):
          shutil.copy(fpath, new_file)#os.path.join(batch_output, new_file))
        if prefs['save_config_json']:
          json_file = new_file.rpartition('.')[0] + '.json'
          with open(os.path.join(stable_dir, json_file), "w") as f:
            json.dump(config_json, f, ensure_ascii=False, indent=4)
          #if save_to_GDrive:
          shutil.copy(os.path.join(stable_dir, json_file), os.path.join(batch_output, json_file))
          if storage_type == "PyDrive Google Drive":
            #batch_output
            out_file = gdrive.CreateFile({'title': json_file})
            out_file.SetContentFile(os.path.join(stable_dir, json_file))
            out_file.Upload()
        output_files.append(os.path.join(batch_output if save_to_GDrive else txt2img_output, new_file))
        if (prefs['display_upscaled_image'] and prefs['apply_ESRGAN_upscale']):
          upscaled_path = os.path.join(batch_output if save_to_GDrive else txt2img_output, new_file)
          #time.sleep(0.4)
          #prt(Row([GestureDetector(content=Img(src_base64=get_base64(upscaled_path), width=arg['width'] * float(prefs["enlarge_scale"]), height=arg['height'] * float(prefs["enlarge_scale"]), fit=ImageFit.CONTAIN, gapless_playback=True), data=upscaled_path, on_long_press_end=download_image, on_secondary_tap=download_image)], alignment=MainAxisAlignment.CENTER))
          #prt(Row([GestureDetector(content=Img(src=upscaled_path, width=arg['width'] * float(prefs["enlarge_scale"]), height=arg['height'] * float(prefs["enlarge_scale"]), fit=ImageFit.CONTAIN, gapless_playback=True), data=upscaled_path, on_long_press_end=download_image, on_secondary_tap=download_image)], alignment=MainAxisAlignment.CENTER))
          prt(Row([ImageButton(src=upscaled_path, width=arg['width'] * float(prefs["enlarge_scale"]), height=arg['height'] * float(prefs["enlarge_scale"]), data=upscaled_path, subtitle=pr[0] if type(pr) == list else pr, page=page)], alignment=MainAxisAlignment.CENTER))
          #prt(Row([Img(src=upscaled_path, width=arg['width'] * float(prefs["enlarge_scale"]), height=arg['height'] * float(prefs["enlarge_scale"]), fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
          #prt(Img(src=upscaled_path))
          #upscaled = PILImage.open(os.path.join(batch_output, new_file))
          #display(upscaled)
        #else:
          #time.sleep(0.4)
          #prt(Row([Img(src=new_file, width=arg['width'], height=arg['height'], fit=ImageFit.FILL, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
        elif not prefs['apply_ESRGAN_upscale'] or not status['installed_ESRGAN']:
          upscaled_path = os.path.join(batch_output if save_to_GDrive else txt2img_output, new_file)
          prt(Row([ImageButton(src=upscaled_path, width=arg['width'], height=arg['height'], data=upscaled_path, subtitle=pr[0] if type(pr) == list else pr, page=page)], alignment=MainAxisAlignment.CENTER))

        prt(Row([Text(fpath.rpartition(slash)[2])], alignment=MainAxisAlignment.CENTER))
        idx += 1
        if abort_run:
          prt(Text("🛑   Aborting Current Diffusion Run..."))
          abort_run = False
          return
      p_idx += 1
      if abort_run:
        prt(Text("🛑   Aborting Current Diffusion Run..."))
        abort_run = False
        return
    if prefs['enable_sounds']: page.snd_alert.play()
  else:
    clear_pipes("interpolation")
    if pipe_interpolation is None:
      pipe_interpolation = get_interpolation_pipe()
    txt2img_output = os.path.join(stable_dir, prefs['batch_folder_name'] if bool(prefs['batch_folder_name']) else 'dreams')
    batch_output = prefs['image_output']
    if not os.path.exists(txt2img_output):
      os.makedirs(txt2img_output)
    #dream_name = prefs['batch_folder_name'] if bool(prefs['batch_folder_name']) else None
    #first = prompts[0]
    arg = args.copy()
    arg['width'] = int(arg['width'])
    arg['height'] = int(arg['height'])
    arg['seed'] = int(arg['seed'])
    arg['guidance_scale'] = float(arg['guidance_scale'])
    arg['steps'] = int(arg['steps'])
    arg['eta'] = float(arg['eta'])
    walk_prompts = []
    walk_seeds = []
    for p in prompts:
      walk_prompts.append(p.prompt)
      if int(p.arg['seed']) < 1 or arg['seed'] is None:
        walk_seeds.append(random.randint(0,4294967295))
      else:
        walk_seeds.append(int(p.arg['seed']))
    img_idx = 0
    from watchdog.observers import Observer
    from watchdog.events import LoggingEventHandler, FileSystemEventHandler
    class Handler(FileSystemEventHandler):
      def __init__(self):
        super().__init__()
      def on_created(self,event):
        nonlocal img_idx
        if event.is_directory:
          return None
        elif event.event_type == 'created':
          page.auto_scrolling(True)
          clear_last()
          #p_count = f'[{img_idx + 1} of {(len(walk_prompts) -1) * int(prefs['num_interpolation_steps'])}]  '
          #prt(Divider(height=6, thickness=2))
          #prt(Row([Text(p_count), Text(walk_prompts[img_idx], expand=True, weight=FontWeight.BOLD), Text(f'seed: {walk_seeds[img_idx]}')]))
          #prt(Row([Img(src=event.src_path, width=arg['width'], height=arg['height'], fit=ImageFit.FILL, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
          prt(Row([ImageButton(src=event.src_path, data=event.src_path, width=arg['width'], height=arg['height'], subtitle=f"Frame {img_idx} - {event.src_path}", center=True, page=page)], alignment=MainAxisAlignment.CENTER))
          prt(Row([Text(f'{event.src_path}')], alignment=MainAxisAlignment.CENTER))
          page.update()
          time.sleep(0.2)
          prt(pb)
          page.auto_scrolling(False)
          img_idx += 1
    # TODO: Rename files with to-from prompt text between each
    image_handler = Handler()
    observer = Observer()
    observer.schedule(image_handler, txt2img_output, recursive = True)
    observer.start()
    prt(f"Interpolating latent space between {len(walk_prompts)} prompts with {int(prefs['num_interpolation_steps'])} steps between each.")
    prt(Divider(height=6, thickness=2))
    prt(pb)
    page.auto_scrolling(False)
    #prt(Row([Text(p_count), Text(pr[0] if type(pr) == list else pr, expand=True, weight=FontWeight.BOLD), Text(f'seed: {arg["seed"]}')]))
    images = pipe_interpolation.walk(prompts=walk_prompts, seeds=walk_seeds, num_interpolation_steps=int(prefs['num_interpolation_steps']), batch_size=int(prefs['batch_size']), output_dir=txt2img_output, width=arg['width'], height=arg['height'], guidance_scale=arg['guidance_scale'], num_inference_steps=int(arg['steps']), eta=arg['eta'], callback=callback_fn, callback_steps=1)
    observer.stop()
    clear_last()
    fpath = images[0].rpartition(slash)[0]
    bfolder = fpath.rpartition(slash)[2]
    if prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
      prt('Applying Real-ESRGAN Upscaling to images...')
      os.chdir(os.path.join(dist_dir, 'Real-ESRGAN'))
      upload_folder = 'upload'
      result_folder = 'results'
      if os.path.isdir(upload_folder):
          shutil.rmtree(upload_folder)
      if os.path.isdir(result_folder):
          shutil.rmtree(result_folder)
      os.mkdir(upload_folder)
      os.mkdir(result_folder)
      for i in images:
        fname = i.rpartition(slash)[2]
        dst_path = os.path.join(dist_dir, 'Real-ESRGAN', upload_folder, fname)
        shutil.move(i, dst_path)
      faceenhance = ' --face_enhance' if prefs["face_enhance"] else ''
      run_sp(f'python inference_realesrgan.py -n realesr-general-x4v3 -i upload --outscale {prefs["enlarge_scale"]}{faceenhance}', cwd=os.path.join(dist_dir, 'Real-ESRGAN'), realtime=False)
      filenames = os.listdir(os.path.join(dist_dir, 'Real-ESRGAN', 'results'))
      for oname in filenames:
        fparts = oname.rpartition('_out')
        fname_clean = fparts[0] + fparts[2]
        opath = os.path.join(fpath, fname_clean)
        shutil.move(os.path.join(dist_dir, 'Real-ESRGAN', result_folder, oname), opath)
      os.chdir(stable_dir)
    os.makedirs(os.path.join(batch_output, bfolder), exist_ok=True)
    imgs = os.listdir(fpath)
    for i in imgs:
      #prt(f'Created {i}')
      #fname = i.rpartition(slash)[2]
      if save_to_GDrive:
        shutil.copy(os.path.join(fpath, i), os.path.join(batch_output, bfolder, i))
      elif storage_type == "PyDrive Google Drive":
        #batch_output
        out_file = gdrive.CreateFile({'title': i})
        out_file.SetContentFile(fpath)
        out_file.Upload()
      elif bool(prefs['image_output']):
        shutil.copy(os.path.join(fpath, i), os.path.join(batch_output, bfolder, i))
    if prefs['enable_sounds']: page.snd_alert.play()


nspterminology = None

def nsp_parse(prompt):
    import random, os, json
    global nspterminology
    new_prompt = ''
    new_prompts = []
    new_dict = {}
    ptype = type(prompt)
    #if not os.path.exists('./nsp_pantry.json'):
    #    wget('https://raw.githubusercontent.com/WASasquatch/noodle-soup-prompts/main/nsp_pantry.json', f'.{slash}nsp_pantry.json')
    if nspterminology is None:
        response = requests.get("https://raw.githubusercontent.com/WASasquatch/noodle-soup-prompts/main/nsp_pantry.json")
        nspterminology = json.loads(response.content)
    if ptype == dict:
        for pstep, pvalue in prompt.items():
            if type(pvalue) == list:
                for prompt in pvalue:
                    new_prompt = prompt
                    for term in nspterminology:
                        tkey = f'_{term}_'
                        tcount = prompt.count(tkey)
                        for i in range(tcount):
                            new_prompt = new_prompt.replace(tkey, random.choice(nspterminology[term]), 1)
                    new_prompts.append(new_prompt)
                new_dict[pstep] = new_prompts
                new_prompts = []
        return new_dict
    elif ptype == list:
        for pstr in prompt:
            new_prompt = pstr
            for term in nspterminology:
                tkey = f'_{term}_'
                tcount = new_prompt.count(tkey)
                for i in range(tcount):
                    new_prompt = new_prompt.replace(tkey, random.choice(nspterminology[term]), 1)
            new_prompts.append(new_prompt)
            new_prompt = None
        return new_prompts
    elif ptype == str:
        new_prompt = prompt
        for term in nspterminology:
            tkey = f'_{term}_'
            tcount = new_prompt.count(tkey)
            for i in range(tcount):
                new_prompt = new_prompt.replace(tkey, random.choice(nspterminology[term]), 1)
        return new_prompt
    else:
        return


#Code a function in Python programming language named list_variations, which takes a list and returns a set of lists with possible permutations of the list. Example list_variations([1,2,3]) returns [[1,2,3],[1,2],[1,3],[2,3],[1],[2],[3]] */
def list_variations(lst):
    result = []
    for i in range(len(lst)):
        for j in range(i+1, len(lst)+1):
            result.append(lst[i:j])
    return result
#print(str(list_variations([1,2,3])))
def and_list(lst):
  return " and ".join([", ".join(lst[:-1]),lst[-1]] if len(lst) > 2 else lst)

generator_request_modes = ["visually detailed",
  "with long detailed colorful interesting artistic scenic visual descriptions",
  "that is highly detailed, artistically interesting, describes a scene, colorful poetic language, with intricate visual descriptions",
  "that are strange, descriptive, graphically visual, full of interesting subjects described in great detail, painted by an artist",
  "that is technical, wordy, extra detailed, confusingly tangental, colorfully worded, dramatically narrative",
  "that is creative, imaginative, funny, interesting, scenic, dark, witty, visual, unexpected, wild",
  "that includes many subjects with descriptions, color details, artistic expression, point of view",
  "complete sentence using many words to describe a landscape in an epic fantasy genre that includes a lot adjectives",]

def run_prompt_generator(page):
  import random as rnd
  global artists, styles, status
  try:
    import openai
  except:
    page.prompt_generator_list.controls.append(Installing("Installing OpenAI Library..."))
    page.prompt_generator_list.update()
    run_sp("pip install --upgrade openai", realtime=False)
    import openai
    del page.prompt_generator_list.controls[-1]
    page.prompt_generator_list.update()
    pass
  try:
    openai.api_key = prefs['OpenAI_api_key']
  except:
    alert_msg(page, "Invalid OpenAI API Key. Change in Settings...")
    return
  status['installed_OpenAI'] = True
  prompts_gen = []
  prompt_results = []
  subject = ""
  if bool(prefs['prompt_generator']['subject_detail']):
      subject = ", and " + prefs['prompt_generator']['subject_detail']

  def prompt_gen():
    prompt = f'''Write a list of {prefs['prompt_generator']['amount'] if prefs['prompt_generator']['phrase_as_subject'] else (prefs['prompt_generator']['amount'] + 4)} image generation prompts about "{prefs['prompt_generator']['phrase']}"{subject}, {generator_request_modes[int(prefs['prompt_generator']['request_mode'])]}, and unique without repetition:

'''
    #print(prompt)
    if prefs['prompt_generator']['phrase_as_subject']:
      prompt += "\n*"
    else:
      prompt += f"""* A beautiful painting of a serene landscape with a river running through it, lush trees, golden sun illuminating
* Fireflies illuminating autumnal woods, an Autumn in the Brightwood glade, with warm yellow lantern lights
* The Fabric of spacetime continuum over a large cosmological vista, pieces of dark matter, space dust and nebula doted with small dots that seem to form fractal patterns and glowing bright lanterns in distances, also with an stardust effect towards the plane of the galaxy
* Midnight landscape painting of a city under a starry sky, owl in the shaman forest knowing the ways of magic, warm glow over the buildings
* {prefs['prompt_generator']['phrase']}"""
    if prefs['prompt_generator']['AI_engine'] == "OpenAI GPT-3":
      response = openai.Completion.create(engine="text-davinci-003", prompt=prompt, max_tokens=2400, temperature=prefs['prompt_generator']['AI_temperature'], presence_penalty=1)
      #print(response)
      result = response["choices"][0]["text"].strip()
    elif prefs['prompt_generator']['AI_engine'] == "ChatGPT-3.5 Turbo":
      response = openai.ChatCompletion.create(
        model="gpt-3.5-turbo-16k",
        messages=[{"role": "user", "content": prompt}]
      )
      #print(str(response))
      result = response["choices"][0]["message"]["content"].strip()
    #if result[-1] == '.': result = result[:-1]
    #print(str(result))
    for p in result.split('\n'):
      pr = p.strip()
      if not bool(pr): continue
      if pr[-1] == '.': pr = pr[:-1]
      if pr[0] == '*': pr = pr[1:].strip()
      elif '.' in pr: # Sometimes got 1. 2.
        pr = pr.partition('.')[2].strip()
      if '"' in pr: pr = pr.replace('"', '')
      if pr.endswith("."):
        pr = pr[:(-1)]
      prompt_results.append(pr)
  #print(f"Request mode influence: {request_modes[prefs['prompt_generator']['request_mode']]}\n")
  page.prompt_generator_list.controls.append(Installing("Requesting Prompts from the AI..."))
  page.prompt_generator_list.update()
  prompt_gen()
  del page.prompt_generator_list.controls[-1]
  page.prompt_generator_list.update()
  if len(prompt_results) < prefs['prompt_generator']['amount']:
    additional = prefs['prompt_generator']['amount'] - len(prompt_results)
    print(f"Didn't make enough prompts.. Needed {additional} more.")
  n=1
  for p in prompt_results:
    random_artist=[]
    for a in range(prefs['prompt_generator']['random_artists']):
      random_artist.append(rnd.choice(artists))
    #print(list_variations(random_artist))
    artist = " and ".join([", ".join(random_artist[:-1]),random_artist[-1]] if len(random_artist) > 2 else random_artist)
    random_style = []
    for s in range(prefs['prompt_generator']['random_styles']):
      random_style.append(rnd.choice(styles))
    style = ", ".join(random_style)
    if not prefs['prompt_generator']['phrase_as_subject'] and n == 1:
      p = prefs['prompt_generator']['phrase'] + " " + p
    text_prompt = p
    if prefs['prompt_generator']['random_artists'] > 0 or prefs['prompt_generator']['random_styles'] > 0:
      prompts_gen.append(text_prompt)
    if prefs['prompt_generator']['random_artists'] > 0: text_prompt += f", by {artist}"
    if prefs['prompt_generator']['random_styles'] > 0: text_prompt += f", style of {style}"
    #if prefs['prompt_generator']['random_styles'] != 0 and prefs['prompt_generator']['permutate_artists']:
    #  prompts_gen.append(text_prompt)
    if prefs['prompt_generator']['permutate_artists']:

      for a in list_variations(random_artist):
        prompt_variation = p + f", by {and_list(a)}"
        prompts_gen.append(prompt_variation)
      if prefs['prompt_generator']['random_styles'] > 0:
        prompts_gen.append(p + f", style of {style}")
    else: prompts_gen.append(text_prompt)
    n += 1
  for item in prompts_gen:
    page.add_to_prompt_generator(item)
    #print(f'   "{item}",')

remixer_request_modes = [
      "visually detailed wording, flowing sentences, extra long descriptions",
      "that is similar but with more details, themes, imagination, interest, subjects, artistic style, poetry, tone, settings, adjectives, visualizations",
      "that is completely rewritten, inspired by, paints a complete picture of an artistic scene",
      "with detailed colorful interesting artistic scenic visual descriptions, described to a blind person",
      "that is highly detailed, artistically interesting, describes a scene, colorful poetic language, with intricate visual descriptions",
      "that replaces every noun, adjective, verb, pronoun, with related words",
      "that is strange, descriptive, graphically visual, full of interesting subjects described in great detail, painted by an artist",
      "that is highly technical, extremely wordy, extra detailed, confusingly tangental, colorfully worded, dramatically narrative",
      "that is creative, imaginative, funny, interesting, scenic, dark, witty, visual, unexpected, wild",
      "that includes more subjects with descriptions, textured color details, expressive",]
      #"complete sentence using many words to describe a landscape in an epic fantasy genre that includes a lot adjectives",

def run_prompt_remixer(page):
  import random as rnd
  global artists, styles, status
  try:
    import openai
  except:
    run_sp("pip install --upgrade openai")
    import openai
    pass
  try:
    openai.api_key = prefs['OpenAI_api_key']
  except:
    alert_msg(page, "Invalid OpenAI API Key. Change in Settings...")
    return
  status['installed_OpenAI'] = True
  prompts_remix = []
  prompt_results = []

  if '_' in prefs['prompt_remixer']['seed_prompt']:
    seed_prompt = nsp_parse(prefs['prompt_remixer']['seed_prompt'])
  else:
    seed_prompt = prefs['prompt_remixer']['seed_prompt']
  if '_' in prefs['prompt_remixer']['optional_about_influencer']:
    optional_about_influencer = nsp_parse(prefs['prompt_remixer']['optional_about_influencer'])
  else:
    optional_about_influencer = prefs['prompt_remixer']['optional_about_influencer']
  about =  f" about {optional_about_influencer}" if bool(optional_about_influencer) else ""
  prompt = f'Write a list of {prefs["prompt_remixer"]["amount"]} remixed variations from the following image generation prompt{about}, "{prefs["prompt_remixer"]["seed_prompt"]}", {remixer_request_modes[int(prefs["prompt_remixer"]["request_mode"])]}, and unique without repetition:\n\n*'
  prompt_results = []

  def prompt_remix():
    if prefs['prompt_remixer']['AI_engine'] == "OpenAI GPT-3":
      response = openai.Completion.create(engine="text-davinci-003", prompt=prompt, max_tokens=2400, temperature=prefs["prompt_remixer"]['AI_temperature'], presence_penalty=1)
      #print(response)
      result = response["choices"][0]["text"].strip()
    elif prefs['prompt_remixer']['AI_engine'] == "ChatGPT-3.5 Turbo":
      response = openai.ChatCompletion.create(model="gpt-3.5-turbo-16k", messages=[{"role": "user", "content": prompt}])
      #print(str(response))
      result = response["choices"][0]["message"]["content"].strip()

    #if result[-1] == '.': result = result[:-1]
    #print(str(result))
    for p in result.split('\n'):
      pr = p.strip()
      if not bool(pr): continue
      if pr[-1] == '.': pr = pr[:-1]
      if pr[0] == '*': pr = pr[1:].strip()
      elif '.' in pr: # Sometimes got 1. 2.
        pr = pr.partition('.')[2].strip()
      prompt_results.append(pr)
  page.prompt_remixer_list.controls.append(Text(f"Remixing {seed_prompt}" + (f", about {optional_about_influencer}" if bool(optional_about_influencer) else "") + f"\nRequest mode influence: {remixer_request_modes[int(prefs['prompt_remixer']['request_mode'])]}\n"))
  page.prompt_remixer_list.update()
  #page.add_to_prompt_remixer(f"Remixing {seed_prompt}" + (f", about {optional_about_influencer}" if bool(optional_about_influencer) else "") + f"\nRequest mode influence: {remixer_request_modes[int(prefs['prompt_remixer']['request_mode'])]}\n")
  #print(f"Remixing {seed_prompt}" + (f", about {optional_about_influencer}" if bool(optional_about_influencer) else ""))
  #print(f"Request mode influence: {remixer_request_modes[int(prefs['prompt_remixer']['request_mode'])]}\n")
  page.prompt_remixer_list.controls.append(Installing("Requesting Prompt Remixes..."))
  page.prompt_remixer_list.update()
  prompt_remix()
  del page.prompt_remixer_list.controls[-1]
  del page.prompt_remixer_list.controls[-1]
  page.prompt_remixer_list.update()

  for p in prompt_results:
    random_artist=[]
    for a in range(prefs['prompt_remixer']['random_artists']):
      random_artist.append(rnd.choice(artists))
    #print(list_variations(random_artist))
    artist = " and ".join([", ".join(random_artist[:-1]),random_artist[-1]] if len(random_artist) > 2 else random_artist)
    random_style = []
    for s in range(prefs['prompt_remixer']['random_styles']):
      random_style.append(rnd.choice(styles))
    style = ", ".join(random_style)
    text_prompt = p
    if prefs['prompt_remixer']['random_artists'] > 0: text_prompt += f", by {artist}"
    if prefs['prompt_remixer']['random_styles'] > 0: text_prompt += f", style of {style}"
    if prefs['prompt_remixer']['random_styles'] == 0 and prefs['prompt_remixer']['permutate_artists']:
      prompts_remix.append(text_prompt)
    if prefs['prompt_remixer']['permutate_artists']:
      for a in list_variations(random_artist):
        prompt_variation = p + f", by {and_list(a)}"
        prompts_remix.append(prompt_variation)
      if prefs['prompt_remixer']['random_styles'] > 0:
        prompts_remix.append(p + f", style of {style}")
    else: prompts_remix.append(text_prompt)
  for item in prompts_remix:
    page.add_to_prompt_remixer(item)

def get_stable_lm(ai_model="StableLM 3b"):
    global pipe_stable_lm, tokenizer_stable_lm
    clear_pipes('stable_lm')
    if pipe_stable_lm != None:
      return pipe_stable_lm
    try:
      import accelerate
    except ModuleNotFoundError:
      run_sp("pip install accelerate", realtime=False)
      import accelerate
      pass
    try:
      os.environ['LD_LIBRARY_PATH'] += "/usr/lib/wsl/lib:$LD_LIBRARY_PATH"
      import bitsandbytes
    except ModuleNotFoundError:
      if sys.platform.startswith("win"):
          run_sp("pip install bitsandbytes-windows", realtime=False)
      else:
          run_sp("pip install bitsandbytes", realtime=False)
      import bitsandbytes
      pass
    try:
      import transformers
    except ModuleNotFoundError:
      run_sp("pip install -q transformers==4.21.3 --upgrade --force-reinstall", realtime=False)
      import transformers
      pass
    import torch
    from transformers import AutoModelForCausalLM, AutoTokenizer
    if ai_model == "StableLM 3b":
        model_name = "stabilityai/stablelm-tuned-alpha-3b"
    else:
        model_name = "stabilityai/stablelm-base-alpha-7b"
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        pipe_stable_lm = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype="float16",
            load_in_8bit=True,
            device_map="auto",
            offload_folder="./offload",
            cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None
        )
    except Exception as e:
      print(str(e))
      return None
    return pipe_stable_lm

def stable_lm_request(input_sentence, temperature=0.5, max_tokens=2048, top_k=0, top_p=0.9, do_sample=True):
    global pipe_stable_lm, tokenizer_stable_lm
    inputs = tokenizer_stable_lm(input_sentence, return_tensors="pt")
    inputs.to(pipe_stable_lm.device)
    tokens = pipe_stable_lm.generate(
      **inputs,
      max_new_tokens=max_tokens,
      temperature=temperature,
      top_k=top_k,
      top_p=top_p,
      do_sample=do_sample,
      pad_token_id=tokenizer_stable_lm.eos_token_id,
    )
    completion_tokens = tokens[0][inputs['input_ids'].size(1):]
    completion = tokenizer_stable_lm.decode(completion_tokens, skip_special_tokens=True)
    return completion

brainstorm_request_modes = {
    "Brainstorm":"Brainstorm visual ideas for an image prompt about ",
    "Write":"Write an interesting visual scene about ",
    "Rewrite":"Rewrite new variations of ",
    "Edit":"Edit this text to improve details and structure: ",
    "Story":"Write an interesting story with visual details and poetic subjects about ",
    "Description":"Describe in graphic detail ",
    "Picture":"Paint a picture with words about ",
    "Raw Request":"",
}

def run_prompt_brainstormer(page):
    import random as rnd
    global artists, styles, brainstorm_request_modes
    global pipe_stable_lm, tokenizer_stable_lm
    textsynth_engine = "gptj_6B" #param ["gptj_6B", "boris_6B", "fairseq_gpt_13B", "gptneox_20B", "m2m100_1_2B"]
    #markdown HuggingFace Bloom AI Settings
    max_tokens_length = 128 #param {type:'slider', min:1, max:64, step:1}
    seed = int(2222 * prefs['prompt_brainstormer']['AI_temperature']) #param {type:'integer'}
    API_URL = "https://api-inference.huggingface.co/models/bigscience/bloom"

    good_key = True
    if prefs['prompt_brainstormer']['AI_engine'] == "TextSynth GPT-J":
      try:
        if not bool(prefs['TextSynth_api_key']): good_key = False
      except NameError: good_key = False
      if not good_key:
        print(f"\33[91mMissing TextSynth_api_key...\33[0m Define your key up above.")
        return
      else:
        try:
          from textsynthpy import TextSynth, Complete
        except ImportError:
          run_sp("pip install textsynthpy", realtime=False)
          clear_output()
        finally:
          from textsynthpy import TextSynth, Complete
        textsynth = TextSynth(prefs['TextSynth_api_key'], engine=textsynth_engine) # Insert your API key in the previous cell
    if prefs['prompt_brainstormer']['AI_engine'] == "OpenAI GPT-3" or  prefs['prompt_brainstormer']['AI_engine'] == "ChatGPT-3.5 Turbo":
      try:
        if not bool(prefs['OpenAI_api_key']): good_key = False
      except NameError: good_key = False
      if not good_key:
        alert_msg(page, f"Missing OpenAI_api_key... Define your key in Settings.")
        return
      else:
        try:
          import openai
        except ModuleNotFoundError:
          run_sp("pip install --upgrade openai -qq", realtime=False)
          #clear_output()
          pass
        finally:
          import openai
        try:
          openai.api_key = prefs['OpenAI_api_key']
        except:
          alert_msg(page, "Invalid OpenAI API Key. Change in Settings...")
          return
    if prefs['prompt_brainstormer']['AI_engine'] == "HuggingFace Bloom 176B" or prefs['prompt_brainstormer']['AI_engine'] == "HuggingFace Flan-T5 XXL":
      try:
        if not bool(prefs['HuggingFace_api_key']): good_key = False
      except NameError: good_key = False
      if not good_key:
        alert_msg(page, f"Missing HuggingFace_api_key... Define your key in Settings.")
        return
    #ask_OpenAI_instead = False #@param {type:'boolean'}

    prompt_request_modes = [
        "visually detailed wording, flowing sentences, extra long descriptions",
        "that is similar but with more details, themes, imagination, interest, subjects, artistic style, poetry, tone, settings, adjectives, visualizations",
        "that is completely rewritten, inspired by, paints a complete picture of an artistic seen",
        "with detailed colorful interesting artistic scenic visual descriptions, described to a blind person",
        "that is highly detailed, artistically interesting, describes a scene, colorful poetic language, with intricate visual descriptions",
        "that replaces every noun, adjective, verb, pronoun, with related words",
        "that is strange, descriptive, graphically visual, full of interesting subjects described in great detail, painted by an artist",
        "that is highly technical, extremely wordy, extra detailed, confusingly tangental, colorfully worded, dramatically narrative",
        "that is creative, imaginative, funny, interesting, scenic, dark, witty, visual, unexpected, wild",
        "that includes more subjects with descriptions, textured color details, expressive",]
        #"complete sentence using many words to describe a landscape in an epic fantasy genre that includes a lot adjectives",

    request = f'{brainstorm_request_modes[prefs["prompt_brainstormer"]["request_mode"]]}"{prefs["prompt_brainstormer"]["about_prompt"]}":' if prefs['prompt_brainstormer']['request_mode'] != "Raw Request" else prefs['prompt_brainstormer']['about_prompt']

    def query(payload):
        #print(payload)
        response = requests.request("POST", API_URL, json=payload, headers={"Authorization": f"Bearer {prefs['HuggingFace_api_key']}"})
        #print(response.text)
        return json.loads(response.content.decode("utf-8"))

    def bloom_request(input_sentence):
        parameters = {
            "max_new_tokens": max_tokens_length,
            "do_sample": False,
            "seed": seed,
            "early_stopping": False,
            "length_penalty": 0.0,
            "eos_token_id": None,}
        payload = {"inputs": input_sentence, "parameters": parameters,"options" : {"use_cache": False} }
        data = query(payload)
        if "error" in data:
            return f"\33[31mERROR: {data['error']}\33[0m"

        generation = data[0]["generated_text"].split(input_sentence, 1)[1]
        #return data[0]["generated_text"]
        return generation

    def flan_query(payload):
        #print(payload)
        response = requests.request("POST", "https://api-inference.huggingface.co/models/google/flan-t5-xxl", json=payload, headers={"Authorization": f"Bearer {prefs['HuggingFace_api_key']}"})
        #print(response.text)
        return json.loads(response.content.decode("utf-8"))

    def flan_request(input_sentence):
        parameters = {
            "max_new_tokens": max_tokens_length,
            "do_sample": False,
            "seed": seed,
            "early_stopping": False,
            "length_penalty": 0.0,
            "eos_token_id": None,}
        payload = {"inputs": input_sentence, "parameters": parameters,"options" : {"use_cache": False} }
        data = flan_query(payload)
        if "error" in data:
            return f"\33[31mERROR: {data['error']}\33[0m"

        generation = data[0]["generated_text"].split(input_sentence, 1)[1]
        #return data[0]["generated_text"]
        return generation

    def stable_lm_request(input_sentence):
        inputs = tokenizer_stable_lm(input_sentence, return_tensors="pt")
        inputs.to(pipe_stable_lm.device)
        tokens = pipe_stable_lm.generate(
          **inputs,
          max_new_tokens=2048,
          temperature=prefs['prompt_brainstormer']['AI_temperature'],
          top_k=0,
          top_p=0.9,
          do_sample=True,
          pad_token_id=tokenizer_stable_lm.eos_token_id,
        )
        completion_tokens = tokens[0][inputs['input_ids'].size(1):]
        completion = tokenizer_stable_lm.decode(completion_tokens, skip_special_tokens=True)
        return completion

    def prompt_brainstormer():
      #(prompt=prompt, temperature=AI_temperature, presence_penalty=1, stop= "\n")
      page.prompt_brainstormer_list.controls.append(Installing("Storming the AI's Brain..."))
      page.prompt_brainstormer_list.update()

      if prefs['prompt_brainstormer']['AI_engine'] == "TextSynth GPT-J":
        response = textsynth.text_complete(prompt=request, max_tokens=200, temperature=prefs['prompt_brainstormer']['AI_temperature'], presence_penalty=1)
        #print(str(response))
        result = response.text.strip()
      elif prefs['prompt_brainstormer']['AI_engine'] == "OpenAI GPT-3":
        response = openai.Completion.create(engine="text-davinci-003", prompt=request, max_tokens=2400, temperature=prefs['prompt_brainstormer']['AI_temperature'], presence_penalty=1)
        result = response["choices"][0]["text"].strip()
      elif prefs['prompt_brainstormer']['AI_engine'] == "ChatGPT-3.5 Turbo":
        response = openai.ChatCompletion.create(model="gpt-3.5-turbo-16k", messages=[{"role": "user", "content": request}])
        result = response["choices"][0]["message"]["content"].strip()
      elif prefs['prompt_brainstormer']['AI_engine'] == "HuggingFace Bloom 176B":
        result = bloom_request(request)
      elif prefs['prompt_brainstormer']['AI_engine'] == "HuggingFace Flan-T5":
        result = flan_request(request)
      elif prefs['prompt_brainstormer']['AI_engine'].startswith("Stable"):
        if pipe_stable_lm != None and tokenizer_stable_lm != None:
          page.add_to_prompt_brainstormer(Installing("Installing StableLM-Alpha Pipeline..."))
          pipe_stable_lm = get_stable_lm(prefs['prompt_brainstormer']['AI_engine'])
          del page.prompt_brainstormer_list.controls[-1]
          page.prompt_brainstormer_list.update()
        result = stable_lm_request(request, temperature=prefs['prompt_brainstormer']['AI_temperature'])
      page.add_to_prompt_brainstormer(str(result) + '\n')
    #print(f"Remixing {seed_prompt}" + (f", about {optional_about_influencer}" if bool(optional_about_influencer) else ""))
    if good_key:
      #print(request)
      prompt_brainstormer()

def run_prompt_writer(page):
    '''try:
        import nsp_pantry
        from nsp_pantry import nsp_parse
    except ModuleNotFoundError:
        run_sp("wget -qq --show-progress --no-cache --backups=1 https://raw.githubusercontent.com/WASasquatch/noodle-soup-prompts/main/nsp_pantry.py")
        #print(subprocess.run(['wget', '-q', '--show-progress', '--no-cache', '--backups=1', 'https://raw.githubusercontent.com/WASasquatch/noodle-soup-prompts/main/nsp_pantry.py'], stdout=subprocess.PIPE).stdout.decode('utf-8'))
    finally:
        import nsp_pantry
        from nsp_pantry import nsp_parse'''
    import random as rnd
    global artists, styles
    def generate_prompt():
      text_prompts = []
      global art_Subjects, by_Artists, art_Styles
      nsSubjects = nsp_parse(prefs['prompt_writer']['art_Subjects'])
      nsArtists = nsp_parse(prefs['prompt_writer']['by_Artists'])
      nsStyles = nsp_parse(prefs['prompt_writer']['art_Styles'])
      prompt = nsSubjects
      random_artist=[]
      if nsArtists: random_artist.append(nsArtists)
      for a in range(prefs['prompt_writer']['random_artists']):
        random_artist.append(rnd.choice(artists))
      artist = and_list(random_artist)
      #artist = random.choice(artists) + " and " + random.choice(artists)
      random_style = []
      if prefs['prompt_writer']['art_Styles']: random_style.append(nsStyles)
      for s in range(prefs['prompt_writer']['random_styles']):
        random_style.append(rnd.choice(styles))
      style = ", ".join(random_style)
      subject_prompt = prompt
      if len(artist) > 0: prompt += f", by {artist}"
      if len(style) > 0: prompt += f", style of {style}"
      if not prefs['prompt_writer']['permutate_artists']:
        return prompt
      if prefs['prompt_writer']['random_styles'] > 0 and prefs['prompt_writer']['permutate_artists']:
        text_prompts.append(prompt)
      if prefs['prompt_writer']['permutate_artists']:
        for a in list_variations(random_artist):
          prompt_variation = subject_prompt + f", by {and_list(a)}"
          text_prompts.append(prompt_variation)
        if prefs['prompt_writer']['random_styles'] > 0:
          text_prompts.append(subject_prompt + f", style of {style}")
        return text_prompts
      #if mod_Custom and mod_Custom.strip(): prompt += mod_Custom)
      #return prompt
    prompts_writer = []
    for p in range(prefs['prompt_writer']['amount']):
      prompts_writer.append(generate_prompt())
    for item in prompts_writer:
      if type(item) is str:
        page.add_to_prompt_writer(item)
      if type(item) is list:
        for i in item:
          page.add_to_prompt_writer(i)

def run_magic_prompt(page):
    #import random as rnd
    global artists, styles, magic_prompt_prefs, pipe_gpt2
    def prt(line):
      if type(line) == str:
        line = Text(line, size=17)
      page.magic_prompt_output.controls.append(line)
      page.magic_prompt_output.update()
    def clear_last():
      del page.magic_prompt_output.controls[-1]
      page.magic_prompt_output.update()
    progress = ProgressBar(bar_height=8)
    prt(Installing("Installing Magic Prompt GPT-2 Pipeline..."))
    try:
        import jinja2
    except:
        run_sp("pip install -q jinja2==3.0.3")
        pass
    try:
        from transformers import pipeline, set_seed
    except:
        run_sp("pip install -qq --upgrade git+https://github.com/huggingface/transformers")
        #run_sp("pip install torch")
        from transformers import pipeline, set_seed
        pass
    try:
        import sentencepiece
    except:
        run_sp("pip install -q sentencepiece")
        pass
    import re
    ideas = os.path.join(root_dir, "ideas.txt")
    if not os.path.exists(ideas):
        download_file("https://huggingface.co/spaces/Gustavosta/MagicPrompt-Stable-Diffusion/raw/main/ideas.txt")
    prompts_magic = []
    prompt_results = []
    if '_' in magic_prompt_prefs['seed_prompt']:
        seed_prompt = nsp_parse(magic_prompt_prefs['seed_prompt'])
    else:
        seed_prompt = magic_prompt_prefs['seed_prompt']
    clear_pipes("gpt2")
    if pipe_gpt2 == None:
        try:
            pipe_gpt2 = pipeline('text-generation', model='Gustavosta/MagicPrompt-Stable-Diffusion', tokenizer='gpt2')
        except Exception as e:
            clear_last()
            alert_msg(page, "Error Initializing GPT-2 Pipeline", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))
            return
    with open(ideas, "r") as f:
        line = f.readlines()
    clear_last()
    prt("Generating Magic Prompts from your Text Input...")
    prt(progress)

    def generate(starting_text):
        random_seed = int(magic_prompt_prefs['seed']) if int(magic_prompt_prefs['seed']) > 0 else rnd.randint(100, 1000000)
        set_seed(random_seed)
        if starting_text == "":
            starting_text: str = line[rnd.randrange(0, len(line))].replace("\n", "").lower().capitalize()
            starting_text: str = re.sub(r"[,:\-–.!;?_]", '', starting_text)
        response = pipe_gpt2(starting_text, max_length=(len(starting_text) + rnd.randint(60, 90)), num_return_sequences=int(magic_prompt_prefs['amount']))
        response_list = []
        for x in response:
            resp = x['generated_text'].strip()
            if resp != starting_text and len(resp) > (len(starting_text) + 4) and resp.endswith((":", "-", "—")) is False:
                response_list.append(resp)
        response_end = "\n".join(response_list)
        response_end = re.sub('[^ ]+\.[^ ]+','', response_end)
        response_end = response_end.replace("<", "").replace(">", "")
        if response_end != "":
            return response_end.split("\n")
        else:
            prt("Error Generating Magic Prompt Responses...")
            return []

    #txt = grad.Textbox(lines=1, label="Initial Text", placeholder="English Text here")
    #out = grad.Textbox(lines=4, label="Generated Prompts")
    #examples = []
    #for x in range(8):
    #    examples.append(line[rnd.randrange(0, len(line))].replace("\n", "").lower().capitalize())
    #title = "Stable Diffusion Prompt Generator"
    #description = 'This is a demo of the model series: "MagicPrompt", in this case, aimed at: "Stable Diffusion". To use it, simply submit your text or click on one of the examples. To learn more about the model, [click here](https://huggingface.co/Gustavosta/MagicPrompt-Stable-Diffusion).<br>'
    prompt_results = generate(seed_prompt)
    clear_last()
    clear_last()
    for p in prompt_results:
        random_artist=[]
        for a in range(magic_prompt_prefs['random_artists']):
            random_artist.append(rnd.choice(artists))
        #print(list_variations(random_artist))
        artist = " and ".join([", ".join(random_artist[:-1]),random_artist[-1]] if len(random_artist) > 2 else random_artist)
        random_style = []
        for s in range(magic_prompt_prefs['random_styles']):
            random_style.append(rnd.choice(styles))
        style = ", ".join(random_style)
        text_prompt = p
        prompts_magic.append(text_prompt)
        if magic_prompt_prefs['random_artists'] > 0: text_prompt += f", by {artist}"
        if magic_prompt_prefs['random_styles'] > 0: text_prompt += f", style of {style}"
        #if magic_prompt_prefs['random_styles'] == 0 and magic_prompt_prefs['permutate_artists']:
        #    prompts_magic.append(text_prompt)
        if magic_prompt_prefs['permutate_artists']:
            for a in list_variations(random_artist):
                prompt_variation = p + f", by {and_list(a)}"
                prompts_magic.append(prompt_variation)
            if magic_prompt_prefs['random_styles'] > 0:
                prompts_magic.append(p + f", style of {style}")
        else: prompts_magic.append(text_prompt)
    for item in prompts_magic:
        page.add_to_magic_prompt(item)
    if prefs['enable_sounds']: page.snd_alert.play()

def run_distil_gpt2(page):
    #import random as rnd
    global artists, styles, distil_gpt2_prefs, pipe_distil_gpt2
    def prt(line):
      if type(line) == str:
        line = Text(line, size=17)
      page.distil_gpt2_output.controls.append(line)
      page.distil_gpt2_output.update()
    def clear_last():
      del page.distil_gpt2_output.controls[-1]
      page.distil_gpt2_output.update()
    progress = ProgressBar(bar_height=8)
    prt(Installing("Installing Distil GPT-2 Pipeline..."))

    try:
        from transformers import GPT2Tokenizer, GPT2LMHeadModel, set_seed
    except:
        run_sp("pip install -qq --upgrade git+https://github.com/huggingface/transformers")
        #run_sp("pip install torch")
        from transformers import GPT2Tokenizer, GPT2LMHeadModel, set_seed
        pass

    prompts_distil = []
    prompt_results = []
    if '_' in distil_gpt2_prefs['seed_prompt']:
        seed_prompt = nsp_parse(distil_gpt2_prefs['seed_prompt'])
    else:
        seed_prompt = distil_gpt2_prefs['seed_prompt']
    clear_pipes("distil_gpt2")
    tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')
    tokenizer.add_special_tokens({'pad_token': '[PAD]'})
    if pipe_distil_gpt2 == None:
        try:
            pipe_distil_gpt2 = GPT2LMHeadModel.from_pretrained('FredZhang7/distilgpt2-stable-diffusion-v2')
        except Exception as e:
            clear_last()
            alert_msg(page, "Error Initializing Distil GPT-2 Pipeline", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))
            return

    clear_last()
    prt("Generating Distil GPT-2 Results from your Text Input...")
    prt(progress)

    def generate(starting_text):
        random_seed = int(distil_gpt2_prefs['seed']) if int(distil_gpt2_prefs['seed']) > 0 else rnd.randint(100, 1000000)
        set_seed(random_seed)
        input_ids = tokenizer(starting_text, return_tensors='pt').input_ids
        output = pipe_distil_gpt2.generate(input_ids, do_sample=True, temperature=distil_gpt2_prefs['AI_temperature'], top_k=distil_gpt2_prefs['top_k'], max_length=distil_gpt2_prefs['max_length'], num_return_sequences=distil_gpt2_prefs['amount'], repetition_penalty=distil_gpt2_prefs['repetition_penalty'], penalty_alpha=distil_gpt2_prefs['penalty_alpha'], no_repeat_ngram_size=distil_gpt2_prefs['no_repeat_ngram_size'], early_stopping=True)
        results = []
        for i in range(len(output)):
            results.append(tokenizer.decode(output[i], skip_special_tokens=True))
        return results
    prompt_results = generate(seed_prompt)
    clear_last()
    clear_last()
    for p in prompt_results:
        random_artist=[]
        for a in range(distil_gpt2_prefs['random_artists']):
            random_artist.append(rnd.choice(artists))
        #print(list_variations(random_artist))
        artist = " and ".join([", ".join(random_artist[:-1]),random_artist[-1]] if len(random_artist) > 2 else random_artist)
        random_style = []
        for s in range(distil_gpt2_prefs['random_styles']):
            random_style.append(rnd.choice(styles))
        style = ", ".join(random_style)
        text_prompt = p
        prompts_distil.append(text_prompt)
        if distil_gpt2_prefs['random_artists'] > 0: text_prompt += f", by {artist}"
        if distil_gpt2_prefs['random_styles'] > 0: text_prompt += f", style of {style}"
        #if distil_gpt2_prefs['random_styles'] == 0 and distil_gpt2_prefs['permutate_artists']:
        #    prompts_distil.append(text_prompt)
        if distil_gpt2_prefs['permutate_artists']:
            for a in list_variations(random_artist):
                prompt_variation = p + f", by {and_list(a)}"
                prompts_distil.append(prompt_variation)
            if distil_gpt2_prefs['random_styles'] > 0:
                prompts_distil.append(p + f", style of {style}")
        else: prompts_distil.append(text_prompt)
    for item in prompts_distil:
        page.add_to_distil_gpt2(item)
    if prefs['enable_sounds']: page.snd_alert.play()


def run_upscaling(page):
    #print(str(ESRGAN_prefs))
    if not status['installed_ESRGAN']:
      alert_msg(page, "You must Install Real-ESRGAN first")
      return
    import re
    from collections import Counter
    enlarge_scale = ESRGAN_prefs['enlarge_scale']
    face_enhance = ESRGAN_prefs['face_enhance']
    image_path = ESRGAN_prefs['image_path']
    save_to_GDrive = ESRGAN_prefs['save_to_GDrive']
    upload_file = ESRGAN_prefs['upload_file']
    download_locally = ESRGAN_prefs['download_locally']
    display_image = ESRGAN_prefs['display_image']
    dst_image_path = ESRGAN_prefs['dst_image_path']
    filename_suffix = ESRGAN_prefs['filename_suffix']
    split_image_grid = ESRGAN_prefs['split_image_grid']
    rows = ESRGAN_prefs['rows']
    cols = ESRGAN_prefs['cols']
    def split(im, rows, cols, img_path, should_cleanup=False):
        im_width, im_height = im.size
        row_width = int(im_width / rows)
        row_height = int(im_height / cols)
        n = 0
        for i in range(0, cols):
            for j in range(0, rows):
                box = (j * row_width, i * row_height, j * row_width +
                      row_width, i * row_height + row_height)
                outp = im.crop(box)
                name, ext = os.path.splitext(img_path)
                outp_path = name + "-" + str(n) + ext
                #print("Exporting image tile: " + outp_path)
                outp.save(outp_path)
                n += 1
        if should_cleanup:
            #print("Cleaning up: " + img_path)
            os.remove(img_path)

    os.chdir(os.path.join(dist_dir, 'Real-ESRGAN'))
    upload_folder = 'upload'
    result_folder = 'results'
    if os.path.isdir(upload_folder):
        shutil.rmtree(upload_folder)
    if os.path.isdir(result_folder):
        shutil.rmtree(result_folder)
    os.mkdir(upload_folder)
    os.mkdir(result_folder)

    uploaded = None
    if not upload_file:
      if not image_path:
         alert_msg(page, 'Provide path to image, local or url')
         return
      if '.' in image_path:
        if os.path.exists(image_path):
          uploaded = {image_path: image_path.rpartition(slash)[2]}
        else:
          alert_msg(page, 'File does not exist')
          return
      else:
        if os.path.isdir(image_path):
          uploaded = {}
          for f in os.listdir(image_path):
            uploaded[ os.path.join(image_path, f)] = f
        else:
          alert_msg(page, 'Image Path directory does not exist')
          return
    else:
      uploaded = files.upload()
    page.clear_ESRGAN_output(uploaded)
    page.add_to_ESRGAN_output(Text(f"Upscaling {len(uploaded)} images.."))
    for filename in uploaded.keys():
      if not os.path.isfile(filename):
        #print("Skipping " + filename)
        continue
      fname = filename.rpartition(slash)[2] if slash in filename else filename
      dst_path = os.path.join(upload_folder, fname)
      #print(f'Copy {filename} to {dst_path}')
      shutil.copy(filename, dst_path)
      if split_image_grid:
        img = PILImage.open(dst_path)
        split(img, rows, cols, dst_path, True)
    os.chdir(os.path.join(dist_dir, 'Real-ESRGAN'))
    faceenhance = ' --face_enhance' if face_enhance else ''
    run_sp(f'python inference_realesrgan.py -n realesr-general-x4v3 -i {upload_folder} --outscale {enlarge_scale}{faceenhance}', cwd=os.path.join(dist_dir, 'Real-ESRGAN'), realtime=False)
    os.chdir(root_dir)
    if is_Colab:
      from google.colab import files
    if not bool(dst_image_path.strip()):
      if os.path.isdir(image_path):
          dst_image_path = image_path
      else:
          dst_image_path = prefs['image_output'] #image_path.rpartition(slash)[0]
    filenames = os.listdir(os.path.join(dist_dir, 'Real-ESRGAN', 'results'))
    for fname in filenames:
      fparts = fname.rpartition('_out')
      fname_clean = fparts[0] + filename_suffix + fparts[2]
      #print(f'Copying {fname_clean}')
      #if save_to_GDrive:
      if not os.path.isdir(dst_image_path):
        os.makedirs(dst_image_path)
      shutil.copy(os.path.join(dist_dir, 'Real-ESRGAN', 'results', fname), os.path.join(dst_image_path, fname_clean))
      if download_locally:
        files.download(os.path.join(dist_dir, 'Real-ESRGAN', 'results', fname))
      if display_image:
        page.add_to_ESRGAN_output(Image(src=os.path.join(dist_dir, 'Real-ESRGAN', 'results', fname)))
      page.add_to_ESRGAN_output(Row([Text(os.path.join(dst_image_path, fname_clean))], alignment=MainAxisAlignment.CENTER))

def run_retrieve(page):
    upload_file = retrieve_prefs['upload_file']
    image_path = retrieve_prefs['image_path']
    display_full_metadata = retrieve_prefs['display_full_metadata']
    display_image = retrieve_prefs['display_image']
    add_to_prompts = retrieve_prefs['add_to_prompts']

    import json
    if is_Colab:
      from google.colab import files
    def meta_dream(meta):
      if meta is not None and len(meta) > 1:
          #d = Dream(meta["prompt"])
          print(str(meta))
          arg = {}
          p = ''
          dream = '    Dream('
          if meta.get('title'):
            dream += f'"{meta["title"]}"'
            p = meta["title"]
          if meta.get('prompt'):
            dream += f'"{meta["prompt"]}"'
            p = meta["prompt"]
          if meta.get('config'):
            meta = meta['config']
          if meta.get('prompt'):
            #dream += f'"{meta["prompt"]}"'
            p = meta["prompt"]
          if meta.get('prompt2'):
            dream += f', prompt2="{meta["prompt2"]}"'
            arg["prompt2"] = meta["prompt2"]
          if meta.get('negative_prompt'):
            dream += f', negative="{meta["negative_prompt"]}"'
            arg["negative_prompt"] = meta["negative_prompt"]
          if meta.get('tweens'):
            dream += f', tweens={meta["tweens"]}'
            arg["tweens"] = meta["tweens"]
          if meta.get('width'):
            dream += f', width={meta["width"]}'
            arg["width"] = meta["width"]
          if meta.get('height'):
            dream += f', height={meta["height"]}'
            arg["height"] = meta["height"]
          if meta.get('guidance_scale'):
            dream += f', guidance_scale={meta["guidance_scale"]}'
            arg["guidance_scale"] = meta["guidance_scale"]
          elif meta.get('CGS'):
            dream += f', guidance_scale={meta["CGS"]}'
            arg["guidance_scale"] = meta["CGS"]
          if meta.get('steps'):
            dream += f', steps={meta["steps"]}'
            arg["steps"] = meta["steps"]
          if meta.get('eta'):
            dream += f', eta={meta["eta"]}'
            arg["eta"] = meta["eta"]
          if meta.get('seed'):
            dream += f', seed={meta["seed"]}'
            arg["seed"] = meta["seed"]
          if meta.get('init_image'):
            dream += f', init_image="{meta["init_image"]}"'
            arg["init_image"] = meta["init_image"]
          if meta.get('mask_image'):
            dream += f', mask_image="{meta["mask_image"]}"'
            arg["mask_image"] = meta["mask_image"]
          if meta.get('init_image_strength'):
            dream += f', init_image_strength={meta["init_image_strength"]}'
            arg["init_image_strength"] = meta["init_image_strength"]
          dream += '),'
          page.add_to_retrieve_output(Text(dream, selectable=True))
          if display_full_metadata:
            page.add_to_retrieve_output(Text(str(metadata)))
          if add_to_prompts:
            page.add_to_prompts(p, arg)
      else:
          alert_msg(page, 'Problem reading your config json image meta data.')
          return
    uploaded = {}
    if not upload_file:
      if not bool(image_path):
        alert_msg(page, 'Provide path to image, local or url')
        return
      if '.' in image_path:
        if os.path.exists(image_path):
          uploaded = {image_path: image_path.rpartition(slash)[2]}
        else:
          alert_msg(page, 'File does not exist')
          return
      else:
        if os.path.isdir(image_path):
          uploaded = {}
          for f in os.listdir(image_path):
            uploaded[ os.path.join(image_path, f)] = f
        else:
          alert_msg(page, 'The image_path directory does not exist')
          return
    else:
      if not is_Colab:
        uploaded = files.upload()
        alert_msg(page, "Can't upload an image easily from non-Colab systems")
        return
    if len(uploaded) > 1:
      page.add_to_retrieve_output(Text(f"Revealing Dream of {len(uploaded)} images..\n"))
    for filename in uploaded.keys():
      if not os.path.isfile(filename):
        #print("Skipping subfolder " + filename)
        continue
      #print(filename)
      if filename.rpartition('.')[2] == 'json':
        meta = json.load(filename)
        meta_dream(meta)
      elif filename.rpartition('.')[2] == 'png':
        img = PILImage.open(filename)
        metadata = img.info
        if display_image:
          page.add_to_retrieve_output(Img(src=filename, gapless_playback=True))
          #display(img)
        if metadata is None or len(metadata) < 1:
          alert_msg(page, 'Sorry, image has no exif data.')
          return
          #print(metadata)
        else:
          if metadata.get('config_json'):
            json_txt = metadata['config_json']
            #print(json_txt)
            meta = json.loads(json_txt)
            meta_dream(meta)
          elif metadata.get('config'):
            config = metadata['config']
            meta = {}
            key = ""
            val = ""
            if metadata.get('title'):
              meta['prompt'] = metadata['title']
            for col in config.split(':'):
              #print(col.strip())
              if ',' not in col:
                key = col
              else:
                parts = col.rpartition(',')
                val = parts[0].strip()
                if bool(key) and bool(val):
                  meta[key] = val
                  val = ''
                key = parts[2].strip()
            #print(meta)
            meta_dream(meta)
            #print(dream)
          else:
            alert_msg(page, "No Enhanced Stable Diffusion config metadata found inside image.")

def run_initfolder(page):
    prompt_string = initfolder_prefs['prompt_string']
    init_folder = initfolder_prefs['init_folder']
    include_strength = initfolder_prefs['include_strength']
    image_strength = initfolder_prefs['image_strength']
    #init_image='/content/ pic.png', init_image_strength=0.4
    if bool(prompt_string):
      p_str = f'"{prompt_string.strip()}"'
      skip_str = f', init_image_strength={image_strength}' if bool(include_strength) else ''
      if os.path.isdir(init_folder):
        arg = {}
        #print("prompts = [")
        for f in os.listdir(init_folder):
          init_path = os.path.join(init_folder, f)
          if os.path.isdir(init_path): continue
          if f.lower().endswith(('.png', '.jpg', '.jpeg')):
            page.add_to_initfolder_output(Text(f'    Dream({p_str}, init_image="{init_path}"{skip_str}),'))
            if bool(initfolder_prefs['negative_prompt']):
              arg['negative_prompt'] = initfolder_prefs['negative_prompt']
            arg['init_image'] = init_path
            if bool(include_strength):
              arg['init_image_strength'] = image_strength
            page.add_to_prompts(prompt_string, arg)
        if not bool(status['installed_img2img']):
          alert_msg(page, 'Make sure you Install the Image2Image module before running Stable Diffusion on prompts...')
       # print("]")
      else:
        alert_msg(page, 'The init_folder directory does not exist.')
    else: alert_msg(page, 'Your prompt_string is empty. What do you want to apply to images?')

def run_init_video(page):
    prompt = init_video_prefs['prompt']
    video_file = init_video_prefs['video_file']
    file_prefix = init_video_prefs['file_prefix']
    try:
        start_time = float(init_video_prefs['start_time'])
        end_time = float(init_video_prefs['end_time'])
        fps = int(init_video_prefs['fps'])
    except Exception:
        alert_msg(page, "Make sure your Numbers are actual numbers...")
        return
    max_size = init_video_prefs['max_size']
    show_images = init_video_prefs['show_images']
    output_dir = os.path.join(stable_dir, init_video_prefs['batch_folder_name'])
    if not bool(prompt):
        alert_msg(page, "Provide a good prompt to apply to All Frames in List.")
        return
    def clear_last():
      del page.init_video_output.controls[-1]
      page.init_video_output.update()
    page.init_video_output.controls.clear()
    page.init_video_output.update()
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    if video_file.startswith("http"):
        page.add_to_init_video_output(Text("Downloading the Video File..."))
        local = download_file(video_file)
        vname = local.rpartition(slash)
        vout = os.path.join(root_dir, vname)
        shutil.move(local, vout)
        video_file = vout
        clear_last()
    if not bool(video_file):
        alert_msg(page, "Provide a valid Video File to Extract...")
        return
    else:
        if not os.path.exists(video_file):
            alert_msg(page, "The provided Video File doesn't Exist...")
            return

    '''try:
        import ffmpeg
    except Exception:
        run_process("pip install -q ffmpeg", page=page)
        import ffmpeg
        pass
    def convert_video_to_images(video_file, fps, start_time, end_time, output_dir):
        """
        Convert a video file to a sequence of images.
        """
        # Check if the output directory exists, otherwise create it.
        command = ['ffmpeg',
                '-i', video_file,
                #'-r', str(fps),
                #'-ss', str(start_time),
                #'-t', str(end_time),
                '-vf', f'fps={fps}'
                #'-vf', "select='between(t,2,6)+between(t,15,24)'"
                #'-qscale:v', '2',
                '-vsync', "0"
                '-f', 'image2',
                os.path.join(output_dir, '%08d.png')]

        run_process(command, page, print=True)
    try:
        import imageio
    except Exception:
        run_process("pip install -q imageio", page=page)
        import imageio
        pass
    def convert_video_to_images(video_file, fps, start_time, end_time, resolution):
        # create video reader object
        reader = imageio.get_reader(video_file)
        # set the reader parameters
        reader.set_fps(fps)
        reader.set_start_time(start_time)
        reader.set_end_time(end_time)
        # get list of frames
        frames = reader.get_meta_data()['nframes']
        # loop through each frame and save as png with frame number in filename
        for frame in range(frames):
            frame_img = reader.get_data(frame)
            frame_img = frame_img.resize(resolution)
            imageio.imwrite("frame_{}.png".format(frame), frame_img)
        reader.close()'''
    progress = ProgressBar(bar_height=8)
    page.add_to_init_video_output(Installing("Processing Video File..."))
    page.add_to_init_video_output(progress)
    try:
        import cv2
    except ModuleNotFoundError:
        run_process("pip install -q opencv-python", page=page)
        import cv2
        pass
    try:
        cap = cv2.VideoCapture(video_file)
    except Exception as e:
        alert_msg(page, "ERROR Reading Video File. May be Incompatible Format...")
        clear_last()
        return
    count = 0
    files = []
    frames = []
    w = h = 0
    cap.set(cv2.CAP_PROP_FPS, fps)
    video_length = cap.get(cv2.CAP_PROP_FRAME_COUNT)
    start_frame = int(start_time * fps)
    if end_time == 0 or end_time == 0.0:
        end_frame = int(video_length)
    else:
        end_frame = int(end_time * fps)
    total = end_frame - start_frame

    #print(f"Length: {video_length}, start_frame: {start_frame}, end_frame: {end_frame}")
    for i in range(start_frame, end_frame):
        # Read frame
        cap.set(cv2.CAP_PROP_POS_FRAMES, i)
        success, image = cap.read()

        # Save frame as png
        if success:
            filename = os.path.join(output_dir, f'{file_prefix}{count}.png')
            if w == 0:
                shape = image.shape
                w, h = scale_dimensions(shape[1], shape[0], max=max_size, multiple=16)
                clear_last()
                page.add_to_init_video_output(Text(f'Extracting {len(frames)} frames at {w}x{h}, {video_length} seconds long...'))
            image = cv2.resize(image, (w, h), interpolation = cv2.INTER_AREA)
            percent = count / total
            progress.value = percent
            progress.update()
            #print(f"{count / total}% - Saving {filename} - {shape}")
            cv2.imwrite(os.path.join(output_dir, filename), image)
            files.append(filename)
            if show_images:
                page.add_to_init_video_output(Row([Img(src=filename, width=w, height=h, fit=ImageFit.FILL, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
                #page.add_to_init_video_output(Row([Text(filename)], alignment=MainAxisAlignment.CENTER))
            count += 1
    cap.release()
    if show_images:
        progress.value = 0.0
        progress.update()
    else:
        clear_last()
        clear_last()
    '''
    cap.set(cv2.CAP_PROP_POS_MSEC, start_time * 1000)
    end_time = min(end_time, video_length / fps)
    cap.set(cv2.CAP_PROP_FPS, fps)
    print(f"Length: {video_length}, end_time: {end_time}")

    while cap.isOpened():
        ret, frame = cap.read()
        current_time = cap.get(cv2.CAP_PROP_POS_MSEC) / 1000.0
        # Break if past end time
        if current_time > end_time and (end_time != 0.0 or end_time == 0):
            break
        frames.append(frame)
        print(f"{count} - ret:{ret}, time: {current_time}")
        count += 1
    for i, frame in enumerate(frames):
        filename = os.path.join(output_dir, f'{file_prefix}{i}.png')
        if w == 0:
            shape = frame.shape
            w, h = scale_dimensions(shape[1], shape[0], max=max_size, multiple=16)
            clear_last()
            page.add_to_init_video_output(Text(f'Extracting {len(frames)} frames at {w}x{h}, {video_length} seconds long...'))
        frame = cv2.resize(frame, (w, h), interpolation = cv2.INTER_AREA)
        cv2.imwrite(filename, frame)
        files.append(filename)
        if show_images:
            page.add_to_init_video_output(Row([Img(src=filename, width=w, height=h, fit=ImageFit.FILL, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
            page.add_to_init_video_output(Row([Text(filename)], alignment=MainAxisAlignment.CENTER))
    cap.release()'''
    if not bool(files):
        alert_msg(page, "ERROR Creating Images from Video...")
        return

    for f in files:
        arg = {}
        if bool(init_video_prefs['negative_prompt']):
            arg['negative_prompt'] = init_video_prefs['negative_prompt']
        arg['init_image'] = f
        arg['width'] = w
        arg['height'] = h
        if bool(init_video_prefs['include_strength']):
            arg['init_image_strength'] = init_video_prefs['image_strength']
        page.add_to_prompts(prompt, arg)
    page.add_to_init_video_output(Text(f'Added {len(files)} Files as Init-Image in Prompts List...', weight=FontWeight.BOLD))
    page.add_to_init_video_output(Text(f'Saved to {output_dir}'))
    if prefs['enable_sounds']: page.snd_drop.play()

def multiple_of_64(x):
    return int(round(x/64)*64)
def multiple_of_8(x):
    return int(round(x/8)*8)
def multiple_of(x, num):
    return int(round(x/num)*num)
def scale_dimensions(width, height, max=1024, multiple=16):
  max = int(max)
  r_width = width
  r_height = height
  if width < max and height < max:
    if width >= height:
      ratio = max / width
      r_width = max
      r_height = int(height * ratio)
    else:
      ratio = max / height
      r_height = max
      r_width = int(width * ratio)
    width = r_width
    height = r_height
  if width >= height:
    if width > max:
      r_width = max
      r_height = int(height * (max/width))
    else:
      r_width = width
      r_height = height
  else:
    if height > max:
      r_height = max
      r_width = int(width * (max/height))
    else:
      r_width = width
      r_height = height
  return multiple_of(r_width, multiple), multiple_of(r_height, multiple)

def run_repainter(page):
    global repaint_prefs, prefs, status, pipe_repaint
    if not status['installed_diffusers']:
      alert_msg(page, "You need to Install HuggingFace Diffusers before using...")
      return
    if not bool(repaint_prefs['original_image']) or not bool(repaint_prefs['mask_image']):
      alert_msg(page, "You must provide the Original Image and the Mask Image to process...")
      return
    def prt(line):
      if type(line) == str:
        line = Text(line, size=17)
      page.RePainter.controls.append(line)
      page.RePainter.update()
    def clear_last():
      del page.RePainter.controls[-1]
      page.RePainter.update()
    def autoscroll(scroll=True):
      page.RePainter.auto_scroll = scroll
      page.RePainter.update()
    def clear_list():
      page.RePainter.controls = page.RePainter.controls[:1]
    progress = ProgressBar(bar_height=8)
    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress
      total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep}"
      progress.update()
      #print(f'{type(latents)} {len(latents)}- {str(latents)}')
    autoscroll(True)
    clear_list()
    prt(Installing("Installing RePaint Pipeline..."))
    import requests, random
    from io import BytesIO
    from PIL import ImageOps
    if repaint_prefs['original_image'].startswith('http'):
      #response = requests.get(repaint_prefs['original_image'])
      #original_img = PILImage.open(BytesIO(response.content)).convert("RGB")
      original_img = PILImage.open(requests.get(repaint_prefs['original_image'], stream=True).raw)
    else:
      if os.path.isfile(repaint_prefs['original_image']):
        original_img = PILImage.open(repaint_prefs['original_image'])
      else:
        alert_msg(page, f"ERROR: Couldn't find your original_image {repaint_prefs['original_image']}")
        return
    width, height = original_img.size
    width, height = scale_dimensions(width, height, repaint_prefs['max_size'])
    original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS)
    original_img = ImageOps.exif_transpose(original_img).convert("RGB")
    mask_img = None
    if repaint_prefs['mask_image'].startswith('http'):
      #response = requests.get(repaint_prefs['mask_image'])
      #mask_img = PILImage.open(BytesIO(response.content)).convert("RGB")
      mask_img = PILImage.open(requests.get(repaint_prefs['mask_image'], stream=True).raw)
    else:
      if os.path.isfile(repaint_prefs['mask_image']):
        mask_img = PILImage.open(repaint_prefs['mask_image'])
      else:
        alert_msg(page, f"ERROR: Couldn't find your mask_image {repaint_prefs['mask_image']}")
        return
    #mask_img = mask_img.convert("L")
    #mask_img = mask_img.convert("1")
    if repaint_prefs['invert_mask']:
       mask_img = ImageOps.invert(mask_img.convert('RGB'))
    mask_img = mask_img.resize((width, height), resample=PILImage.NEAREST)
    mask_img = ImageOps.exif_transpose(mask_img).convert("RGB")
    #print(f'Resize to {width}x{height}')
    clear_pipes('repaint')
    if not status['installed_repaint']:
      get_repaint(page)
      status['installed_repaint'] = True
    if pipe_repaint is None:
      pipe_repaint = get_repaint_pipe()
    clear_last()
    prt("Generating Repaint of your Image...")
    prt(progress)
    autoscroll(False)
    random_seed = int(repaint_prefs['seed']) if int(repaint_prefs['seed']) > 0 else random.randint(0,4294967295)
    generator = torch.Generator(device=torch_device).manual_seed(random_seed)
#Sizes of tensors must match except in dimension 1. Expected size 58 but got size 59 for tensor number 1 in the list.
    try:
      #from IPython.utils.capture import capture_output
      #with capture_output() as captured:
      image = pipe_repaint(image=original_img, mask_image=mask_img, num_inference_steps=repaint_prefs['num_inference_steps'], eta=repaint_prefs['eta'], jump_length=repaint_prefs['jump_length'], jump_n_sample=repaint_prefs['jump_length'], generator=generator, callback=callback_fnc, callback_steps=1).images[0]
      #print(str(captured.stdout))
    except Exception as e:
      clear_last()
      alert_msg(page, f"ERROR: Couldn't Repaint your image for some reason.  Possibly out of memory or something wrong with my code...", content=Text(str(e)))
      return
    fname = repaint_prefs['original_image'].rpartition('.')[0]
    fname = fname.rpartition(slash)[2]
    if prefs['file_suffix_seed']: fname += f"-{random_seed}"
    image_path = available_file(stable_dir, fname, 1)
    image.save(image_path)
    out_path = image_path
    clear_last()
    clear_last()
    autoscroll(True)
    prt(Row([ImageButton(src=image_path, width=width, height=height, data=image_path, page=page)], alignment=MainAxisAlignment.CENTER))
    #prt(Row([Img(src=image_path, width=width, height=height, fit=ImageFit.FILL, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
    #TODO: ESRGAN, Metadata & PyDrive
    if storage_type == "Colab Google Drive":
      new_file = available_file(prefs['image_output'], fname, 1)
      out_path = new_file
      shutil.copy(image_path, new_file)
    elif bool(prefs['image_output']):
      new_file = available_file(prefs['image_output'], fname, 1)
      out_path = new_file
      shutil.copy(image_path, new_file)
    prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))
    autoscroll(False)
    if prefs['enable_sounds']: page.snd_alert.play()

def run_image_variation(page):
    global image_variation_prefs, pipe_image_variation
    if not status['installed_diffusers']:
      alert_msg(page, "You must Install the HuggingFace Diffusers Library first... ")
      return
    def prt(line):
      if type(line) == str:
        line = Text(line)
      page.ImageVariation.controls.append(line)
      page.ImageVariation.update()
    def clear_last():
      del page.ImageVariation.controls[-1]
      page.ImageVariation.update()
    def autoscroll(scroll=True):
      page.ImageVariation.auto_scroll = scroll
      page.ImageVariation.update()
    def clear_list():
      page.ImageVariation.controls = page.ImageVariation.controls[:1]
    progress = ProgressBar(bar_height=8)
    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress
      total_steps = image_variation_prefs['num_inference_steps']#len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep}"
      progress.update()
    autoscroll(True)
    clear_list()
    from io import BytesIO
    from PIL import ImageOps
    if image_variation_prefs['init_image'].startswith('http'):
      init_img = PILImage.open(requests.get(image_variation_prefs['init_image'], stream=True).raw)
    else:
      if os.path.isfile(image_variation_prefs['init_image']):
        init_img = PILImage.open(image_variation_prefs['init_image'])
      else:
        alert_msg(page, f"ERROR: Couldn't find your init_image {image_variation_prefs['init_image']}")
        return
    width, height = init_img.size
    width, height = scale_dimensions(width, height, image_variation_prefs['max_size'])
    tform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Resize(
            (width, height),
            interpolation=transforms.InterpolationMode.BICUBIC,
            antialias=False,
            ),
        transforms.Normalize(
          [0.48145466, 0.4578275, 0.40821073],
          [0.26862954, 0.26130258, 0.27577711]),
    ])
    init_img = tform(init_img).to(torch_device)
    #init_img = init_img.resize((width, height), resample=PILImage.Resampling.LANCZOS)
    #init_img = ImageOps.exif_transpose(init_img).convert("RGB")
    clear_pipes('image_variation')
    if pipe_image_variation == None:
        from diffusers import StableDiffusionImageVariationPipeline
        prt(Installing("Downloading Image Variation Pipeline"))
        model_id = "fusing/sd-image-variations-diffusers"
        pipe_image_variation = StableDiffusionImageVariationPipeline.from_pretrained(model_id, safety_checker=None, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
        #pipe_image_variation.to(torch_device)
        pipe_image_variation = pipeline_scheduler(pipe_image_variation)
        pipe_image_variation = optimize_pipe(pipe_image_variation)
        #pipe_image_variation.set_progress_bar_config(disable=True)
        clear_last()
    s = "s" if image_variation_prefs['num_images'] > 1 else ""
    prt(f"Generating Variation{s} of your Image...")
    prt(progress)
    autoscroll(False)
    random_seed = int(image_variation_prefs['seed']) if int(image_variation_prefs['seed']) > 0 else rnd.randint(0,4294967295)
    generator = torch.Generator(device=torch_device).manual_seed(random_seed)

    try:
        images = pipe_image_variation(image=init_img, height=height, width=width, num_inference_steps=image_variation_prefs['num_inference_steps'], guidance_scale=image_variation_prefs['guidance_scale'], eta=image_variation_prefs['eta'], num_images_per_prompt=image_variation_prefs['num_images'], generator=generator, callback=callback_fnc, callback_steps=1).images
    except Exception as e:
        clear_last()
        clear_last()
        alert_msg(page, "Error running pipeline", content=Text(str(e)))
        return
    clear_last()
    clear_last()
    autoscroll(True)
    fname = image_variation_prefs['init_image'].rpartition('.')[0]
    fname = fname.rpartition(slash)[2]
    if prefs['file_suffix_seed']: fname += f"-{random_seed}"
    for image in images:
        image_path = available_file(stable_dir, fname, 1)
        image.save(image_path)
        out_path = image_path
        #prt(Row([Img(src=image_path, width=width, height=height, fit=ImageFit.FILL, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
        prt(Row([ImageButton(src=image_path, width=width, height=height, data=image_path, page=page)], alignment=MainAxisAlignment.CENTER))
        #TODO: ESRGAN, Metadata & PyDrive
        if storage_type == "Colab Google Drive":
            new_file = available_file(prefs['image_output'], fname, 1)
            out_path = new_file
            shutil.copy(image_path, new_file)
        elif bool(prefs['image_output']):
            new_file = available_file(prefs['image_output'], fname, 1)
            out_path = new_file
            shutil.copy(image_path, new_file)
        prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))
    page.ImageVariation.auto_scroll = False
    page.ImageVariation.update()
    autoscroll(True)
    if prefs['enable_sounds']: page.snd_alert.play()

def run_background_remover(page):
    global background_remover_prefs, pipe_background_remover
    #if not status['installed_diffusers']:
    #  alert_msg(page, "You must Install the HuggingFace Diffusers Library first... ")
    #  return
    def prt(line):
      if type(line) == str:
        line = Text(line)
      page.BackgroundRemover.controls.append(line)
      page.BackgroundRemover.update()
    def clear_last():
      del page.BackgroundRemover.controls[-1]
      page.BackgroundRemover.update()
    def autoscroll(scroll=True):
      page.BackgroundRemover.auto_scroll = scroll
      page.BackgroundRemover.update()
    def clear_list():
      page.BackgroundRemover.controls = page.BackgroundRemover.controls[:1]
    progress = ProgressBar(bar_height=8)
    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress
      total_steps = background_remover_prefs['num_inference_steps']#len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep}"
      progress.update()
    autoscroll(True)
    clear_list()
    installer = Installing("Downloading Background Remover Pipeline")
    prt(installer)
    try:
        from huggingface_hub import hf_hub_download
    except ModuleNotFoundError:
        installer.set_details("...HuggingFace Hub")
        run_process("pip install huggingface_hub --upgrade", page=page)
        from huggingface_hub import hf_hub_download
        pass
    try:
        import cv2
    except ModuleNotFoundError:
        installer.set_details("...installing opencv")
        run_sp("pip install opencv-python", realtime=False)
        import cv2
        pass
    try:
        import onnx
    except ImportError as e:
        installer.set_details("...installing onnx")
        run_sp("pip install -q onnx==1.14.0", realtime=False)
        pass
    try:
        import onnxruntime
    except ImportError as e:
        installer.set_details("...installing onnxruntime")
        run_sp("pip install -q onnxruntime-gpu==1.15.0", realtime=False)
        pass
    if pipe_background_remover == None:
        installer.set_details("...downloading model")
        pipe_background_remover = hf_hub_download('nateraw/background-remover-files', 'modnet.onnx', repo_type='dataset')
    from io import BytesIO
    from PIL import ImageOps
    import cv2
    import numpy as np
    import onnxruntime
    import requests
    def get_scale_factor(im_h, im_w, ref_size=512):
        if max(im_h, im_w) < ref_size or min(im_h, im_w) > ref_size:
            if im_w >= im_h:
                im_rh = ref_size
                im_rw = int(im_w / im_h * ref_size)
            elif im_w < im_h:
                im_rw = ref_size
                im_rh = int(im_h / im_w * ref_size)
        else:
            im_rh = im_h
            im_rw = im_w
        im_rw = im_rw - im_rw % 32
        im_rh = im_rh - im_rh % 32
        x_scale_factor = im_rw / im_w
        y_scale_factor = im_rh / im_h
        return x_scale_factor, y_scale_factor
    if background_remover_prefs['init_image'].startswith('http'):
        installer.set_details("...downloading image")
        init_file = background_remover_prefs['init_image'].rpartition("/")[2].rpartition(".")[0]
        init_img = PILImage.open(requests.get(background_remover_prefs['init_image'], stream=True).raw)
    else:
        if os.path.isfile(background_remover_prefs['init_image']):
            init_file = background_remover_prefs['init_image'].rpartition(slash)[2].rpartition(".")[0]
            init_img = PILImage.open(background_remover_prefs['init_image'])
        else:
            alert_msg(page, f"ERROR: Couldn't find your init_image {background_remover_prefs['init_image']}")
            return
    if bool(background_remover_prefs['output_name']):
        init_file = format_filename(background_remover_prefs['output_name'], force_underscore=True)
    batch_output = os.path.join(stable_dir, background_remover_prefs['batch_folder_name'])
    init_path = os.path.join(batch_output, f"{init_file}.jpg")
    image_path = available_file(batch_output, init_file, 1)
    mask_path = available_file(batch_output, f"{init_file}-mask.png", 1)
    mask_file = available_file(os.path.join(prefs['image_output'], background_remover_prefs['batch_folder_name']), f"{init_file}-mask.png", 1)
    if not os.path.isdir(batch_output):
        os.makedirs(batch_output)
    if not os.path.isdir(os.path.join(prefs['image_output'], background_remover_prefs['batch_folder_name'])):
        os.makedirs(os.path.join(prefs['image_output'], background_remover_prefs['batch_folder_name']))
    init_img.save(init_path)
    clear_pipes('background_remover')
    clear_last()
    prt(f"Generating Foreground of your Image...")
    prt(progress)
    autoscroll(False)
    im = cv2.imread(init_path)
    im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)
    if len(im.shape) == 2:
        im = im[:, :, None]
    if im.shape[2] == 1:
        im = np.repeat(im, 3, axis=2)
    elif im.shape[2] == 4:
        im = im[:, :, 0:3]
    im = (im - 127.5) / 127.5
    im_h, im_w, im_c = im.shape
    x, y = get_scale_factor(im_h, im_w)
    im = cv2.resize(im, None, fx=x, fy=y, interpolation=cv2.INTER_AREA)
    im = np.transpose(im)
    im = np.swapaxes(im, 1, 2)
    im = np.expand_dims(im, axis=0).astype('float32')
    session = onnxruntime.InferenceSession(pipe_background_remover, None, providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])
    input_name = session.get_inputs()[0].name
    output_name = session.get_outputs()[0].name
    result = session.run([output_name], {input_name: im})
    matte = (np.squeeze(result[0]) * 255).astype('uint8')
    matte = cv2.resize(matte, dsize=(im_w, im_h), interpolation=cv2.INTER_AREA)
    cv2.imwrite(mask_path, matte)
    if background_remover_prefs['save_mask']:
        clear_last()
        clear_last()
        prt(Row([ImageButton(src=mask_path, width=im_w, height=im_h, data=mask_file, page=page)], alignment=MainAxisAlignment.CENTER))
    image = PILImage.open(init_path)
    matte = PILImage.open(mask_path)
    image = np.asarray(image)
    if len(image.shape) == 2:
        image = image[:, :, None]
    if image.shape[2] == 1:
        image = np.repeat(image, 3, axis=2)
    elif image.shape[2] == 4:
        image = image[:, :, 0:3]
    b, g, r = cv2.split(image)
    mask = np.asarray(matte)
    a = np.ones(mask.shape, dtype='uint8') * 255
    alpha_im = cv2.merge([b, g, r, a], 4)
    bg = np.zeros(alpha_im.shape)
    new_mask = np.stack([mask, mask, mask, mask], axis=2)
    foreground = np.where(new_mask > background_remover_prefs['threshold'], alpha_im, bg).astype(np.uint8)
    img = PILImage.fromarray(foreground)
    width, height = img.size
    if not background_remover_prefs['save_mask']:
        clear_last()
        clear_last()
    autoscroll(True)
    img.save(image_path)
    out_path = image_path
    #prt(Row([Img(src=image_path, width=width, height=height, fit=ImageFit.FILL, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
    prt(Row([ImageButton(src=image_path, width=width, height=height, data=image_path, page=page)], alignment=MainAxisAlignment.CENTER))
    #TODO: ESRGAN, Metadata & PyDrive
    if storage_type == "Colab Google Drive":
        new_file = available_file(prefs['image_output'], init_file, 1)
        out_path = new_file
        shutil.copy(image_path, new_file)
        if background_remover_prefs['save_mask']:
            shutil.copy(mask_path, mask_file)
    elif bool(prefs['image_output']):
        new_file = available_file(prefs['image_output'], init_file, 1)
        out_path = new_file
        shutil.copy(image_path, new_file)
        if background_remover_prefs['save_mask']:
            shutil.copy(mask_path, mask_file)
    prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))
    page.BackgroundRemover.auto_scroll = False
    page.BackgroundRemover.update()
    autoscroll(False)
    if prefs['enable_sounds']: page.snd_alert.play()


def run_reference(page, from_list=False):
    global reference_prefs, pipe_reference
    if not status['installed_diffusers']:
      alert_msg(page, "You must Install the HuggingFace Diffusers Library first... ")
      return
    def prt(line, update=True):
      if type(line) == str:
        line = Text(line)
      if from_list:
        page.imageColumn.controls.append(line)
        if update:
          page.imageColumn.update()
      else:
        page.Reference.controls.append(line)
        if update:
          page.Reference.update()
    def clear_last():
      if from_list:
        del page.imageColumn.controls[-1]
        page.imageColumn.update()
      else:
        del page.Reference.controls[-1]
        page.Reference.update()
    def clear_list():
      if from_list:
        page.imageColumn.controls.clear()
      else:
        page.Reference.controls = page.Reference.controls[:1]
    def autoscroll(scroll=True):
      if from_list:
        page.imageColumn.auto_scroll = scroll
        page.imageColumn.update()
        page.Reference.auto_scroll = scroll
        page.Reference.update()
      else:
        page.Reference.auto_scroll = scroll
        page.Reference.update()
    progress = ProgressBar(bar_height=8)
    total_steps = reference_prefs['num_inference_steps']
    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep}"
      progress.update()
    reference_prompts = []
    if from_list:
      if len(prompts) < 1:
        alert_msg(page, "You need to add Prompts to your List first... ")
        return
      for p in prompts:
        reference = {'prompt': p.prompt, 'negative_prompt': p['negative_prompt'], 'ref_image': p['init_image'] if bool(p['init_image']) else reference_prefs['ref_image'], 'guidance_scale':p['guidance_scale'], 'num_inference_steps': p['steps'], 'width': p['width'], 'height': p['height'], 'seed': p['seed']}
        reference_prompts.append(reference)
    else:
        if not bool(reference_prefs['prompt']):
            alert_msg(page, "You need to add a Text Prompt first... ")
            return
        reference = {'prompt':reference_prefs['prompt'], 'negative_prompt': reference_prefs['negative_prompt'], 'ref_image': reference_prefs['ref_image'], 'guidance_scale':reference_prefs['guidance_scale'], 'num_inference_steps': reference_prefs['num_inference_steps'], 'width': reference_prefs['width'], 'height': reference_prefs['height'], 'seed': reference_prefs['seed']}
        reference_prompts.append(reference)
    if from_list:
      page.tabs.selected_index = 4
      page.tabs.update()
    else:
      clear_list()
    autoscroll(True)
    from io import BytesIO
    from PIL.PngImagePlugin import PngInfo
    from PIL import ImageOps
    model = get_model(prefs['model_ckpt'])['path']
    if reference_prefs['last_model'] == model:
        clear_pipes('reference')
    else:
        clear_pipes()
        reference_prefs['last_model'] = model
    #torch.cuda.empty_cache()
    #torch.cuda.reset_max_memory_allocated()
    #torch.cuda.reset_peak_memory_stats()
    if pipe_reference == None:
        prt(Installing(f"Installing Reference Pipeline with {model} Model... "))
        diffusers_dir = os.path.join(root_dir, "diffusers")
        if not os.path.exists(diffusers_dir):
          run_process("git clone https://github.com/Skquark/diffusers.git", realtime=False, cwd=root_dir)
        sys.path.append(os.path.join(diffusers_dir, "examples", "community"))
        from stable_diffusion_reference import StableDiffusionReferencePipeline
        try:
            pipe_reference = StableDiffusionReferencePipeline.from_pretrained(model, torch_dtype=torch.float16, safety_checker=None, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
            #pipe_reference.to(torch_device)
            pipe_reference = pipeline_scheduler(pipe_reference)
            pipe_reference = optimize_pipe(pipe_reference)
            pipe_reference.set_progress_bar_config(disable=True)
        except Exception as e:
            clear_last()
            alert_msg(page, "Error Installing Reference Pipeline", content=Text(str(e)))
            return
        #pipe_reference.set_progress_bar_config(disable=True)
        clear_last()
    else:
        pipe_reference = pipeline_scheduler(pipe_reference)
    s = "s" if reference_prefs['num_images'] > 1 or reference_prefs['batch_size'] > 1 else ""
    prt(f"Generating Reference{s} of your Image...")
    batch_output = os.path.join(stable_dir, reference_prefs['batch_folder_name'])
    if not os.path.isdir(batch_output):
      os.makedirs(batch_output)
    batch_output = os.path.join(prefs['image_output'], reference_prefs['batch_folder_name'])
    if not os.path.isdir(batch_output):
      os.makedirs(batch_output)
    for pr in reference_prompts:
        if pr['ref_image'].startswith('http'):
            init_img = PILImage.open(requests.get(pr['ref_image'], stream=True).raw)
        else:
            if os.path.isfile(pr['ref_image']):
                init_img = PILImage.open(pr['ref_image'])
            else:
                alert_msg(page, f"ERROR: Couldn't find your ref_image {pr['ref_image']}")
                return
        width, height = init_img.size
        width, height = scale_dimensions(width, height, reference_prefs['max_size'])
        init_img = init_img.resize((width, height), resample=PILImage.BICUBIC)
        init_img = ImageOps.exif_transpose(init_img).convert("RGB")
        width = pr['width']
        height = pr['height']
        total_steps = pr['num_inference_steps']
        for num in range(reference_prefs['num_images']):
            prt(progress)
            autoscroll(False)
            random_seed = (int(pr['seed']) + num) if int(pr['seed']) > 0 else rnd.randint(0,4294967295)
            #generator = torch.Generator(device=torch_device).manual_seed(random_seed)
            try:
                images = pipe_reference(ref_image=init_img, prompt=pr['prompt'], negative_prompt=pr['negative_prompt'], num_inference_steps=pr['num_inference_steps'], attention_auto_machine_weight=reference_prefs['attention_auto_machine_weight'], gn_auto_machine_weight=reference_prefs['gn_auto_machine_weight'], style_fidelity=reference_prefs['style_fidelity'], reference_attn=reference_prefs['reference_attn'], reference_adain=reference_prefs['reference_adain'], guidance_scale=pr['guidance_scale'], width=width, height=height, num_images_per_prompt=reference_prefs['batch_size'], callback=callback_fnc, callback_steps=1).images
            except Exception as e:
                clear_last()
                alert_msg(page, "Error running Reference Pipeline", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))
                return
            autoscroll(True)
            clear_last()
            fname = format_filename(pr['prompt'])
            for image in images:
                if prefs['file_suffix_seed']: fname += f"-{random_seed}"
                #for image in images:
                image_path = available_file(os.path.join(stable_dir, reference_prefs['batch_folder_name']), fname, num)
                unscaled_path = image_path
                output_file = image_path.rpartition(slash)[2]
                image.save(image_path)
                out_path = image_path.rpartition(slash)[0]
                upscaled_path = os.path.join(out_path, output_file)
                if not reference_prefs['display_upscaled_image'] or not reference_prefs['apply_ESRGAN_upscale']:
                    prt(Row([ImageButton(src=unscaled_path, data=upscaled_path, width=width, height=height, page=page)], alignment=MainAxisAlignment.CENTER))
                    #prt(Row([Img(src=unscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
                    time.sleep(0.6)
                if reference_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
                    os.chdir(os.path.join(dist_dir, 'Real-ESRGAN'))
                    upload_folder = 'upload'
                    result_folder = 'results'
                    if os.path.isdir(upload_folder):
                        shutil.rmtree(upload_folder)
                    if os.path.isdir(result_folder):
                        shutil.rmtree(result_folder)
                    os.mkdir(upload_folder)
                    os.mkdir(result_folder)
                    short_name = f'{fname[:80]}-{num}.png'
                    dst_path = os.path.join(dist_dir, 'Real-ESRGAN', upload_folder, short_name)
                    #print(f'Moving {fpath} to {dst_path}')
                    #shutil.move(fpath, dst_path)
                    shutil.copy(image_path, dst_path)
                    #faceenhance = ' --face_enhance' if reference_prefs["face_enhance"] else ''
                    faceenhance = ''
                    run_sp(f'python inference_realesrgan.py -n realesr-general-x4v3 -i upload --outscale {reference_prefs["enlarge_scale"]}{faceenhance}', cwd=os.path.join(dist_dir, 'Real-ESRGAN'), realtime=False)
                    out_file = short_name.rpartition('.')[0] + '_out.png'
                    shutil.move(os.path.join(dist_dir, 'Real-ESRGAN', result_folder, out_file), upscaled_path)
                    image_path = upscaled_path
                    os.chdir(stable_dir)
                    if reference_prefs['display_upscaled_image']:
                        prt(Row([ImageButton(src=upscaled_path, data=upscaled_path, width=width * float(reference_prefs["enlarge_scale"]), height=height * float(reference_prefs["enlarge_scale"]), page=page)], alignment=MainAxisAlignment.CENTER))
                        time.sleep(0.6)
                        #prt(Row([Img(src=upscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
                if prefs['save_image_metadata']:
                    img = PILImage.open(image_path)
                    metadata = PngInfo()
                    metadata.add_text("artist", prefs['meta_ArtistName'])
                    metadata.add_text("copyright", prefs['meta_Copyright'])
                    metadata.add_text("software", "Stable Diffusion Deluxe" + f", upscaled {reference_prefs['enlarge_scale']}x with ESRGAN" if reference_prefs['apply_ESRGAN_upscale'] else "")
                    metadata.add_text("pipeline", "Reference")
                    if prefs['save_config_in_metadata']:
                      metadata.add_text("title", pr['prompt'])
                      config_json = reference_prefs.copy()
                      config_json['model_path'] = model
                      config_json['seed'] = random_seed
                      del config_json['num_images'], config_json['batch_size']
                      del config_json['display_upscaled_image']
                      del config_json['batch_folder_name']
                      del config_json['max_size']
                      if not config_json['apply_ESRGAN_upscale']:
                        del config_json['enlarge_scale']
                        del config_json['apply_ESRGAN_upscale']
                      metadata.add_text("config_json", json.dumps(config_json, ensure_ascii=True, indent=4))
                    img.save(image_path, pnginfo=metadata)
                #TODO: PyDrive
                if storage_type == "Colab Google Drive":
                    new_file = available_file(os.path.join(prefs['image_output'], reference_prefs['batch_folder_name']), fname, num)
                    out_path = new_file
                    shutil.copy(image_path, new_file)
                elif bool(prefs['image_output']):
                    new_file = available_file(os.path.join(prefs['image_output'], reference_prefs['batch_folder_name']), fname, num)
                    out_path = new_file
                    shutil.copy(image_path, new_file)
                time.sleep(0.2)
                prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))
    autoscroll(False)
    if prefs['enable_sounds']: page.snd_alert.play()

def run_controlnet_qr(page, from_list=False):
    global controlnet_qr_prefs, pipe_controlnet_qr, pipe_controlnet
    if not status['installed_diffusers']:
      alert_msg(page, "You must Install the HuggingFace Diffusers Library first... ")
      return
    def prt(line, update=True):
      if type(line) == str:
        line = Text(line)
      if from_list:
        page.imageColumn.controls.append(line)
        if update:
          page.imageColumn.update()
      else:
        page.ControlNetQR.controls.append(line)
        if update:
          page.ControlNetQR.update()
    def clear_last():
      if from_list:
        del page.imageColumn.controls[-1]
        page.imageColumn.update()
      else:
        del page.ControlNetQR.controls[-1]
        page.ControlNetQR.update()
    def clear_list():
      if from_list:
        page.imageColumn.controls.clear()
      else:
        page.ControlNetQR.controls = page.ControlNetQR.controls[:1]
    def autoscroll(scroll=True):
      if from_list:
        page.imageColumn.auto_scroll = scroll
        page.imageColumn.update()
        page.ControlNetQR.auto_scroll = scroll
        page.ControlNetQR.update()
      else:
        page.ControlNetQR.auto_scroll = scroll
        page.ControlNetQR.update()
    progress = ProgressBar(bar_height=8)
    total_steps = controlnet_qr_prefs['num_inference_steps'] * 2
    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep}"
      progress.update()
    controlnet_qr_prompts = []
    if from_list:
      if len(prompts) < 1:
        alert_msg(page, "You need to add Prompts to your List first... ")
        return
      for p in prompts:
        controlnet_qr = {'prompt': p.prompt, 'negative_prompt': p['negative_prompt'], 'init_image': p['init_image'] if bool(p['init_image']) else controlnet_qr_prefs['init_image'], 'guidance_scale':p['guidance_scale'], 'num_inference_steps': p['steps'], 'width': p['max_size'], 'height': p['max_size'], 'strength':p['init_image_strength'], 'seed': p['seed']}
        controlnet_qr_prompts.append(controlnet_qr)
    else:
        if not bool(controlnet_qr_prefs['prompt']):
            alert_msg(page, "You need to add a Text Prompt first... ")
            return
        controlnet_qr = {'prompt':controlnet_qr_prefs['prompt'], 'negative_prompt': controlnet_qr_prefs['negative_prompt'], 'init_image': controlnet_qr_prefs['init_image'], 'guidance_scale':controlnet_qr_prefs['guidance_scale'], 'num_inference_steps': controlnet_qr_prefs['num_inference_steps'], 'width': controlnet_qr_prefs['max_size'], 'height': controlnet_qr_prefs['max_size'], 'strength': controlnet_qr_prefs['strength'], 'seed': controlnet_qr_prefs['seed']}
        controlnet_qr_prompts.append(controlnet_qr)
    if from_list:
      page.tabs.selected_index = 4
      page.tabs.update()
    else:
      clear_list()
    autoscroll(True)
    from io import BytesIO
    from PIL.PngImagePlugin import PngInfo
    from PIL import ImageOps
    try:
        import qrcode
    except ImportError:
        prt(Installing(f"Installing QRCode Library... "))
        run_sp("pip install qrcode", realtime=False)
        import qrcode
        clear_last()
        pass
    sd_model = get_model(prefs['model_ckpt'])['path']
    controlnet_model = "DionTimmer/controlnet_qrcode-control_v1p_sd15" if controlnet_qr_prefs['controlnet_version'].endswith("1.5") else "DionTimmer/controlnet_qrcode-control_v11p_sd21"
    if controlnet_qr_prefs['last_model'] == sd_model and controlnet_qr_prefs['last_controlnet_model'] == controlnet_model and pipe_controlnet_qr != None:
        clear_pipes('controlnet_qr')
    else:
        clear_pipes()
        controlnet_qr_prefs['last_model'] = sd_model
        controlnet_qr_prefs['last_controlnet_model'] = controlnet_model
    #torch.cuda.empty_cache()
    #torch.cuda.reset_max_memory_allocated()
    #torch.cuda.reset_peak_memory_stats()
    if pipe_controlnet_qr == None:
        prt(Installing(f"Installing ControlNet QRCode Pipeline with {sd_model} Model... "))
        from diffusers import StableDiffusionControlNetImg2ImgPipeline, ControlNetModel
        try:
            pipe_controlnet = ControlNetModel.from_pretrained(controlnet_model, torch_dtype=torch.float16)
            pipe_controlnet_qr = StableDiffusionControlNetImg2ImgPipeline.from_pretrained(sd_model, controlnet=pipe_controlnet, torch_dtype=torch.float16, safety_checker=None, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
            pipe_controlnet_qr = pipeline_scheduler(pipe_controlnet_qr)
            pipe_controlnet_qr = optimize_pipe(pipe_controlnet_qr)
            pipe_controlnet_qr.set_progress_bar_config(disable=True)
        except Exception as e:
            clear_last()
            alert_msg(page, "Error Installing ControlNet QRCode Pipeline", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            return
        #pipe_controlnet_qr.set_progress_bar_config(disable=True)
        clear_last()
    else:
        pipe_controlnet_qr = pipeline_scheduler(pipe_controlnet_qr)
    s = "s" if controlnet_qr_prefs['num_images'] > 1 or controlnet_qr_prefs['batch_size'] > 1 else ""
    prt(f"Generating ControlNet QR{s} of your Image...")
    batch_output = os.path.join(stable_dir, controlnet_qr_prefs['batch_folder_name'])
    if not os.path.isdir(batch_output):
        os.makedirs(batch_output)
    batch_output = os.path.join(prefs['image_output'], controlnet_qr_prefs['batch_folder_name'])
    if not os.path.isdir(batch_output):
        os.makedirs(batch_output)
    batch_size = controlnet_qr_prefs['batch_size']
    if controlnet_qr_prefs['use_image']:
        if not bool(controlnet_qr_prefs['ref_image']):
            alert_msg(page, f"ERROR: If using your own QR image, you must provide it.")
            return
        if controlnet_qr_prefs['ref_image'].startswith('http'):
            qrcode_image = PILImage.open(requests.get(controlnet_qr_prefs['ref_image'], stream=True).raw)
        else:
            if os.path.isfile(pr['ref_image']):
                qrcode_image = PILImage.open(controlnet_qr_prefs['ref_image'])
            else:
                alert_msg(page, f"ERROR: Couldn't find your ref_image {controlnet_qr_prefs['ref_image']}")
                return
    else:
        if not bool(controlnet_qr_prefs['qr_content']):
            alert_msg(page, f"ERROR: If Generating QR Code, you must provide content like a URL.")
            return
        prt("Generating QR Code from content...")
        qr = qrcode.QRCode(
            version=1,
            error_correction=qrcode.constants.ERROR_CORRECT_H,
            box_size=10,
            border=controlnet_qr_prefs['border_thickness'],
        )
        qr.add_data(controlnet_qr_prefs['qr_content'])
        qr.make(fit=True)
        qrcode_image = qr.make_image(fill_color="black", back_color="white")
        clear_last()
    width, height = qrcode_image.size
    width, height = scale_dimensions(width, height, controlnet_qr_prefs['max_size'], multiple=32)
    qrcode_image = qrcode_image.resize((width, height), resample=PILImage.LANCZOS)

    for pr in controlnet_qr_prompts:
        if bool(pr['init_image']):
            if pr['init_image'].startswith('http'):
                init_img = PILImage.open(requests.get(pr['init_image'], stream=True).raw)
            else:
                if os.path.isfile(pr['init_image']):
                    init_img = PILImage.open(pr['init_image'])
                else:
                    alert_msg(page, f"ERROR: Couldn't find your init_image {pr['init_image']}")
                    return
            init_img = ImageOps.exif_transpose(init_img).convert("RGB")
            init_img = init_img.resize((width, height), resample=PILImage.LANCZOS)
            #width = pr['width']
            #height = pr['height']
        else:
            init_img = qrcode_image
            pr['strength'] = 1.0
        total_steps = pr['num_inference_steps'] * 2
        for num in range(controlnet_qr_prefs['num_images']):
            prt(progress)
            prt(Container(content=None))
            clear_last()
            autoscroll(False)
            random_seed = (int(pr['seed']) + num) if int(pr['seed']) > 0 else rnd.randint(0,4294967295)
            generator = torch.Generator(device=torch_device).manual_seed(random_seed)
            try:
                images = pipe_controlnet_qr(prompt=[pr['prompt']] * batch_size, negative_prompt=[pr['negative_prompt']] * batch_size, image=[init_img] * batch_size, control_image=qrcode_image, num_inference_steps=pr['num_inference_steps'], guidance_scale=pr['guidance_scale'], controlnet_conditioning_scale=float(controlnet_qr_prefs['conditioning_scale']), control_guidance_start=controlnet_qr_prefs['control_guidance_start'], control_guidance_end=controlnet_qr_prefs['control_guidance_end'], width=width, height=height, num_images_per_prompt=controlnet_qr_prefs['batch_size'], strength=pr['strength'], generator=generator, callback=callback_fnc, callback_steps=1).images
            except Exception as e:
                clear_last()
                alert_msg(page, "Error running ControlNet-QRCode Pipeline", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
                return
            autoscroll(True)
            clear_last()
            fname = format_filename(pr['prompt'])
            i = 0
            for image in images:
                if prefs['file_suffix_seed']: fname += f"-{random_seed}"
                #for image in images:
                image_path = available_file(os.path.join(stable_dir, controlnet_qr_prefs['batch_folder_name']), fname, num)
                unscaled_path = image_path
                output_file = image_path.rpartition(slash)[2]
                image.save(image_path)
                out_path = image_path.rpartition(slash)[0]
                upscaled_path = os.path.join(out_path, output_file)
                if not controlnet_qr_prefs['display_upscaled_image'] or not controlnet_qr_prefs['apply_ESRGAN_upscale']:
                    prt(Row([ImageButton(src=unscaled_path, data=upscaled_path, width=width, height=height, page=page)], alignment=MainAxisAlignment.CENTER))
                    #prt(Row([Img(src=unscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
                if controlnet_qr_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
                    os.chdir(os.path.join(dist_dir, 'Real-ESRGAN'))
                    upload_folder = 'upload'
                    result_folder = 'results'
                    if os.path.isdir(upload_folder):
                        shutil.rmtree(upload_folder)
                    if os.path.isdir(result_folder):
                        shutil.rmtree(result_folder)
                    os.mkdir(upload_folder)
                    os.mkdir(result_folder)
                    short_name = f'{fname[:80]}-{num}.png'
                    dst_path = os.path.join(dist_dir, 'Real-ESRGAN', upload_folder, short_name)
                    #print(f'Moving {fpath} to {dst_path}')
                    #shutil.move(fpath, dst_path)
                    shutil.copy(image_path, dst_path)
                    #faceenhance = ' --face_enhance' if controlnet_qr_prefs["face_enhance"] else ''
                    faceenhance = ''
                    run_sp(f'python inference_realesrgan.py -n realesr-general-x4v3 -i upload --outscale {controlnet_qr_prefs["enlarge_scale"]}{faceenhance}', cwd=os.path.join(dist_dir, 'Real-ESRGAN'), realtime=False)
                    out_file = short_name.rpartition('.')[0] + '_out.png'
                    shutil.move(os.path.join(dist_dir, 'Real-ESRGAN', result_folder, out_file), upscaled_path)
                    image_path = upscaled_path
                    os.chdir(stable_dir)
                    if controlnet_qr_prefs['display_upscaled_image']:
                        time.sleep(0.6)
                        prt(Row([ImageButton(src=upscaled_path, data=upscaled_path, width=width * float(controlnet_qr_prefs["enlarge_scale"]), height=height * float(controlnet_qr_prefs["enlarge_scale"]), page=page)], alignment=MainAxisAlignment.CENTER))
                        #prt(Row([Img(src=upscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
                if prefs['save_image_metadata']:
                    img = PILImage.open(image_path)
                    metadata = PngInfo()
                    metadata.add_text("artist", prefs['meta_ArtistName'])
                    metadata.add_text("copyright", prefs['meta_Copyright'])
                    metadata.add_text("software", "Stable Diffusion Deluxe" + f", upscaled {controlnet_qr_prefs['enlarge_scale']}x with ESRGAN" if controlnet_qr_prefs['apply_ESRGAN_upscale'] else "")
                    metadata.add_text("pipeline", "ControlNet-QRCode")
                    if prefs['save_config_in_metadata']:
                      metadata.add_text("title", pr['prompt'])
                      config_json = controlnet_qr_prefs.copy()
                      config_json['controlnet_model_path'] = controlnet_model
                      config_json['model_path'] = sd_model
                      config_json['seed'] = random_seed + i
                      del config_json['num_images'], config_json['batch_size']
                      del config_json['display_upscaled_image']
                      del config_json['batch_folder_name']
                      del config_json['max_size']
                      del config_json['last_model']
                      del config_json['last_controlnet_model']
                      if not config_json['apply_ESRGAN_upscale']:
                        del config_json['enlarge_scale']
                        del config_json['apply_ESRGAN_upscale']
                      metadata.add_text("config_json", json.dumps(config_json, ensure_ascii=True, indent=4))
                    img.save(image_path, pnginfo=metadata)
                #TODO: PyDrive
                if storage_type == "Colab Google Drive":
                    new_file = available_file(os.path.join(prefs['image_output'], controlnet_qr_prefs['batch_folder_name']), fname, num)
                    out_path = new_file
                    shutil.copy(image_path, new_file)
                elif bool(prefs['image_output']):
                    new_file = available_file(os.path.join(prefs['image_output'], controlnet_qr_prefs['batch_folder_name']), fname, num)
                    out_path = new_file
                    shutil.copy(image_path, new_file)
                time.sleep(0.2)
                prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))
                i += 1
    autoscroll(False)
    if prefs['enable_sounds']: page.snd_alert.play()

def run_controlnet_segment(page, from_list=False):
    global controlnet_segment_prefs, pipe_controlnet_segment
    if not status['installed_diffusers']:
      alert_msg(page, "You must Install the HuggingFace Diffusers Library first... ")
      return
    def prt(line, update=True):
      if type(line) == str:
        line = Text(line)
      if from_list:
        page.imageColumn.controls.append(line)
        if update:
          page.imageColumn.update()
      else:
        page.ControlNetSegmentAnything.controls.append(line)
        if update:
          page.ControlNetSegmentAnything.update()
    def clear_last():
      if from_list:
        del page.imageColumn.controls[-1]
        page.imageColumn.update()
      else:
        del page.ControlNetSegmentAnything.controls[-1]
        page.ControlNetSegmentAnything.update()
    def clear_list():
      if from_list:
        page.imageColumn.controls.clear()
      else:
        page.ControlNetSegmentAnything.controls = page.ControlNetSegmentAnything.controls[:1]
    def autoscroll(scroll=True):
      if from_list:
        page.imageColumn.auto_scroll = scroll
        page.imageColumn.update()
        page.ControlNetSegmentAnything.auto_scroll = scroll
        page.ControlNetSegmentAnything.update()
      else:
        page.ControlNetSegmentAnything.auto_scroll = scroll
        page.ControlNetSegmentAnything.update()
    progress = ProgressBar(bar_height=8)
    total_steps = controlnet_segment_prefs['num_inference_steps']
    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep}"
      progress.update()
    controlnet_segment_prompts = []
    if from_list:
      if len(prompts) < 1:
        alert_msg(page, "You need to add Prompts to your List first... ")
        return
      for p in prompts:
        controlnet_segment = {'prompt': p.prompt, 'negative_prompt': p['negative_prompt'], 'ref_image': p['init_image'] if bool(p['init_image']) else controlnet_segment_prefs['ref_image'], 'guidance_scale':p['guidance_scale'], 'num_inference_steps': p['steps'], 'width': p['width'], 'height': p['height'], 'seed': p['seed']}
        controlnet_segment_prompts.append(controlnet_segment)
    else:
        if not bool(controlnet_segment_prefs['prompt']):
            alert_msg(page, "You need to add a Text Prompt first... ")
            return
        controlnet_segment = {'prompt':controlnet_segment_prefs['prompt'], 'negative_prompt': controlnet_segment_prefs['negative_prompt'], 'ref_image': controlnet_segment_prefs['ref_image'], 'guidance_scale':controlnet_segment_prefs['guidance_scale'], 'num_inference_steps': controlnet_segment_prefs['num_inference_steps'], 'width': controlnet_segment_prefs['width'], 'height': controlnet_segment_prefs['height'], 'seed': controlnet_segment_prefs['seed']}
        controlnet_segment_prompts.append(controlnet_segment)
    if from_list:
      page.tabs.selected_index = 4
      page.tabs.update()
    else:
      clear_list()
    from io import BytesIO
    from PIL.PngImagePlugin import PngInfo
    from PIL import ImageOps
    model = "runwayml/stable-diffusion-v1-5"#get_model(prefs['model_ckpt'])['path']
    model_name = "1.5"#get_model(prefs['model_ckpt'])['name']
    controlnet_model = "mfidabel/controlnet-segment-anything"
    autoscroll(True)
    installer = Installing(f"Installing ControlNet Segment-Anything Pipeline with {model} Model... ")
    prt(installer)
    clear_pipes('controlnet_segment')
    #torch.cuda.empty_cache()
    #torch.cuda.reset_max_memory_allocated()
    #torch.cuda.reset_peak_memory_stats()
    try:
        from segment_anything import sam_model_registry, SamAutomaticMaskGenerator
    except ModuleNotFoundError:
        installer.set_details("...facebookresearch/segment-anything")
        run_sp("pip install git+https://github.com/facebookresearch/segment-anything.git", realtime=False)
        from segment_anything import sam_model_registry, SamAutomaticMaskGenerator
        pass
    SAM_URL = "https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth"
    SAM_dir = os.path.join(root_dir, "sam_vit_h_4b8939.pth")
    if not os.path.exists(SAM_dir):
        installer.set_details("...Downloading SAM Weights")
        r = requests.get(SAM_URL, allow_redirects=True)
        installer.set_details("...Writing SAM Weights")
        with open(SAM_dir, "wb") as sam_weights:
            sam_weights.write(r.content)
        del r

    gc.collect()
    sam = sam_model_registry["vit_h"](checkpoint=SAM_dir).to(torch_device)
    mask_generator = SamAutomaticMaskGenerator(sam)
    gc.collect()

    from diffusers import StableDiffusionControlNetPipeline, ControlNetModel
    #prt(installer)
    if pipe_controlnet_segment == None:
        try:
            installer.set_details(f"...ControlNet Model {controlnet_model}")
            pipe_controlnet = ControlNetModel.from_pretrained(controlnet_model, torch_dtype=torch.float16).to(torch_device)
            installer.set_details(f"...SD Model {model_name}")
            pipe_controlnet_segment = StableDiffusionControlNetPipeline.from_pretrained(model, controlnet=pipe_controlnet, torch_dtype=torch.float16, safety_checker=None, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
            pipe_controlnet_segment = pipeline_scheduler(pipe_controlnet_segment)
            pipe_controlnet_segment = optimize_pipe(pipe_controlnet_segment)
            pipe_controlnet_segment.set_progress_bar_config(disable=True)
        except Exception as e:
            clear_last()
            alert_msg(page, "Error Installing ControlNet Segment-Anything Pipeline", content=Text(str(e)))
            return
        #pipe_controlnet_segment.set_progress_bar_config(disable=True)
    else:
        pipe_controlnet_segment = pipeline_scheduler(pipe_controlnet_segment)

    def show_anns(anns):
        if len(anns) == 0:
            return
        sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)
        h, w =  anns[0]['segmentation'].shape
        final_img = PILImage.fromarray(np.zeros((h, w, 3), dtype=np.uint8), mode="RGB")
        for ann in sorted_anns:
            m = ann['segmentation']
            img = np.empty((m.shape[0], m.shape[1], 3), dtype=np.uint8)
            for i in range(3):
                img[:,:,i] = np.random.randint(255, dtype=np.uint8)
            final_img.paste(PILImage.fromarray(img, mode="RGB"), (0, 0), PILImage.fromarray(np.uint8(m*255)))
        return final_img

    clear_last()
    s = "s" if controlnet_segment_prefs['num_images'] > 1 or controlnet_segment_prefs['batch_size'] > 1 else ""
    prt(f"Generating ControlNet Segment-Anything{s} of your Image...")
    batch_output = os.path.join(stable_dir, controlnet_segment_prefs['batch_folder_name'])
    if not os.path.isdir(batch_output):
      os.makedirs(batch_output)
    batch_output = os.path.join(prefs['image_output'], controlnet_segment_prefs['batch_folder_name'])
    if not os.path.isdir(batch_output):
      os.makedirs(batch_output)
    for pr in controlnet_segment_prompts:
        if pr['ref_image'].startswith('http'):
            init_img = PILImage.open(requests.get(pr['ref_image'], stream=True).raw)
        else:
            if os.path.isfile(pr['ref_image']):
                init_img = PILImage.open(pr['ref_image'])
            else:
                alert_msg(page, f"ERROR: Couldn't find your ref_image {pr['ref_image']}")
                return
        width, height = init_img.size
        width, height = scale_dimensions(width, height, controlnet_segment_prefs['max_size'], multiple=32)
        init_img = init_img.resize((width, height), resample=PILImage.BICUBIC)
        init_img = ImageOps.exif_transpose(init_img).convert("RGB")
        #width = pr['width']
        #height = pr['height']
        total_steps = pr['num_inference_steps']
        batch_size = controlnet_segment_prefs['batch_size']
        fname = format_filename(pr['prompt'])
        segmented_image = os.path.join(stable_dir, controlnet_segment_prefs['batch_folder_name'], f"{fname}-segmented.png")
        for num in range(controlnet_segment_prefs['num_images']):
            prt("Creating Segmented Mask Image...")
            random_seed = (int(pr['seed']) + num) if int(pr['seed']) > 0 else rnd.randint(0,4294967295)
            generator = torch.Generator(device=torch_device).manual_seed(random_seed)
            np.random.seed(int(random_seed))
            init_img_np = np.asarray(init_img)
            masks = mask_generator.generate(init_img_np)
            segmented_map = show_anns(masks)
            segmented_map.save(segmented_image)
            clear_last()#src_base64=pil_to_base64(segmented_map)
            prt(Row([Img(src=segmented_image, width=width, height=height, fit=ImageFit.FILL, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
            del masks
            flush()
            prt(progress)
            autoscroll(False)
            #segmented_map = segment_image(init_img, random_seed)
            #yield segmented_map, [PILImage.fromarray(np.zeros((width, height, 3), dtype=np.uint8))] * batch_size]
            try:
                images = pipe_controlnet_segment([pr['prompt']] * batch_size, [segmented_map] * batch_size, negative_prompt=[pr['negative_prompt']] * batch_size, num_inference_steps=pr['num_inference_steps'], guidance_scale=pr['guidance_scale'], width=width, height=height, generator=generator, callback=callback_fnc, callback_steps=1).images
            except Exception as e:
                clear_last()
                alert_msg(page, "Error running ControlNet Segment-Anything Pipeline", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))
                return
            autoscroll(True)
            clear_last()
            for image in images:
                if prefs['file_suffix_seed']: fname += f"-{random_seed}"
                #for image in images:
                image_path = available_file(os.path.join(stable_dir, controlnet_segment_prefs['batch_folder_name']), fname, num)
                unscaled_path = image_path
                output_file = image_path.rpartition(slash)[2]
                image.save(image_path)
                out_path = image_path.rpartition(slash)[0]
                upscaled_path = os.path.join(out_path, output_file)
                if not controlnet_segment_prefs['display_upscaled_image'] or not controlnet_segment_prefs['apply_ESRGAN_upscale']:
                    prt(Row([ImageButton(src=unscaled_path, data=upscaled_path, width=width, height=height, page=page)], alignment=MainAxisAlignment.CENTER))
                    #prt(Row([Img(src=unscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
                    time.sleep(0.8)
                if controlnet_segment_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
                    os.chdir(os.path.join(dist_dir, 'Real-ESRGAN'))
                    upload_folder = 'upload'
                    result_folder = 'results'
                    if os.path.isdir(upload_folder):
                        shutil.rmtree(upload_folder)
                    if os.path.isdir(result_folder):
                        shutil.rmtree(result_folder)
                    os.mkdir(upload_folder)
                    os.mkdir(result_folder)
                    short_name = f'{fname[:80]}-{num}.png'
                    dst_path = os.path.join(dist_dir, 'Real-ESRGAN', upload_folder, short_name)
                    #print(f'Moving {fpath} to {dst_path}')
                    #shutil.move(fpath, dst_path)
                    shutil.copy(image_path, dst_path)
                    #faceenhance = ' --face_enhance' if controlnet_segment_prefs["face_enhance"] else ''
                    faceenhance = ''
                    run_sp(f'python inference_realesrgan.py -n realesr-general-x4v3 -i upload --outscale {controlnet_segment_prefs["enlarge_scale"]}{faceenhance}', cwd=os.path.join(dist_dir, 'Real-ESRGAN'), realtime=False)
                    out_file = short_name.rpartition('.')[0] + '_out.png'
                    shutil.move(os.path.join(dist_dir, 'Real-ESRGAN', result_folder, out_file), upscaled_path)
                    image_path = upscaled_path
                    os.chdir(stable_dir)
                    if controlnet_segment_prefs['display_upscaled_image']:
                        prt(Row([ImageButton(src=upscaled_path, data=upscaled_path, width=width * float(controlnet_segment_prefs["enlarge_scale"]), height=height * float(controlnet_segment_prefs["enlarge_scale"]), page=page)], alignment=MainAxisAlignment.CENTER))
                        time.sleep(0.6)
                        #prt(Row([Img(src=upscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
                if prefs['save_image_metadata']:
                    img = PILImage.open(image_path)
                    metadata = PngInfo()
                    metadata.add_text("artist", prefs['meta_ArtistName'])
                    metadata.add_text("copyright", prefs['meta_Copyright'])
                    metadata.add_text("software", "Stable Diffusion Deluxe" + f", upscaled {controlnet_segment_prefs['enlarge_scale']}x with ESRGAN" if controlnet_segment_prefs['apply_ESRGAN_upscale'] else "")
                    metadata.add_text("pipeline", "ControlNetSegmentAnything")
                    if prefs['save_config_in_metadata']:
                      metadata.add_text("title", pr['prompt'])
                      config_json = controlnet_segment_prefs.copy()
                      config_json['model_path'] = model
                      config_json['seed'] = random_seed
                      del config_json['num_images'], config_json['batch_size']
                      del config_json['display_upscaled_image']
                      del config_json['batch_folder_name']
                      del config_json['max_size']
                      if not config_json['apply_ESRGAN_upscale']:
                        del config_json['enlarge_scale']
                        del config_json['apply_ESRGAN_upscale']
                      metadata.add_text("config_json", json.dumps(config_json, ensure_ascii=True, indent=4))
                    img.save(image_path, pnginfo=metadata)
                #TODO: PyDrive
                if storage_type == "Colab Google Drive":
                    new_file = available_file(os.path.join(prefs['image_output'], controlnet_segment_prefs['batch_folder_name']), fname, num)
                    out_path = new_file
                    shutil.copy(image_path, new_file)
                elif bool(prefs['image_output']):
                    new_file = available_file(os.path.join(prefs['image_output'], controlnet_segment_prefs['batch_folder_name']), fname, num)
                    out_path = new_file
                    shutil.copy(image_path, new_file)
                time.sleep(0.2)
                prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))
    autoscroll(False)
    if prefs['enable_sounds']: page.snd_alert.play()


def run_EDICT(page):
    global EDICT_prefs, prefs, status, pipe_EDICT, text_encoder_EDICT
    if not status['installed_diffusers']:
      alert_msg(page, "You need to Install HuggingFace Diffusers before using...")
      return
    if not bool(EDICT_prefs['init_image']):
      alert_msg(page, "You must provide the Original Image and the Mask Image to process...")
      return
    if not bool(EDICT_prefs['base_prompt']) or not bool(EDICT_prefs['target_prompt']):
      alert_msg(page, "You must provide a base prompt describing image and target prompt to process...")
      return
    def prt(line):
      if type(line) == str:
        line = Text(line, size=17)
      page.EDICT.controls.append(line)
      page.EDICT.update()
    def clear_last():
      del page.EDICT.controls[-1]
      page.EDICT.update()
    def autoscroll(scroll=True):
      page.EDICT.auto_scroll = scroll
      page.EDICT.update()
    def clear_list():
      page.EDICT.controls = page.EDICT.controls[:1]
    progress = ProgressBar(bar_height=8)
    total_steps = EDICT_prefs['num_inference_steps']
    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep}"
      progress.update()
      #print(f'{type(latents)} {len(latents)}- {str(latents)}')
    def center_crop_resize(im):
        width, height = im.size
        d = min(width, height)
        left = (width - d) / 2
        upper = (height - d) / 2
        right = (width + d) / 2
        lower = (height + d) / 2
        return im.crop((left, upper, right, lower)).resize((EDICT_prefs['max_size'], EDICT_prefs['max_size']))
    autoscroll(True)
    clear_list()
    prt(Installing("Installing EDICT Editor Pipeline..."))
    import requests, random
    from io import BytesIO
    from PIL import ImageOps
    from PIL.PngImagePlugin import PngInfo
    if EDICT_prefs['init_image'].startswith('http'):
      #response = requests.get(EDICT_prefs['init_image'])
      #original_img = PILImage.open(BytesIO(response.content)).convert("RGB")
      original_img = PILImage.open(requests.get(EDICT_prefs['init_image'], stream=True).raw)
    else:
      if os.path.isfile(EDICT_prefs['init_image']):
        original_img = PILImage.open(EDICT_prefs['init_image'])
      else:
        alert_msg(page, f"ERROR: Couldn't find your init_image {EDICT_prefs['init_image']}")
        return
    #width, height = original_img.size
    #width, height = scale_dimensions(width, height, EDICT_prefs['max_size'])
    original_img = center_crop_resize(original_img)
    clear_pipes('EDICT')
    torch.cuda.empty_cache()
    torch.cuda.reset_max_memory_allocated()
    torch.cuda.reset_peak_memory_stats()
    torch_dtype = torch.float16
    model_id = get_model(prefs['model_ckpt'])['path']
    if pipe_EDICT is None:
        from diffusers import DiffusionPipeline, DDIMScheduler
        from transformers import CLIPTextModel
        try:
            scheduler = DDIMScheduler(
                num_train_timesteps=1000,
                beta_start=0.00085,
                beta_end=0.012,
                beta_schedule="scaled_linear",
                set_alpha_to_one=False,
                clip_sample=False,
            )
            text_encoder_EDICT = CLIPTextModel.from_pretrained(pretrained_model_name_or_path="openai/clip-vit-large-patch14", torch_dtype=torch_dtype)
            pipe_EDICT = DiffusionPipeline.from_pretrained(
                pretrained_model_name_or_path=model_id,
                custom_pipeline="edict_pipeline",
                revision="fp16",
                scheduler=scheduler,
                text_encoder=text_encoder_EDICT,
                leapfrog_steps=True,
                torch_dtype=torch_dtype,
                cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,
            ).to(torch_device)
        except Exception as e:
            clear_last()
            alert_msg(page, f"ERROR: Couldn't Initialize EDICT Pipeline for some reason.  Possibly out of memory or something wrong with my code...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            return
    clear_last()
    prt("Generating EDICT Edit of your Image...")
    prt(progress)
    autoscroll(False)
    batch_output = os.path.join(stable_dir, EDICT_prefs['batch_folder_name'])
    if not os.path.isdir(batch_output):
      os.makedirs(batch_output)
    batch_output = os.path.join(prefs['image_output'], EDICT_prefs['batch_folder_name'])
    if not os.path.isdir(batch_output):
      os.makedirs(batch_output)
    random_seed = int(EDICT_prefs['seed']) if int(EDICT_prefs['seed']) > 0 else random.randint(0,4294967295)
    for i in range(EDICT_prefs['num_images']):
        generator = torch.Generator(device="cpu").manual_seed(random_seed)
        #generator = torch.manual_seed(random_seed)
        try:
            images = pipe_EDICT(base_prompt=EDICT_prefs['base_prompt'], target_prompt=EDICT_prefs['target_prompt'], image=original_img, num_inference_steps=EDICT_prefs['num_inference_steps'], strength=EDICT_prefs['strength'], guidance_scale=EDICT_prefs['guidance_scale'], generator=generator)#.images
        except Exception as e:
            clear_last()
            alert_msg(page, f"ERROR: Couldn't EDICT Edit your image for some reason.  Possibly out of memory or something wrong with my code...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            return
        clear_last()
        clear_last()
        filename = format_filename(EDICT_prefs['target_prompt'])
        #if prefs['file_suffix_seed']: fname += f"-{random_seed}"
        num = 0
        for image in images:
            random_seed += num
            fname = filename + (f"-{random_seed}" if prefs['file_suffix_seed'] else "")
            image_path = available_file(os.path.join(stable_dir, EDICT_prefs['batch_folder_name']), fname, i)
            unscaled_path = image_path
            output_file = image_path.rpartition(slash)[2]
            image.save(image_path)
            width, height = image.size
            out_path = image_path.rpartition(slash)[0]
            upscaled_path = os.path.join(out_path, output_file)
            if not EDICT_prefs['display_upscaled_image'] or not EDICT_prefs['apply_ESRGAN_upscale']:
                prt(Row([ImageButton(src=unscaled_path, data=upscaled_path, width=width, height=height, page=page)], alignment=MainAxisAlignment.CENTER))
                #prt(Row([Img(src=unscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
            if EDICT_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
                os.chdir(os.path.join(dist_dir, 'Real-ESRGAN'))
                upload_folder = 'upload'
                result_folder = 'results'
                if os.path.isdir(upload_folder):
                    shutil.rmtree(upload_folder)
                if os.path.isdir(result_folder):
                    shutil.rmtree(result_folder)
                os.mkdir(upload_folder)
                os.mkdir(result_folder)
                short_name = f'{fname[:80]}-{num}.png'
                dst_path = os.path.join(dist_dir, 'Real-ESRGAN', upload_folder, short_name)
                #print(f'Moving {fpath} to {dst_path}')
                #shutil.move(fpath, dst_path)
                shutil.copy(image_path, dst_path)
                #faceenhance = ' --face_enhance' if EDICT_prefs["face_enhance"] else ''
                faceenhance = ''
                run_sp(f'python inference_realesrgan.py -n realesr-general-x4v3 -i upload --outscale {EDICT_prefs["enlarge_scale"]}{faceenhance}', cwd=os.path.join(dist_dir, 'Real-ESRGAN'), realtime=False)
                out_file = short_name.rpartition('.')[0] + '_out.png'
                shutil.move(os.path.join(dist_dir, 'Real-ESRGAN', result_folder, out_file), upscaled_path)
                image_path = upscaled_path
                os.chdir(stable_dir)
                if EDICT_prefs['display_upscaled_image']:
                    time.sleep(0.6)
                    prt(Row([ImageButton(src=upscaled_path, data=upscaled_path, width=width * float(EDICT_prefs["enlarge_scale"]), height=height * float(EDICT_prefs["enlarge_scale"]), page=page)], alignment=MainAxisAlignment.CENTER))
                    #prt(Row([Img(src=upscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
            if prefs['save_image_metadata']:
                img = PILImage.open(image_path)
                metadata = PngInfo()
                metadata.add_text("artist", prefs['meta_ArtistName'])
                metadata.add_text("copyright", prefs['meta_Copyright'])
                metadata.add_text("software", "Stable Diffusion Deluxe" + f", upscaled {EDICT_prefs['enlarge_scale']}x with ESRGAN" if EDICT_prefs['apply_ESRGAN_upscale'] else "")
                metadata.add_text("pipeline", "EDICT Editor")
                if prefs['save_config_in_metadata']:
                    config_json = EDICT_prefs.copy()
                    config_json['model_path'] = model_id
                    config_json['seed'] = random_seed
                    del config_json['num_images']
                    del config_json['max_size']
                    del config_json['display_upscaled_image']
                    del config_json['batch_folder_name']
                    if not config_json['apply_ESRGAN_upscale']:
                        del config_json['enlarge_scale']
                        del config_json['apply_ESRGAN_upscale']
                metadata.add_text("config_json", json.dumps(config_json, ensure_ascii=True, indent=4))
                img.save(image_path, pnginfo=metadata)
            #TODO: PyDrive
            if storage_type == "Colab Google Drive":
                new_file = available_file(os.path.join(prefs['image_output'], EDICT_prefs['batch_folder_name']), fname, num)
                out_path = new_file
                shutil.copy(image_path, new_file)
            elif bool(prefs['image_output']):
                new_file = available_file(os.path.join(prefs['image_output'], EDICT_prefs['batch_folder_name']), fname, num)
                out_path = new_file
                shutil.copy(image_path, new_file)
            time.sleep(0.2)
            prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))
            num += 1
        random_seed += 1
    autoscroll(False)
    if prefs['enable_sounds']: page.snd_alert.play()

def run_DiffEdit(page):
    global DiffEdit_prefs, prefs, status, pipe_DiffEdit, text_encoder_DiffEdit
    if not status['installed_diffusers']:
      alert_msg(page, "You need to Install HuggingFace Diffusers before using...")
      return
    if not bool(DiffEdit_prefs['init_image']):
      alert_msg(page, "You must provide the Original Image and the Mask Image to process...")
      return
    if not bool(DiffEdit_prefs['target_prompt']):
      alert_msg(page, "You must provide a target prompt to process...")
      return
    def prt(line):
      if type(line) == str:
        line = Text(line, size=17)
      page.DiffEdit.controls.append(line)
      page.DiffEdit.update()
    def clear_last():
      del page.DiffEdit.controls[-1]
      page.DiffEdit.update()
    def autoscroll(scroll=True):
      page.DiffEdit.auto_scroll = scroll
      page.DiffEdit.update()
    def clear_list():
      page.DiffEdit.controls = page.DiffEdit.controls[:1]
    progress = ProgressBar(bar_height=8)
    total_steps = DiffEdit_prefs['num_inference_steps']
    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep}"
      progress.update()
      #print(f'{type(latents)} {len(latents)}- {str(latents)}')
    def center_crop_resize(im):
        width, height = im.size
        d = min(width, height)
        left = (width - d) / 2
        upper = (height - d) / 2
        right = (width + d) / 2
        lower = (height + d) / 2
        return im.crop((left, upper, right, lower)).resize((DiffEdit_prefs['max_size'], DiffEdit_prefs['max_size']))
    autoscroll(True)
    clear_list()
    source_prompt = DiffEdit_prefs['source_prompt']
    target_prompt = DiffEdit_prefs['target_prompt']
    prt(Installing(f'Installing DiffEdit Pipeline{" and Caption Generator" if not bool(source_prompt) else ""}...'))
    clear_pipes('DiffEdit')
    import requests, random
    from io import BytesIO
    from PIL import ImageOps
    from PIL.PngImagePlugin import PngInfo
    if DiffEdit_prefs['init_image'].startswith('http'):
      #response = requests.get(DiffEdit_prefs['init_image'])
      #original_img = PILImage.open(BytesIO(response.content)).convert("RGB")
      original_img = PILImage.open(requests.get(DiffEdit_prefs['init_image'], stream=True).raw)
    else:
      if os.path.isfile(DiffEdit_prefs['init_image']):
        original_img = PILImage.open(DiffEdit_prefs['init_image'])
      else:
        alert_msg(page, f"ERROR: Couldn't find your init_image {DiffEdit_prefs['init_image']}")
        return
    #width, height = original_img.size
    #width, height = scale_dimensions(width, height, DiffEdit_prefs['max_size'])
    original_img = center_crop_resize(original_img)

    @torch.no_grad()
    def generate_caption(images, caption_generator, caption_processor):
        text = "a photograph of"
        inputs = caption_processor(images, text, return_tensors="pt").to(device="cuda", dtype=caption_generator.dtype)
        caption_generator.to("cuda")
        outputs = caption_generator.generate(**inputs, max_new_tokens=128)
        # offload caption generator
        caption_generator.to("cpu")
        caption = caption_processor.batch_decode(outputs, skip_special_tokens=True)[0]
        print(f"Caption: {caption}")
        del inputs
        del caption_generator
        return caption
    if not bool(source_prompt):
        prt("Generating Caption from Image with Blip...")
        from transformers import BlipForConditionalGeneration, BlipProcessor
        captioner_id = "Salesforce/blip-image-captioning-base"
        processor = BlipProcessor.from_pretrained(captioner_id)
        blip_model = BlipForConditionalGeneration.from_pretrained(captioner_id, torch_dtype=torch.float16, low_cpu_mem_usage=True)
        source_prompt = generate_caption(original_img, blip_model, processor)
        del processor
        del blip_model
        clear_last()

    torch.cuda.empty_cache()
    torch.cuda.reset_max_memory_allocated()
    torch.cuda.reset_peak_memory_stats()
    torch_dtype = torch.float16
    model_id = get_model(prefs['model_ckpt'])['path']
    if pipe_DiffEdit is None:
        from diffusers import DDIMScheduler, DDIMInverseScheduler, StableDiffusionDiffEditPipeline
        try:
            pipe_DiffEdit = StableDiffusionDiffEditPipeline.from_pretrained(
                model_id,
                torch_dtype=torch.float16,
                cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,
                safety_checker=None,
            )
            pipe_DiffEdit.scheduler = DDIMScheduler.from_config(pipe_DiffEdit.scheduler.config)
            #pipe_DiffEdit.scheduler = pipeline_scheduler(pipe_DiffEdit)
            pipe_DiffEdit.inverse_scheduler = DDIMInverseScheduler.from_config(pipe_DiffEdit.scheduler.config)
            pipe_DiffEdit.enable_model_cpu_offload()
            pipe_DiffEdit.enable_vae_slicing()
        except Exception as e:
            clear_last()
            alert_msg(page, f"ERROR: Couldn't Initialize DiffEdit Pipeline for some reason.  Possibly out of memory or something wrong with my code...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            return
    clear_last()
    prt("Generating DiffEdit of your Image...")
    prt(progress)
    autoscroll(False)
    batch_output = os.path.join(stable_dir, DiffEdit_prefs['batch_folder_name'])
    if not os.path.isdir(batch_output):
      os.makedirs(batch_output)
    batch_output = os.path.join(prefs['image_output'], DiffEdit_prefs['batch_folder_name'])
    if not os.path.isdir(batch_output):
      os.makedirs(batch_output)
    random_seed = int(DiffEdit_prefs['seed']) if int(DiffEdit_prefs['seed']) > 0 else random.randint(0,4294967295)

    for i in range(DiffEdit_prefs['num_images']):
        generator = torch.manual_seed(random_seed)
        #generator = torch.manual_seed(random_seed)
        try:
            mask_image = pipe_DiffEdit.generate_mask(
                image=original_img,
                source_prompt=source_prompt,
                target_prompt=target_prompt,
                generator=generator,
            )
            inv_latents = pipe_DiffEdit.invert(prompt=source_prompt, image=original_img, generator=generator).latents
        except Exception as e:
            clear_last()
            alert_msg(page, f"ERROR: Couldn't generate mask for DiffEdit your image for some reason.  Possibly out of memory or something wrong with my code...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            return
        try:
            images = pipe_DiffEdit(prompt=target_prompt, mask_image=mask_image, image_latents=inv_latents, negative_prompt=source_prompt, num_inference_steps=DiffEdit_prefs['num_inference_steps'], inpaint_strength=DiffEdit_prefs['strength'], guidance_scale=DiffEdit_prefs['guidance_scale'], generator=generator, callback=callback_fnc, callback_steps=1).images
            #images = pipe_DiffEdit(source_prompt=source_prompt, target_prompt=target_prompt, mask_image=mask_image, image_latents=inv_latents, num_inference_steps=DiffEdit_prefs['num_inference_steps'], inpaint_strength=DiffEdit_prefs['strength'], guidance_scale=DiffEdit_prefs['guidance_scale'], generator=generator, callback=callback_fnc, callback_steps=1).images
        except Exception as e:
            clear_last()
            alert_msg(page, f"ERROR: Couldn't generate DiffEdit your image for some reason.  Possibly out of memory or something wrong with my code...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            return
        clear_last()
        clear_last()
        filename = format_filename(DiffEdit_prefs['target_prompt'])
        #if prefs['file_suffix_seed']: fname += f"-{random_seed}"
        num = 0
        for image in images:
            random_seed += num
            fname = filename + (f"-{random_seed}" if prefs['file_suffix_seed'] else "")
            image_path = available_file(os.path.join(stable_dir, DiffEdit_prefs['batch_folder_name']), fname, i)
            unscaled_path = image_path
            output_file = image_path.rpartition(slash)[2]
            image.save(image_path)
            width, height = image.size
            out_path = image_path.rpartition(slash)[0]
            upscaled_path = os.path.join(out_path, output_file)
            if not DiffEdit_prefs['display_upscaled_image'] or not DiffEdit_prefs['apply_ESRGAN_upscale']:
                prt(Row([ImageButton(src=unscaled_path, data=upscaled_path, width=width, height=height, page=page)], alignment=MainAxisAlignment.CENTER))
                #prt(Row([Img(src=unscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
            if DiffEdit_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
                os.chdir(os.path.join(dist_dir, 'Real-ESRGAN'))
                upload_folder = 'upload'
                result_folder = 'results'
                if os.path.isdir(upload_folder):
                    shutil.rmtree(upload_folder)
                if os.path.isdir(result_folder):
                    shutil.rmtree(result_folder)
                os.mkdir(upload_folder)
                os.mkdir(result_folder)
                short_name = f'{fname[:80]}-{num}.png'
                dst_path = os.path.join(dist_dir, 'Real-ESRGAN', upload_folder, short_name)
                #print(f'Moving {fpath} to {dst_path}')
                #shutil.move(fpath, dst_path)
                shutil.copy(image_path, dst_path)
                #faceenhance = ' --face_enhance' if DiffEdit_prefs["face_enhance"] else ''
                faceenhance = ''
                run_sp(f'python inference_realesrgan.py -n realesr-general-x4v3 -i upload --outscale {DiffEdit_prefs["enlarge_scale"]}{faceenhance}', cwd=os.path.join(dist_dir, 'Real-ESRGAN'), realtime=False)
                out_file = short_name.rpartition('.')[0] + '_out.png'
                shutil.move(os.path.join(dist_dir, 'Real-ESRGAN', result_folder, out_file), upscaled_path)
                image_path = upscaled_path
                os.chdir(stable_dir)
                if DiffEdit_prefs['display_upscaled_image']:
                    time.sleep(0.6)
                    prt(Row([ImageButton(src=upscaled_path, data=upscaled_path, width=width * float(DiffEdit_prefs["enlarge_scale"]), height=height * float(DiffEdit_prefs["enlarge_scale"]), page=page)], alignment=MainAxisAlignment.CENTER))
                    #prt(Row([Img(src=upscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
            if prefs['save_image_metadata']:
                img = PILImage.open(image_path)
                metadata = PngInfo()
                metadata.add_text("artist", prefs['meta_ArtistName'])
                metadata.add_text("copyright", prefs['meta_Copyright'])
                metadata.add_text("software", "Stable Diffusion Deluxe" + f", upscaled {DiffEdit_prefs['enlarge_scale']}x with ESRGAN" if DiffEdit_prefs['apply_ESRGAN_upscale'] else "")
                metadata.add_text("pipeline", "DiffEdit")
                if prefs['save_config_in_metadata']:
                    config_json = DiffEdit_prefs.copy()
                    config_json['model_path'] = model_id
                    config_json['seed'] = random_seed
                    del config_json['num_images']
                    del config_json['max_size']
                    del config_json['display_upscaled_image']
                    del config_json['batch_folder_name']
                    if not config_json['apply_ESRGAN_upscale']:
                        del config_json['enlarge_scale']
                        del config_json['apply_ESRGAN_upscale']
                metadata.add_text("config_json", json.dumps(config_json, ensure_ascii=True, indent=4))
                img.save(image_path, pnginfo=metadata)
            #TODO: PyDrive
            if storage_type == "Colab Google Drive":
                new_file = available_file(os.path.join(prefs['image_output'], DiffEdit_prefs['batch_folder_name']), fname, num)
                out_path = new_file
                shutil.copy(image_path, new_file)
            elif bool(prefs['image_output']):
                new_file = available_file(os.path.join(prefs['image_output'], DiffEdit_prefs['batch_folder_name']), fname, num)
                out_path = new_file
                shutil.copy(image_path, new_file)
            time.sleep(0.2)
            prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))
            num += 1
        random_seed += 1
    autoscroll(False)
    if prefs['enable_sounds']: page.snd_alert.play()

def run_CLIPstyler(page):
    def prt(line):
      if type(line) == str:
        line = Text(line)
      page.CLIPstyler.controls.append(line)
      page.CLIPstyler.update()
    def clear_last():
      del page.CLIPstyler.controls[-1]
      page.CLIPstyler.update()
    def autoscroll(scroll=True):
      page.CLIPstyler.auto_scroll = scroll
      page.CLIPstyler.update()
    def clear_list():
      page.CLIPstyler.controls = page.CLIPstyler.controls[:1]
    clipstyler_dir = os.path.join(root_dir, "CLIPstyler")
    if not os.path.exists(clipstyler_dir):
          os.mkdir(clipstyler_dir)
    if CLIPstyler_prefs['original_image'].startswith('http'):
        import requests
        from io import BytesIO
        response = requests.get(CLIPstyler_prefs['original_image'])
        fpath = os.path.join(clipstyler_dir, CLIPstyler_prefs['original_image'].rpartition(slash)[2])
        original_img = PILImage.open(BytesIO(response.content)).convert("RGB")
        #width, height = original_img.size
        #width, height = scale_dimensions(width, height)
        original_img = original_img.resize((CLIPstyler_prefs['width'], CLIPstyler_prefs['height']), resample=PILImage.Resampling.LANCZOS).convert("RGB")
        original_img.save(fpath)
        CLIPstyler_prefs['image_dir'] = fpath
    elif os.path.isfile(CLIPstyler_prefs['original_image']):
        fpath = os.path.join(clipstyler_dir, CLIPstyler_prefs['original_image'].rpartition(slash)[2])
        original_img = PILImage.open(CLIPstyler_prefs['original_image'])
        #width, height = original_img.size
        #width, height = scale_dimensions(width, height)
        original_img = original_img.resize((CLIPstyler_prefs['width'], CLIPstyler_prefs['height']), resample=PILImage.Resampling.LANCZOS).convert("RGB")
        original_img.save(fpath)
        CLIPstyler_prefs['image_dir'] = fpath
    else:
        alert_msg(page, "Couldn't find a valid File, Path or URL...")
        return
    progress = ProgressBar(bar_height=8)
    autoscroll(True)
    clear_list()
    prt(Installing("Downloading CLIP-Styler Packages..."))
    run_process("pip install ftfy regex tqdm", realtime=False, page=page)
    run_sp("pip install git+https://github.com/openai/CLIP.git", realtime=False)
    #os.chdir(clipstyler_dir)
    os.chdir(root_dir)
    run_sp("pip install git+https://github.com/cyclomon/CLIPstyler.git", realtime=True)
    #!git clone https://github.com/cyclomon/CLIPstyler/
    run_sp(f"git clone https://github.com/cyclomon/CLIPstyler/ {clipstyler_dir}", realtime=True)
    os.chdir(root_dir)
    #run_process(f"git clone https://github.com/paper11667/CLIPstyler/ {clipstyler_dir}", realtime=False, page=page)
    sys.path.append(clipstyler_dir)

    import torch.nn
    import torch.optim as optim
    from torchvision import transforms, models
    import StyleNet
    import utils
    import clip
    import torch.nn.functional as F
    from template import imagenet_templates
    from torchvision import utils as vutils
    import argparse
    from torchvision.transforms.functional import adjust_contrast
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    VGG = models.vgg19(pretrained=True).features
    VGG.to(device)
    save_dir = stable_dir
    if bool(CLIPstyler_prefs['batch_folder_name']):
        save_dir = os.path.join(stable_dir, CLIPstyler_prefs['batch_folder_name'])
    new_file = format_filename(CLIPstyler_prefs["prompt_text"])
    images = []
    for parameter in VGG.parameters():
        parameter.requires_grad_(False)

    def img_denormalize(image):
        mean=torch.tensor([0.485, 0.456, 0.406]).to(device)
        std=torch.tensor([0.229, 0.224, 0.225]).to(device)
        mean = mean.view(1,-1,1,1)
        std = std.view(1,-1,1,1)
        image = image*std +mean
        return image

    def img_normalize(image):
        mean=torch.tensor([0.485, 0.456, 0.406]).to(device)
        std=torch.tensor([0.229, 0.224, 0.225]).to(device)
        mean = mean.view(1,-1,1,1)
        std = std.view(1,-1,1,1)
        image = (image-mean)/std
        return image

    def clip_normalize(image,device):
        image = F.interpolate(image,size=224,mode='bicubic')
        mean=torch.tensor([0.48145466, 0.4578275, 0.40821073]).to(device)
        std=torch.tensor([0.26862954, 0.26130258, 0.27577711]).to(device)
        mean = mean.view(1,-1,1,1)
        std = std.view(1,-1,1,1)
        image = (image-mean)/std
        return image

    def get_image_prior_losses(inputs_jit):
        diff1 = inputs_jit[:, :, :, :-1] - inputs_jit[:, :, :, 1:]
        diff2 = inputs_jit[:, :, :-1, :] - inputs_jit[:, :, 1:, :]
        diff3 = inputs_jit[:, :, 1:, :-1] - inputs_jit[:, :, :-1, 1:]
        diff4 = inputs_jit[:, :, :-1, :-1] - inputs_jit[:, :, 1:, 1:]
        loss_var_l2 = torch.norm(diff1) + torch.norm(diff2) + torch.norm(diff3) + torch.norm(diff4)
        return loss_var_l2

    from argparse import Namespace
    source = CLIPstyler_prefs['source']

    training_args = {
        "lambda_tv": 2e-3,
        "lambda_patch": 9000,
        "lambda_dir": 500,
        "lambda_c": 150,
        "crop_size": CLIPstyler_prefs['crop_size'],
        "num_crops":CLIPstyler_prefs['num_crops'],
        "img_height":CLIPstyler_prefs['height'],
        "img_width":CLIPstyler_prefs['width'],
        "max_step":CLIPstyler_prefs['training_iterations'],
        "lr":5e-4,
        "thresh":0.7,
        "content_path":CLIPstyler_prefs['image_dir'],
        "text":CLIPstyler_prefs['prompt_text']
    }

    style_args = Namespace(**training_args)

    def compose_text_with_templates(text: str, templates=imagenet_templates) -> list:
        return [template.format(text) for template in templates]

    content_path = style_args.content_path
    content_image = utils.load_image2(content_path, img_height=style_args.img_height,img_width =style_args.img_width)
    content_image = content_image.to(device)
    content_features = utils.get_features(img_normalize(content_image), VGG)
    target = content_image.clone().requires_grad_(True).to(device)
    style_net = StyleNet.UNet()
    style_net.to(device)

    style_weights = {'conv1_1': 0.1,
                    'conv2_1': 0.2,
                    'conv3_1': 0.4,
                    'conv4_1': 0.8,
                    'conv5_1': 1.6}
    clear_last()
    prt("Generating Stylized Image from your source... Check console output for progress.")
    prt(progress)
    autoscroll(False)

    content_weight = style_args.lambda_c
    show_every = 20
    optimizer = optim.Adam(style_net.parameters(), lr=style_args.lr)
    s_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.5)
    steps = style_args.max_step
    content_loss_epoch = []
    style_loss_epoch = []
    total_loss_epoch = []
    output_image = content_image
    m_cont = torch.mean(content_image,dim=(2,3),keepdim=False).squeeze(0)
    m_cont = [m_cont[0].item(),m_cont[1].item(),m_cont[2].item()]
    cropper = transforms.Compose([transforms.RandomCrop(style_args.crop_size)])
    augment = transforms.Compose([
        transforms.RandomPerspective(fill=0, p=1,distortion_scale=0.5),
        transforms.Resize(224)
    ])

    clip_model, preprocess = clip.load('ViT-B/32', device, jit=False)
    prompt = style_args.text

    with torch.no_grad():
        template_text = compose_text_with_templates(prompt, imagenet_templates)
        tokens = clip.tokenize(template_text).to(device)
        text_features = clip_model.encode_text(tokens).dcrop_sizech()
        text_features = text_features.mean(axis=0, keepdim=True)
        text_features /= text_features.norm(dim=-1, keepdim=True)
        template_source = compose_text_with_templates(source, imagenet_templates)
        tokens_source = clip.tokenize(template_source).to(device)
        text_source = clip_model.encode_text(tokens_source).dcrop_sizech()
        text_source = text_source.mean(axis=0, keepdim=True)
        text_source /= text_source.norm(dim=-1, keepdim=True)
        source_features = clip_model.encode_image(clip_normalize(content_image,device))
        source_features /= (source_features.clone().norm(dim=-1, keepdim=True))


    num_crops = style_args.num_crops
    for epoch in range(0, steps+1):
        s_scheduler.step()
        target = style_net(content_image,use_sigmoid=True).to(device)
        target.requires_grad_(True)
        target_features = utils.get_features(img_normalize(target), VGG)
        content_loss = 0
        content_loss += torch.mean((target_features['conv4_2'] - content_features['conv4_2']) ** 2)
        content_loss += torch.mean((target_features['conv5_2'] - content_features['conv5_2']) ** 2)
        loss_patch=0
        img_proc =[]
        for n in range(num_crops):
            target_crop = cropper(target)
            target_crop = augment(target_crop)
            img_proc.append(target_crop)
        img_proc = torch.cat(img_proc,dim=0)
        img_aug = img_proc
        image_features = clip_model.encode_image(clip_normalize(img_aug,device))
        image_features /= (image_features.clone().norm(dim=-1, keepdim=True))
        img_direction = (image_features-source_features)
        img_direction /= img_direction.clone().norm(dim=-1, keepdim=True)
        text_direction = (text_features-text_source).repeat(image_features.size(0),1)
        text_direction /= text_direction.norm(dim=-1, keepdim=True)
        loss_temp = (1- torch.cosine_similarity(img_direction, text_direction, dim=1))
        loss_temp[loss_temp<style_args.thresh] =0
        loss_patch+=loss_temp.mean()
        glob_features = clip_model.encode_image(clip_normalize(target,device))
        glob_features /= (glob_features.clone().norm(dim=-1, keepdim=True))
        glob_direction = (glob_features-source_features)
        glob_direction /= glob_direction.clone().norm(dim=-1, keepdim=True)
        loss_glob = (1- torch.cosine_similarity(glob_direction, text_direction, dim=1)).mean()
        reg_tv = style_args.lambda_tv*get_image_prior_losses(target)
        total_loss = style_args.lambda_patch*loss_patch + content_weight * content_loss+ reg_tv+ style_args.lambda_dir*loss_glob
        total_loss_epoch.append(total_loss)
        optimizer.zero_grad()
        total_loss.backward()
        optimizer.step()
        autoscroll(True)
        if epoch % show_every == 0:
            prt("After %d iters:" % epoch)
            prt('  Total loss: ', total_loss.item())
            prt('  Content loss: ', content_loss.item())
            prt('  patch loss: ', loss_patch.item())
            prt('  dir loss: ', loss_glob.item())
            prt('  TV loss: ', reg_tv.item())

        if epoch % show_every == 0:
            output_image = target.clone()
            output_image = torch.clamp(output_image,0,1)
            output_image = adjust_contrast(output_image,1.5)
            img = utils.im_convert2(output_image)
            save_file = available_file(save_dir, new_file, 1)
            img.save(save_file)
            prt(Row([ImageButton(src=save_file, width=CLIPstyler_prefs['width'], height=CLIPstyler_prefs['height'], subtitle=save_file, show_subtitle=True, page=page)], alignment=MainAxisAlignment.CENTER))
            #prt(Row([Img(src=save_file, width=CLIPstyler_prefs['width'], height=CLIPstyler_prefs['height'], fit=ImageFit.FILL, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
            #prt(Row([Text(save_file)], alignment=MainAxisAlignment.CENTER))
            images.append(save_file)
            #plt.imshow(utils.im_convert2(output_image))
            #plt.show()
        progress.value = (epoch) / steps
        progress.tooltip = f'[{(epoch)} / {steps}]'
        progress.update()
    #clear_last()
    # TODO: ESRGAN and copy to GDrive and Metadata
    autoscroll(False)
    if prefs['enable_sounds']: page.snd_alert.play()

def run_semantic(page):
    global semantic_prefs, prefs, status, pipe_semantic, model_path
    if not status['installed_diffusers']:
      alert_msg(page, "You need to Install HuggingFace Diffusers before using...")
      return
    def prt(line):
      if type(line) == str:
        line = Text(line, size=17)
      page.SemanticGuidance.controls.append(line)
      page.SemanticGuidance.update()
    def clear_last():
      del page.SemanticGuidance.controls[-1]
      page.SemanticGuidance.update()
    def autoscroll(scroll=True):
      page.SemanticGuidance.auto_scroll = scroll
      page.SemanticGuidance.update()
    def clear_list():
      page.SemanticGuidance.controls = page.SemanticGuidance.controls[:1]
    progress = ProgressBar(bar_height=8)
    total_steps = semantic_prefs['num_inference_steps']
    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep}"
      progress.update()
      #print(f'{type(latents)} {len(latents)}- {str(latents)}')
    clear_list()
    autoscroll(True)
    prt(Installing("Installing Semantic Guidance Pipeline..."))

    clear_pipes('semantic')
    if pipe_semantic is None:
        from diffusers import SemanticStableDiffusionPipeline
        pipe_semantic = SemanticStableDiffusionPipeline.from_pretrained(model_path, torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, **safety)
        pipe_semantic = pipeline_scheduler(pipe_semantic)
        pipe_semantic = optimize_pipe(pipe_semantic, vae=False)
        pipe_semantic.set_progress_bar_config(disable=True)
    else:
        pipe_semantic = pipeline_scheduler(pipe_semantic)
    clear_last()
    prt("Generating Semantic Guidance of your Image...")
    prt(progress)
    autoscroll(False)
    batch_output = os.path.join(stable_dir, semantic_prefs['batch_folder_name'])
    if not os.path.isdir(batch_output):
      os.makedirs(batch_output)
    batch_output = os.path.join(prefs['image_output'], semantic_prefs['batch_folder_name'])
    if not os.path.isdir(batch_output):
      os.makedirs(batch_output)
    random_seed = int(semantic_prefs['seed']) if int(semantic_prefs['seed']) > 0 else rnd.randint(0,4294967295)
    generator = torch.Generator(device=torch_device).manual_seed(random_seed)
    #generator = torch.manual_seed(random_seed)
    width = semantic_prefs['width']
    height = semantic_prefs['height']
    editing_prompt = []
    edit_warmup_steps = []
    edit_guidance_scale = []
    edit_threshold = []
    edit_weights = []
    reverse_editing_direction = []
    for ep in semantic_prefs['editing_prompts']:
        editing_prompt.append(ep['editing_prompt'])
        edit_warmup_steps.append(int(ep['edit_warmup_steps']))
        edit_guidance_scale.append(ep['edit_guidance_scale'])
        edit_threshold.append(ep['edit_threshold'])
        edit_weights.append(ep['edit_weights'])
        reverse_editing_direction.append(ep['reverse_editing_direction'])
    try:
      #print(f"prompt={semantic_prefs['prompt']}, negative_prompt={semantic_prefs['negative_prompt']}, editing_prompt={editing_prompt}, edit_warmup_steps={edit_warmup_steps}, edit_guidance_scale={edit_guidance_scale}, edit_threshold={edit_threshold}, edit_weights={edit_weights}, reverse_editing_direction={reverse_editing_direction}, edit_momentum_scale={semantic_prefs['edit_momentum_scale']}, edit_mom_beta={semantic_prefs['edit_mom_beta']}, num_inference_steps={semantic_prefs['num_inference_steps']}, eta={semantic_prefs['eta']}, guidance_scale={semantic_prefs['guidance_scale']}")
      images = pipe_semantic(prompt=semantic_prefs['prompt'], negative_prompt=semantic_prefs['negative_prompt'], editing_prompt=editing_prompt, edit_warmup_steps=edit_warmup_steps, edit_guidance_scale=edit_guidance_scale, edit_threshold=edit_threshold, edit_weights=edit_weights, reverse_editing_direction=reverse_editing_direction, edit_momentum_scale=semantic_prefs['edit_momentum_scale'], edit_mom_beta=semantic_prefs['edit_mom_beta'], num_inference_steps=semantic_prefs['num_inference_steps'], eta=semantic_prefs['eta'], guidance_scale=semantic_prefs['guidance_scale'], width=width, height=height, num_images_per_prompt=semantic_prefs['num_images'], generator=generator, callback=callback_fnc, callback_steps=1).images
    except Exception as e:
      clear_last()
      alert_msg(page, f"ERROR: Couldn't Semantic Guidance your image for some reason.  Possibly out of memory or something wrong with my code...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
      return
    clear_last()
    clear_last()
    filename = f"{prefs['file_prefix']}{format_filename(semantic_prefs['prompt'])}"
    filename = filename[:int(prefs['file_max_length'])]
    #if prefs['file_suffix_seed']: filename += f"-{random_seed}"
    autoscroll(True)
    num = 0
    for image in images:
        random_seed += num
        fname = filename + (f"-{random_seed}" if prefs['file_suffix_seed'] else "")
        image_path = available_file(os.path.join(stable_dir, semantic_prefs['batch_folder_name']), fname, num)
        unscaled_path = image_path
        output_file = image_path.rpartition(slash)[2]
        image.save(image_path)
        out_path = image_path.rpartition(slash)[0]
        upscaled_path = os.path.join(out_path, output_file)
        if not semantic_prefs['display_upscaled_image'] or not semantic_prefs['apply_ESRGAN_upscale']:
            prt(Row([ImageButton(src=unscaled_path, data=upscaled_path, width=width, height=height, page=page)], alignment=MainAxisAlignment.CENTER))
        if semantic_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
            os.chdir(os.path.join(dist_dir, 'Real-ESRGAN'))
            upload_folder = 'upload'
            result_folder = 'results'
            if os.path.isdir(upload_folder):
                shutil.rmtree(upload_folder)
            if os.path.isdir(result_folder):
                shutil.rmtree(result_folder)
            os.mkdir(upload_folder)
            os.mkdir(result_folder)
            short_name = f'{fname[:80]}-{num}.png'
            dst_path = os.path.join(dist_dir, 'Real-ESRGAN', upload_folder, short_name)
            #print(f'Moving {fpath} to {dst_path}')
            #shutil.move(fpath, dst_path)
            shutil.copy(image_path, dst_path)
            #faceenhance = ' --face_enhance' if semantic_prefs["face_enhance"] else ''
            faceenhance = ''
            run_sp(f'python inference_realesrgan.py -n realesr-general-x4v3 -i upload --outscale {semantic_prefs["enlarge_scale"]}{faceenhance}', cwd=os.path.join(dist_dir, 'Real-ESRGAN'), realtime=False)
            out_file = short_name.rpartition('.')[0] + '_out.png'
            shutil.move(os.path.join(dist_dir, 'Real-ESRGAN', result_folder, out_file), upscaled_path)
            image_path = upscaled_path
            os.chdir(stable_dir)
            if semantic_prefs['display_upscaled_image']:
                time.sleep(0.6)
                prt(Row([ImageButton(src=upscaled_path, data=upscaled_path, width=width * float(semantic_prefs["enlarge_scale"]), height=height * float(semantic_prefs["enlarge_scale"]), page=page)], alignment=MainAxisAlignment.CENTER))
                #prt(Row([Img(src=upscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
        if prefs['save_image_metadata']:
            img = PILImage.open(image_path)
            metadata = PngInfo()
            metadata.add_text("artist", prefs['meta_ArtistName'])
            metadata.add_text("copyright", prefs['meta_Copyright'])
            metadata.add_text("software", "Stable Diffusion Deluxe" + f", upscaled {semantic_prefs['enlarge_scale']}x with ESRGAN" if semantic_prefs['apply_ESRGAN_upscale'] else "")
            metadata.add_text("pipeline", "Semantic Guidance")
            if prefs['save_config_in_metadata']:
              config_json = semantic_prefs.copy()
              config_json['model_path'] = model_path
              config_json['scheduler_mode'] = prefs['scheduler_mode']
              config_json['seed'] = random_seed
              del config_json['num_images']
              del config_json['width']
              del config_json['height']
              del config_json['display_upscaled_image']
              del config_json['batch_folder_name']
              if not config_json['apply_ESRGAN_upscale']:
                del config_json['enlarge_scale']
                del config_json['apply_ESRGAN_upscale']
              metadata.add_text("config_json", json.dumps(config_json, ensure_ascii=True, indent=4))
            img.save(image_path, pnginfo=metadata)
        #TODO: PyDrive
        if storage_type == "Colab Google Drive":
            new_file = available_file(os.path.join(prefs['image_output'], semantic_prefs['batch_folder_name']), fname, num)
            out_path = new_file
            shutil.copy(image_path, new_file)
        elif bool(prefs['image_output']):
            new_file = available_file(os.path.join(prefs['image_output'], semantic_prefs['batch_folder_name']), fname, num)
            out_path = new_file
            shutil.copy(image_path, new_file)
        prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))
        num += 1
    autoscroll(False)
    if prefs['enable_sounds']: page.snd_alert.play()

def run_image2text(page):
    def prt(line):
      if type(line) == str:
        line = Text(line)
      page.image2text_output.controls.append(line)
      page.image2text_output.update()
    def clear_last():
      del page.image2text_output.controls[-1]
      page.image2text_output.update()
    if image2text_prefs['use_AIHorde'] and not bool(prefs['AIHorde_api_key']):
      alert_msg(page, "To use AIHorde API service, you must provide an API key in settings")
      return
    progress = ProgressBar(bar_height=8)
    #if not status['installed_diffusers']:
    #  alert_msg(page, "You must Install the HuggingFace Diffusers Library first... ")
    #  return
    if len(image2text_prefs['images']) < 1:
      alert_msg(page, "You must add one or more files to interrogate first... ")
      return
    if not image2text_prefs['use_AIHorde']:
        prt(Installing("Downloading Image2Text CLIP-Interrogator Blips..."))
        try:
            if transformers.__version__ != "4.21.3": # Diffusers conflict
              run_process("pip uninstall -y transformers", realtime=False)
        except Exception:
            pass
        run_process("pip install ftfy regex tqdm timm fairscale requests", realtime=False)
        #run_sp("pip install --upgrade transformers==4.21.2", realtime=False)
        run_process("pip install -q transformers==4.21.3 --upgrade --force-reinstall", realtime=False)
        run_process("pip install -e git+https://github.com/openai/CLIP.git@main#egg=clip", realtime=False)
        run_process("pip install -e git+https://github.com/pharmapsychotic/BLIP.git@lib#egg=blip", realtime=False)
        run_process("pip clone https://github.com/pharmapsychotic/clip-interrogator.git", realtime=False)
            #['pip', 'install', 'ftfy', 'gradio', 'regex', 'tqdm', 'transformers==4.21.2', 'timm', 'fairscale', 'requests'],
        #    pass
        # Have to force downgrade of transformers because error with cache_dir, but should upgrade after run
        run_process("pip install clip-interrogator", realtime=False)

        '''def setup():
            install_cmds = [
                ['pip', 'install', 'ftfy', 'gradio', 'regex', 'tqdm', 'transformers==4.21.2', 'timm', 'fairscale', 'requests'],
                ['pip', 'install', 'git+https://github.com/openai/CLIP.git@main#egg=clip'],
                ['pip', 'install', 'git+https://github.com/pharmapsychotic/BLIP.git@lib#egg=blip'],
                ['git', 'clone', 'https://github.com/pharmapsychotic/clip-interrogator.git'],
                ['pip', 'install', 'clip-interrogator'],
            ]
            for cmd in install_cmds:
                print(subprocess.run(cmd, stdout=subprocess.PIPE).stdout.decode('utf-8'))
        setup()'''
        #run_sp("pip install git+https://github.com/openai/CLIP.git", realtime=False)
        import argparse, sys, time
        sys.path.append('src/blip')
        sys.path.append('src/clip')
        sys.path.append('clip-interrogator')
        import clip
        import torch
        from clip_interrogator import Interrogator, Config
        clear_last()
        prt("Interrogating Images to Describe Prompt... Check console output for progress.")
        prt(progress)
        try:
            ci = Interrogator(Config())
        except Exception as e:
            clear_last()
            alert_msg(page, "ERROR: Problem running Interrogator. Try running before installing Diffusers...", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip(), selectable=True)]))
            return
        def inference(image, mode):
            nonlocal ci
            image = image.convert('RGB')
            if mode == 'best':
                return ci.interrogate(image)
            elif mode == 'classic':
                return ci.interrogate_classic(image)
            else:
                return ci.interrogate_fast(image)
        folder_path = image2text_prefs['folder_path']
        mode = image2text_prefs['mode'] #'best' #param ["best","classic", "fast"]
        #files = [f for f in os.listdir(folder_path) if f.endswith('.jpg') or  f.endswith('.png')] if os.path.exists(folder_path) else []
        files = image2text_prefs['images']
        clear_last()
        i2t_prompts = []
        for file in files:
            image = PILImage.open(os.path.join(folder_path, file)).convert('RGB')
            prompt = inference(image, mode)
            i2t_prompts.append(prompt)
            page.add_to_image2text(prompt)
            #thumb = image.copy()
            #thumb.thumbnail([256, 256])
            #display(thumb)
            #print(prompt)
        if image2text_prefs['save_csv']:
            if len(i2t_prompts):
                import csv
                csv_path = os.path.join(folder_path, 'img2txt_prompts.csv')
                with open(csv_path, 'w', encoding='utf-8', newline='') as f:
                    w = csv.writer(f, quoting=csv.QUOTE_MINIMAL)
                    w.writerow(['image', 'prompt'])
                    for file, prompt in zip(files, i2t_prompts):
                        w.writerow([file, prompt])

                prt(f"\n\n\nGenerated {len(i2t_prompts)} and saved to {csv_path}, enjoy!")
            else:
                prt(f"Sorry, we couldn't find any images in {folder_path}")
        run_process("pip uninstall -y git+https://github.com/pharmapsychotic/BLIP.git@lib#egg=blip", realtime=False)
        run_process("pip uninstall -y clip-interrogator", realtime=False)
        run_process("pip uninstall -y transformers", realtime=False)
        run_process("pip install --upgrade transformers", realtime=False)
        clear_last()
    else:
        if not status['installed_AIHorde']:
          prt(Installing("Installing AIHorde API Library..."))
          get_AIHorde(page)
          clear_last()
        import requests
        from io import BytesIO

        api_host = 'https://stablehorde.net/api'
        api_check_url = f"{api_host}/v2/generate/check/"
        api_get_result_url = f"{api_host}/v2/interrogate/status/"
        interrogate_url = f"{api_host}/v2/interrogate/async"
        mode = image2text_prefs['mode'] #'best' #param ["best","classic", "fast"]
        headers = {
            #'Content-Type': 'application/json',
            #'Accept': 'application/json',
            'apikey': prefs['AIHorde_api_key'],
        }
        payload = {
            "forms": [{"name": "caption"}],
            "slow_workers": False if mode == "fast" else True,
        }
        def get_captions(filename):
          with open(filename, 'r') as f:
            lines = f.readlines()
          captions = []
          for line in lines:
            if 'caption result:' in line:
              captions.append(line.split('caption result: ')[1])
          return captions
        def interrogate_line(j, cat):
          l = []
          for t in j[cat]:
            l.append(f"{t['text']}:{round(t['confidence'], 1)}")
          return f"**{cat.capitalize()}:** {', '.join(l)}\n  \n  "
        def get_interrogation(j):
          attr = ""
          for t in j.keys():
            attr += interrogate_line(j, t)
          return attr
        def get_interrogation_cat(j, cat):
          l = []
          for t in j[cat]:
            if t['confidence'] > 3:
              l.append(t['text'])
          if cat == "artist" or cat == "artists":
            return and_list(l)
          else:
            return ', '.join(l)
        def get_interrogation_prompt(j):
          attrs = {}
          for t in j.keys():
            attrs[t] = get_interrogation_cat(j, t)
          full = f", by {attrs['artist' if 'artist' in attrs else 'artists']}, as {attrs['medium' if 'medium' in attrs else 'mediums']}, style of {attrs['flavors' if 'flavors' in attrs else 'flavor']}, {attrs['tags' if 'tags' in attrs else 'tag']}, {attrs['movement' if 'movement' in attrs else 'movements']}, technique of {attrs['techniques' if 'techniques' in attrs else 'technique']}, trending on {attrs['trending' if 'trending' in attrs else 'sites']}"
          return full
        import yaml
        AI_Horde = os.path.join(dist_dir, "AI-Horde-CLI")
        cli_response = os.path.join(AI_Horde, "cliRequests.log")
        alchemy_yml = os.path.join(AI_Horde, "cliRequestsData_Alchemy.yml")
        interrogation_txt = os.path.join(AI_Horde, "cliRequestsData_Alchemy.yml_interrogation.txt")
        def make_yml(request):
            alchemy = f'''
filename: "horde_alchemy"
api_key: "{prefs["AIHorde_api_key"]}"
submit_dict:
    trusted_workers: {image2text_prefs["trusted_workers"]}
    slow_workers: {image2text_prefs["slow_workers"]}
    forms:
        - name: "{request}"'''

            al = yaml.safe_load(alchemy)

            with open(alchemy_yml, 'w') as file:
                yaml.dump(al, file)
        if image2text_prefs["request_mode"] == "Caption" or image2text_prefs["request_mode"] == "Interrogation":
            make_yml(image2text_prefs["request_mode"].lower())
        elif image2text_prefs["request_mode"] == "Full Prompt":
            make_yml("caption")
        #print(open('names.yaml').read())
        folder_path = image2text_prefs['folder_path']
        #files = [f for f in image2text_prefs['images'] if f.endswith('.jpg') or  f.endswith('.png')] else []
        #files = [f for f in os.listdir(folder_path) if f.endswith('.jpg') or  f.endswith('.png')] if os.path.exists(folder_path) else []
        #print(f"Files: {len(files)}")
        i2t_prompts = []
        for file in image2text_prefs['images']:
            prt(f"Getting {image2text_prefs['request_mode']} with the Horde to Describe Image...")
            prt(progress)
            stats = Text("Stable Horde API Interrogation ")
            #prt(stats)
            img_file = os.path.join(folder_path, file)
            run_process(f'python cli_request_alchemy.py --api_key {prefs["AIHorde_api_key"]} --file "{alchemy_yml}" --source_image "{img_file}"', cwd=AI_Horde, realtime=True)
            clear_last()
            clear_last()
            if image2text_prefs["request_mode"] == "Caption":
                captions = get_captions(cli_response)
                for r in captions:
                  prompt = to_title(r.strip(), sentence=True)
                  i2t_prompts.append(prompt)
                  page.add_to_image2text(prompt)
                new_log = available_file(AI_Horde, "cliRequests", 0, ext="log")
                shutil.move(cli_response, new_log)
            elif image2text_prefs["request_mode"] == "Interrogation":
                with open(interrogation_txt) as json_file:
                  interrogation = json.load(json_file)
                prt(Markdown(get_interrogation(interrogation), selectable=True))
                new_interrogation = available_file(AI_Horde, "cliRequestsData_Alchemy.yml_interrogation", 0, ext="txt")
                shutil.move(interrogation_txt, new_interrogation)
                os.remove(cli_response)
            elif image2text_prefs["request_mode"] == "Full Prompt":
                captions = get_captions(cli_response)
                make_yml("interrogation")
                prt(f"Getting Interrogation with the Horde to Describe Image...")
                prt(progress)
                run_process(f'python cli_request_alchemy.py --api_key {prefs["AIHorde_api_key"]} --file "{alchemy_yml}" --source_image "{img_file}"', cwd=AI_Horde, realtime=True)
                clear_last()
                clear_last()
                with open(interrogation_txt) as json_file:
                  interrogation = json.load(json_file)
                prt(Markdown(get_interrogation(interrogation), selectable=True))
                new_interrogation = available_file(AI_Horde, "cliRequestsData_Alchemy.yml_interrogation", 0, ext="txt")
                shutil.move(interrogation_txt, new_interrogation)
                styles = get_interrogation_prompt(interrogation)
                for r in captions:
                  prompt = to_title(r.strip(), sentence=True)
                  i2t_prompts.append(f"{prompt}{styles}")
                  page.add_to_image2text(f"{prompt}{styles}")
                new_log = available_file(AI_Horde, "cliRequests", 0, ext="log")
                shutil.move(cli_response, new_log)
            ''' API Method, want to go back to if I can upload source_image
            image = PILImage.open(os.path.join(folder_path, file)).convert('RGB')
            buff = io.BytesIO()
            image.save(buff, format="PNG")
            buff.seek(0)
            img_str = io.BufferedReader(buff).read()
            payload['source_image'] = img_str.decode()

            response = requests.post(interrogate_url, headers=headers, json=json.dumps(payload))
            if response != None:
              if response.status_code != 202:
                if response.status_code == 400:
                  alert_msg(page, "Stable Horde-API ERROR: Validation Error...", content=Text(str(response.text)))
                  return
                else:
                  prt(Text(f"Stable Horde-API ERROR {response.status_code}: " + str(response.text), selectable=True))
                  print(payload)
                  continue
            artifacts = json.loads(response.content)
            q_id = artifacts['id']
            print(f"ID: {q_id}")
            elapsed_seconds = 0
            try:
              while True:
                check_response = requests.get(api_check_url + q_id)
                check = json.loads(check_response.content)
                div = check['wait_time'] + elapsed_seconds
                if div == 0: continue
                try:
                  percentage = (1 - check['wait_time'] / div)
                except Exception:
                  continue
                pb.value = percentage
                pb.update()
                status_txt = f"Stable Horde API Interrogation - Queued Position: {check['queue_position']} - Waiting: {check['waiting']} - Wait Time: {check['wait_time']}"
                stats.value = status_txt #str(check)
                stats.update()
                if bool(check['done']):
                  break
                time.sleep(1)
                elapsed_seconds += 1
            except Exception as e:
              alert_msg(page, f"EXCEPTION ERROR: Unknown error processing image. Check parameters and try again. Restart app if persists.", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip(), selectable=True)]))
              return
            get_response = requests.get(api_get_result_url + q_id)
            final_results = json.loads(get_response.content)
            clear_last()
            clear_last()
            prt(str(final_results))
            for r in final_results['forms']:
              prompt = r['result']['*']
              i2t_prompts.append(prompt)
              page.add_to_image2text(prompt)'''
    if prefs['enable_sounds']: page.snd_alert.play()

def run_BLIP2_image2text(page):
    def prt(line):
      if type(line) == str:
        line = Text(line)
      page.BLIP2_image2text_output.controls.append(line)
      page.BLIP2_image2text_output.update()
    def clear_last():
      if len(page.BLIP2_image2text_output.controls) < 1: return
      del page.BLIP2_image2text_output.controls[-1]
      page.BLIP2_image2text_output.update()
    progress = ProgressBar(bar_height=8)
    #if not status['installed_diffusers']:
    #  alert_msg(page, "You must Install the HuggingFace Diffusers Library first... ")
    #  return
    prt(Installing("Installing BLIP2 Image2Text from Salesforce LAVIS..."))

    try:
        import lavis
        #from lavis.models import load_model_and_preprocess
    except ModuleNotFoundError:
        try:
            #run_process("pip install clip-salesforce-lavis", page=page, show=True)
            run_sp("pip install -q salesforce-lavis", realtime=True)
        except Exception as e:
            clear_last()
            alert_msg(page, "ERROR Installing salesforce-lavis. Try running before installing Diffusers...", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip(), selectable=True)]))
            return
        pass
    finally:
        from lavis.models import load_model_and_preprocess
    import requests
    device = torch.device(torch_device)
    clear_last()
    prt(Installing("Downloading BLIP2 Image2Text from Salesforce LAVIS... It's huge, see console for progress."))
    model_name = "blip2_t5"
    if '.' in BLIP2_image2text_prefs['model_type']:
      model_name = "blip2_opt"
    if BLIP2_image2text_prefs['model_type'] == "base":
      model_name = "img2prompt_vqa"
    try:
        model, vis_processors, _ = load_model_and_preprocess(
            name=model_name, model_type=BLIP2_image2text_prefs['model_type'], is_eval=True, device=device)
    except Exception as e:
        clear_last()
        alert_msg(page, "ERROR: Problem running Interrogator. Try running before installing Diffusers...", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip(), selectable=True)]))
        return
    vis_keys = vis_processors.keys()
    print(str(vis_keys))
    prt("  Examining Images to Describe Prompt... Check console output for progress, first run is slow.")
    prt(progress)
    folder_path = BLIP2_image2text_prefs['folder_path']
    files = [f for f in os.listdir(folder_path) if f.endswith('.jpg') or  f.endswith('.png')] if os.path.exists(folder_path) else []
    clear_last()
    BLIP2_i2t_prompts = []
    try:
        for file in files:
            img = PILImage.open(os.path.join(folder_path, file)).convert('RGB')
            image = vis_processors["eval"](img).unsqueeze(0).to(device)
            if BLIP2_image2text_prefs['num_captions'] == 1:
                prompt = model.generate({"image": image})
            else:
                prompt = model.generate({"image": image}, use_nucleus_sampling=True, num_captions=BLIP2_image2text_prefs['num_captions'])
            print(prompt)
            if isinstance(prompt, str):
                BLIP2_i2t_prompts.append(prompt)
                page.add_to_BLIP2_image2text(prompt)
            else:
                for p in prompt:
                    BLIP2_i2t_prompts.append(p)
                    page.add_to_BLIP2_image2text(p)
            if bool(BLIP2_image2text_prefs['question_prompt']):
                question = f"Question: {BLIP2_image2text_prefs['question_prompt']} Answer:"
                answer = model.generate({"image": image, "prompt": question})
                a = answer.rpartition(':')[2].strip()
                prt(f"Answer: {a}")
    except Exception as e:
        clear_last()
        alert_msg(page, "ERROR: Problem Generating from Model. Probably out of memory...", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip(), selectable=True)]))
        return
    clear_last()
    if prefs['enable_sounds']: page.snd_alert.play()

def run_dance_diffusion(page):
    if not status['installed_diffusers']:
      alert_msg(page, "You must Install the HuggingFace Diffusers Library first... ")
      return
    global dance_pipe, dance_prefs
    if dance_prefs['dance_model'] == 'Community' or dance_prefs['dance_model'] == 'Custom':
      alert_msg(page, "Custom Community Checkpoints are not functional yet, working on it so check back later... ")
      return
    from diffusers import DanceDiffusionPipeline
    import scipy.io.wavfile, random
    try:
      import gdown
    except ImportError:
      run_sp("pip -q install gdown")
    finally:
      import gdown
    #import sys
    #sys.path.append('drive/gdrive/MyDrive/NotebookDatasets/CMVRLG')
    #print(dir(os))
    #print(dir(os.path))
    def prt(line):
      if type(line) == str:
        line = Text(line)
      page.dance_output.controls.append(line)
      page.dance_output.update()
    def clear_last():
      del page.dance_output.controls[-1]
      page.dance_output.update()
    def play_audio(e):
      e.control.data.play()
    progress = ProgressBar(bar_height=8)
    prt(Installing("Downloading Dance Diffusion Models"))
    diffusers_dir = os.path.join(root_dir, "diffusers")
    if not os.path.exists(diffusers_dir):
      run_process("git clone https://github.com/Skquark/diffusers.git", realtime=False, cwd=root_dir)
    dance_model_file = f"harmonai/{dance_prefs['dance_model']}"
    if dance_prefs['dance_model'] == 'Community':
      models_path = os.path.join(root_dir, 'dancediffusion_models')
      os.makedirs(models_path, exist_ok=True)
      for c in community_dance_diffusion_models:
        if c['name'] == dance_prefs['community_model']:
          community = c
      model_out = os.path.join(models_path, format_filename(community['name'], use_dash=True))
      if not os.path.exists(model_out):
        prt("Converting Checkpoint to Diffusers...")
        os.makedirs(model_out, exist_ok=True)
        if bool(community['download']):
          dance_model_file = os.path.join(models_path, community['ckpt'])
          if community['download'].startswith('https://drive'):
            gdown.download(community['download'], dance_model_file, quiet=True)
          elif community['download'].startswith('http'):
            local = download_file(community['download'])
            print(f"Download {community['download']} local:{local}")
            shutil.move(local, os.path.join(models_path, community['ckpt']))
          else:
            dance_model_file = community['download']
          sample_generator = os.path.join(root_dir, 'sample-generator')
          if not os.path.exists(sample_generator):
            run_process("git clone https://github.com/harmonai-org/sample-generator", page=page, cwd=root_dir)
          import sys
          sys.path.insert(1, os.path.join(sample_generator, 'audio_diffusion'))
          run_sp(f"pip install {sample_generator}", cwd=sample_generator, realtime=False)
          v_diffusion_pytorch = os.path.join(root_dir, 'v-diffusion-pytorch')
          if not os.path.exists(v_diffusion_pytorch):
            run_sp("git clone --recursive https://github.com/crowsonkb/v-diffusion-pytorch", cwd=root_dir, realtime=False)
          run_sp(f"pip install {v_diffusion_pytorch}", cwd=v_diffusion_pytorch, realtime=False)
          run_sp(f"python3 {os.path.join(diffusers_dir, 'scripts', 'convert_dance_diffusion_to_diffusers.py')} --model_path {dance_model_file} --checkpoint_path {model_out}", cwd=os.path.join(sample_generator, 'audio_diffusion'))#os.path.join(diffusers_dir, 'scripts'))
          clear_last()
          dance_model_file = model_out
          #run_sp(f'gdown {community['download']} {dance_model_file}')
          #run_sp(f"wget {community['download']} -O {models_path}")
      else:
        dance_model_file = model_out
    if dance_prefs['dance_model'] == 'Custom':
      models_path = os.path.join(root_dir, 'dancediffusion_models')
      os.makedirs(models_path, exist_ok=True)
      if bool(dance_prefs['custom_model']):
        if dance_prefs['custom_model'].startswith('https://drive'):
          dance_model_file = os.path.join(models_path, "custom_dance.ckpt")
          gdown.download(dance_prefs['custom_model'], dance_model_file, quiet=True)
        elif dance_prefs['custom_model'].startswith('http'):
          fname = dance_prefs['custom_model'].rpartition('/')[2]
          local = download_file(dance_prefs['custom_model'])
          dance_model_file = os.path.join(models_path, fname)
          print(f"Download {dance_prefs['custom_model']} local:{local}")
          shutil.move(local, dance_model_file)
        else:
          dance_model_file = dance_prefs['custom_model']
    if dance_prefs['train_custom']:
      dance_audio = os.path.join(root_dir, 'dance-audio')
      sample_generator = os.path.join(root_dir, 'sample-generator')
      if not os.path.exists(sample_generator):
        run_process("git clone https://github.com/harmonai-org/sample-generator", page=page, cwd=root_dir)
      run_process(f"pip install {sample_generator}", page=page, cwd=root_dir, show=True)
      v_diffusion_pytorch = os.path.join(root_dir, 'v-diffusion-pytorch')
      if not os.path.exists(v_diffusion_pytorch):
        run_sp("git clone --recursive https://github.com/crowsonkb/v-diffusion-pytorch", cwd=root_dir, realtime=False)
      run_sp(f"pip install {v_diffusion_pytorch}", cwd=v_diffusion_pytorch, realtime=False)
      run_cmd = "python3 " + os.path.join(sample_generator, 'train_uncond.py')
      custom_name = format_filename(dance_prefs['custom_name'], use_dash=True)
      output_dir = os.path.join(dance_audio, custom_name)
      output_dir = output_dir.replace(f" ", f"\ ")
      random_crop_str = f"--random-crop True" if dance_prefs['random_crop'] else ""
      run_cmd += f''' --ckpt-path {dance_model_file}\
          --name {custom_name}\
          --training-dir {dance_audio}\
          --sample-size {dance_prefs['sammple_size']}\
          --accum-batches {dance_prefs['accumulate_batches']}\
          --sample-rate {dance_prefs['sample_rate']}\
          --batch-size {dance_prefs['finetune_batch_size']}\
          --demo-every {dance_prefs['demo_every']}\
          --checkpoint-every {dance_prefs['checkpoint_every']}\
          --num-workers 2\
          --num-gpus 1\
          {random_crop_str}\
          --save-path {output_dir}'''
      prt(" Training with " + run_cmd)
      prt(progress)
      try:
        run_process(run_cmd, page=page, cwd=sample_generator, show=True)
      except Exception as e:
        clear_last()
        alert_msg(page, f"ERROR: CUDA Out of Memory, or something else. Try changing parameters and try again...", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip())]))
        with torch.no_grad():
          torch.cuda.empty_cache()
        return
      if dance_prefs['save_model']:
        print("Upload to HuggingFace (todo)")
      clear_last()
      clear_last()
      #dance_model_file = os.path.join(output_dir, custom_name + '.ckpt')
    try:
      dance_pipe = DanceDiffusionPipeline.from_pretrained(dance_model_file, torch_dtype=torch.float16, device_map="auto")
      dance_pipe = dance_pipe.to(torch_device)
    except Exception as e:
      clear_last()
      alert_msg(page, f"ERROR: Problem getting DanceDiffusion Pipeline. Try changing parameters and try again...", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip())]))
      return
    dance_pipe.set_progress_bar_config(disable=True)
    random_seed = int(dance_prefs['seed']) if int(dance_prefs['seed']) > 0 else random.randint(0,4294967295)
    dance_generator = torch.Generator(device=torch_device).manual_seed(random_seed)
    clear_last()
    pb.width=(page.width if page.web else page.window_width) - 50
    prt(pb)
    if prefs['higher_vram_mode']:
      output = dance_pipe(generator=dance_generator, batch_size=int(dance_prefs['batch_size']), num_inference_steps=int(dance_prefs['inference_steps']), audio_length_in_s=float(dance_prefs['audio_length_in_s']))
    else:
      output = dance_pipe(generator=dance_generator, batch_size=int(dance_prefs['batch_size']), num_inference_steps=int(dance_prefs['inference_steps']), audio_length_in_s=float(dance_prefs['audio_length_in_s'])) #, torch_dtype=torch.float16
    #, callback=callback_fn, callback_steps=1)
    audio = output.audios
    audio_slice = audio[0, -3:, -3:]
    clear_last()
    #prt(f'audio: {type(audio[0])}, audio_slice: {type(audio_slice)}, len:{len(audio)}')
    #audio_slice.tofile("/content/dance-test.wav")
    audio_name = f"dance-{dance_prefs['dance_model']}" + (f"-{random_seed}" if prefs['file_suffix_seed'] else '')
    audio_local = os.path.join(root_dir, "audio_out")
    audio_out = audio_local
    os.makedirs(audio_local, exist_ok=True)
    if storage_type == "Colab Google Drive":
      audio_out = prefs['image_output'].rpartition(slash)[0] + slash + 'audio_out'
      os.makedirs(audio_out, exist_ok=True)
    i = 0
    for a in audio:
      fname = available_file(audio_local, audio_name, i, ext="wav")
      scipy.io.wavfile.write(fname, dance_pipe.unet.sample_rate, a.transpose())
      os.path.abspath(fname)
      #a_out = Audio(src=fname, autoplay=False)
      #page.overlay.append(a_out)
      #page.update()
      display_name = fname
      #a.tofile(f"/content/dance-{i}.wav")
      if storage_type == "Colab Google Drive":
        audio_save = available_file(audio_out, audio_name, i, ext='wav')
        shutil.copy(fname, audio_save)
        display_name = audio_save
      prt(AudioPlayer(src=fname, display=display_name, data=audio_save, page=page))
      #prt(Row([IconButton(icon=icons.PLAY_CIRCLE_FILLED, icon_size=48, on_click=play_audio, data=a_out), Text(display_name)]))
      i += 1
    if prefs['enable_sounds']: page.snd_alert.play()

def run_audio_diffusion(page):
    global audio_diffusion_prefs, pipe_audio_diffusion, prefs
    def prt(line):
      if type(line) == str:
        line = Text(line)
      page.audio_diffusion_output.controls.append(line)
      page.audio_diffusion_output.update()
    def clear_last():
      if len(page.audio_diffusion_output.controls) < 1: return
      del page.audio_diffusion_output.controls[-1]
      page.audio_diffusion_output.update()
    def play_audio(e):
      e.control.data.play()
    #if not bool(audio_diffusion_prefs['text']):
    #  alert_msg(page, "Provide Text for the AI to create the sound of...")
    #  return
    if not status['installed_diffusers']:
      alert_msg(page, "You must Install the HuggingFace Diffusers Library first... ")
      return
    total_steps = audio_diffusion_prefs['steps']
    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep}"
      progress.update()
    progress = ProgressBar(bar_height=8)
    installer = Installing("Downloading Audio Diffusion Pipeline...")
    prt(installer)
    audio_diffusion_dir = os.path.join(root_dir, "audio_diffusion")

    try:
        import mel
    except ModuleNotFoundError:
        installer.set_details("...installing mel")
        try:
            run_process("pip install -q mel", page=page, show=True, print=True)
        except Exception as e:
            clear_last()
            alert_msg(page, "Error Installing AudioDiffusion requirements", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip())]))
            return
        pass
    finally:
        import mel
    import scipy.io.wavfile
    from diffusers import DiffusionPipeline, DDIMScheduler
    model_id = audio_diffusion_prefs['audio_model']
    if audio_diffusion_prefs['loaded_model'] != model_id:
      clear_pipes()
    else:
      clear_pipes('audio_diffusion')
    if pipe_audio_diffusion == None:
      installer.set_details("...initializing audio_diffusion pipe")
      try:
          # TODO: Switch DDPM
        a_scheduler = DDIMScheduler()
        pipe_audio_diffusion = DiffusionPipeline.from_pretrained(model_id, scheduler=a_scheduler, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
        pipe_audio_diffusion = pipe_audio_diffusion.to(torch_device)
        pipe_audio_diffusion.set_progress_bar_config(disable=True)
        audio_diffusion_prefs['loaded_model'] = model_id
      except Exception as e:
        clear_last()
        alert_msg(page, "Error setting up Audio Diffusion Pipeline", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))
        return
    init = audio_diffusion_prefs['audio_file']
    if init.startswith('http'):
        installer.set_details("...downloading audio file")
        init_audio = download_file(init)
    else:
        if os.path.isfile(init):
            init_audio = init
        else:
            init_audio = None
    clear_last()
    prt(Text("  Generating Audio Diffusion Sounds...", weight=FontWeight.BOLD))
    prt(progress)
    random_seed = int(audio_diffusion_prefs['seed']) if int(audio_diffusion_prefs['seed']) > 0 else rnd.randint(0,4294967295)
    generator = torch.Generator(device=torch_device).manual_seed(random_seed)
    try:
        output = pipe_audio_diffusion(audio_file=init_audio, slice=audio_diffusion_prefs['slice'], steps=audio_diffusion_prefs['steps'], start_step=audio_diffusion_prefs['start_step'], mask_start_secs=audio_diffusion_prefs['mask_start_secs'], mask_end_secs=audio_diffusion_prefs['mask_end_secs'], eta=audio_diffusion_prefs['eta'], batch_size=int(audio_diffusion_prefs['batch_size']), generator=generator)#, callback=callback_fnc)
    except Exception as e:
        clear_last()
        alert_msg(page, "Error Generating Audio Diffusion Output", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))
        return
    images = output.images
    audios = output.audios
    sample_rate = pipe_audio_diffusion.mel.get_sample_rate()
    save_dir = os.path.join(root_dir, 'audio_out', audio_diffusion_prefs['batch_folder_name'])
    if not os.path.exists(save_dir):
      os.makedirs(save_dir, exist_ok=True)
    audio_out = os.path.join(prefs['image_output'].rpartition(slash)[0], 'audio_out')
    if bool(audio_diffusion_prefs['batch_folder_name']):
      audio_out = os.path.join(audio_out, audio_diffusion_prefs['batch_folder_name'])
    os.makedirs(audio_out, exist_ok=True)
    #voice_dirs = os.listdir(os.path.join(root_dir, "audio_diffusion-tts", 'audio_diffusion', 'voices'))
    #print(str(voice_dirs))
    clear_last()
    clear_last()
    a_name = audio_diffusion_prefs['audio_name']
    if not bool(a_name):
        if bool(init_audio):
            a_name = init_audio.rpartition(slash)[2].rpartition('.')[0]
        else:
            a_name = f"audio_diffusion-{model_id.rpartition('/')[2]}"
    fname = format_filename(a_name, force_underscore=True)
    if fname[-1] == '.': fname = fname[:-1]
    file_prefix = audio_diffusion_prefs['file_prefix']
    audio_name = f'{file_prefix}{fname}'
    audio_name = audio_name[:int(prefs['file_max_length'])]
    for image in images:
        iname = available_file(save_dir, audio_name, 0)
        image.save(iname)
        out_path = iname
        prt(Row([Img(src=iname, fit=ImageFit.FILL, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
        if storage_type == "Colab Google Drive":
            new_file = available_file(prefs['image_output'], fname, 0)
            out_path = new_file
            shutil.copy(iname, new_file)
        elif bool(prefs['image_output']):
            new_file = available_file(prefs['image_output'], fname, 0)
            out_path = new_file
            shutil.copy(iname, new_file)
        #prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))
    for a in audios:
        aname = available_file(save_dir, audio_name, 0, ext="wav")
        #for i in range(waveform.shape[0]):
        #    sf.write(aname, waveform[i, 0], samplerate=sample_rate)
        #torchaudio.save(fname, gen.squeeze(0).cpu(), 24000)
        #IPython.display.Audio('generated.wav')
        scipy.io.wavfile.write(aname, sample_rate, a.transpose())
        #a_out = Audio(src=aname, autoplay=False)
        #page.overlay.append(a_out)
        #page.update()
        display_name = aname
        #a.tofile(f"/content/dance-{i}.wav")
        if storage_type == "Colab Google Drive":
            audio_save = available_file(audio_out, audio_name, 0, ext='wav')
            shutil.copy(aname, audio_save)
        elif bool(prefs['image_output']):
            audio_save = available_file(audio_out, audio_name, 0, ext='wav')
            shutil.copy(aname, audio_save)
        else: audio_save = aname
        display_name = audio_save
        prt(AudioPlayer(src=aname, display=audio_save, data=audio_save))
        #prt(Row([IconButton(icon=icons.PLAY_CIRCLE_FILLED, icon_size=48, on_click=play_audio, data=a_out), Text(display_name)]))
    if prefs['enable_sounds']: page.snd_alert.play()

def run_music_gen(page):
    global music_gen_prefs, pipe_music_gen, prefs
    def prt(line):
      if type(line) == str:
        line = Text(line)
      page.music_gen_output.controls.append(line)
      page.music_gen_output.update()
    def clear_last():
      if len(page.music_gen_output.controls) < 1: return
      del page.music_gen_output.controls[-1]
      page.music_gen_output.update()
    def play_audio(e):
      e.control.data.play()
    if not bool(music_gen_prefs['prompt']):
      alert_msg(page, "Provide Text for the AI to create the sound of...")
      return
    #if not status['installed_diffusers']:
    #  alert_msg(page, "You must Install the HuggingFace Diffusers Library first... ")
    #  return
    '''total_steps = music_gen_prefs['steps']
    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep}"
      progress.update()'''
    progress = ProgressBar(bar_height=8)
    installer = Installing("Downloading MusicGen Pipeline...")
    prt(installer)
    music_gen_dir = os.path.join(root_dir, "music_gen")
    try:
        import sentencepiece
    except ModuleNotFoundError: #>=0.1.99
        run_sp("pip install --upgrade sentencepiece~=0.1", realtime=False)
        import sentencepiece
        pass
    try:
        import audiocraft
        #from audiocraft.models import musicgen
    except ModuleNotFoundError:
        installer.set_details("...facebookresearch/audiocraft")
        run_sp("pip install -U ffmpeg", realtime=False)
        #run_sp("pip install -U audiocraft", realtime=True)
        #run_sp("pip install -U git+https://github.com/facebookresearch/audiocraft#egg=audiocraft", realtime=False)
        run_sp("pip install -U git+https://github.com/Oncorporation/audiocraft#egg=audiocraft", realtime=False)
        pass
    finally:
        from audiocraft.models import musicgen
        from audiocraft.data.audio import audio_write
        from audiocraft.data.audio_utils import apply_fade, apply_tafade
        from audiocraft.utils.extend import generate_music_segments, add_settings_to_image, INTERRUPTING
    text = music_gen_prefs['prompt']
    init = music_gen_prefs['audio_file']
    model_id = music_gen_prefs['audio_model'] if not bool(init) else "melody"
    if music_gen_prefs['loaded_model'] != model_id:
        installer.set_details("...clear_pipes")
        clear_pipes()
    else:
      clear_pipes('music_gen')
    if pipe_music_gen == None:
        installer.set_details(f"...MusicGen pretrained {model_id}")
        try:
            pipe_music_gen = musicgen.MusicGen.get_pretrained(model_id, device='cuda')
            music_gen_prefs['loaded_model'] = model_id
        except Exception as e:
            clear_last()
            alert_msg(page, "Error setting up MusicGen Pipeline", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            return
    clear_last()
    def get_melody(melody_filepath):
        audio_data= list(librosa.load(melody_filepath, sr=None))
        audio_data[0], audio_data[1] = audio_data[1], audio_data[0]
        melody = tuple(audio_data)
        return melody
    for num in range(music_gen_prefs['num_samples']):
        if init.startswith('http'):
            installer.set_details("...download audio_file")
            melody = download_file(init)
        else:
            if os.path.isfile(init):
                melody = init
            else:
                melody = None
        if melody is None:
            melody = get_melody(melody)
        duration = music_gen_prefs['duration']
        overlap = music_gen_prefs['overlap']
        dimension = music_gen_prefs['dimension']
        seed = music_gen_prefs['seed']
        output = None
        first_chunk = None
        total_samples = duration * 50 + 3
        segment_duration = duration
        initial_duration = duration
        output_segments = []
        gen_status = Text("  Generating MusicGen Sounds...", weight=FontWeight.BOLD)
        prt(gen_status)
        prt(progress)
        if seed <= 0:
            random_seed = rnd.randint(0, 0xffff_ffff_ffff)
        else:
            random_seed = seed + num
        torch.manual_seed(random_seed)
        #if melody != None and duration > 30:
        #    duration = 30
        chunk = 0
        while duration > 0:
            chunk += 1
            if not output_segments: # first pass of long or short song
                if segment_duration > pipe_music_gen.lm.cfg.dataset.segment_duration:
                    segment_duration = pipe_music_gen.lm.cfg.dataset.segment_duration
                else:
                    segment_duration = duration
            else: # next pass of long song
                if duration + overlap < pipe_music_gen.lm.cfg.dataset.segment_duration:
                    segment_duration = duration + overlap
                else:
                    segment_duration = pipe_music_gen.lm.cfg.dataset.segment_duration
            gen_status.value = f"  Generating MusicGen... Segment duration: {segment_duration}, Remaining duration: {duration}, overlap: {overlap}, chunk: {chunk}"
            page.music_gen_output.update()
            #print(f'Segment duration: {segment_duration}, duration: {duration}, overlap: {overlap}')
            pipe_music_gen.set_generation_params(
                use_sampling=music_gen_prefs['use_sampling'],
                two_step_cfg=music_gen_prefs['two_step_cfg'],
                top_k=music_gen_prefs['top_k'],
                top_p=music_gen_prefs['top_p'],
                temperature=music_gen_prefs['temperature'],
                cfg_coef=music_gen_prefs['guidance'],
                duration=segment_duration,
                rep_penalty=0.5
            )
            try:
                if melody:
                    if duration > pipe_music_gen.lm.cfg.dataset.segment_duration:
                        output_segments, duration = generate_music_segments(text, melody, random_seed, pipe_music_gen, duration, overlap, pipe_music_gen.lm.cfg.dataset.segment_duration, prompt_index, harmony_only=music_gen_prefs['harmony_only'])
                    else:
                        # pure original code
                        sr, melody = melody[0], torch.from_numpy(melody[1]).to(pipe_music_gen.device).float().t().unsqueeze(0)
                        print(melody.shape)
                        if melody.dim() == 2:
                            melody = melody[None]
                        melody = melody[..., :int(sr * pipe_music_gen.lm.cfg.dataset.segment_duration)]
                        output = pipe_music_gen.generate_with_chroma(
                            descriptions=[text],
                            melody_wavs=melody,
                            melody_sample_rate=sr,
                            progress=True
                        )
                    # All output_segments are populated, so we can break the loop or set duration to 0
                    break
                    '''import torchaudio
                    sr, melody = melody[0], torch.from_numpy(melody[1]).to(pipe_music_gen.device).float().t().unsqueeze(0)
                    melody, sr = torchaudio.load(init_audio)
                    #sr = 32000
                    #melody = torch.from_numpy(init_audio).to(pipe_music_gen.device).float().t().unsqueeze(0)
                    #if melody.dim() == 2:
                    #    melody = melody[None]
                    melody = melody[None].expand(3, -1, -1)
                    #melody = melody[..., :int(sr * pipe_music_gen.lm.cfg.dataset.segment_duration)]
                    output = pipe_music_gen.generate_with_chroma(
                        descriptions=[music_gen_prefs['prompt'] * music_gen_prefs['batch_size']],
                        melody_wavs=melody,
                        melody_sample_rate=sr,
                        progress=True
                    )
                    duration -= segment_duration'''
                else:
                    if not output_segments:
                        next_segment = pipe_music_gen.generate(descriptions=[text], progress=True)
                        duration -= segment_duration
                    else:
                        last_chunk = output_segments[-1][:, :, -overlap*pipe_music_gen.sample_rate:]
                        next_segment = pipe_music_gen.generate_continuation(last_chunk, pipe_music_gen.sample_rate, descriptions=[text], progress=False)
                        duration -= segment_duration - overlap
                    output_segments.append(next_segment)
                    '''if output is None:
                        next_segment = pipe_music_gen.generate(descriptions=[music_gen_prefs['prompt'] * music_gen_prefs['batch_size']], progress=True)
                                                      #progress=updateProgress)
                        duration -= segment_duration
                    else:
                        if first_chunk is None and pipe_music_gen.name == "melody" and music_gen_prefs['recondition']:
                            first_chunk = output[:, :, :pipe_music_gen.lm.cfg.dataset.segment_duration*pipe_music_gen.sample_rate]
                        last_chunk = output[:, :, -overlap*pipe_music_gen.sample_rate:]
                        next_segment = pipe_music_gen.generate_continuation(last_chunk,
                            pipe_music_gen.sample_rate, descriptions=[[music_gen_prefs['prompt'] * music_gen_prefs['batch_size']]], progress=True) #, melody_wavs=(first_chunk), resample=False
                        duration -= segment_duration - overlap'''
                    #output = pipe_music_gen.generate(descriptions=[music_gen_prefs['prompt'] * music_gen_prefs['batch_size']], progress=True)
                #if output is None:
                #    output = next_segment
                #else:
                #    output = torch.cat([output[:, :, :-overlap*pipe_music_gen.sample_rate], next_segment], 2)
            except Exception as e:
                clear_last()
                alert_msg(page, "Error Generating Music Output", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
                return
            #output = output.detach().cpu().float()[0]
        if output_segments:
            try:
                output = output_segments[0]
                for i in range(1, len(output_segments)):
                    if overlap > 0:
                        overlap_samples = overlap * pipe_music_gen.sample_rate
                        overlapping_output_fadeout = output[:, :, -overlap_samples:]
                        #overlapping_output_fadeout = apply_fade(overlapping_output_fadeout,sample_rate=pipe_music_gen.sample_rate,duration=overlap,out=True,start=True, curve_end=0.0, current_device=pipe_music_gen.device)
                        overlapping_output_fadeout = apply_tafade(overlapping_output_fadeout,sample_rate=pipe_music_gen.sample_rate,duration=overlap,out=True,start=True,shape="linear")
                        overlapping_output_fadein = output_segments[i][:, :, :overlap_samples]
                        #overlapping_output_fadein = apply_fade(overlapping_output_fadein,sample_rate=pipe_music_gen.sample_rate,duration=overlap,out=False,start=False, curve_start=0.0, current_device=pipe_music_gen.device)
                        overlapping_output_fadein = apply_tafade(overlapping_output_fadein,sample_rate=pipe_music_gen.sample_rate,duration=overlap,out=False,start=False, shape="linear")
                        overlapping_output = torch.cat([overlapping_output_fadeout[:, :, :-(overlap_samples // 2)], overlapping_output_fadein],dim=2)
                        gen_status.value = f"  Saving MusicGen... Overlap size Fade:{overlapping_output.size()}\n output: {output.size()}\n segment: {output_segments[i].size()}"
                        page.music_gen_output.update()
                        #print(f" Overlap size Fade:{overlapping_output.size()}\n output: {output.size()}\n segment: {output_segments[i].size()}")
                        output = torch.cat([output[:, :, :-overlap_samples], overlapping_output, output_segments[i][:, :, overlap_samples:]], dim=dimension)
                    else:
                        output = torch.cat([output, output_segments[i]], dim=dimension)
                output = output.detach().cpu().float()[0]
            except Exception as e:
                alert_msg(page, f"Error combining segments: {e}. Using the first segment only.")
                output = output_segments[0].detach().cpu().float()[0]
        else:
            try:
                output = output.detach().cpu().float()[0]
            except Exception as e:
                clear_last()
                alert_msg(page, "Error Saving Music Output", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
                return

        #sample_rate = pipe_music_gen.mel.get_sample_rate()
        save_dir = os.path.join(root_dir, 'audio_out', music_gen_prefs['batch_folder_name'])
        if not os.path.exists(save_dir):
            os.makedirs(save_dir, exist_ok=True)
        audio_out = os.path.join(prefs['image_output'].rpartition(slash)[0], 'audio_out')
        if bool(music_gen_prefs['batch_folder_name']):
            audio_out = os.path.join(audio_out, music_gen_prefs['batch_folder_name'])
        os.makedirs(audio_out, exist_ok=True)
        #voice_dirs = os.listdir(os.path.join(root_dir, "music_gen-tts", 'music_gen', 'voices'))
        #print(str(voice_dirs))
        clear_last()
        clear_last()
        a_name = music_gen_prefs['audio_name']
        if not bool(a_name):
            a_name = music_gen_prefs['prompt']
        fname = format_filename(a_name, force_underscore=True)
        if fname[-1] == '.': fname = fname[:-1]
        audio_name = f'{music_gen_prefs["file_prefix"]}{fname}'
        audio_name = audio_name[:int(prefs['file_max_length'])]
        #audios = output.audios
        idx = 0
        for wav in output:
            aname = available_file(save_dir, audio_name, num + idx, ext="wav")
            with open(aname, "wb") as file:
                audio_write(file.name, wav.cpu(), pipe_music_gen.sample_rate, strategy="loudness", loudness_compressor=True, loudness_headroom_db=18, add_suffix=False, channels=2)
                #waveform_video = gr.make_waveform(file.name)
            #for i in range(waveform.shape[0]):
            #    sf.write(aname, waveform[i, 0], samplerate=sample_rate)
            #torchaudio.save(fname, gen.squeeze(0).cpu(), 24000)
            #IPython.display.Audio('generated.wav')
            #scipy.io.wavfile.write(aname, sample_rate, a.transpose())
            #a_out = Audio(src=aname, autoplay=False)
            #page.overlay.append(a_out)
            #page.update()
            display_name = aname
            #a.tofile(f"/content/dance-{i}.wav")
            if storage_type == "Colab Google Drive":
                audio_save = available_file(audio_out, audio_name, num + idx, ext='wav')
                shutil.copy(aname, audio_save)
            elif bool(prefs['image_output']):
                audio_save = available_file(audio_out, audio_name, num + idx, ext='wav')
                shutil.copy(aname, audio_save)
            else: audio_save = aname
            display_name = audio_save
            prt(AudioPlayer(src=aname, display=audio_save, data=audio_out, page=page))
            #prt(Row([IconButton(icon=icons.PLAY_CIRCLE_FILLED, icon_size=48, on_click=play_audio, data=a_out), Text(display_name)]))
            idx += 1
        output = None
        first_chunk = None
        output_segments = []
    flush()
    torch.cuda.ipc_collect()
    if prefs['enable_sounds']: page.snd_alert.play()

# https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_dreambooth_training.ipynb
def run_dreambooth(page):
    global dreambooth_prefs, prefs
    def prt(line):
      if type(line) == str:
        line = Text(line)
      page.dreambooth_output.controls.append(line)
      page.dreambooth_output.update()
    def clear_last():
      del page.dreambooth_output.controls[-1]
      page.dreambooth_output.update()
    if not status['installed_diffusers']:
      alert_msg(page, "You must Install the HuggingFace Diffusers Library first... ")
      return
    save_path = os.path.join(root_dir, "my_concept")
    error = False
    if not os.path.exists(save_path):
      error = True
    elif len(os.listdir(save_path)) == 0:
      error = True
    if len(page.db_file_list.controls) == 0:
      error = True
    if error:
      alert_msg(page, "Couldn't find a list of images to train concept. Add image files to the list...")
      return
    prt(Installing("Downloading DreamBooth Conceptualizers"))
    #os.chdir(root_dir)
    #run_process("pip clone https://github.com/Skquark/diffusers.git", realtime=False)
    #run_process("pip install -e .", realtime=False)
    #os.chdir(os.path.join(root_dir, "diffusers", "examples", "dreambooth"))
    #run_process("pip install -r requirements.txt", realtime=False)
    os.chdir(root_dir)
    try:
        os.environ['LD_LIBRARY_PATH'] += "/usr/lib/wsl/lib:$LD_LIBRARY_PATH"
        #run_sp("export LD_LIBRARY_PATH=/usr/lib/wsl/lib:$LD_LIBRARY_PATH", realtime=False)
        import bitsandbytes
    except Exception:
        if sys.platform.startswith("win"):
            run_sp("pip install bitsandbytes-windows", realtime=False)
        else:
            run_sp("pip install --upgrade bitsandbytes", realtime=False)
        import bitsandbytes
        pass
    import argparse
    import itertools
    import math
    from contextlib import nullcontext
    import random
    import torch.nn.functional as F
    import torch.utils.checkpoint
    from torch.utils.data import Dataset
    from accelerate import Accelerator
    from accelerate.logging import get_logger
    from accelerate.utils import set_seed
    from diffusers import AutoencoderKL, DDPMScheduler, PNDMScheduler, StableDiffusionPipeline, UNet2DConditionModel
    #from diffusers.hub_utils import init_git_repo, push_to_hub
    from diffusers.optimization import get_scheduler
    from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker

    from torchvision import transforms
    from tqdm.auto import tqdm
    from transformers import CLIPFeatureExtractor, CLIPTextModel, CLIPTokenizer

    import bitsandbytes as bnb
    import gc
    import glob
    from io import BytesIO

    def download_image(url):
      try:
        response = requests.get(url)
      except:
        return None
      return PILImage.open(BytesIO(response.content)).convert("RGB")
    #images = list(filter(None,[download_image(url) for url in dreambooth_prefs['urls']]))
    #save_path = "./my_concept"
    #if not os.path.exists(save_path):
    #  os.mkdir(save_path)
    #[image.save(f"{save_path}/{i}.jpeg") for i, image in enumerate(images)]
    #image_grid(images, 1, len(images))

    prior_preservation_class_folder = dreambooth_prefs['prior_preservation_class_folder']
    class_data_root=prior_preservation_class_folder
    class_prompt=dreambooth_prefs['prior_preservation_class_prompt']
    class_data_root=prior_preservation_class_folder
    dreambooth_prefs['instance_prompt'] = dreambooth_prefs['instance_prompt'].strip()
    clear_pipes()
    num_new_images = None
    if(dreambooth_prefs['prior_preservation']):
        class_images_dir = Path(class_data_root)
        if not class_images_dir.exists():
            class_images_dir.mkdir(parents=True)
        cur_class_images = len(list(class_images_dir.iterdir()))

        if cur_class_images < dreambooth_prefs['num_class_images']:
            if prefs['higher_vram_mode']:
              pipeline = StableDiffusionPipeline.from_pretrained(model_path).to("cuda")
            else:
              pipeline = StableDiffusionPipeline.from_pretrained(model_path, revision="fp16", torch_dtype=torch.float16).to("cuda")
            if prefs['enable_attention_slicing']:
              pipeline.enable_attention_slicing()
            pipeline.set_progress_bar_config(disable=True)

            num_new_images = dreambooth_prefs['num_class_images'] - cur_class_images
            print(f"Number of class images to sample: {num_new_images}.")

            sample_dataset = PromptDataset(class_prompt, num_new_images)
            sample_dataloader = torch.utils.data.DataLoader(sample_dataset, batch_size=dreambooth_prefs['sample_batch_size'])

            for example in tqdm(sample_dataloader, desc="Generating class images"):
                images = pipeline(example["prompt"]).images

                for i, image in enumerate(images):
                    image.save(class_images_dir / f"{example['index'][i] + cur_class_images}.jpg")
            pipeline = None
            gc.collect()
            del pipeline
            with torch.no_grad():
              torch.cuda.empty_cache()
    text_encoder = CLIPTextModel.from_pretrained(model_path, subfolder="text_encoder")
    vae = AutoencoderKL.from_pretrained(model_path, subfolder="vae")
    unet = UNet2DConditionModel.from_pretrained(model_path, subfolder="unet")
    tokenizer = CLIPTokenizer.from_pretrained(model_path,subfolder="tokenizer")

    from argparse import Namespace
    dreambooth_args = Namespace(
        pretrained_model_name_or_path=model_path,
        resolution=dreambooth_prefs['max_size'],
        center_crop=True,
        instance_data_dir=save_path,
        instance_prompt=dreambooth_prefs['instance_prompt'],
        learning_rate=dreambooth_prefs['learning_rate'],#5e-06,
        max_train_steps=dreambooth_prefs['max_train_steps'],#450,
        train_batch_size=1,
        gradient_accumulation_steps=2,
        max_grad_norm=1.0,
        mixed_precision="no", # set to "fp16" for mixed-precision training.
        gradient_checkpointing=True, # set this to True to lower the memory usage.
        use_8bit_adam=not prefs['higher_vram_mode'], # use 8bit optimizer from bitsandbytes
        seed=dreambooth_prefs['seed'],#3434554,
        with_prior_preservation=dreambooth_prefs['prior_preservation'],
        prior_loss_weight=dreambooth_prefs['prior_loss_weight'],
        sample_batch_size=dreambooth_prefs['sample_batch_size'],
        class_data_dir=dreambooth_prefs['prior_preservation_class_folder'],
        class_prompt=class_prompt,
        num_class_images=dreambooth_prefs['num_class_images'],
        output_dir="dreambooth-concept",
    )

    from accelerate.utils import set_seed
    def training_function(text_encoder, vae, unet):
        logger = get_logger(__name__)
        accelerator = Accelerator(
            gradient_accumulation_steps=dreambooth_args.gradient_accumulation_steps,
            mixed_precision=dreambooth_args.mixed_precision,
        )
        set_seed(dreambooth_args.seed)
        if dreambooth_args.gradient_checkpointing:
            unet.enable_gradient_checkpointing()
        # Use 8-bit Adam for lower memory usage or to fine-tune the model in 16GB GPUs
        if dreambooth_args.use_8bit_adam:
            optimizer_class = bnb.optim.AdamW8bit
        else:
            optimizer_class = torch.optim.AdamW
        optimizer = optimizer_class(unet.parameters(),lr=dreambooth_args.learning_rate)
        noise_scheduler = DDPMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule="scaled_linear", num_train_timesteps=1000)
        train_dataset = DreamBoothDataset(
            instance_data_root=dreambooth_args.instance_data_dir,
            instance_prompt=dreambooth_args.instance_prompt,
            class_data_root=dreambooth_args.class_data_dir if dreambooth_args.with_prior_preservation else None,
            class_prompt=dreambooth_args.class_prompt,
            tokenizer=tokenizer,
            size=dreambooth_args.resolution,
            center_crop=dreambooth_args.center_crop,
        )
        def collate_fn(examples):
            input_ids = [example["instance_prompt_ids"] for example in examples]
            pixel_values = [example["instance_images"] for example in examples]

            # concat class and instance examples for prior preservation
            if dreambooth_args.with_prior_preservation:
                input_ids += [example["class_prompt_ids"] for example in examples]
                pixel_values += [example["class_images"] for example in examples]

            pixel_values = torch.stack(pixel_values)
            pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()

            input_ids = tokenizer.pad({"input_ids": input_ids}, padding=True, return_tensors="pt").input_ids

            batch = {
                "input_ids": input_ids,
                "pixel_values": pixel_values,
            }
            return batch
        train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=dreambooth_args.train_batch_size, shuffle=True, collate_fn=collate_fn)
        unet, optimizer, train_dataloader = accelerator.prepare(unet, optimizer, train_dataloader)
        # Move text_encode and vae to gpu
        text_encoder.to(accelerator.device)
        vae.to(accelerator.device)
        # We need to recalculate our total training steps as the size of the training dataloader may have changed.
        num_update_steps_per_epoch = math.ceil(len(train_dataloader) / dreambooth_args.gradient_accumulation_steps)
        num_train_epochs = math.ceil(dreambooth_args.max_train_steps / num_update_steps_per_epoch)
        clear_last()
        total_batch_size = dreambooth_args.train_batch_size * accelerator.num_processes * dreambooth_args.gradient_accumulation_steps

        prt("***** Running training *****")
        prt(f"  Number of examples = {len(train_dataset)}")
        if num_new_images != None: prt(f"  Number of class images to sample: {num_new_images}.")
        prt(f"  Instantaneous batch size per device = {dreambooth_args.train_batch_size}")
        prt(f"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}")
        prt(f"  Gradient Accumulation steps = {dreambooth_args.gradient_accumulation_steps}")
        prt(f"  Total optimization steps = {dreambooth_args.max_train_steps}")
        progress = ProgressBar(bar_height=8)
        prt(progress)
        progress_bar = tqdm(range(dreambooth_args.max_train_steps), disable=not accelerator.is_local_main_process)
        progress_bar.set_description("Steps")
        global_step = 0

        for epoch in range(num_train_epochs):
            unet.train()
            for step, batch in enumerate(train_dataloader):
                with accelerator.accumulate(unet):
                    # Convert images to latent space
                    with torch.no_grad():
                        latents = vae.encode(batch["pixel_values"]).latent_dist.sample()
                        latents = latents * 0.18215
                    # Sample noise that we'll add to the latents
                    noise = torch.randn(latents.shape).to(latents.device)
                    bsz = latents.shape[0]
                    # Sample a random timestep for each image
                    timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz,), device=latents.device).long()

                    # Add noise to the latents according to the noise magnitude at each timestep
                    # (this is the forward diffusion process)
                    noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)

                    # Get the text embedding for conditioning
                    with torch.no_grad():
                        encoder_hidden_states = text_encoder(batch["input_ids"])[0]

                    # Predict the noise residual
                    noise_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample

                    if dreambooth_args.with_prior_preservation:
                        # Chunk the noise and noise_pred into two parts and compute the loss on each part separately.
                        noise_pred, noise_pred_prior = torch.chunk(noise_pred, 2, dim=0)
                        noise, noise_prior = torch.chunk(noise, 2, dim=0)

                        # Compute instance loss
                        loss = F.mse_loss(noise_pred, noise, reduction="none").mean([1, 2, 3]).mean()

                        # Compute prior loss
                        prior_loss = F.mse_loss(noise_pred_prior, noise_prior, reduction="none").mean([1, 2, 3]).mean()

                        # Add the prior loss to the instance loss.
                        loss = loss + dreambooth_args.prior_loss_weight * prior_loss
                    else:
                        loss = F.mse_loss(noise_pred, noise, reduction="none").mean([1, 2, 3]).mean()

                    accelerator.backward(loss)
                    if accelerator.sync_gradients:
                        accelerator.clip_grad_norm_(unet.parameters(), dreambooth_args.max_grad_norm)
                    optimizer.step()
                    optimizer.zero_grad()

                # Checks if the accelerator has performed an optimization step behind the scenes
                if accelerator.sync_gradients:
                    progress_bar.update(1)
                    global_step += 1

                logs = {"loss": loss.detach().item()}
                progress_bar.set_postfix(**logs)
                progress.value = (global_step + 1) / dreambooth_args.max_train_steps
                progress.tooltip = f'[{(global_step + 1)} / {dreambooth_args.max_train_steps}]'
                progress.update()

                if global_step >= dreambooth_args.max_train_steps:
                    break

            accelerator.wait_for_everyone()

        # Create the pipeline using the trained modules and save it.
        if accelerator.is_main_process:
            pipeline = StableDiffusionPipeline(
                text_encoder=text_encoder,
                vae=vae,
                unet=accelerator.unwrap_model(unet),
                tokenizer=tokenizer,
                scheduler=PNDMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule="scaled_linear", skip_prk_steps=True),
                feature_extractor=CLIPFeatureExtractor.from_pretrained("openai/clip-vit-base-patch32"),
                **safety,
            )
            pipeline.save_pretrained(dreambooth_args.output_dir)

    import accelerate
    try:
      accelerate.notebook_launcher(training_function, args=(text_encoder, vae, unet))
    except Exception as e:
      clear_last()
      alert_msg(page, "ERROR: CUDA Ran Out of Memory. Try reducing parameters and try again...", content=Text(str(e)))
      with torch.no_grad():
        torch.cuda.empty_cache()
      return
    clear_last()
    with torch.no_grad():
        torch.cuda.empty_cache()
    name_of_your_concept = dreambooth_prefs['name_of_your_concept']
    if(dreambooth_prefs['save_concept']):
      from huggingface_hub import HfApi, HfFolder, CommitOperationAdd
      from huggingface_hub import create_repo
      from IPython.display import display_markdown
      api = HfApi()
      your_username = api.whoami()["name"]
      dreambooth_pipe = StableDiffusionPipeline.from_pretrained(
        dreambooth_args.output_dir,
        torch_dtype=torch.float16,
      ).to("cuda")
      os.makedirs("fp16_model",exist_ok=True)
      dreambooth_pipe.save_pretrained("fp16_model")
      del dreambooth_pipe
      hf_token = prefs['HuggingFace_api_key']
      if(dreambooth_prefs['where_to_save_concept'] == "Public Library"):
        repo_id = f"sd-dreambooth-library/{format_filename(name_of_your_concept, use_dash=True)}"
        #Join the Concepts Library organization if you aren't part of it already
        run_sp(f"curl -X POST -H 'Authorization: Bearer '{hf_token} -H 'Content-Type: application/json' https://huggingface.co/organizations/sd-dreambooth-library/share/SSeOwppVCscfTEzFGQaqpfcjukVeNrKNHX", realtime=False)
        #!curl -X POST -H 'Authorization: Bearer '$hf_token -H 'Content-Type: application/json' https://huggingface.co/organizations/sd-dreambooth-library/share/SSeOwppVCscfTEzFGQaqpfcjukVeNrKNHX
      else:
        repo_id = f"{your_username}/{format_filename(name_of_your_concept, use_dash=True)}"
      output_dir = dreambooth_args.output_dir
      if(not prefs['HuggingFace_api_key']):
        with open(HfFolder.path_token, 'r') as fin: hf_token = fin.read();
      else:
        hf_token = prefs['HuggingFace_api_key']

      images_upload = os.listdir(save_path)
      image_string = ""
      #repo_id = f"sd-dreambooth-library/{slugify(name_of_your_concept)}"
      print(f"repo_id={repo_id}")
      for i, image in enumerate(images_upload):
          image_string = f'''{image_string}![image {i}](https://huggingface.co/{repo_id}/resolve/main/concept_images/{image})
    '''
      description = dreambooth_prefs['readme_description']
      if bool(description.strip()):
        description = dreambooth_prefs['readme_description'] + '\n\n'
      readme_text = f'''---
license: mit
---
### {name_of_your_concept} on Stable Diffusion via Dreambooth using [Stable Diffusion Deluxe](https://colab.research.google.com/github/Skquark/AI-Friends/blob/main/Stable_Diffusion_Deluxe.ipynb)
#### model by {api.whoami()["name"]}
This your the Stable Diffusion model fine-tuned the {name_of_your_concept} concept taught to Stable Diffusion with Dreambooth.
It can be used by modifying the `instance_prompt`: **{dreambooth_prefs['instance_prompt']}**

{description}You can also train your own concepts and upload them to the library by using [this notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_dreambooth_training.ipynb).
And you can run your new concept via `diffusers`: [Colab Notebook for Inference](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_dreambooth_inference.ipynb), [Spaces with the Public Concepts loaded](https://huggingface.co/spaces/sd-dreambooth-library/stable-diffusion-dreambooth-concepts)

Here are the images used for training this concept:
{image_string}
'''
      #Save the readme to a file
      readme_file = open("README.md", "w")
      readme_file.write(readme_text)
      readme_file.close()
      #Save the token identifier to a file
      text_file = open("token_identifier.txt", "w")
      text_file.write(dreambooth_prefs['instance_prompt'])
      text_file.close()
      operations = [
        CommitOperationAdd(path_in_repo="token_identifier.txt", path_or_fileobj="token_identifier.txt"),
        CommitOperationAdd(path_in_repo="README.md", path_or_fileobj="README.md"),
      ]
      try:
        create_repo(repo_id, private=True, token=hf_token)
      except Exception as e:
        alert_msg(page, f"ERROR Creating repo {repo_id}... Make sure your HF token has Write access.", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip(), selectable=True)]))
        return

      api.create_commit(repo_id=repo_id, operations=operations, commit_message=f"Upload the concept {name_of_your_concept} embeds and token",token=hf_token)
      api.upload_folder(folder_path="fp16_model", path_in_repo="", repo_id=repo_id,token=hf_token)
      api.upload_folder(folder_path=save_path, path_in_repo="concept_images", repo_id=repo_id, token=hf_token)
      prefs['custom_model'] = repo_id
      prefs['custom_models'].append({'name': name_of_your_concept, 'path':repo_id})
      page.custom_model.value = repo_id
      try:
        page.custom_model.update()
      except Exception: pass
      prt(Markdown(f"## Your concept was saved successfully to _{repo_id}_.<br>[Click here to access it](https://huggingface.co/{repo_id}) and go to _Installers->Model Checkpoint->Custom Model Path_ to use. Include Token in prompts.", on_tap_link=lambda e: e.page.launch_url(e.data)))
    if prefs['enable_sounds']: page.snd_alert.play()

try:
    from torchvision import transforms
except Exception:
    run_sp("pip install torchvision", realtime=False)
    from torchvision import transforms
    pass
from torch.utils.data import Dataset

class DreamBoothDataset(Dataset):
    def __init__(
        self,
        instance_data_root,
        instance_prompt,
        tokenizer,
        class_data_root=None,
        class_prompt=None,
        size=dreambooth_prefs['max_size'],
        center_crop=False,
    ):
        self.size = size
        self.center_crop = center_crop
        self.tokenizer = tokenizer

        self.instance_data_root = Path(instance_data_root)
        if not self.instance_data_root.exists():
            raise ValueError("Instance images root doesn't exists.")

        self.instance_images_path = list(Path(instance_data_root).iterdir())
        self.num_instance_images = len(self.instance_images_path)
        self.instance_prompt = instance_prompt
        self._length = self.num_instance_images

        if class_data_root is not None:
            self.class_data_root = Path(class_data_root)
            self.class_data_root.mkdir(parents=True, exist_ok=True)
            self.class_images_path = list(Path(class_data_root).iterdir())
            self.num_class_images = len(self.class_images_path)
            self._length = max(self.num_class_images, self.num_instance_images)
            self.class_prompt = class_prompt
        else:
            self.class_data_root = None

        self.image_transforms = transforms.Compose([
            transforms.Resize(size, interpolation=transforms.InterpolationMode.BILINEAR),
            transforms.CenterCrop(size) if center_crop else transforms.RandomCrop(size),
            transforms.ToTensor(),
            transforms.Normalize([0.5], [0.5])]
        )

    def __len__(self):
        return self._length

    def __getitem__(self, index):
        example = {}
        instance_image = PILImage.open(self.instance_images_path[index % self.num_instance_images])
        if not instance_image.mode == "RGB":
            instance_image = instance_image.convert("RGB")
        example["instance_images"] = self.image_transforms(instance_image)
        example["instance_prompt_ids"] = self.tokenizer(
            self.instance_prompt,
            padding="do_not_pad",
            truncation=True,
            max_length=self.tokenizer.model_max_length,
        ).input_ids

        if self.class_data_root:
            class_image = PILImage.open(self.class_images_path[index % self.num_class_images])
            if not class_image.mode == "RGB":
                class_image = class_image.convert("RGB")
            example["class_images"] = self.image_transforms(class_image)
            example["class_prompt_ids"] = self.tokenizer(
                self.class_prompt,
                padding="do_not_pad",
                truncation=True,
                max_length=self.tokenizer.model_max_length,
            ).input_ids

        return example

class PromptDataset(Dataset):
    def __init__(self, prompt, num_samples):
        self.prompt = prompt
        self.num_samples = num_samples

    def __len__(self):
        return self.num_samples

    def __getitem__(self, index):
        example = {}
        example["prompt"] = self.prompt
        example["index"] = index
        return example


def run_dreambooth2(page):
    global dreambooth_prefs, prefs
    def prt(line):
      if type(line) == str:
        line = Text(line)
      page.dreambooth_output.controls.append(line)
      page.dreambooth_output.update()
    def clear_last():
      del page.dreambooth_output.controls[-1]
      page.dreambooth_output.update()
    if not status['installed_diffusers']:
      alert_msg(page, "You must Install the HuggingFace Diffusers Library first... ")
      return
    save_path = os.path.join(root_dir, "my_concept")
    error = False
    if not os.path.exists(save_path):
      error = True
    elif len(os.listdir(save_path)) == 0:
      error = True
    if len(page.db_file_list.controls) == 0:
      error = True
    if error:
      alert_msg(page, "Couldn't find a list of images to train concept. Add image files to the list...")
      return
    prt(Installing("Downloading DreamBooth Conceptualizers"))
    diffusers_dir = os.path.join(root_dir, "diffusers")
    if not os.path.exists(diffusers_dir):
      os.chdir(root_dir)
      run_process("git clone https://github.com/Skquark/diffusers.git", realtime=False, cwd=root_dir)
    os.chdir(diffusers_dir)
    #run_process('pip install -e ".[training]"', cwd=diffusers_dir, realtime=False)
    run_process('pip install "git+https://github.com/Skquark/diffusers.git#egg=diffusers[training]"', cwd=root_dir, realtime=False)
    dreambooth_dir = os.path.join(diffusers_dir, "examples", "dreambooth")
    os.chdir(dreambooth_dir)
    run_process("pip install -r requirements.txt", cwd=dreambooth_dir, realtime=False)
    try:
      os.environ['LD_LIBRARY_PATH'] += "/usr/lib/wsl/lib:$LD_LIBRARY_PATH"
      import bitsandbytes
    except ModuleNotFoundError:
      if sys.platform.startswith("win"):
          run_sp("pip install bitsandbytes-windows", realtime=False)
      else:
          run_sp("pip install bitsandbytes", realtime=False)
      import bitsandbytes
      pass
    #from accelerate.utils import write_basic_config
    #write_basic_config()
    import argparse
    from io import BytesIO
    #save_path = "./my_concept"
    #if not os.path.exists(save_path):
    #  os.mkdir(save_path)

    clear_pipes()
    clear_last()
    num_new_images = None

    from argparse import Namespace
    dreambooth_args = Namespace(
        pretrained_model_name_or_path=model_path,
        resolution=dreambooth_prefs['max_size'],
        center_crop=True,
        instance_data_dir=save_path,
        instance_prompt=dreambooth_prefs['instance_prompt'].strip(),
        learning_rate=dreambooth_prefs['learning_rate'],#5e-06,
        max_train_steps=dreambooth_prefs['max_train_steps'],#450,
        train_batch_size=dreambooth_prefs['train_batch_size'],
        gradient_accumulation_steps=2,
        max_grad_norm=1.0,
        #mixed_precision="no", # set to "fp16" for mixed-precision training.
        gradient_checkpointing=True, # set this to True to lower the memory usage.
        use_8bit_adam=not prefs['higher_vram_mode'], # use 8bit optimizer from bitsandbytes
        enable_xformers_memory_efficient_attention = status['installed_xformers'],
        seed=dreambooth_prefs['seed'],#3434554,
        with_prior_preservation=dreambooth_prefs['prior_preservation'],
        prior_loss_weight=dreambooth_prefs['prior_loss_weight'],
        sample_batch_size=dreambooth_prefs['sample_batch_size'],
        class_data_dir=dreambooth_prefs['prior_preservation_class_folder'],
        class_prompt=dreambooth_prefs['prior_preservation_class_prompt'],
        num_class_images=dreambooth_prefs['num_class_images'],
        output_dir=os.path.join(root_dir, "dreambooth-concept"),
    )
    if not os.path.exists(dreambooth_args.output_dir): os.mkdir(dreambooth_args.output_dir)
    arg_str = "accelerate launch ./train_dreambooth.py"
    for k, v in vars(dreambooth_args).items():
      if isinstance(v, str):
        if ' ' in v:
          v = f'"{v}"'
      if isinstance(v, bool):
        if bool(v):
          arg_str += f" --{k}"
      else:
        arg_str += f" --{k}={v}"
    prt("***** Running training *****")
    #if num_new_images != None: prt(f"  Number of class images to sample: {num_new_images}.")
    #prt(f"  Instantaneous batch size per device = {dreambooth_args.train_batch_size}")
    #prt(f"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}")
    #prt(f"  Gradient Accumulation steps = {dreambooth_args.gradient_accumulation_steps}")
    #prt(f"  Total optimization steps = {dreambooth_args.max_train_steps}")
    prt(arg_str)
    progress = ProgressBar(bar_height=8)
    prt(progress)
    try:
      run_process(arg_str, page=page, cwd=dreambooth_dir)
      #run_sp(arg_str, cwd=dreambooth_dir)
    except Exception as e:
      clear_last()
      alert_msg(page, f"ERROR: CUDA Ran Out of Memory. Try reducing parameters and try again...", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip())]))
      with torch.no_grad():
        torch.cuda.empty_cache()
      return
    clear_last()
    name_of_your_concept = dreambooth_prefs['name_of_your_concept']
    if(dreambooth_prefs['save_concept']):
      from huggingface_hub import HfApi, HfFolder, CommitOperationAdd
      from huggingface_hub import create_repo
      from diffusers import StableDiffusionPipeline
      api = HfApi()
      your_username = api.whoami()["name"]
      dreambooth_pipe = StableDiffusionPipeline.from_pretrained(
        dreambooth_args.output_dir,
        torch_dtype=torch.float16,
      ).to("cuda")
      os.makedirs("fp16_model",exist_ok=True)
      dreambooth_pipe.save_pretrained("fp16_model")
      hf_token = prefs['HuggingFace_api_key']
      if(dreambooth_prefs['where_to_save_concept'] == "Public Library"):
        repo_id = f"sd-dreambooth-library/{format_filename(name_of_your_concept, use_dash=True)}"
        #Join the Concepts Library organization if you aren't part of it already
        run_sp(f"curl -X POST -H 'Authorization: Bearer '{hf_token} -H 'Content-Type: application/json' https://huggingface.co/organizations/sd-dreambooth-library/share/SSeOwppVCscfTEzFGQaqpfcjukVeNrKNHX", realtime=False)
        #!curl -X POST -H 'Authorization: Bearer '$hf_token -H 'Content-Type: application/json' https://huggingface.co/organizations/sd-dreambooth-library/share/SSeOwppVCscfTEzFGQaqpfcjukVeNrKNHX
      else:
        repo_id = f"{your_username}/{format_filename(name_of_your_concept, use_dash=True)}"
      output_dir = dreambooth_args.output_dir
      if(not prefs['HuggingFace_api_key']):
        with open(HfFolder.path_token, 'r') as fin: hf_token = fin.read();
      else:
        hf_token = prefs['HuggingFace_api_key']

      images_upload = os.listdir(save_path)
      image_string = ""
      #repo_id = f"sd-dreambooth-library/{slugify(name_of_your_concept)}"
      for i, image in enumerate(images_upload):
          image_string = f'''{image_string}![image {i}](https://huggingface.co/{repo_id}/resolve/main/concept_images/{image})
'''
      description = dreambooth_prefs['readme_description']
      if bool(description.strip()):
        description = dreambooth_prefs['readme_description'] + '\n\n'
      readme_text = f'''---
license: mit
---
### {name_of_your_concept} on Stable Diffusion via Dreambooth using [Stable Diffusion Deluxe](https://colab.research.google.com/github/Skquark/AI-Friends/blob/main/Stable_Diffusion_Deluxe.ipynb)
#### model by {api.whoami()["name"]}
This your the Stable Diffusion model fine-tuned the {name_of_your_concept} concept taught to Stable Diffusion with Dreambooth.
It can be used by modifying the `instance_prompt`: **{dreambooth_prefs['instance_prompt']}**

{description}You can also train your own concepts and upload them to the library by using [this notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_dreambooth_training.ipynb).
And you can run your new concept via `diffusers`: [Colab Notebook for Inference](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_dreambooth_inference.ipynb), [Spaces with the Public Concepts loaded](https://huggingface.co/spaces/sd-dreambooth-library/stable-diffusion-dreambooth-concepts)

Here are the images used for training this concept:
{image_string}
'''
      #Save the readme to a file
      readme_file = open("README.md", "w")
      readme_file.write(readme_text)
      readme_file.close()
      #Save the token identifier to a file
      text_file = open("token_identifier.txt", "w")
      text_file.write(dreambooth_prefs['instance_prompt'])
      text_file.close()
      operations = [
        CommitOperationAdd(path_in_repo="token_identifier.txt", path_or_fileobj="token_identifier.txt"),
        CommitOperationAdd(path_in_repo="README.md", path_or_fileobj="README.md"),
      ]
      print(repo_id)
      print(readme_text)
      try:
        create_repo(repo_id, private=True, token=hf_token)
      except Exception as e:
        alert_msg(page, f"ERROR Creating repo {repo_id}... Make sure your HF token has Write access.", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip(), selectable=True)]))
        return
      api.create_commit(repo_id=repo_id, operations=operations, commit_message=f"Upload the concept {name_of_your_concept} embeds and token",token=hf_token)
      api.upload_folder(folder_path="fp16_model", path_in_repo="", repo_id=repo_id,token=hf_token)
      api.upload_folder(folder_path=save_path, path_in_repo="concept_images", repo_id=repo_id, token=hf_token)
      prefs['custom_model'] = repo_id
      prefs['custom_models'].append({'name': name_of_your_concept, 'path':repo_id})
      page.custom_model.value = repo_id
      try:
        page.custom_model.update()
      except Exception: pass
      prt(Markdown(f"## Your concept was saved successfully to _{repo_id}_.<br>[Click here to access it](https://huggingface.co/{repo_id}) and go to _Installers->Model Checkpoint->Custom Model Path_ to use. Include Token in prompts.", on_tap_link=lambda e: e.page.launch_url(e.data)))
    if prefs['enable_sounds']: page.snd_alert.play()

def run_textualinversion(page):
    global textualinversion_prefs, prefs
    def prt(line):
      if type(line) == str:
        line = Text(line)
      page.textualinversion_output.controls.append(line)
      page.textualinversion_output.update()
    def clear_last():
      del page.textualinversion_output.controls[-1]
      page.textualinversion_output.update()
    if not status['installed_diffusers']:
      alert_msg(page, "You must Install the HuggingFace Diffusers Library first... ")
      return
    save_path = os.path.join(root_dir, "my_concept")
    error = False
    if not os.path.exists(save_path):
      error = True
    elif len(os.listdir(save_path)) == 0:
      error = True
    if len(page.ti_file_list.controls) == 0:
      error = True
    if error:
      alert_msg(page, "Couldn't find a list of images to train concept. Add image files to the list...")
      return
    page.textualinversion_output.controls.clear()
    page.textualinversion_output.update()
    prt(Installing("Downloading Textual-Inversion Training Models"))

    placeholder_token = textualinversion_prefs['placeholder_token'].strip()
    if not placeholder_token.startswith('<'): placeholder_token = '<' + placeholder_token
    if not placeholder_token.endswith('>'): placeholder_token = placeholder_token + '>'
    initializer_token = textualinversion_prefs['initializer_token'].strip()
    if bool(initializer_token):
      if not initializer_token.startswith('<'): initializer_token = '<' + initializer_token
      if not initializer_token.endswith('>'): initializer_token = initializer_token + '>'
    diffusers_dir = os.path.join(root_dir, "diffusers")
    if not os.path.exists(diffusers_dir):
      os.chdir(root_dir)
      run_process("git clone https://github.com/Skquark/diffusers.git", realtime=False, cwd=root_dir)
    run_process('pip install git+https://github.com/Skquark/diffusers.git#egg=diffusers[training]', cwd=root_dir, realtime=False)
    os.chdir(diffusers_dir)
    run_sp('pip install -e ".[training]"', cwd=diffusers_dir, realtime=False)
    textualinversion_dir = os.path.join(diffusers_dir, "examples", "textual_inversion")
    #textualinversion_dir = os.path.join(diffusers_dir, "examples", "text_to_image")
    os.chdir(textualinversion_dir)
    run_sp("pip install -r requirements.txt", cwd=textualinversion_dir, realtime=False)
    run_process("pip install -qq bitsandbytes", page=page)
    run_sp("accelerate config default", realtime=False)
    #from accelerate.utils import write_basic_config
    #write_basic_config()
    import argparse
    from io import BytesIO

    clear_pipes()
    clear_last()
    #num_new_images = None
    random_seed = int(textualinversion_prefs['seed']) if int(textualinversion_prefs['seed']) > 0 else rnd.randint(0,4294967295)
    name_of_your_model = textualinversion_prefs['name_of_your_model']
    from argparse import Namespace
    textualinversion_args = Namespace(
        pretrained_model_name_or_path=model_path,
        resolution=textualinversion_prefs['resolution'],
        center_crop=True,
        #train_data_dir=save_path,
        #caption_column=textualinversion_prefs['instance_prompt'].strip(),
        train_data_dir=save_path,
        validation_prompt=textualinversion_prefs['validation_prompt'].strip(),
        placeholder_token=placeholder_token,
        initializer_token=initializer_token,
        learnable_property=textualinversion_prefs['what_to_teach'],
        learning_rate=textualinversion_prefs['learning_rate'],#5e-06,'
        lr_scheduler=textualinversion_prefs['lr_scheduler'],
        lr_warmup_steps=textualinversion_prefs['lr_warmup_steps'],
        scale_lr=textualinversion_prefs['scale_lr'],
        max_train_steps=textualinversion_prefs['max_train_steps'],#450,
        train_batch_size=textualinversion_prefs['train_batch_size'],
        checkpointing_steps=textualinversion_prefs['checkpointing_steps'],
        gradient_accumulation_steps=textualinversion_prefs['gradient_accumulation_steps'],
        validation_steps=textualinversion_prefs['validation_steps'],
        num_vectors=textualinversion_prefs['num_vectors'],
        #mixed_precision="no", # set to "fp16" for mixed-precision training.
        gradient_checkpointing=True, # set this to True to lower the memory usage.
        use_8bit_adam=not prefs['higher_vram_mode'], # use 8bit optimizer from bitsandbytes
        enable_xformers_memory_efficient_attention = status['installed_xformers'],
        seed=random_seed,
        #with_prior_preservation=textualinversion_prefs['prior_preservation'],
        #prior_loss_weight=textualinversion_prefs['prior_loss_weight'],
        #sample_batch_size=textualinversion_prefs['sample_batch_size'],
        #class_data_dir=textualinversion_prefs['class_data_dir'],
        #class_prompt=textualinversion_prefs['class_prompt'],
        num_class_images=textualinversion_prefs['num_class_images'],
        output_dir=os.path.join(save_path, format_filename(textualinversion_prefs['name_of_your_concept'], use_dash=True)),
    )
    output_dir = textualinversion_args.output_dir
    arg_str = "accelerate launch textual_inversion.py"
    #arg_str = 'accelerate --mixed_precision="fp16" launch train_text_to_image_lora.py'
    for k, v in vars(textualinversion_args).items():
      if isinstance(v, str):
        if ' ' in v:
          v = f'"{v}"'
      if isinstance(v, bool):
        if bool(v):
          arg_str += f" --{k}"
      else:
        arg_str += f" --{k}={v}"
    prt(Text("*** Running Training *** See Console for Progress", weight=FontWeight.BOLD))
    #if num_new_images != None: prt(f"  Number of class images to sample: {num_new_images}.")
    #prt(f"  Instantaneous batch size per device = {textualinversion_args.train_batch_size}")
    #prt(f"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}")
    #prt(f"  Gradient Accumulation steps = {textualinversion_args.gradient_accumulation_steps}")
    #prt(f"  Total optimization steps = {textualinversion_args.max_train_steps}")
    prt(arg_str)
    progress = ProgressBar(bar_height=8)
    prt(progress)
    if(textualinversion_prefs['save_model']):
      from huggingface_hub import HfApi, HfFolder, CommitOperationAdd
      from huggingface_hub import Repository, create_repo, whoami
      #from diffusers import StableDiffusionPipeline
      api = HfApi()
      your_username = api.whoami()["name"]
      #textualinversion_pipe = StableDiffusionPipeline.from_pretrained(
      #  textualinversion_args.output_dir,
      #  torch_dtype=torch.float16,
      #).to("cuda")
      #os.makedirs("fp16_model",exist_ok=True)
      #textualinversion_pipe.save_pretrained("fp16_model")
      hf_token = prefs['HuggingFace_api_key']
      private = False if textualinversion_prefs['where_to_save_model'] == "Public HuggingFace" else True
      repo_id = f"{your_username}/{format_filename(name_of_your_model, use_dash=True)}"
      output_dir = textualinversion_args.output_dir
      if(not prefs['HuggingFace_api_key']):
        with open(HfFolder.path_token, 'r') as fin: hf_token = fin.read();
      else:
        hf_token = prefs['HuggingFace_api_key']
      try:
        create_repo(repo_id, private=private, exist_ok=True, token=hf_token)
        repo = Repository(output_dir, clone_from=repo_id, token=hf_token)
      except Exception as e:
        alert_msg(page, f"ERROR Creating repo {repo_id}... Make sure your HF token has Write access.", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip(), selectable=True)]))
        return
    else:
      if not os.path.exists(output_dir): os.makedirs(output_dir, exist_ok=True)

    try:
      os.chdir(textualinversion_dir)
      #!accelerate $arg_str # type: ignore
      os.system(arg_str)
      #run_sp(arg_str, cwd=textualinversion_dir, realtime=True)
      #run_sp(arg_str, cwd=textualinversion_dir)
    except Exception as e:
      clear_last()
      alert_msg(page, f"ERROR: Out of Memory (or something else). Try reducing parameters and try again...", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip())]))
      with torch.no_grad():
        torch.cuda.empty_cache()
      return
    clear_last()

    #title Save your newly created concept to the [library of concepts](https://huggingface.co/sd-concepts-library)?
    save_concept_to_public_library = textualinversion_prefs['save_concept']
    name_of_your_concept = textualinversion_prefs['name_of_your_concept']
    # `hf_token_write`: leave blank if you logged in with a token with `write access` in the [Initial Setup](#scrollTo=KbzZ9xe6dWwf). If not, [go to your tokens settings and create a write access token](https://huggingface.co/settings/tokens)
    hf_token_write = prefs['HuggingFace_api_key']

    if(save_concept_to_public_library):
        from huggingface_hub import HfApi, HfFolder, CommitOperationAdd
        from huggingface_hub import create_repo
        api = HfApi()
        your_username = api.whoami()["name"]
        repo_id = f"sd-concepts-library/{format_filename(name_of_your_concept, use_dash=True)}"
        #output_dir = textualinversion_prefs["output_dir"]
        if(not hf_token_write):
            with open(HfFolder.path_token, 'r') as fin: hf_token = fin.read();
        else:
            hf_token = hf_token_write
        if(textualinversion_prefs['where_to_save_concept'] == "Public Library"):
            #Join the Concepts Library organization if you aren't part of it already
            run_sp(f"curl -X POST -H 'Authorization: Bearer '{hf_token} -H 'Content-Type: application/json' https://huggingface.co/organizations/sd-concepts-library/share/VcLXJtzwwxnHYCkNMLpSJCdnNFZHQwWywv", realtime=False)
            # curl -X POST -H 'Authorization: Bearer '$hf_token -H 'Content-Type: application/json' https://huggingface.co/organizations/sd-concepts-library/share/VcLXJtzwwxnHYCkNMLpSJCdnNFZHQwWywv
        else:
            repo_id = f"{your_username}/{format_filename(name_of_your_concept, use_dash=True)}"
        images_upload = os.listdir(save_path)
        image_string = ""
        repo_id = f"sd-concepts-library/{format_filename(name_of_your_concept, use_dash=True)}"
        for i, image in enumerate(images_upload):
            image_string = f'''{image_string}![{placeholder_token} {i}](https://huggingface.co/{repo_id}/resolve/main/concept_images/{image})
        '''
        if(textualinversion_prefs['what_to_teach'] == "style"):
            what_to_teach_article = f"a `{textualinversion_prefs['what_to_teach']}`"
        else:
            what_to_teach_article = f"an `{textualinversion_prefs['what_to_teach']}`"
        description = textualinversion_prefs['readme_description']
        if bool(description.strip()):
            description = textualinversion_prefs['readme_description'] + '\n\n'
        readme_text = f'''---
license: mit
base_model: {model_path}
tags:
- stable-diffusion
- stable-diffusion-diffusers
- text-to-image
- diffusers
- textual_inversion
inference: true
---
# Textual inversion text2image fine-tuning - {repo_id}
### {name_of_your_concept} by {your_username} using [Stable Diffusion Deluxe](https://colab.research.google.com/github/Skquark/AI-Friends/blob/main/Stable_Diffusion_Deluxe.ipynb)
This is the `{placeholder_token}` concept taught to Stable Diffusion via Textual Inversion. You can load this concept into the [Stable Diffusion Deluxe](https://colab.research.google.com/github/Skquark/AI-Friends/blob/main/Stable_Diffusion_Deluxe.ipynb) notebook. You can also train your own concepts and load them into the concept libraries there too, or using [this notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_textual_inversion_training.ipynb).

{description}Here is the new concept you will be able to use as {what_to_teach_article}:
{image_string}
'''
        #Save the readme to a file
        readme_file = open("README.md", "w")
        readme_file.write(readme_text)
        readme_file.close()
        #Save the token identifier to a file
        text_file = open("token_identifier.txt", "w")
        text_file.write(placeholder_token)
        text_file.close()
        #Save the type of teached thing to a file
        type_file = open("type_of_concept.txt","w")
        type_file.write(textualinversion_prefs['what_to_teach'])
        type_file.close()
        operations = [
            CommitOperationAdd(path_in_repo="learned_embeds.bin", path_or_fileobj=f"{output_dir}/learned_embeds.bin"),
            CommitOperationAdd(path_in_repo="token_identifier.txt", path_or_fileobj="token_identifier.txt"),
            CommitOperationAdd(path_in_repo="type_of_concept.txt", path_or_fileobj="type_of_concept.txt"),
            CommitOperationAdd(path_in_repo="README.md", path_or_fileobj="README.md"),
        ]
        try:
          create_repo(repo_id, private=True, token=hf_token)
        except Exception as e:
          alert_msg(page, f"ERROR Creating repo {repo_id}... Make sure your HF token has Write access.", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip(), selectable=True)]))
          return
        api = HfApi()
        api.create_commit(
            repo_id=repo_id,
            operations=operations,
            commit_message=f"Upload the concept {name_of_your_concept} embeds and token",
            token=hf_token
        )
        api.upload_folder(
            folder_path=save_path,
            path_in_repo="concept_images",
            repo_id=repo_id,
            token=hf_token
        )
        prefs['custom_model'] = repo_id
        prefs['custom_models'].append({'name': name_of_your_concept, 'path':repo_id})
        page.custom_model.value = repo_id
        try:
          page.custom_model.update()
        except Exception: pass
        prt(Markdown(f"## Your concept was saved successfully to _{repo_id}_.<br>[Click here to access it](https://huggingface.co/{repo_id}) and go to _Installers->Model Checkpoint->Custom Model Path_ to use. Include Token to your Prompt text.", on_tap_link=lambda e: e.page.launch_url(e.data)))
    if prefs['enable_sounds']: page.snd_alert.play()

def run_textualinversion2(page):
    global textualinversion_prefs, prefs
    def prt(line):
      if type(line) == str:
        line = Text(line)
      page.textualinversion_output.controls.append(line)
      page.textualinversion_output.update()
    def clear_last():
      del page.textualinversion_output.controls[-1]
      page.textualinversion_output.update()
    if not status['installed_diffusers']:
      alert_msg(page, "You must Install the HuggingFace Diffusers Library first... ")
      return
    save_path = os.path.join(root_dir, "my_concept")
    error = False
    if not os.path.exists(save_path):
      error = True
    elif len(os.listdir(save_path)) == 0:
      error = True
    if len(page.ti_file_list.controls) == 0:
      error = True
    if error:
      alert_msg(page, "Couldn't find a list of images to train concept. Add image files to the list...")
      return
    prt(Installing("Downloading Textual-Inversion Training Models"))
    #run_process("pip install -qq bitsandbytes")
    import argparse
    import itertools
    import math
    import random

    import torch.nn.functional as F
    import torch.utils.checkpoint
    from torch.utils.data import Dataset
    from accelerate import Accelerator
    from accelerate.logging import get_logger
    from accelerate.utils import set_seed
    from diffusers import AutoencoderKL, DDPMScheduler, PNDMScheduler, StableDiffusionPipeline, UNet2DConditionModel
    #from diffusers.hub_utils import init_git_repo, push_to_hub
    from diffusers.optimization import get_scheduler
    from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker
    from torchvision import transforms
    from tqdm.auto import tqdm
    from transformers import CLIPFeatureExtractor, CLIPTextModel, CLIPTokenizer

    imagenet_templates_small = [
        "a photo of a {}",
        "a rendering of a {}",
        "a cropped photo of the {}",
        "the photo of a {}",
        "a photo of a clean {}",
        "a photo of a dirty {}",
        "a dark photo of the {}",
        "a photo of my {}",
        "a photo of the cool {}",
        "a close-up photo of a {}",
        "a bright photo of the {}",
        "a cropped photo of a {}",
        "a photo of the {}",
        "a good photo of the {}",
        "a photo of one {}",
        "a close-up photo of the {}",
        "a rendition of the {}",
        "a photo of the clean {}",
        "a rendition of a {}",
        "a photo of a nice {}",
        "a good photo of a {}",
        "a photo of the nice {}",
        "a photo of the small {}",
        "a photo of the weird {}",
        "a photo of the large {}",
        "a photo of a cool {}",
        "a photo of a small {}",
    ]

    imagenet_style_templates_small = [
        "a painting in the style of {}",
        "a rendering in the style of {}",
        "a cropped painting in the style of {}",
        "the painting in the style of {}",
        "a clean painting in the style of {}",
        "a dirty painting in the style of {}",
        "a dark painting in the style of {}",
        "a picture in the style of {}",
        "a cool painting in the style of {}",
        "a close-up painting in the style of {}",
        "a bright painting in the style of {}",
        "a cropped painting in the style of {}",
        "a good painting in the style of {}",
        "a close-up painting in the style of {}",
        "a rendition in the style of {}",
        "a nice painting in the style of {}",
        "a small painting in the style of {}",
        "a weird painting in the style of {}",
        "a large painting in the style of {}",
    ]
    tokenizer = CLIPTokenizer.from_pretrained(
        model_path,
        subfolder="tokenizer",
    )
    placeholder_token = textualinversion_prefs['placeholder_token'].strip()
    if not placeholder_token.startswith('<'): placeholder_token = '<' + placeholder_token
    if not placeholder_token.endswith('>'): placeholder_token = placeholder_token + '>'
    # Add the placeholder token in tokenizer
    num_added_tokens = tokenizer.add_tokens(placeholder_token)
    if num_added_tokens == 0:
        raise ValueError(
            f"The tokenizer already contains the token {placeholder_token}. Please pass a different"
            " `placeholder_token` that is not already in the tokenizer."
        )

    token_ids = tokenizer.encode(textualinversion_prefs['initializer_token'], add_special_tokens=False)
    # Check if initializer_token is a single token or a sequence of tokens
    if len(token_ids) > 1:
        raise ValueError("The initializer token must be a single token.")

    initializer_token_id = token_ids[0]
    placeholder_token_id = tokenizer.convert_tokens_to_ids(placeholder_token)

    # Load the Stable Diffusion model
    # Load models and create wrapper for stable diffusion
    text_encoder = CLIPTextModel.from_pretrained(
        model_path, subfolder="text_encoder"
    )
    vae = AutoencoderKL.from_pretrained(
        model_path, subfolder="vae"
    )
    unet = UNet2DConditionModel.from_pretrained(
        model_path, subfolder="unet"
    )

    text_encoder.resize_token_embeddings(len(tokenizer))
    token_embeds = text_encoder.get_input_embeddings().weight.data
    token_embeds[placeholder_token_id] = token_embeds[initializer_token_id]

    def freeze_params(params):
        for param in params:
            param.requires_grad = False

    # Freeze vae and unet
    freeze_params(vae.parameters())
    freeze_params(unet.parameters())
    # Freeze all parameters except for the token embeddings in text encoder
    params_to_freeze = itertools.chain(
        text_encoder.text_model.encoder.parameters(),
        text_encoder.text_model.final_layer_norm.parameters(),
        text_encoder.text_model.embeddings.position_embedding.parameters(),
    )
    freeze_params(params_to_freeze)
    train_dataset = TextualInversionDataset(
        data_root=save_path,
        tokenizer=tokenizer,
        size=512,
        placeholder_token=placeholder_token,
        repeats=100,
        learnable_property=textualinversion_prefs['what_to_teach'], #Option selected above between object and style
        center_crop=False,
        set="train",
    )
    def create_dataloader(train_batch_size=1):
        return torch.utils.data.DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)
    noise_scheduler = DDPMScheduler.from_pretrained(model_path, subfolder="scheduler")
    #noise_scheduler = DDPMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule="scaled_linear", num_train_timesteps=1000, tensor_format="pt")
    def training_function(text_encoder, vae, unet):
        logger = get_logger(__name__)

        train_batch_size = textualinversion_prefs["train_batch_size"]
        gradient_accumulation_steps = textualinversion_prefs["gradient_accumulation_steps"]
        learning_rate = textualinversion_prefs["learning_rate"]
        max_train_steps = textualinversion_prefs["max_train_steps"]
        output_dir = textualinversion_prefs["output_dir"]
        accelerator = Accelerator(gradient_accumulation_steps=gradient_accumulation_steps)
        train_dataloader = create_dataloader(train_batch_size)
        if textualinversion_prefs["scale_lr"]:
            learning_rate = (learning_rate * gradient_accumulation_steps * train_batch_size * accelerator.num_processes)
        # Initialize the optimizer
        optimizer = torch.optim.AdamW(
            text_encoder.get_input_embeddings().parameters(),  # only optimize the embeddings
            lr=learning_rate,
        )
        text_encoder, optimizer, train_dataloader = accelerator.prepare(text_encoder, optimizer, train_dataloader)
        vae.to(accelerator.device)
        unet.to(accelerator.device)
        vae.eval()
        unet.eval()
        # We need to recalculate our total training steps as the size of the training dataloader may have changed.
        num_update_steps_per_epoch = math.ceil(len(train_dataloader) / gradient_accumulation_steps)
        num_train_epochs = math.ceil(max_train_steps / num_update_steps_per_epoch)
        clear_last()
        # Train!
        total_batch_size = train_batch_size * accelerator.num_processes * gradient_accumulation_steps

        prt("***** Running training *****")
        prt(f"  Num examples = {len(train_dataset)}")
        prt(f"  Instantaneous batch size per device = {train_batch_size}")
        prt(f"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}")
        prt(f"  Gradient Accumulation steps = {gradient_accumulation_steps}")
        prt(f"  Total optimization steps = {max_train_steps}")
        progress = ProgressBar(bar_height=8)
        prt(progress)
        progress_bar = tqdm(range(max_train_steps), disable=not accelerator.is_local_main_process)
        progress_bar.set_description("Steps")
        global_step = 0

        for epoch in range(num_train_epochs):
            text_encoder.train()
            for step, batch in enumerate(train_dataloader):
                with accelerator.accumulate(text_encoder):
                    # Convert images to latent space
                    latents = vae.encode(batch["pixel_values"]).latent_dist.sample().detach()
                    latents = latents * 0.18215

                    # Sample noise that we'll add to the latents
                    noise = torch.randn(latents.shape).to(latents.device)
                    bsz = latents.shape[0]
                    # Sample a random timestep for each image
                    timesteps = torch.randint(0, noise_scheduler.num_train_timesteps, (bsz,), device=latents.device).long()

                    # Add noise to the latents according to the noise magnitude at each timestep
                    # (this is the forward diffusion process)
                    noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)

                    # Get the text embedding for conditioning
                    encoder_hidden_states = text_encoder(batch["input_ids"])[0]

                    # Predict the noise residual
                    noise_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample

                    loss = F.mse_loss(noise_pred, noise, reduction="none").mean([1, 2, 3]).mean()
                    accelerator.backward(loss)

                    # Zero out the gradients for all token embeddings except the newly added
                    # embeddings for the concept, as we only want to optimize the concept embeddings
                    if accelerator.num_processes > 1:
                        grads = text_encoder.module.get_input_embeddings().weight.grad
                    else:
                        grads = text_encoder.get_input_embeddings().weight.grad
                    # Get the index for tokens that we want to zero the grads for
                    index_grads_to_zero = torch.arange(len(tokenizer)) != placeholder_token_id
                    grads.data[index_grads_to_zero, :] = grads.data[index_grads_to_zero, :].fill_(0)

                    optimizer.step()
                    optimizer.zero_grad()

                # Checks if the accelerator has performed an optimization step behind the scenes
                if accelerator.sync_gradients:
                    progress_bar.update(1)
                    global_step += 1
                    progress.value = (global_step + 1) / max_train_steps
                    progress.tooltip = f'[{(global_step + 1)} / {max_train_steps}]'
                    progress.update()

                logs = {"loss": loss.detach().item()}
                progress_bar.set_postfix(**logs)

                if global_step >= max_train_steps:
                    break

            accelerator.wait_for_everyone()


        # Create the pipeline using using the trained modules and save it.
        if accelerator.is_main_process:
            pipeline = StableDiffusionPipeline(
                text_encoder=accelerator.unwrap_model(text_encoder),
                vae=vae,
                unet=unet,
                tokenizer=tokenizer,
                scheduler=PNDMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule="scaled_linear", skip_prk_steps=True),
                feature_extractor=CLIPFeatureExtractor.from_pretrained("openai/clip-vit-base-patch32"),
                **safety,
            )
            pipeline.save_pretrained(output_dir)
            # Also save the newly trained embeddings
            learned_embeds = accelerator.unwrap_model(text_encoder).get_input_embeddings().weight[placeholder_token_id]
            learned_embeds_dict = {placeholder_token: learned_embeds.detach().cpu()}
            torch.save(learned_embeds_dict, os.path.join(output_dir, "learned_embeds.bin"))

    import accelerate
    try:
        accelerate.notebook_launcher(training_function, args=(text_encoder, vae, unet))
    except Exception as e:
      clear_last()
      alert_msg(page, "ERROR: CUDA Ran Out of Memory. Try reducing parameters and try again...", content=Text(str(e)))
      with torch.no_grad():
        torch.cuda.empty_cache()
      return
    clear_last()
    #title Save your newly created concept to the [library of concepts](https://huggingface.co/sd-concepts-library)?
    save_concept_to_public_library = textualinversion_prefs['save_concept']
    name_of_your_concept = textualinversion_prefs['name_of_your_concept']
    # `hf_token_write`: leave blank if you logged in with a token with `write access` in the [Initial Setup](#scrollTo=KbzZ9xe6dWwf). If not, [go to your tokens settings and create a write access token](https://huggingface.co/settings/tokens)
    hf_token_write = prefs['HuggingFace_api_key']

    if(save_concept_to_public_library):
        from huggingface_hub import HfApi, HfFolder, CommitOperationAdd
        from huggingface_hub import create_repo
        api = HfApi()
        your_username = api.whoami()["name"]
        repo_id = f"sd-concepts-library/{format_filename(name_of_your_concept, use_dash=True)}"
        output_dir = textualinversion_prefs["output_dir"]
        if(not hf_token_write):
            with open(HfFolder.path_token, 'r') as fin: hf_token = fin.read();
        else:
            hf_token = hf_token_write
        if(textualinversion_prefs['where_to_save_concept'] == "Public Library"):
            #Join the Concepts Library organization if you aren't part of it already
            run_sp(f"curl -X POST -H 'Authorization: Bearer '{hf_token} -H 'Content-Type: application/json' https://huggingface.co/organizations/sd-concepts-library/share/VcLXJtzwwxnHYCkNMLpSJCdnNFZHQwWywv", realtime=False)
            # curl -X POST -H 'Authorization: Bearer '$hf_token -H 'Content-Type: application/json' https://huggingface.co/organizations/sd-concepts-library/share/VcLXJtzwwxnHYCkNMLpSJCdnNFZHQwWywv
        else:
            repo_id = f"{your_username}/{format_filename(name_of_your_concept, use_dash=True)}"
        images_upload = os.listdir(save_path)
        image_string = ""
        repo_id = f"sd-concepts-library/{format_filename(name_of_your_concept, use_dash=True)}"
        for i, image in enumerate(images_upload):
            image_string = f'''{image_string}![{placeholder_token} {i}](https://huggingface.co/{repo_id}/resolve/main/concept_images/{image})
        '''
        if(textualinversion_prefs['what_to_teach'] == "style"):
            what_to_teach_article = f"a `{textualinversion_prefs['what_to_teach']}`"
        else:
            what_to_teach_article = f"an `{textualinversion_prefs['what_to_teach']}`"
        description = textualinversion_prefs['readme_description']
        if bool(description.strip()):
            description = textualinversion_prefs['readme_description'] + '\n\n'
        readme_text = f'''---
license: mit
---
### {name_of_your_concept} by {your_username} using [Stable Diffusion Deluxe](https://colab.research.google.com/github/Skquark/AI-Friends/blob/main/Stable_Diffusion_Deluxe.ipynb)
This is the `{placeholder_token}` concept taught to Stable Diffusion via Textual Inversion. You can load this concept into the [Stable Diffusion Deluxe](https://colab.research.google.com/github/Skquark/AI-Friends/blob/main/Stable_Diffusion_Deluxe.ipynb) notebook. You can also train your own concepts and load them into the concept libraries there too, or using [this notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_textual_inversion_training.ipynb).

{description}Here is the new concept you will be able to use as {what_to_teach_article}:
{image_string}
'''
        #Save the readme to a file
        readme_file = open("README.md", "w")
        readme_file.write(readme_text)
        readme_file.close()
        #Save the token identifier to a file
        text_file = open("token_identifier.txt", "w")
        text_file.write(placeholder_token)
        text_file.close()
        #Save the type of teached thing to a file
        type_file = open("type_of_concept.txt","w")
        type_file.write(textualinversion_prefs['what_to_teach'])
        type_file.close()
        operations = [
            CommitOperationAdd(path_in_repo="learned_embeds.bin", path_or_fileobj=f"{output_dir}/learned_embeds.bin"),
            CommitOperationAdd(path_in_repo="token_identifier.txt", path_or_fileobj="token_identifier.txt"),
            CommitOperationAdd(path_in_repo="type_of_concept.txt", path_or_fileobj="type_of_concept.txt"),
            CommitOperationAdd(path_in_repo="README.md", path_or_fileobj="README.md"),
        ]
        try:
          create_repo(repo_id, private=True, token=hf_token)
        except Exception as e:
          alert_msg(page, f"ERROR Creating repo {repo_id}... Make sure your HF token has Write access.", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip(), selectable=True)]))
          return
        api = HfApi()
        api.create_commit(
            repo_id=repo_id,
            operations=operations,
            commit_message=f"Upload the concept {name_of_your_concept} embeds and token",
            token=hf_token
        )
        api.upload_folder(
            folder_path=save_path,
            path_in_repo="concept_images",
            repo_id=repo_id,
            token=hf_token
        )
        prefs['custom_model'] = repo_id
        prefs['custom_models'].append({'name': name_of_your_concept, 'path':repo_id})
        page.custom_model.value = repo_id
        try:
          page.custom_model.update()
        except Exception: pass
        prt(Markdown(f"## Your concept was saved successfully to _{repo_id}_.<br>[Click here to access it](https://huggingface.co/{repo_id}) and go to _Installers->Model Checkpoint->Custom Model Path_ to use. Include Token to your Prompt text.", on_tap_link=lambda e: e.page.launch_url(e.data)))
    if prefs['enable_sounds']: page.snd_alert.play()

imagenet_templates_small = [
    "a photo of a {}",
    "a rendering of a {}",
    "a cropped photo of the {}",
    "the photo of a {}",
    "a photo of a clean {}",
    "a photo of a dirty {}",
    "a dark photo of the {}",
    "a photo of my {}",
    "a photo of the cool {}",
    "a close-up photo of a {}",
    "a bright photo of the {}",
    "a cropped photo of a {}",
    "a photo of the {}",
    "a good photo of the {}",
    "a photo of one {}",
    "a close-up photo of the {}",
    "a rendition of the {}",
    "a photo of the clean {}",
    "a rendition of a {}",
    "a photo of a nice {}",
    "a good photo of a {}",
    "a photo of the nice {}",
    "a photo of the small {}",
    "a photo of the weird {}",
    "a photo of the large {}",
    "a photo of a cool {}",
    "a photo of a small {}",
]

imagenet_style_templates_small = [
    "a painting in the style of {}",
    "a rendering in the style of {}",
    "a cropped painting in the style of {}",
    "the painting in the style of {}",
    "a clean painting in the style of {}",
    "a dirty painting in the style of {}",
    "a dark painting in the style of {}",
    "a picture in the style of {}",
    "a cool painting in the style of {}",
    "a close-up painting in the style of {}",
    "a bright painting in the style of {}",
    "a cropped painting in the style of {}",
    "a good painting in the style of {}",
    "a close-up painting in the style of {}",
    "a rendition in the style of {}",
    "a nice painting in the style of {}",
    "a small painting in the style of {}",
    "a weird painting in the style of {}",
    "a large painting in the style of {}",
]

class TextualInversionDataset(Dataset):
    import random as rnd
    def __init__(
        self,
        data_root,
        tokenizer,
        learnable_property="object",  # [object, style]
        size=textualinversion_prefs['max_size'],
        repeats=100,
        interpolation="bicubic",
        flip_p=0.5,
        set="train",
        placeholder_token="*",
        center_crop=False,
    ):
        self.data_root = data_root
        self.tokenizer = tokenizer
        self.learnable_property = learnable_property
        self.size = size
        self.placeholder_token = placeholder_token
        self.center_crop = center_crop
        self.flip_p = flip_p
        self.image_paths = [os.path.join(self.data_root, file_path) for file_path in os.listdir(self.data_root)]
        self.num_images = len(self.image_paths)
        self._length = self.num_images
        if set == "train":
            self._length = self.num_images * repeats
        self.interpolation = {
            "linear": PIL.Image.LINEAR,
            "bilinear": PIL.Image.BILINEAR,
            "bicubic": PIL.Image.BICUBIC,
            "lanczos": PIL.Image.LANCZOS,
        }[interpolation]
        self.templates = imagenet_style_templates_small if learnable_property == "style" else imagenet_templates_small
        self.flip_transform = transforms.RandomHorizontalFlip(p=self.flip_p)

    def __len__(self):
        return self._length

    def __getitem__(self, i):
        import random as rnd
        example = {}
        image = PILImage.open(self.image_paths[i % self.num_images])
        if not image.mode == "RGB":
            image = image.convert("RGB")
        placeholder_string = self.placeholder_token
        text = rnd.choice(self.templates).format(placeholder_string)
        example["input_ids"] = self.tokenizer(
            text,
            padding="max_length",
            truncation=True,
            max_length=self.tokenizer.model_max_length,
            return_tensors="pt",
        ).input_ids[0]
        # default to score-sde preprocessing
        img = np.array(image).astype(np.uint8)
        if self.center_crop:
            crop = min(img.shape[0], img.shape[1])
            h, w, = (
                img.shape[0],
                img.shape[1],
            )
            img = img[(h - crop) // 2 : (h + crop) // 2, (w - crop) // 2 : (w + crop) // 2]
        image = PILImage.fromarray(img)
        image = image.resize((self.size, self.size), resample=self.interpolation)
        image = self.flip_transform(image)
        image = np.array(image).astype(np.uint8)
        image = (image / 127.5 - 1.0).astype(np.float32)
        example["pixel_values"] = torch.from_numpy(image).permute(2, 0, 1)
        return example


def run_LoRA_dreambooth(page):
    global LoRA_dreambooth_prefs, prefs
    def prt(line):
      if type(line) == str:
        line = Text(line)
      page.LoRA_dreambooth_output.controls.append(line)
      page.LoRA_dreambooth_output.update()
    def clear_last():
      del page.LoRA_dreambooth_output.controls[-1]
      page.LoRA_dreambooth_output.update()
    if not status['installed_diffusers']:
      alert_msg(page, "You must Install the HuggingFace Diffusers Library first... ")
      return
    save_path = os.path.join(root_dir, "my_model")
    error = False
    if not os.path.exists(save_path):
      error = True
    elif len(os.listdir(save_path)) == 0:
      error = True
    if len(page.lora_dreambooth_file_list.controls) == 0:
      error = True
    if error:
      alert_msg(page, "Couldn't find a list of images to train model. Add image files to the list...")
      return
    page.LoRA_dreambooth_output.controls.clear()
    page.LoRA_dreambooth_output.update()
    prt(Installing("Downloading LoRA DreamBooth Conceptualizers"))
    diffusers_dir = os.path.join(root_dir, "diffusers")
    if not os.path.exists(diffusers_dir):
      os.chdir(root_dir)
      run_process("git clone https://github.com/Skquark/diffusers.git", realtime=False, cwd=root_dir)
    run_process('pip install git+https://github.com/Skquark/diffusers.git#egg=diffusers[training]', cwd=root_dir, realtime=False)
    os.chdir(diffusers_dir)
    run_sp('pip install -e ".[training]"', cwd=diffusers_dir, realtime=False)
    LoRA_dreambooth_dir = os.path.join(diffusers_dir, "examples", "dreambooth")
    #LoRA_dreambooth_dir = os.path.join(diffusers_dir, "examples", "text_to_image")
    os.chdir(LoRA_dreambooth_dir)
    run_sp("pip install -r requirements.txt", cwd=LoRA_dreambooth_dir, realtime=False)
    run_process("pip install -qq bitsandbytes", page=page)
    run_sp("accelerate config default", realtime=False)
    #from accelerate.utils import write_basic_config
    #write_basic_config()
    import argparse
    from io import BytesIO
    #save_path = "./my_model"
    #if not os.path.exists(save_path):
    #  os.mkdir(save_path)

    clear_pipes()
    clear_last()
    #num_new_images = None
    random_seed = int(LoRA_dreambooth_prefs['seed']) if int(LoRA_dreambooth_prefs['seed']) > 0 else rnd.randint(0,4294967295)
    name_of_your_model = LoRA_dreambooth_prefs['name_of_your_model']
    from argparse import Namespace
    LoRA_dreambooth_args = Namespace(
        pretrained_model_name_or_path=model_path if not LoRA_dreambooth_prefs['use_SDXL'] else "stabilityai/stable-diffusion-xl-base-1.0",
        resolution=LoRA_dreambooth_prefs['resolution'],
        center_crop=True,
        mixed_precision="fp16",
        #train_data_dir=save_path,
        #caption_column=LoRA_dreambooth_prefs['instance_prompt'].strip(),
        instance_data_dir=save_path,
        instance_prompt=LoRA_dreambooth_prefs['instance_prompt'].strip(),
        learning_rate=LoRA_dreambooth_prefs['learning_rate'],#5e-06,'
        lr_scheduler=LoRA_dreambooth_prefs['lr_scheduler'],
        lr_warmup_steps=LoRA_dreambooth_prefs['lr_warmup_steps'],
        lr_num_cycles=LoRA_dreambooth_prefs['lr_num_cycles'],
        lr_power=LoRA_dreambooth_prefs['lr_power'],
        scale_lr=LoRA_dreambooth_prefs['scale_lr'],
        max_train_steps=LoRA_dreambooth_prefs['max_train_steps'],#450,
        train_batch_size=LoRA_dreambooth_prefs['train_batch_size'],
        checkpointing_steps=LoRA_dreambooth_prefs['checkpointing_steps'],
        gradient_accumulation_steps=LoRA_dreambooth_prefs['gradient_accumulation_steps'],
        max_grad_norm=1.0,
        #mixed_precision="no", # set to "fp16" for mixed-precision training.
        gradient_checkpointing=True, # set this to True to lower the memory usage.
        use_8bit_adam=not prefs['higher_vram_mode'], # use 8bit optimizer from bitsandbytes
        enable_xformers_memory_efficient_attention = status['installed_xformers'],
        seed=random_seed,
        with_prior_preservation=LoRA_dreambooth_prefs['prior_preservation'],
        prior_loss_weight=LoRA_dreambooth_prefs['prior_loss_weight'],
        rank=4,#LoRA_dreambooth_prefs['rank'], TODO: "The dimension of the LoRA update matrices."
        sample_batch_size=LoRA_dreambooth_prefs['sample_batch_size'],
        #class_data_dir=LoRA_dreambooth_prefs['class_data_dir'],
        #class_prompt=LoRA_dreambooth_prefs['class_prompt'],
        num_class_images=LoRA_dreambooth_prefs['num_class_images'],
        output_dir=os.path.join(root_dir, "LoRA-model", format_filename(LoRA_dreambooth_prefs['name_of_your_model'], use_dash=True)),
    )
    output_dir = LoRA_dreambooth_args.output_dir
    if not os.path.exists(os.path.join(root_dir, "LoRA-model")): os.makedirs(os.path.join(root_dir, "LoRA-model"))
    if LoRA_dreambooth_prefs['use_SDXL']:
      arg_str = "launch train_dreambooth_lora_sdxl.py"
    else:
      arg_str = "launch train_dreambooth_lora.py"
    #arg_str = 'accelerate --mixed_precision="fp16" launch train_text_to_image_lora.py'
    for k, v in vars(LoRA_dreambooth_args).items():
      if isinstance(v, str):
        if ' ' in v:
          v = f'"{v}"'
      if isinstance(v, bool):
        if bool(v):
          arg_str += f" --{k}"
      else:
        arg_str += f" --{k}={v}"
    prt(Text("*** Running training ***", weight=FontWeight.BOLD))
    #if num_new_images != None: prt(f"  Number of class images to sample: {num_new_images}.")
    #prt(f"  Instantaneous batch size per device = {LoRA_dreambooth_args.train_batch_size}")
    #prt(f"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}")
    #prt(f"  Gradient Accumulation steps = {LoRA_dreambooth_args.gradient_accumulation_steps}")
    #prt(f"  Total optimization steps = {LoRA_dreambooth_args.max_train_steps}")
    prt(arg_str)
    progress = ProgressBar(bar_height=8)
    prt(progress)
    if(LoRA_dreambooth_prefs['save_model']):
      from huggingface_hub import HfApi, HfFolder, CommitOperationAdd
      from huggingface_hub import Repository, create_repo, whoami
      #from diffusers import StableDiffusionPipeline
      api = HfApi()
      your_username = api.whoami()["name"]
      #LoRA_dreambooth_pipe = StableDiffusionPipeline.from_pretrained(
      #  LoRA_dreambooth_args.output_dir,
      #  torch_dtype=torch.float16,
      #).to("cuda")
      #os.makedirs("fp16_model",exist_ok=True)
      #LoRA_dreambooth_pipe.save_pretrained("fp16_model")
      hf_token = prefs['HuggingFace_api_key']
      private = False if LoRA_dreambooth_prefs['where_to_save_model'] == "Public HuggingFace" else True
      repo_id = f"{your_username}/{format_filename(name_of_your_model, use_dash=True)}"
      output_dir = LoRA_dreambooth_args.output_dir
      if(not prefs['HuggingFace_api_key']):
        with open(HfFolder.path_token, 'r') as fin: hf_token = fin.read();
      else:
        hf_token = prefs['HuggingFace_api_key']
      try:
        create_repo(repo_id, private=private, exist_ok=True, token=hf_token)
        repo = Repository(output_dir, clone_from=repo_id, token=hf_token)
      except Exception as e:
        alert_msg(page, f"ERROR Creating repo {repo_id}... Make sure your HF token has Write access.", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip(), selectable=True)]))
        return
    else:
      if not os.path.exists(output_dir): os.makedirs(output_dir, exist_ok=True)

    try:
      #%cd $LoRA_dreambooth_dir # type: ignore
      os.chdir(LoRA_dreambooth_dir)
      #!accelerate $arg_str # type: ignore
      os.system("accelerate" + arg_str)
      #run_sp(arg_str, cwd=LoRA_dreambooth_dir, realtime=True)
      #run_sp(arg_str, cwd=LoRA_dreambooth_dir)
    except Exception as e:
      clear_last()
      alert_msg(page, f"ERROR: Out of Memory (or something else). Try reducing parameters and try again...", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip())]))
      with torch.no_grad():
        torch.cuda.empty_cache()
      return
    clear_last()
    if(LoRA_dreambooth_prefs['save_model']):
      model_images = os.path.join(output_dir, 'model_images')
      if not os.path.exists(model_images): os.makedirs(model_images, exist_ok=True)
      images_upload = os.listdir(save_path)
      image_string = ""
      #repo_id = f"sd-LoRA-library/{slugify(name_of_your_model)}"
      for i, image in enumerate(images_upload):
          img_name = f"image_{i}.png"
          shutil.copy(os.path.join(save_path, image), os.path.join(model_images, image))
          #image.save(os.path.join(repo_folder, f"image_{i}.png"))
          #img_str += f"![img_{i}](./image_{i}.png)\n"
          image_string = f'''{image_string}![img_{i}-{image}](https://huggingface.co/{repo_id}/resolve/main/model_images/{image})
'''
      description = LoRA_dreambooth_prefs['readme_description']
      if bool(description.strip()):
        description = LoRA_dreambooth_prefs['readme_description'] + '\n\n'
      readme_text = f'''---
license: mit
---
### {name_of_your_model} on Stable Diffusion via LoRA Dreambooth using [Stable Diffusion Deluxe](https://colab.research.google.com/github/Skquark/AI-Friends/blob/main/Stable_Diffusion_Deluxe.ipynb)
#### model by {api.whoami()["name"]}
This is a model fine-tuned on the {model_path} model taught to Stable Diffusion with LoRA.
It can be used by modifying the `instance_prompt`: **{LoRA_dreambooth_prefs['instance_prompt']}**

{description}You can also train your own models and upload them to the library by using [this notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_dreambooth_training.ipynb).
And you can run your new model via `diffusers`: [Colab Notebook for Inference](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_dreambooth_inference.ipynb), [Spaces with the Public Concepts loaded](https://huggingface.co/spaces/sd-dreambooth-library/stable-diffusion-dreambooth-models)

Here are the images used for training this model:
{image_string}
'''
      #Save the readme to a file
      #readme_file = open(os.path.join(output_dir, "README.md"), "w")
      #readme_file.write(readme_text)
      #readme_file.close()
      yaml = f"""
---
license: creativeml-openrail-m
base_model: {model_path}
tags:
- stable-diffusion
- stable-diffusion-diffusers
- stable-diffusion-deluxe
- text-to-image
- diffusers
inference: true
---
      """
      model_card = f"""
# LoRA DreamBooth - {name_of_your_model}
These are LoRA adaption weights for {name_of_your_model}. The weights were trained on {LoRA_dreambooth_args.instance_prompt} using [DreamBooth](https://dreambooth.github.io/).\n
### {repo_id} on Stable Diffusion via LoRA Dreambooth using [Stable Diffusion Deluxe](https://colab.research.google.com/github/Skquark/AI-Friends/blob/main/Stable_Diffusion_Deluxe.ipynb)
#### Model by {api.whoami()["name"]}

{description}You can also train your own models and upload them to the library by using [Stable Diffusion Deluxe](https://colab.research.google.com/github/Skquark/AI-Friends/blob/main/Stable_Diffusion_Deluxe.ipynb) or [this notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_dreambooth_training.ipynb).

Images used for training this model:
{image_string}
"""
      readme_file = open(os.path.join(output_dir, "README.md"), "w")
      readme_file.write(yaml + model_card)#(readme_text)
      readme_file.close()
      #with open(os.path.join(output_dir, "README.md"), "w") as f:
      #    f.write(yaml + model_card)
      #Save the token identifier to a file
      #text_file = open("token_identifier.txt", "w")
      #text_file.write(LoRA_dreambooth_prefs['instance_prompt'])
      #text_file.close()
      #operations = [
        #CommitOperationAdd(path_in_repo="token_identifier.txt", path_or_fileobj="token_identifier.txt"),
        #CommitOperationAdd(path_in_repo="README.md", path_or_fileobj="README.md"),
      #]
      print(repo_id)
      print(model_card)


      with open(os.path.join(output_dir, ".gitignore"), "w+") as gitignore:
        if "step_*" not in gitignore:
            gitignore.write("step_*\n")
        if "epoch_*" not in gitignore:
            gitignore.write("epoch_*\n")
      try:
        #api.upload_folder(folder_path=output_dir, path_in_repo="", repo_id=repo_id, token=hf_token)
        #api.upload_folder(folder_path=save_path, path_in_repo="model_images", repo_id=repo_id, token=hf_token)
        #api.create_commit(repo_id=repo_id, operations=operations, commit_message=f"Upload the model {name_of_your_model} embeds and token",token=hf_token)
        repo.push_to_hub(commit_message=f"Upload the LoRA model {name_of_your_model} embeds and weights", blocking=False, auto_lfs_prune=True)
      except Exception as e:
        alert_msg(page, f"ERROR Pushing {name_of_your_model} Repository {repo_id}... Make sure your HF token has Write access.", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip(), selectable=True)]))
        return
      #api.create_commit(repo_id=repo_id, operations=operations, commit_message=f"Upload the model {name_of_your_model} embeds and token",token=hf_token)
      #api.upload_folder(folder_path="fp16_model", path_in_repo="", repo_id=repo_id,token=hf_token)
      prefs['LoRA_dreambooth_model'] = name_of_your_model
      prefs['custom_models'].append({'name': name_of_your_model, 'path':repo_id})
      page.LoRA_dreambooth_model.options.insert(0, dropdown.Option(name_of_your_model))
      page.LoRA_dreambooth_model.value = name_of_your_model
      page.LoRA_dreambooth_model.update()
      save_settings_file(page)
      prt(Markdown(f"## Your model was saved successfully to _{repo_id}_.\n[Click here to access it](https://huggingface.co/{repo_id}). Use it in _Parameters->Use LaRA Model_ dropdown on top of any other Model loaded.", on_tap_link=lambda e: e.page.launch_url(e.data)))
    if prefs['enable_sounds']: page.snd_alert.play()


def run_LoRA(page):
    global LoRA_prefs, prefs
    def prt(line):
      if type(line) == str:
        line = Text(line)
      page.LoRA_output.controls.append(line)
      page.LoRA_output.update()
    def clear_last():
      del page.LoRA_output.controls[-1]
      page.LoRA_output.update()
    if not status['installed_diffusers']:
      alert_msg(page, "You must Install the HuggingFace Diffusers Library first... ")
      return
    save_path = os.path.join(root_dir, "my_model")
    error = False
    if not os.path.exists(save_path):
      error = True
    elif len(os.listdir(save_path)) == 0:
      error = True
    if len(page.lora_file_list.controls) == 0:
      error = True
    if error:
      alert_msg(page, "Couldn't find a list of images to train model. Add image files to the list...")
      return
    page.LoRA_output.controls.clear()
    page.LoRA_output.update()
    prt(Installing("Downloading LoRA Conceptualizers"))
    diffusers_dir = os.path.join(root_dir, "diffusers")
    if not os.path.exists(diffusers_dir):
      os.chdir(root_dir)
      run_process("git clone https://github.com/Skquark/diffusers.git", realtime=False, cwd=root_dir)
    run_process('pip install git+https://github.com/Skquark/diffusers.git#egg=diffusers[training]', cwd=root_dir, realtime=False)
    os.chdir(diffusers_dir)
    run_sp('pip install -e ".[training]"', cwd=diffusers_dir, realtime=False)
    #LoRA_dir = os.path.join(diffusers_dir, "examples", "dreambooth")
    LoRA_dir = os.path.join(diffusers_dir, "examples", "text_to_image")
    os.chdir(LoRA_dir)
    run_sp("pip install -r requirements.txt", cwd=LoRA_dir, realtime=False)
    run_process("pip install -qq bitsandbytes", page=page)
    run_sp("accelerate config default", realtime=False)
    #from accelerate.utils import write_basic_config
    #write_basic_config()
    import argparse
    from io import BytesIO
    from huggingface_hub import HfApi, HfFolder, CommitOperationAdd
    from huggingface_hub import Repository, create_repo, whoami
    #from diffusers import StableDiffusionPipeline
    api = HfApi()
    your_username = api.whoami()["name"]
    hf_token = prefs['HuggingFace_api_key']
    metadata_jsonl = []
    for fl in page.lora_file_list.controls:
        f = fl.title.value
        fn = f.rpartition(slash)[2]
        text = fl.subtitle.value
        metadata_jsonl.append({'file_name':fn, 'text':text})
    with open(os.path.join(save_path, "metadata.jsonl"), "w") as f:
        for meta in metadata_jsonl:
          print(json.dumps(meta), file=f)
        #json.dump(metadata_jsonl, f, ensure_ascii=False, indent=4)
    clear_pipes()
    clear_last()
    #num_new_images = None
    random_seed = int(LoRA_prefs['seed']) if int(LoRA_prefs['seed']) > 0 else rnd.randint(0,4294967295)
    name_of_your_model = LoRA_prefs['name_of_your_model']
    repo_id = f"{your_username}/{format_filename(name_of_your_model, use_dash=True)}"
    from argparse import Namespace
    #--lr_num_cycles=1 --lr_power=1 --prior_loss_weight=1.0 --sample_batch_size=4 --num_class_images=100
    LoRA_args = Namespace(
        pretrained_model_name_or_path=model_path,
        #dataset_name=repo_id,
        train_data_dir=save_path,
        resolution=LoRA_prefs['resolution'],
        center_crop=True,
        image_column="image",
        caption_column="text",
        #caption_column=LoRA_prefs['instance_prompt'].strip(),
        #instance_data_dir=save_path,
        validation_prompt=LoRA_prefs['validation_prompt'].strip(),
        num_validation_images = LoRA_prefs['num_validation_images'],
        validation_epochs=LoRA_prefs['validation_epochs'],
        learning_rate=LoRA_prefs['learning_rate'],#5e-06,'
        lr_scheduler=LoRA_prefs['lr_scheduler'],
        lr_warmup_steps=LoRA_prefs['lr_warmup_steps'],
        #lr_num_cycles=LoRA_prefs['lr_num_cycles'],
        #lr_power=LoRA_prefs['lr_power'],
        scale_lr=LoRA_prefs['scale_lr'],
        max_train_steps=LoRA_prefs['max_train_steps'],#450,
        train_batch_size=LoRA_prefs['train_batch_size'],
        checkpointing_steps=LoRA_prefs['checkpointing_steps'],
        gradient_accumulation_steps=LoRA_prefs['gradient_accumulation_steps'],
        max_grad_norm=1.0,
        mixed_precision="fp16", # set to "fp16" for mixed-precision training.
        gradient_checkpointing=LoRA_prefs['gradient_checkpointing'], # set this to True to lower the memory usage.
        use_8bit_adam=not prefs['higher_vram_mode'], # use 8bit optimizer from bitsandbytes
        enable_xformers_memory_efficient_attention = status['installed_xformers'],
        seed=random_seed,
        with_prior_preservation=LoRA_prefs['prior_preservation'],
        #prior_loss_weight=LoRA_prefs['prior_loss_weight'],
        #sample_batch_size=LoRA_prefs['sample_batch_size'],
        #class_data_dir=LoRA_prefs['class_data_dir'],
        #class_prompt=LoRA_prefs['class_prompt'],
        #num_class_images=LoRA_prefs['num_class_images'],
        cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None,
        hub_model_id=repo_id,
        output_dir=os.path.join(root_dir, "LoRA-model", format_filename(LoRA_prefs['name_of_your_model'], use_dash=True)),
    )
    output_dir = LoRA_args.output_dir
    if not os.path.exists(os.path.join(root_dir, "LoRA-model")): os.makedirs(os.path.join(root_dir, "LoRA-model"))
    #arg_str = "accelerate launch train_dreambooth_lora.py"
    arg_str = 'launch train_text_to_image_lora.py'
    for k, v in vars(LoRA_args).items():
      if isinstance(v, str):
        if ' ' in v:
          v = f'"{v}"'
      if isinstance(v, bool) or v == None:
        if bool(v):
          arg_str += f" --{k}"
      else:
        arg_str += f" --{k}={v}"
    prt(Text("*** Running training ***", weight=FontWeight.BOLD))
    #if num_new_images != None: prt(f"  Number of class images to sample: {num_new_images}.")
    #prt(f"  Instantaneous batch size per device = {LoRA_args.train_batch_size}")
    #prt(f"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}")
    #prt(f"  Gradient Accumulation steps = {LoRA_args.gradient_accumulation_steps}")
    #prt(f"  Total optimization steps = {LoRA_args.max_train_steps}")
    prt(arg_str)
    progress = ProgressBar(bar_height=8)
    prt(progress)
    if(LoRA_prefs['save_model']):

      private = False if LoRA_prefs['where_to_save_model'] == "Public HuggingFace" else True
      output_dir = LoRA_args.output_dir
      if(not prefs['HuggingFace_api_key']):
        with open(HfFolder.path_token, 'r') as fin: hf_token = fin.read();
      else:
        hf_token = prefs['HuggingFace_api_key']
      try:
        create_repo(repo_id, private=private, exist_ok=True, token=hf_token)
        repo = Repository(output_dir, clone_from=repo_id, token=hf_token)
      except Exception as e:
        alert_msg(page, f"ERROR Creating repo {repo_id}... Make sure your HF token has Write access.", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip(), selectable=True)]))
        return
    else:
      if not os.path.exists(output_dir): os.makedirs(output_dir, exist_ok=True)

    try:
      #run_sp("accelerate " + arg_str, cwd=LoRA_dir, realtime=True)
      #run_sp(arg_str, cwd=LoRA_dir)
      #%cd $LoRA_dir # type: ignore
      #!accelerate $arg_str # type: ignore
      os.chdir(LoRA_dir)
      os.system("accelerate" + arg_str)
    except Exception as e:
      clear_last()
      alert_msg(page, f"ERROR: Out of Memory (or something else). Try reducing parameters and try again...", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip())]))
      with torch.no_grad():
        torch.cuda.empty_cache()
      return
    clear_last()
    if(LoRA_prefs['save_model']):
      model_images = os.path.join(output_dir, 'model_images')
      if not os.path.exists(model_images): os.makedirs(model_images, exist_ok=True)
      images_upload = os.listdir(save_path)
      image_string = ""
      #repo_id = f"sd-LoRA-library/{slugify(name_of_your_model)}"
      for i, image in enumerate(images_upload):
          if image.endswith("jsonl"): continue
          img_name = f"image_{i}.png"
          shutil.copy(os.path.join(save_path, image), os.path.join(model_images, image))
          #image.save(os.path.join(repo_folder, f"image_{i}.png"))
          #img_str += f"![img_{i}](./image_{i}.png)\n"
          image_string = f'''{image_string}![img_{i}-{image}](https://huggingface.co/{repo_id}/resolve/main/model_images/{image})
'''
      shutil.copy(os.path.join(save_path, "metadata.jsonl"), os.path.join(model_images, "metadata.jsonl"))
      description = LoRA_prefs['readme_description']
      if bool(description.strip()):
        description = LoRA_prefs['readme_description'] + '\n\n'
      readme_text = f'''---
license: mit
---
### {name_of_your_model} on Stable Diffusion via LoRA Dreambooth using [Stable Diffusion Deluxe](https://colab.research.google.com/github/Skquark/AI-Friends/blob/main/Stable_Diffusion_Deluxe.ipynb)
#### model by {api.whoami()["name"]}
This your the Stable Diffusion model fine-tuned the {name_of_your_model} model taught to Stable Diffusion with LoRA.
It can be used by modifying the `validation_prompt`: **{LoRA_prefs['validation_prompt']}**

{description}You can also train your own models and upload them to the library by using [this notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_dreambooth_training.ipynb).
And you can run your new model via `diffusers`: [Colab Notebook for Inference](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_dreambooth_inference.ipynb), [Spaces with the Public Concepts loaded](https://huggingface.co/spaces/sd-dreambooth-library/stable-diffusion-dreambooth-models)

Here are the images used for training this model:
{image_string}
'''
      #Save the readme to a file
      #readme_file = open(os.path.join(output_dir, "README.md"), "w")
      #readme_file.write(readme_text)
      #readme_file.close()
      yaml = f"""
---
license: creativeml-openrail-m
base_model: {model_path}
tags:
- stable-diffusion
- stable-diffusion-diffusers
- stable-diffusion-deluxe
- text-to-image
- diffusers
- lora
inference: true
---
      """
      model_card = f"""
# LoRA Model - {name_of_your_model}
These are LoRA adaption weights for {model_path}. The weights were validated with {LoRA_args.validation_prompt} using [DreamBooth](https://dreambooth.github.io/).\n
### {repo_id} on Stable Diffusion via LoRA using [Stable Diffusion Deluxe](https://colab.research.google.com/github/Skquark/AI-Friends/blob/main/Stable_Diffusion_Deluxe.ipynb)
#### Model by {your_username}

{description}You can also train your own models and upload them to the library by using [Stable Diffusion Deluxe](https://colab.research.google.com/github/Skquark/AI-Friends/blob/main/Stable_Diffusion_Deluxe.ipynb) or [this notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_dreambooth_training.ipynb).

Images used for training this model:
{image_string}
"""
      readme_file = open(os.path.join(output_dir, "README.md"), "w")
      readme_file.write(yaml + model_card)#(readme_text)
      readme_file.close()
      #with open(os.path.join(output_dir, "README.md"), "w") as f:
      #    f.write(yaml + model_card)
      #Save the token identifier to a file
      #text_file = open("token_identifier.txt", "w")
      #text_file.write(LoRA_prefs['instance_prompt'])
      #text_file.close()
      #operations = [
        #CommitOperationAdd(path_in_repo="token_identifier.txt", path_or_fileobj="token_identifier.txt"),
        #CommitOperationAdd(path_in_repo="README.md", path_or_fileobj="README.md"),
      #]
      print(repo_id)
      print(model_card)


      with open(os.path.join(output_dir, ".gitignore"), "w+") as gitignore:
        if "step_*" not in gitignore:
            gitignore.write("step_*\n")
        if "epoch_*" not in gitignore:
            gitignore.write("epoch_*\n")
      try:
        #api.upload_folder(folder_path=output_dir, path_in_repo="", repo_id=repo_id, token=hf_token)
        #api.upload_folder(folder_path=save_path, path_in_repo="model_images", repo_id=repo_id, token=hf_token)
        #api.create_commit(repo_id=repo_id, operations=operations, commit_message=f"Upload the model {name_of_your_model} embeds and token",token=hf_token)
        repo.push_to_hub(commit_message=f"Upload the LoRA model {name_of_your_model} embeds and weights", blocking=False, auto_lfs_prune=True)
      except Exception as e:
        alert_msg(page, f"ERROR Pushing {name_of_your_model} Repository {repo_id}... Make sure your HF token has Write access.", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip(), selectable=True)]))
        return
      #api.create_commit(repo_id=repo_id, operations=operations, commit_message=f"Upload the model {name_of_your_model} embeds and token",token=hf_token)
      #api.upload_folder(folder_path="fp16_model", path_in_repo="", repo_id=repo_id,token=hf_token)
      prefs['LoRA_model'] = name_of_your_model
      prefs['custom_LoRA_models'].append({'name': name_of_your_model, 'path':repo_id})
      page.LoRA_model.options.insert(0, dropdown.Option(name_of_your_model))
      page.LoRA_model.value = name_of_your_model
      page.LoRA_model.update()
      save_settings_file(page)
      prt(Markdown(f"## Your model was saved successfully to _{repo_id}_.\n[Click here to access it](https://huggingface.co/{repo_id}). Use it in _Parameters->Use LaRA Model_ dropdown on top of any other Model loaded.", on_tap_link=lambda e: e.page.launch_url(e.data)))
    if prefs['enable_sounds']: page.snd_alert.play()


def run_converter(page):
    global converter_prefs, prefs
    #https://colab.research.google.com/github/camenduru/converter-colab/blob/main/converter_colab.ipynb
    def prt(line):
      if type(line) == str:
        line = Text(line)
      page.converter_output.controls.append(line)
      page.converter_output.update()
    def clear_last():
      if len(page.converter_output.controls) == 0: return
      del page.converter_output.controls[-1]
      page.converter_output.update()
    if not status['installed_diffusers']:
      alert_msg(page, "You must Install the HuggingFace Diffusers Library first... ")
      return
    model_name = converter_prefs['model_name']
    model_path = converter_prefs['model_path']
    if not bool(converter_prefs['model_name']):
      alert(page, "Provide a name to call the converted custom model")
      return
    if not bool(model_path):
      alert(page, "Provide the path to the custom model to convert")
      return
    model_file = format_filename(model_name, use_dash=True)
    if converter_prefs['from_format'] == 'ckpt':
      model_file += '.ckpt'
    page.converter_output.controls.clear()
    custom_models = os.path.join(root_dir, 'custom_models',)
    custom_path = os.path.join(custom_models, format_filename(model_name, use_dash=True))
    checkpoint_file = os.path.join(custom_models, model_file)
    if not os.path.exists(custom_path):
      os.makedirs(custom_path, exist_ok=True)
    try:
      import omegaconf
    except Exception:
      run_sp("pip install omegaconf", realtime=False)
      pass
    diffusers_dir = os.path.join(root_dir, "diffusers")
    if not os.path.exists(diffusers_dir):
      run_process("git clone https://github.com/Skquark/diffusers.git", realtime=False, cwd=root_dir)
    run_process('pip install "git+https://github.com/Skquark/diffusers.git#egg=diffusers[training]"', cwd=root_dir, realtime=False)
    scripts_dir = os.path.join(diffusers_dir, "scripts")
    progress = ProgressBar(bar_height=8)

    if model_path.startswith('https://drive'):
      gdown.download(model_path, checkpoint_file, quiet=True)
    elif model_path.startswith('http'):
      local = download_file(model_path)
      print(f"Download {model_path} local:{local}")
      shutil.move(local, checkpoint_file)
    elif os.path.isfile(model_path):
      shutil.copy(model_path, checkpoint_file)
    elif os.path.isdir(model_path):
      if os.path.exists(custom_path):
        shutil.rmtree(custom_path)
      shutil.copytree(model_path, custom_path, dirs_exist_ok=True)
      checkpoint_file = custom_path
    elif '/' in model_path and not model_path.startswith('/'):
      checkpoint_file = model_path # hopefully Huggingface
    else:
      alert_msg(page, f"Couldn't recognize source model file path {model_path}.")
      return
    prt(Text(f'Converting {model_file} to {converter_prefs["to_format"]}...', weight=FontWeight.BOLD))
    prt(progress)

    if converter_prefs['from_format'] == "lora_safetensors":
      run_cmd = f"python3 {os.path.join(scripts_dir, 'convert_lora_safetensor_to_diffusers.py')}"
    else:
      run_cmd = f"python3 {os.path.join(scripts_dir, 'convert_original_stable_diffusion_to_diffusers.py')}"
    if converter_prefs['from_format'] == "safetensors":
      run_cmd += f' --from_safetensors'
    if converter_prefs['from_format'] == "controlnet":
      run_cmd += f' --controlnet'
    if converter_prefs['from_format'] == "lora_safetensors" and bool(converter_prefs['base_model']):
      run_cmd += f' --base_model {converter_prefs["base_model"]}'
    if converter_prefs['from_format'] == "ckpt" or converter_prefs['from_format'] == "safetensors" or converter_prefs['from_format'] == "lora_safetensors":
      run_cmd += f' --checkpoint_path {checkpoint_file}'
    if converter_prefs['to_format'] == "safetensors":
      run_cmd += f' --to_safetensors'
    #if status['installed_xformers']:
    if converter_prefs['model_type'] == "SD v1.x text2image" and converter_prefs['from_format'] != "lora_safetensors":
      run_cmd += f' --image_size 512'
    elif converter_prefs['model_type'] == "SD v2.x text2image" and converter_prefs['from_format'] != "lora_safetensors":
      run_cmd += f' --image_size 768'
      run_cmd += f' --upcast_attention'
      run_cmd += f' --prediction_type v_prediction'
    run_cmd += f' --scheduler_type {converter_prefs["scheduler_type"]}'
    if converter_prefs['half_percision']:
      run_cmd += f' --half'
    run_cmd += f' --dump_path {custom_path}'
    #TODO: Add this to UI to use
    #if bool(converter_prefs['vae_path']): #"Set to a path, hub id to an already converted vae to not convert it again."
    #  run_cmd += f' --vae_path {converter_prefs["vae_path"]}'
    if converter_prefs['from_format'] == converter_prefs["to_format"]:
      out_file = os.path.join(custom_path, f"{format_filename(model_name, use_dash=True)}.{converter_prefs['to_format']}")
      print(f"From and To Formats are the same. Using file without conversion to {out_file}")
      shutil.move(checkpoint_file, out_file)
      custom_path = out_file
    else:
      prt(f"Running {run_cmd}")
      try:
        run_sp(run_cmd, cwd=scripts_dir, realtime=True)
        #run_process(run_cmd, page=page, cwd=scripts_dir, show=True)
      except Exception as e:
        clear_last()
        alert_msg(page, "Error Running convert_original_stable_diffusion_to_diffusers", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))
        return
      if len(os.listdir(custom_path)) == 0:
        prt(f"Problem converting your model. Check console and source checkpoint and try again...")
        os.rmdir(custom_path)
        return
      clear_last()
    clear_last()
    clear_last()
    prt(f"Done Converting... Saved locally at {custom_path}")
    if converter_prefs['load_custom_model']:
      prefs['custom_model'] = custom_path
      prefs['custom_models'].append({'name': model_name, 'path':custom_path})
    if(converter_prefs['save_model']):
      from huggingface_hub import HfApi, HfFolder, CommitOperationAdd
      from huggingface_hub import model_info, create_repo, create_branch, upload_folder
      from huggingface_hub.utils import RepositoryNotFoundError, revisionNotFoundError
      from diffusers import StableDiffusionPipeline
      api = HfApi()
      your_username = api.whoami()["name"]
      '''dreambooth_pipe = StableDiffusionPipeline.from_pretrained(
        custom_path,
        torch_dtype=torch.float16,
      ).to("cuda")
      os.makedirs("fp16_model",exist_ok=True)
      dreambooth_pipe.save_pretrained("fp16_model")'''
      hf_token = prefs['HuggingFace_api_key']
      private = True
      if(converter_prefs['where_to_save_model'] == "Public Library"):
        private = False
      #  repo_id = f"sd-dreambooth-library/{slugify(model_name)}"
      if '/' in model_path and not model_path.startswith('/'):
        repo_id = model_path
      else:
        repo_id = f"{your_username}/{format_filename(model_name, use_dash=True)}"
      #output_dir = dreambooth_args.output_dir
      if(not bool(prefs['HuggingFace_api_key'])):
        with open(HfFolder.path_token, 'r') as fin: hf_token = fin.read();
      else:
        hf_token = prefs['HuggingFace_api_key']

      description = converter_prefs['readme_description']
      if bool(description.strip()):
        description = converter_prefs['readme_description'] + '\n\n'
      readme_text = f'''---
license: mit
---
### {model_name} model on Stable Diffusion using [Stable Diffusion Deluxe](https://colab.research.google.com/github/Skquark/AI-Friends/blob/main/Stable_Diffusion_Deluxe.ipynb)
#### model by {api.whoami()["name"]}

{description}
You can also train your own models and upload them to the library by using [this notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_dreambooth_training.ipynb).
And you can run your new concept via `diffusers`: [Colab Notebook for Inference](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_dreambooth_inference.ipynb), [Spaces with the Public Concepts loaded](https://huggingface.co/spaces/sd-dreambooth-library/stable-diffusion-dreambooth-concepts)

    '''
      #Save the readme to a file
      readme_file = open("README.md", "w")
      readme_file.write(readme_text)
      readme_file.close()
      #Save the token identifier to a file
      '''text_file = open("token_identifier.txt", "w")
      text_file.write(dreambooth_prefs['instance_prompt'])
      text_file.close()'''
      operations = [
        #CommitOperationAdd(path_in_repo="token_identifier.txt", path_or_fileobj="token_identifier.txt"),
        CommitOperationAdd(path_in_repo="README.md", path_or_fileobj="README.md"),
      ]
      print(repo_id)
      print(readme_text)
      try:
          repo_exists = True
          r_info = model_info(repo_id, token=hf_token)
      except RepositoryNotFoundError:
          repo_exists = False
          pass
      if not repo_exists:
        try:
          create_repo(repo_id, private=private, token=hf_token)
        except Exception as e:
          alert_msg(page, f"ERROR Creating repo {repo_id}... Make sure your HF token has Write access.", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip(), selectable=True)]))
          return
      branch = f"{converter_prefs['from_format']}-to-{converter_prefs['to_format']}"
      try:
          branch_exists = True
          b_info = model_info(repo_id, revision=branch, token=hf_token)
      except revisionNotFoundError:
          branch_exists = False
      finally:
          if branch_exists:
              print(b_info)
          else:
              create_branch(repo_id, branch=branch, token=hf_token)
      api.create_commit(repo_id=repo_id, operations=operations, commit_message=f"Upload the converted model {model_name} embeds",token=hf_token)
      api.upload_folder(folder_path=custom_path, path_in_repo="", revision=branch, repo_id=repo_id, commit_message=f"Upload the converted model {model_name} embeds", token=hf_token)
      #api.upload_folder(folder_path="fp16_model", path_in_repo="", repo_id=repo_id,token=hf_token)
      #api.upload_folder(folder_path=save_path, path_in_repo="concept_images", repo_id=repo_id, token=hf_token)
      prefs['custom_model'] = repo_id
      prefs['custom_models'].append({'name': model_name, 'path':repo_id})
      page.custom_model.value = repo_id
      try:
        page.custom_model.update()
      except Exception: pass
      prt(Markdown(f"## Your model was saved successfully to _{repo_id}_.<br>[Click here to access it](https://huggingface.co/{repo_id}) and go to _Installers->Model Checkpoint->Custom Model Path_ to use. Include Token in prompts.", on_tap_link=lambda e: e.page.launch_url(e.data)))

    def push_ckpt(model_to, token, branch):
      try:
          repo_exists = True
          r_info = model_info(model_to, token=token)
      except RepositoryNotFoundError:
          repo_exists = False
      finally:
          if repo_exists:
              print(r_info)
          else:
              create_repo(model_to, private=True, token=token)
      try:
          branch_exists = True
          b_info = model_info(model_to, revision=branch, token=token)
      except revisionNotFoundError:
          branch_exists = False
      finally:
          if branch_exists:
              print(b_info)
          else:
              create_branch(model_to, branch=branch, token=token)
      upload_folder(folder_path="ckpt", path_in_repo="", revision=branch, repo_id=model_to, commit_message=f"ckpt", token=token)
      return "push ckpt done!"
    if prefs['enable_sounds']: page.snd_alert.play()

def run_checkpoint_merger(page):
    global checkpoint_merger_prefs
    def prt(line):
      if type(line) == str:
        line = Text(line)
      page.checkpoint_merger_output.controls.append(line)
      page.checkpoint_merger_output.update()
    def clear_last():
      del page.checkpoint_merger_output.controls[-1]
      page.checkpoint_merger_output.update()
    if not status['installed_diffusers']:
        alert_msg(page, "You must Install the HuggingFace Diffusers Library first... ")
        return
    if len(checkpoint_merger_prefs['pretrained_models']) < 2:
        alert_msg(page, "Select 2 or more compatible checkpoint models to the list before running...")
        return
    prt(Installing("Downloading Required Models and Merging..."))
    try:
        from diffusers import DiffusionPipeline
        model_path = checkpoint_merger_prefs['pretrained_models'][0]
        checkpoint_merger_pipe = DiffusionPipeline.from_pretrained(model_path, custom_pipeline="checkpoint_merger.py", cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
        merged_pipe = checkpoint_merger_pipe.merge(checkpoint_merger_prefs['pretrained_models'], interp = checkpoint_merger_prefs['interp'] if checkpoint_merger_prefs['interp'] != "weighted_sum" else None, alpha = checkpoint_merger_prefs['alpha'], force = checkpoint_merger_prefs['force'], cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
        merged_pipe.to(torch_device)
    except Exception as e:
        clear_last()
        alert_msg(page, f"ERROR: Problem running Merger. Check parameters and try again...", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip())]))
        with torch.no_grad():
            torch.cuda.empty_cache()
        return
    model_name = format_filename(checkpoint_merger_prefs['name_of_your_model'], force_underscores=True)
    output_dir = os.path.join(root_dir, 'my_models', model_name)
    repo_id = f"{prefs['HuggingFace_username']}/{model_name}"
    if bool(checkpoint_merger_prefs['validation_prompt']):
        prt("Generating Test Validation Image...")
        try:
            image = merged_pipe(checkpoint_merger_prefs['validation_prompt']).images[0]
        except Exception as e:
            clear_last()
            alert_msg(page, f"ERROR: Problem creating image with merged_pipe...", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip())]))
            with torch.no_grad():
                torch.cuda.empty_cache()
            return
        fname = format_filename(checkpoint_merger_prefs['validation_prompt'])
        fpath = available_file(stable_dir, fname, 0)
        image.save(fpath)
        clear_last()
        prt(Img(src=fpath))
    if checkpoint_merger_prefs['save_model']:
        private = False if checkpoint_merger_prefs['where_to_save_model'] == "Public HuggingFace" else True
        if(not prefs['HuggingFace_api_key']):
            with open(HfFolder.path_token, 'r') as fin: hf_token = fin.read();
        else:
            hf_token = prefs['HuggingFace_api_key']
        try:
            create_repo(repo_id, private=private, exist_ok=True, token=hf_token)
            repo = Repository(output_dir, clone_from=repo_id, token=hf_token)
        except Exception as e:
            alert_msg(page, f"ERROR Creating repo {repo_id}... Make sure your HF token has Write access.", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip(), selectable=True)]))
            return
    else:
        if os.path.exists(output_dir):
            os.makedirs(output_dir, exist_ok=True)
    try:
        merged_pipe.save_pretrained(output_dir)
    except Exception as e:
        clear_last()
        alert_msg(page, f"ERROR: Issue saving pretrained.  Check parameters and try again...", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip())]))
        with torch.no_grad():
            torch.cuda.empty_cache()
        return
    del checkpoint_merger_pipe
    del merged_pipe
    if checkpoint_merger_prefs['save_model']:
        description = checkpoint_merger_prefs['readme_description']
        if bool(description.strip()):
            description = checkpoint_merger_prefs['readme_description'] + '\n\n'
        models_string = ""
        for m in checkpoint_merger_prefs['pretrained_models']:
            models_string += f"* [{m}](https://huggingface.co/{m})\n"
        yaml = f"""
---
license: creativeml-openrail-m
base_model: {model_path}
tags:
- stable-diffusion
- stable-diffusion-diffusers
- stable-diffusion-deluxe
- text-to-image
- diffusers
- merge
inference: true
---
"""
        model_card = f"""
# Merged Checkpoint Model - {checkpoint_merger_prefs['name_of_your_model']}
These are fine-tuned combined weights of {' + '.join(checkpoint_merger_prefs['pretrained_models'])}.\n
### {repo_id} on Stable Diffusion via Custom Checkpoint using [Stable Diffusion Deluxe](https://colab.research.google.com/github/Skquark/AI-Friends/blob/main/Stable_Diffusion_Deluxe.ipynb)
#### Model by {prefs['HuggingFace_username']}
{description}

Checkpoints used for training this model:
{models_string}
Alpha Interpolation: {checkpoint_merger_prefs['alpha']}
Interpolation Method: {checkpoint_merger_prefs['interp']}
"""
        readme_file = open(os.path.join(output_dir, "README.md"), "w")
        readme_file.write(yaml + model_card)#(readme_text)
        readme_file.close()
        print(repo_id)
        print(model_card)
        with open(os.path.join(output_dir, ".gitignore"), "w+") as gitignore:
            if "step_*" not in gitignore:
                gitignore.write("step_*\n")
            if "epoch_*" not in gitignore:
                gitignore.write("epoch_*\n")
        try:
            #api.upload_folder(folder_path=output_dir, path_in_repo="", repo_id=repo_id, token=hf_token)
            #api.upload_folder(folder_path=save_path, path_in_repo="model_images", repo_id=repo_id, token=hf_token)
            #api.create_commit(repo_id=repo_id, operations=operations, commit_message=f"Upload the model {name_of_your_model} embeds and token",token=hf_token)
            repo.push_to_hub(commit_message=f"Upload the Merged model {model_name} embeds and weights", blocking=False, auto_lfs_prune=True)
        except Exception as e:
            alert_msg(page, f"ERROR Pushing {model_name} Repository {repo_id}... Make sure your HF token has Write access.", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip(), selectable=True)]))
            return
        prt(Markdown(f"## Your model was saved successfully to _{repo_id}_.\n[Click here to access it](https://huggingface.co/{repo_id}). Use it in _Installers->Diffusers Custom Model_ dropdown.", on_tap_link=lambda e: e.page.launch_url(e.data)))

    if prefs['enable_sounds']: page.snd_alert.play()

def run_tortoise_tts(page):
    #https://github.com/neonbjb/tortoise-tts
    global tortoise_prefs, pipe_tortoise_tts, prefs
    def prt(line):
      if type(line) == str:
        line = Text(line)
      page.tortoise_output.controls.append(line)
      page.tortoise_output.update()
    def clear_last():
      del page.tortoise_output.controls[-1]
      page.tortoise_output.update()
    def play_audio(e):
      e.control.data.play()
    if tortoise_prefs['train_custom'] and not bool(tortoise_prefs['custom_voice_name']):
      alert_msg(page, "Provide a Custom Voice Name when training your audio files.")
      return
    if not bool(tortoise_prefs['text']):
      alert_msg(page, "Provide Text for the AI voice to read...")
      return
    progress = ProgressBar(bar_height=8)
    installer = Installing("Downloading Tortoise-TTS Packages...")
    prt(installer)
    tortoise_dir = os.path.join(root_dir, "tortoise-tts")
    voice_dir = os.path.join(tortoise_dir, 'tortoise', 'voices')
    if not os.path.isdir(tortoise_dir):
      installer.set_details("...cloning jnorberg/toroise-tts")
      os.chdir(root_dir)
      run_process("git clone https://github.com/jnordberg/tortoise-tts.git", page=page)
    os.chdir(tortoise_dir)
    try:
      import pydub
    except Exception:
      installer.set_details("...installing ffmpeg & pydub")
      run_process("pip install -q ffmpeg", page=page)
      run_process("pip install -q pydub", page=page)
      import pydub
      pass
    try:
      from tortoise.api import TextToSpeech
    except Exception:
      installer.set_details("...installing all requirements")
      try:
        run_process("pip3 install -r requirements.txt", page=page, cwd=tortoise_dir)
        run_process("python3 setup.py install", page=page, cwd=tortoise_dir)
      except Exception as e:
        clear_last()
        alert_msg(page, "Error Installing Tortoise TextToSpeech requirements", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip())]))
        return
      pass
    import torch
    import torchaudio
    import torch.nn as nn
    import torch.nn.functional as F
    from tortoise.api import TextToSpeech
    from tortoise.utils.audio import load_audio, load_voice, load_voices
    clear_pipes('tortoise_tts')
    # This will download all the models used by Tortoise from the HuggingFace hub.
    if pipe_tortoise_tts == None:
      installer.set_details("...initialize TextToSpeech pipe")
      try:
        pipe_tortoise_tts = TextToSpeech()
      except Exception as e:
        clear_last()
        alert_msg(page, "Error downloading Tortoise TextToSpeech package", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))
        return
    clear_last()
    prt(Text("  Generating Tortoise Text-to-Speech... (slow, but wins the race)", weight=FontWeight.BOLD))
    prt(progress)
    save_dir = os.path.join(root_dir, 'audio_out', tortoise_prefs['batch_folder_name'])
    if not os.path.exists(save_dir):
      os.makedirs(save_dir, exist_ok=True)
    audio_out = os.path.join(prefs['image_output'].rpartition(slash)[0], 'audio_out')
    custom_voices = os.path.join(audio_out, 'custom_voices')
    if bool(tortoise_prefs['batch_folder_name']):
      audio_out = os.path.join(audio_out, tortoise_prefs['batch_folder_name'])
    os.makedirs(audio_out, exist_ok=True)
    #voice_dirs = os.listdir(os.path.join(root_dir, "tortoise-tts", 'tortoise', 'voices'))
    #print(str(voice_dirs))
    fname = format_filename(tortoise_prefs['text'])
    if fname[-1] == '.': fname = fname[:-1]
    file_prefix = tortoise_prefs['file_prefix']
    tortoise_custom_voices = prefs['tortoise_custom_voices']
    tortoise_prefs['voice'] = []
    if tortoise_prefs['train_custom']:
        if len(tortoise_prefs['custom_wavs']) <2:
          alert_msg(page, "To train a custom voice, provide at least 2 audio files to mimic.")
          return
        CUSTOM_VOICE_NAME = format_filename(tortoise_prefs['custom_voice_name'])
        custom_voice_folder = os.path.join(root_dir, "tortoise-tts", 'tortoise', 'voices', CUSTOM_VOICE_NAME)
        os.makedirs(custom_voice_folder, exist_ok=True)
        for i, f in enumerate(tortoise_prefs['custom_wavs']):
            if f.lower().endswith('mp3'):
              sound = pydub.AudioSpegment.from_mp3(f)
              sound.export(os.path.join(custom_voice_folder, f'{i+1}.wav'), format="wav", bitrate="22050")
            elif f.lower().endswith('wav'):
              sound = pydub.AudioSpegment.from_wav(f)
              sound.export(os.path.join(custom_voice_folder, f'{i+1}.wav'), format="wav", bitrate="22050")
            else:
              alert_msg(f"Unknown file type {f.rpartition('.')[2]}... Use only .wav and .mp3 audio clips.")
              return
              #shutil.copy(f, os.path.join(custom_voice_folder, f'{i+1}.wav'))
        #for i, file_data in enumerate(files.upload().values()):
        #    with open(os.path.join(custom_voice_folder, f'{i}.wav'), 'wb') as f:
        #        f.write(file_data)
        if not CUSTOM_VOICE_NAME in tortoise_prefs['voice']:
          #tortoise_prefs['voice'].append(CUSTOM_VOICE_NAME)
          page.tortoise_voices.controls.append(Checkbox(label=CUSTOM_VOICE_NAME, value=True, fill_color=colors.PRIMARY_CONTAINER, check_color=colors.ON_PRIMARY_CONTAINER, col={'xs':12, 'sm':6, 'md':3, 'lg':3, 'xl': 2}))
          page.tortoise_voices.update()
          output_voice_folder = os.path.join(custom_voices, CUSTOM_VOICE_NAME)
          if os.path.exists(output_voice_folder):
            shutil.rmtree(output_voice_folder)
            print(f'Output Voice Folder already existed: {output_voice_folder}... Deleting to remake.')
          #os.makedirs(output_voice_folder, exist_ok=False)
          shutil.copytree(custom_voice_folder, output_voice_folder)
          prefs['tortoise_custom_voices'].append({'name':CUSTOM_VOICE_NAME, 'folder': output_voice_folder})
          save_settings_file(page)
    for v in page.tortoise_voices.controls:
        if v.value == True:
          tortoise_prefs['voice'].append(v.label)
          if not os.path.exists(os.path.join(voice_dir, v.label)):
            for custom in tortoise_custom_voices:
              if custom['name'] == v.label:
                if os.path.exists(custom['folder']):
                  #os.makedirs(os.path.join(voice_dir, custom['name'], exist_ok=True))
                  shutil.copytree(custom['folder'], os.path.join(voice_dir, custom['name']))
                else:
                  print(f"Couldn't find custom folder {custom['folder']}")
    # Load it and send it through Tortoise.
    voice = tortoise_prefs['voice']
    v_str = voice if isinstance(voice, str) else '+'.join(voice) if isinstance(voice, list) else ''
    if len(voice) == 0: v_str = 'random'
    audio_name = f'{file_prefix}{v_str}-{fname}'
    audio_name = audio_name[:int(prefs['file_max_length'])]
    fname = available_file(save_dir, audio_name, 0, ext="wav")
    #print(str(voice))
    #print(fname)
    if len(voice) == 0:
        voice_samples = conditioning_latents = None
    elif len(voice) == 1:
        voice_samples, conditioning_latents = load_voice(voice[0])
    else:
        voice_samples, conditioning_latents = load_voices(voice)
    gen = pipe_tortoise_tts.tts_with_preset(tortoise_prefs['text'], voice_samples=voice_samples, conditioning_latents=conditioning_latents, preset=tortoise_prefs['preset'])
    torchaudio.save(fname, gen.squeeze(0).cpu(), 24000)
    #IPython.display.Audio('generated.wav')
    clear_last()
    clear_last()
    #a_out = Audio(src=fname, autoplay=False)
    #page.overlay.append(a_out)
    #page.update()
    display_name = fname
    #a.tofile(f"/content/dance-{i}.wav")
    if storage_type == "Colab Google Drive":
      audio_save = available_file(audio_out, audio_name, 0, ext='wav')
      shutil.copy(fname, audio_save)
      display_name = audio_save
    prt(AudioPlayer(src=fname, display=display_name, data=display_name))
    #prt(Row([IconButton(icon=icons.PLAY_CIRCLE_FILLED, icon_size=48, on_click=play_audio, data=a_out), Text(display_name)]))
    if prefs['enable_sounds']: page.snd_alert.play()

def run_audio_ldm(page):
    global audioLDM_prefs, pipe_audio_ldm, prefs
    def prt(line):
      if type(line) == str:
        line = Text(line)
      page.audioLDM_output.controls.append(line)
      page.audioLDM_output.update()
    def clear_last():
      if len(page.audioLDM_output.controls) < 1: return
      del page.audioLDM_output.controls[-1]
      page.audioLDM_output.update()
    def play_audio(e):
      e.control.data.play()
    if not bool(audioLDM_prefs['text']):
      alert_msg(page, "Provide Text for the AI to create the sound of...")
      return
    if not status['installed_diffusers']:
      alert_msg(page, "You must Install the HuggingFace Diffusers Library first... ")
      return
    progress = ProgressBar(bar_height=8)
    state_text = Text(" Downloading Audio LDM Packages...", weight=FontWeight.BOLD)
    prt(Row([ProgressRing(), state_text]))
    audioLDM_dir = os.path.join(root_dir, "audioldm-text-to-audio-generation")
    #voice_dir = os.path.join(audioLDM_dir, 'audioldm', 'voices')
    if not os.path.isdir(audioLDM_dir):
      os.chdir(root_dir)
      run_process("git clone https://huggingface.co/spaces/haoheliu/audioldm-text-to-audio-generation", page=page)
    os.chdir(audioLDM_dir)
    import sys
    sys.path.append(os.path.join(audioLDM_dir, 'audioldm'))
    try:
        from audioldm import text_to_audio, build_model
    except Exception:
        try:
            run_process("pip install einops", page=page)
            run_process("pip install -q pyyaml", page=page)
            run_process("pip install -q soundfile", page=page)
            run_process("pip install -q librosa", page=page)
            run_process("pip install -q pandas", page=page)
            run_process("pip install -q gradio", page=page)
            run_process("pip install -q torchlibrosa", page=page)
        except Exception as e:
            clear_last()
            alert_msg(page, "Error Installing AudioLDM requirements", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip())]))
            return
        pass
    finally:
        from audioldm import text_to_audio, build_model
    import soundfile as sf
    model_id="cvssp/audioldm-s-full-v2"
    clear_pipes('audio_ldm')
    # This will download all the models used by Audio LDM from the HuggingFace hub.
    if pipe_audio_ldm == None:
      try:
        pipe_audio_ldm = build_model()
      except Exception as e:
        clear_last()
        alert_msg(page, "Error downloading Audio LDM package", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))
        return
    clear_last()
    prt(Text("  Generating AudioLDM Sounds...", weight=FontWeight.BOLD))
    prt(progress)
    random_seed = int(audioLDM_prefs['seed']) if int(audioLDM_prefs['seed']) > 0 else rnd.randint(0,4294967295)
    try:
      waveform = text_to_audio(pipe_audio_ldm, audioLDM_prefs['text'], random_seed, duration=audioLDM_prefs['duration'], guidance_scale=audioLDM_prefs['guidance_scale'], n_candidate_gen_per_text=int(audioLDM_prefs['n_candidates']))
    except Exception as e:
      clear_last()
      alert_msg(page, "Error generating text_to_audio waveform...", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))
      return
    save_dir = os.path.join(root_dir, 'audio_out', audioLDM_prefs['batch_folder_name'])
    if not os.path.exists(save_dir):
      os.makedirs(save_dir, exist_ok=True)
    audio_out = os.path.join(prefs['image_output'].rpartition(slash)[0], 'audio_out')
    if bool(audioLDM_prefs['batch_folder_name']):
      audio_out = os.path.join(audio_out, audioLDM_prefs['batch_folder_name'])
    os.makedirs(audio_out, exist_ok=True)
    #voice_dirs = os.listdir(os.path.join(root_dir, "audioldm-tts", 'audioldm', 'voices'))
    #print(str(voice_dirs))
    fname = format_filename(audioLDM_prefs['text'])
    if fname[-1] == '.': fname = fname[:-1]
    file_prefix = audioLDM_prefs['file_prefix']
    audio_name = f'{file_prefix}-{fname}'
    audio_name = audio_name[:int(prefs['file_max_length'])]
    fname = available_file(save_dir, audio_name, 0, ext="wav")
    for i in range(waveform.shape[0]):
        sf.write(fname, waveform[i, 0], samplerate=16000)
    #torchaudio.save(fname, gen.squeeze(0).cpu(), 24000)
    #IPython.display.Audio('generated.wav')
    clear_last()
    clear_last()
    #a_out = Audio(src=fname, autoplay=False)
    #page.overlay.append(a_out)
    #page.update()
    display_name = fname
    #a.tofile(f"/content/dance-{i}.wav")
    if storage_type == "Colab Google Drive":
      audio_save = available_file(audio_out, audio_name, 0, ext='wav')
      shutil.copy(fname, audio_save)
      display_name = audio_save
    #prt(Row([IconButton(icon=icons.PLAY_CIRCLE_FILLED, icon_size=48, on_click=play_audio, data=a_out), Text(display_name)]))
    prt(AudioPlayer(src=fname, display=display_name, data=display_name, page=page))
    if prefs['enable_sounds']: page.snd_alert.play()

def run_audio_ldm2(page):
    global audioLDM2_prefs, pipe_audio_ldm2, prefs, status
    def prt(line):
      if type(line) == str:
        line = Text(line)
      page.audioLDM2_output.controls.append(line)
      page.audioLDM2_output.update()
    def clear_last():
      if len(page.audioLDM2_output.controls) < 1: return
      del page.audioLDM2_output.controls[-1]
      page.audioLDM2_output.update()
    if not bool(audioLDM2_prefs['text']):
      alert_msg(page, "Provide Text for the AI to create the sound of...")
      return
    if not status['installed_diffusers']:
      alert_msg(page, "You must Install the HuggingFace Diffusers Library first... ")
      return
    progress = ProgressBar(bar_height=8)
    installer = Installing("Downloading Audio LDM-2 Packages...")
    prt(installer)
    audioLDM2_dir = os.path.join(root_dir, "AudioLDM2")
    #voice_dir = os.path.join(audioLDM2_dir, 'audioldm2', 'voices')
    if not os.path.isdir(audioLDM2_dir):
      os.chdir(root_dir)
      installer.set_details("...clone haoheliu/AudioLDM2")
      run_process("git clone https://github.com/haoheliu/AudioLDM2", page=page)
    os.chdir(audioLDM2_dir)
    import sys
    sys.path.append(os.path.join(audioLDM2_dir, 'audioldm2'))
    try:
        from audioldm2 import text_to_audio, build_model
    except Exception:
        try:
            installer.set_details("...install einops")
            run_process("pip install einops", page=page)
            installer.set_details("...install pyyaml")
            run_process("pip install -q pyyaml", page=page)
            installer.set_details("...install soundfile")
            run_process("pip install -q soundfile", page=page)
            installer.set_details("...install chardet")
            run_process("pip install -q librosa chardet", page=page)
            installer.set_details("...install scipy")
            run_process("pip install -q pandas scipy", page=page)
            installer.set_details("...install gradio")
            run_process("pip install -q gradio", page=page)
            installer.set_details("...install torchlibrosa")
            run_process("pip install -q torchlibrosa", page=page)
            installer.set_details("...install unidecode phonemizer ftfy timm")
            run_process("pip install -q unidecode phonemizer ftfy timm", page=page)
            installer.set_details("...install haoheliu/AudioLDM2.git")
            run_process("pip install git+https://github.com/haoheliu/AudioLDM2.git", page=page)
            #sudo apt-get install espeak
            #export PHONEMIZER_ESPEAK_LIBRARY=$(find /opt/homebrew/Cellar/espeak -name "libespeak.dylib" | sort -r | head -n 1)
            os.environ["TOKENIZERS_PARALLELISM"] = "true"
        except Exception as e:
            clear_last()
            alert_msg(page, "Error Installing AudioLDM-2 requirements", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip())]))
            return
        pass
    finally:
        from audioldm2 import text_to_audio, build_model
    import soundfile as sf
    model_id=audioLDM2_prefs['model_name']
    if 'loaded_ldm2' not in status:
        status['loaded_ldm2'] = ""
    if status['loaded_ldm2'] == model_id:
        clear_pipes('audio_ldm2')
    else:
        clear_pipes()
    # This will download all the models used by Audio LDM from the HuggingFace hub.
    if pipe_audio_ldm2 == None:
      try:
        installer.set_details("...building model")
        pipe_audio_ldm2 = build_model(model_name=model_id)
        installer.set_details("...torch compile")
        pipe_audio_ldm2 = torch.compile(pipe_audio_ldm2)
        torch.set_float32_matmul_precision("high")
        status['loaded_ldm2'] = model_id
      except Exception as e:
        clear_last()
        alert_msg(page, "Error downloading Audio LDM 2 model", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))
        return
    clear_last()
    prt(Text("  Generating AudioLDM-2 Sounds...", weight=FontWeight.BOLD))
    prt(progress)
    random_seed = int(audioLDM2_prefs['seed']) if int(audioLDM2_prefs['seed']) > 0 else rnd.randint(0,4294967295)
    try:#audioLDM2_prefs['duration']
      waveform = text_to_audio(pipe_audio_ldm2, audioLDM2_prefs['text'], random_seed, duration=10, guidance_scale=audioLDM2_prefs['guidance_scale'], n_candidate_gen_per_text=int(audioLDM2_prefs['n_candidates']), batchsize=int(audioLDM2_prefs['batch_size']), transcript=audioLDM2_prefs['transcription'] if 'speech' in model_id else "")
    except Exception as e:
      clear_last()
      alert_msg(page, "Error generating text_to_audio waveform...", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))
      return
    save_dir = os.path.join(root_dir, 'audio_out', audioLDM2_prefs['batch_folder_name'])
    if not os.path.exists(save_dir):
      os.makedirs(save_dir, exist_ok=True)
    audio_out = os.path.join(prefs['image_output'].rpartition(slash)[0], 'audio_out')
    if bool(audioLDM2_prefs['batch_folder_name']):
      audio_out = os.path.join(audio_out, audioLDM2_prefs['batch_folder_name'])
    os.makedirs(audio_out, exist_ok=True)
    #voice_dirs = os.listdir(os.path.join(root_dir, "audioldm2-tts", 'audioldm2', 'voices'))
    #print(str(voice_dirs))
    # waveform = [(16000, np.random.randn(16000)), (16000, np.random.randn(16000))]
    #TODO: batch
    fname = format_filename(audioLDM2_prefs['text'])
    if fname[-1] == '.': fname = fname[:-1]
    file_prefix = audioLDM2_prefs['file_prefix']
    audio_name = f'{file_prefix}-{fname}'
    audio_name = audio_name[:int(prefs['file_max_length'])]
    fname = available_file(save_dir, audio_name, 0, ext="wav")
    for i in range(waveform.shape[0]):
        sf.write(fname, waveform[i, 0], samplerate=16000)
    #torchaudio.save(fname, gen.squeeze(0).cpu(), 24000)
    #IPython.display.Audio('generated.wav')
    clear_last()
    clear_last()
    #a_out = Audio(src=fname, autoplay=False)
    #page.overlay.append(a_out)
    #page.update()
    display_name = fname
    #a.tofile(f"/content/dance-{i}.wav")
    if storage_type == "Colab Google Drive":
      audio_save = available_file(audio_out, audio_name, 0, ext='wav')
      shutil.copy(fname, audio_save)
      display_name = audio_save
    #prt(Row([IconButton(icon=icons.PLAY_CIRCLE_FILLED, icon_size=48, on_click=play_audio, data=a_out), Text(display_name)]))
    prt(AudioPlayer(src=fname, display=display_name, data=display_name, page=page))
    if prefs['enable_sounds']: page.snd_alert.play()

def run_bark(page):
    global bark_prefs, pipe_bark, prefs
    def prt(line):
      if type(line) == str:
        line = Text(line)
      page.bark_output.controls.append(line)
      page.bark_output.update()
    def clear_last():
      if len(page.bark_output.controls) < 1: return
      del page.bark_output.controls[-1]
      page.bark_output.update()
    def play_audio(e):
      e.control.data.play()
    if not bool(bark_prefs['text']):
      alert_msg(page, "Provide Text for the AI to create the sound of...")
      return
    progress = ProgressBar(bar_height=8)
    installer = Installing("Downloading Bark Packages...")
    prt(installer)
    try:
        import scipy
    except ModuleNotFoundError:
        installer.set_details("...installing scipy")
        run_process("pip install -qq --upgrade scipy", page=page)
        pass
    from scipy.io.wavfile import write as write_wav
    import sys
    sys.path.append(os.path.join(root_dir, 'audioldm'))
    try:
        from bark import SAMPLE_RATE, generate_audio, preload_models
        if force_updates: raise ImportError("Forcing update")
    except Exception:
        installer.set_details("...installing suno-ai/bark")
        try:
            run_process("pip install git+https://github.com/suno-ai/bark.git", page=page)
        except Exception as e:
            clear_last()
            alert_msg(page, "Error Installing Bark requirements", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip())]))
            return
        pass
    finally:
        from bark import SAMPLE_RATE, generate_audio, preload_models
    if bark_prefs['use_bettertransformer']:
      #https://colab.research.google.com/drive/1XO0RhINg4ZZCdJJmPeJ9lOQs98skJ8h_
        try:
            from optimum.bettertransformer import BetterTransformer
        except ModuleNotFoundError:
            installer.set_details("...installing Optimum BetterTransformer")
            run_process("pip install --upgrade git+https://github.com/huggingface/optimum.git", page=page)
            from optimum.bettertransformer import BetterTransformer
            pass
        from transformers import BarkModel, set_seed, AutoProcessor
        installer.set_details("...initializing suno/bark model")
        bark_model = BarkModel.from_pretrained("suno/bark", torch_dtype=torch.float16).to(torch_device)
        installer.set_details("...initializing suno/bark processor")
        processor = AutoProcessor.from_pretrained("suno/bark")
        installer.set_details("...initializing BetterTransformer model")
        bark_model = BetterTransformer.transform(bark_model, keep_original_model=False)
    import soundfile as sf
    clear_pipes()
    if not bark_prefs['use_bettertransformer']: preload_models()
    clear_last()
    audio_out = os.path.join(prefs['image_output'].rpartition(slash)[0], 'audio_out')
    if bool(bark_prefs['batch_folder_name']):
      audio_out = os.path.join(audio_out, bark_prefs['batch_folder_name'])
    os.makedirs(audio_out, exist_ok=True)
    history_prompt = bark_prefs['acoustic_prompt']
    if history_prompt == "Unconditional":
        history_prompt = None
    if history_prompt == "Anouncer":
        history_prompt = "announcer"
    for i in range(bark_prefs['n_iterations']):
        prt(Text("  Generating Bark Audio...", weight=FontWeight.BOLD))
        prt(progress)
        #random_seed = int(bark_prefs['seed']) if int(bark_prefs['seed']) > 0 else rnd.randint(0,4294967295)
        try:
            if not bark_prefs['use_bettertransformer']:
                audio_array = generate_audio(bark_prefs['text'], history_prompt=history_prompt, text_temp=bark_prefs['text_temp'], waveform_temp=bark_prefs['waveform_temp'])
            else:
                set_seed(0)
                inputs = processor(bark_prefs['text'], voice_preset=history_prompt).to(torch_device)
                audio_array = bark_model.generate(**inputs, do_sample = True, fine_temperature=bark_prefs['text_temp'], coarse_temperature=bark_prefs['waveform_temp'])
                audio_array = audio_array.cpu().numpy().squeeze()
                sample_rate = bark_model.generation_config.sample_rate
        except Exception as e:
            clear_last()
            alert_msg(page, "Error generating Bark waveform...", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))
            return
        fname = format_filename(bark_prefs['text'])
        if fname[-1] == '.': fname = fname[:-1]
        file_prefix = bark_prefs['file_prefix']
        audio_name = f'{file_prefix}-{fname}'
        audio_name = audio_name[:int(prefs['file_max_length'])]
        fname = available_file(audio_out, audio_name, i, ext="wav")
        write_wav(fname, SAMPLE_RATE if not bark_prefs['use_bettertransformer'] else sample_rate, audio_array)
        clear_last()
        clear_last()
        #a_out = Audio(src=fname, autoplay=False)
        #page.overlay.append(a_out)
        #page.update()
        display_name = fname
        prt(AudioPlayer(src=fname, display=display_name, data=display_name, page=page))
        #prt(Row([IconButton(icon=icons.PLAY_CIRCLE_FILLED, icon_size=48, on_click=play_audio, data=a_out), Text(display_name)]))
    if prefs['enable_sounds']: page.snd_alert.play()

def run_riffusion(page):
    global riffusion_prefs, pipe_riffusion, prefs
    def prt(line):
      if type(line) == str:
        line = Text(line)
      page.riffusion_output.controls.append(line)
      page.riffusion_output.update()
    def clear_last():
      if len(page.riffusion_output.controls) < 1: return
      del page.riffusion_output.controls[-1]
      page.riffusion_output.update()
    def play_audio(e):
      e.control.data.play()
    if not bool(riffusion_prefs['prompt']):
      alert_msg(page, "Provide Text for the AI to create the sound of...")
      return
    if not status['installed_diffusers']:
      alert_msg(page, "You must Install the HuggingFace Diffusers Library first... ")
      return
    progress = ProgressBar(bar_height=8)
    state_text = Text(" Downloading Riffusion Packages...", weight=FontWeight.BOLD)
    prt(Row([ProgressRing(), state_text]))
    riffusion_dir = os.path.join(root_dir, "riffusion-inference")
    #voice_dir = os.path.join(riffusion_dir, 'audioldm', 'voices')
    if not os.path.isdir(riffusion_dir):
      os.chdir(root_dir) # -b v0.3.0
      run_process("git clone https://github.com/hmartiro/riffusion-inference", page=page)
    os.chdir(riffusion_dir)
    import sys
    sys.path.append(os.path.join(riffusion_dir, 'riffusion'))
    try:
        from riffusion.spectrogram_image_converter import SpectrogramImageConverter
    except Exception:
        try:
            run_process("pip install -r requirements.txt", page=page, cwd=riffusion_dir)
        except Exception as e:
            clear_last()
            alert_msg(page, "Error Installing Riffusion requirements", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip())]))
            return
        pass
    finally:
        from riffusion.spectrogram_image_converter import SpectrogramImageConverter
        from riffusion.spectrogram_params import SpectrogramParams
        try:
            from riffusion.audio import spectrogram_from_waveform
        except Exception:
            print("Still can't find riffusion.audio import spectrogram_from_waveform")
            pass
        #from IPython.display import Audio
        from scipy.io import wavfile
    def image_from_spectrogram(spectrogram: np.ndarray, max_volume: float = 50, power_for_image: float = 0.25) -> PILImage.Image:
        data = np.power(spectrogram, power_for_image)
        data = data * 255 / max_volume
        data = 255 - data
        image = PILImage.fromarray(data.astype(np.uint8))
        image = image.transpose(PILImage.FLIP_TOP_BOTTOM)
        image = image.convert("RGB")
        return image
    model_id="riffusion/riffusion-model-v1"
    save_dir = os.path.join(root_dir, 'audio_out', riffusion_prefs['batch_folder_name'])
    if not os.path.exists(save_dir):
      os.makedirs(save_dir, exist_ok=True)
    audio_out = os.path.join(prefs['image_output'].rpartition(slash)[0], 'audio_out')
    if bool(riffusion_prefs['batch_folder_name']):
      audio_out = os.path.join(audio_out, riffusion_prefs['batch_folder_name'])
    os.makedirs(audio_out, exist_ok=True)
    #voice_dirs = os.listdir(os.path.join(root_dir, "audioldm-tts", 'audioldm', 'voices'))
    #print(str(voice_dirs))
    fname = format_filename(riffusion_prefs['prompt'])
    if fname[-1] == '.': fname = fname[:-1]
    file_prefix = riffusion_prefs['file_prefix']
    audio_name = f'{file_prefix}-{fname}'
    audio_name = audio_name[:int(prefs['file_max_length'])]

    init = riffusion_prefs['audio_file']
    if bool(init):
        if init.startswith('http'):
            init_audio = download_file(init)
        else:
            if os.path.isfile(init):
                init_audio = init
            else:
                init_audio = None
    else:
        init_audio = None

    if pipe_riffusion == None or (init_audio == None and riffusion_prefs['loaded_pipe'] == "image") or (init_audio != None and riffusion_prefs['loaded_pipe'] == "text"):
      clear_pipes()
      try:
        if init_audio == None:
            from diffusers import DiffusionPipeline #, custom_pipeline="AlanB/lpw_stable_diffusion_mod"
            pipe_riffusion = DiffusionPipeline.from_pretrained(model_id, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
            pipe_riffusion = optimize_pipe(pipe_riffusion)
            riffusion_prefs['loaded_pipe'] = "text"
        else:
            from diffusers import StableDiffusionImg2ImgPipeline
            pipe_riffusion = StableDiffusionImg2ImgPipeline.from_pretrained(model_id, torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
            pipe_riffusion = pipe_riffusion.to(torch_device)
            riffusion_prefs['loaded_pipe'] = "image"
      except Exception as e:
        clear_last()
        alert_msg(page, "Error downloading Riffusion package", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))
        return
    else:
        clear_pipes('riffusion')
    clear_last()
    prt(Text("  Generating Riffusion Sounds...", weight=FontWeight.BOLD))
    prt(progress)
    random_seed = int(riffusion_prefs['seed']) if int(riffusion_prefs['seed']) > 0 else rnd.randint(0,4294967295)
    generator = torch.Generator(device=torch_device).manual_seed(random_seed)
    try:
        if init_audio == None:
            params = SpectrogramParams()
            converter = SpectrogramImageConverter(params)
            specs = pipe_riffusion(
                riffusion_prefs['prompt'],
                negative_prompt=riffusion_prefs['negative_prompt'],
                guidance_scale=riffusion_prefs['guidance_scale'],
                steps=riffusion_prefs['steps'],
                width=riffusion_prefs['max_size'],
                height=riffusion_prefs['max_size'],
                batch_size=riffusion_prefs['batch_size'],
                generator=generator,
            ).images
        else:
            rate, data = wavfile.read(init_audio)
            data = np.mean(data, axis=1)
            data = data.astype(np.float32)
            data = data[rate*7:rate*14]
            spectrogram = spectrogram_from_waveform(waveform=data, sample_rate=rate, n_fft=8192, hop_length=512, win_length=8192)
            spec = image_from_spectrogram(spectrogram)
            specs = pipe_riffusion(
                prompt=riffusion_prefs['prompt'],
                negative_prompt=riffusion_prefs['negative_prompt'],
                image=spec,
                strength=riffusion_prefs['strength'],
                guidance_scale=riffusion_prefs['guidance_scale'],
                steps=riffusion_prefs['steps'],
                width=riffusion_prefs['max_size'],
                height=riffusion_prefs['max_size'],
                batch_size=riffusion_prefs['batch_size'],
                generator=generator,
            ).images
    except Exception as e:
      clear_last()
      alert_msg(page, "Error generating Spectrogram Image Converter from Diffusion...", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))
      return

    clear_last()
    clear_last()
    for spec in specs:
        audio_file = available_file(save_dir, audio_name, 0, ext="wav")
        image_file = available_file(save_dir, audio_name, 0)
        wav = converter.audio_from_spectrogram_image(image=spec)
        wav.export(audio_file, format='wav')
        spec.save(image_file)
        #a_out = Audio(src=audio_file, autoplay=False)
        #page.overlay.append(a_out)
        #page.update()
        display_name = audio_file
        if storage_type == "Colab Google Drive":
          audio_save = available_file(audio_out, fname, 0, ext='wav')
          image_save = available_file(audio_out, fname, 0)
          shutil.copy(audio_file, audio_save)
          shutil.copy(image_file, image_save)
          display_name = audio_save
        prt(AudioPlayer(src=audio_file, display=display_name, data=display_name, page=page))
        #prt(Row([IconButton(icon=icons.PLAY_CIRCLE_FILLED, icon_size=48, on_click=play_audio, data=a_out), Text(display_name)]))
    if prefs['enable_sounds']: page.snd_alert.play()


def run_mubert(page):
    def prt(line):
      if type(line) == str:
        line = Text(line)
      page.tortoise_output.controls.append(line)
      page.tortoise_output.update()
    def clear_last():
      if len(page.tortoise_output.controls) == 0: return
      del page.tortoise_output.controls[-1]
      page.tortoise_output.update()
    def play_audio(e):
      e.control.data.play()
    progress = ProgressBar(bar_height=8)
    state_text = Text(" Downloading Mubert Packages...", weight=FontWeight.BOLD)
    prt(Row([ProgressRing(), state_text]))
    mubert_dir = os.path.join(root_dir, "mubert-songs")
    if bool(mubert_prefs['batch_folder_name']):
        mubert_dir = os.path.join(mubert_dir, mubert_prefs['batch_folder_name'])
    if not os.path.exists(mubert_dir):
        os.makedirs(mubert_dir)
    import time
    try:
        from sentence_transformers import SentenceTransformer
    except Exception:
        run_process("pip install sentence_transformers", page=page, show=True, print=True)
        from sentence_transformers import SentenceTransformer
        pass
    try:
        import httpx
    except ModuleNotFoundError:
        run_process("pip install httpx", page=page)
        import httpx
        pass

    MUBERT_TAGS_STRING = 'tribal,action,kids,neo-classic,run 130,pumped,jazz / funk,ethnic,dubtechno,reggae,acid jazz,liquidfunk,funk,witch house,tech house,underground,artists,mystical,disco,sensorium,r&b,agender,psychedelic trance / psytrance,peaceful,run 140,piano,run 160,setting,meditation,christmas,ambient,horror,cinematic,electro house,idm,bass,minimal,underscore,drums,glitchy,beautiful,technology,tribal house,country pop,jazz & funk,documentary,space,classical,valentines,chillstep,experimental,trap,new jack swing,drama,post-rock,tense,corporate,neutral,happy,analog,funky,spiritual,sberzvuk special,chill hop,dramatic,catchy,holidays,fitness 90,optimistic,orchestra,acid techno,energizing,romantic,minimal house,breaks,hyper pop,warm up,dreamy,dark,urban,microfunk,dub,nu disco,vogue,keys,hardcore,aggressive,indie,electro funk,beauty,relaxing,trance,pop,hiphop,soft,acoustic,chillrave / ethno-house,deep techno,angry,dance,fun,dubstep,tropical,latin pop,heroic,world music,inspirational,uplifting,atmosphere,art,epic,advertising,chillout,scary,spooky,slow ballad,saxophone,summer,erotic,jazzy,energy 100,kara mar,xmas,atmospheric,indie pop,hip-hop,yoga,reggaeton,lounge,travel,running,folk,chillrave & ethno-house,detective,darkambient,chill,fantasy,minimal techno,special,night,tropical house,downtempo,lullaby,meditative,upbeat,glitch hop,fitness,neurofunk,sexual,indie rock,future pop,jazz,cyberpunk,melancholic,happy hardcore,family / kids,synths,electric guitar,comedy,psychedelic trance & psytrance,edm,psychedelic rock,calm,zen,bells,podcast,melodic house,ethnic percussion,nature,heavy,bassline,indie dance,techno,drumnbass,synth pop,vaporwave,sad,8-bit,chillgressive,deep,orchestral,futuristic,hardtechno,nostalgic,big room,sci-fi,tutorial,joyful,pads,minimal 170,drill,ethnic 108,amusing,sleepy ambient,psychill,italo disco,lofi,house,acoustic guitar,bassline house,rock,k-pop,synthwave,deep house,electronica,gabber,nightlife,sport & fitness,road trip,celebration,electro,disco house,electronic'
    MUBERT_TAGS = np.array(MUBERT_TAGS_STRING.split(','))
    MUBERT_LICENSE = "ttmmubertlicense#f0acYBenRcfeFpNT4wpYGaTQIyDI4mJGv5MfIhBFz97NXDwDNFHmMRsBSzmGsJwbTpP1A6i07AXcIeAHo5"
    MUBERT_MODE = "loop"
    MUBERT_TOKEN = "4951f6428e83172a4f39de05d5b3ab10d58560b8"

    def get_mubert_tags_embeddings(w2v_model):
        return w2v_model.encode(MUBERT_TAGS)

    def get_pat(email: str):
        r = httpx.post('https://api-b2b.mubert.com/v2/GetServiceAccess', json={"method": "GetServiceAccess", "params": {"email": email, "license": MUBERT_LICENSE, "token": MUBERT_TOKEN, "mode": MUBERT_MODE}})
        rdata = json.loads(r.text)
        if rdata['status'] != 1:
            alert_msg(page, "ERROR Requesting Mubert Service. Probably incorrect e-mail...")
        pat = rdata['data']['pat']
        return pat

    def find_similar(em, embeddings, method='cosine'):
        scores = []
        for ref in embeddings:
            if method == 'cosine':
                scores.append(1 - np.dot(ref, em) / (np.linalg.norm(ref) * np.linalg.norm(em)))
            if method == 'norm':
                scores.append(np.linalg.norm(ref - em))
        return np.array(scores), np.argsort(scores)

    def get_tags_for_prompts(w2v_model, mubert_tags_embeddings, prompts, top_n=3, debug=False):
        prompts_embeddings = w2v_model.encode(prompts)
        ret = []
        for i, pe in enumerate(prompts_embeddings):
            scores, idxs = find_similar(pe, mubert_tags_embeddings)
            top_tags = MUBERT_TAGS[idxs[:top_n]]
            top_prob = 1 - scores[idxs[:top_n]]
            if debug:
                prt(f"Prompt: {prompts[i]}\nTags: {', '.join(top_tags)}\nScores: {top_prob}\n\n\n")
            ret.append((prompts[i], list(top_tags)))
        return ret
    minilm = SentenceTransformer('all-MiniLM-L6-v2')
    mubert_tags_embeddings = get_mubert_tags_embeddings(minilm)

    def get_track_by_tags(tags, pat, duration, maxit=20, loop=False):
        if loop:
            mode = "loop"
        else:
            mode = "track"
        r = httpx.post('https://api-b2b.mubert.com/v2/RecordTrackTTM', json={"method": "RecordTrackTTM", "params": { "pat": pat,"duration": duration, "tags": tags, "mode": mode}})
        rdata = json.loads(r.text)
        assert rdata['status'] == 1, rdata['error']['text']
        trackurl = rdata['data']['tasks'][0]['download_link']
        prt('Generating your Mubert track... ')
        for i in range(maxit):
            r = httpx.get(trackurl)
            if r.status_code == 200:
                return trackurl
            time.sleep(1)

    def generate_track_by_prompt(email, prompt, duration, loop=False):
        try:
            pat = get_pat(email)
            _, tags = get_tags_for_prompts(minilm, mubert_tags_embeddings, [prompt, ])[0]
            return get_track_by_tags(tags, pat, int(duration), loop=loop), "Success", ", ".join(tags)
        except Exception as e:
            return None, str(e), ""
    #btn.click(fn=generate_track_by_prompt, inputs=[email, prompt, duration, is_loop], outputs=[out, result_msg, tags])
    clear_last()
    out, result_msg, tags = generate_track_by_prompt(mubert_prefs['email'], mubert_prefs['prompt'], mubert_prefs['duration'], loop=mubert_prefs['is_loop'])
    if out == None:
      alert_msg(page, "Error generating track by prompt. The API Key problably reached montly limit...",  content=Text(result_msg))
      return
    clear_last()
    clear_last()
    audio_out = os.path.join(prefs['image_output'].rpartition(slash)[0], 'audio_out')
    mubert_songs = os.path.join(audio_out, 'mubert_songs')
    audio_name = format_filename(mubert_prefs['prompt'])
    audio_name = f"{mubert_prefs['file_prefix']}{audio_name}"
    if bool(mubert_prefs['batch_folder_name']):
      mubert_songs = os.path.join(audio_out, mubert_prefs['batch_folder_name'])
    os.makedirs(mubert_songs, exist_ok=True)
    fname = available_file(mubert_songs, audio_name, 0, ext="mp3")
    audio_file = download_file(out)
    shutil.copy(audio_file, fname)
    #a_out = Audio(src=fname, autoplay=False)
    #page.overlay.append(a_out)
    #page.update()
    display_name = fname
    #a.tofile(f"/content/dance-{i}.wav")
    if storage_type == "Colab Google Drive":
      audio_save = available_file(audio_out, audio_name, 0, ext='mp3')
      shutil.copy(fname, audio_save)
      display_name = audio_save
    prt(AudioPlayer(src=fname, display=display_name, data=display_name, page=page))
    #prt(Row([IconButton(icon=icons.PLAY_CIRCLE_FILLED, icon_size=48, on_click=play_audio, data=a_out), Column([Text(display_name), Text(tags, style=TextThemeStyle.DISPLAY_SMALL)])]))
    if prefs['enable_sounds']: page.snd_alert.play()

def run_whisper(page):
    global whisper_prefs, whisper_requests
    def prt(line):
      if type(line) == str:
        line = Text(line, size=17)
      page.Whisper.controls.append(line)
      page.Whisper.update()
    def clear_last():
      del page.Whisper.controls[-1]
      page.Whisper.update()
    def clear_list():
      page.Whisper.controls = page.Whisper.controls[:1]
    def autoscroll(scroll=True):
      page.Whisper.auto_scroll = scroll
      page.Whisper.update()
    clear_list()
    autoscroll(True)
    progress = ProgressBar(bar_height=8)
    installer = Installing("Installing OpenAI Whisper Speech-to-Text Packages...")
    prt(installer)
    try:
        import whisper
    except Exception as e:
        installer.set_details("...openai/whisper.git")
        run_sp("pip install git+https://github.com/openai/whisper.git -q", realtime=False)
        import whisper
        pass
    installer.set_details(f"...load_model {whisper_prefs['model_size']}")
    whisper_model = whisper.load_model(whisper_prefs['model_size'])
    from_language = ""
    def transcribe(audio):
        nonlocal installer, from_language
        installer.set_details("...load_audio")
        audio = whisper.load_audio(audio)
        if whisper_prefs['trim_audio']:
            installer.set_details("...pad_or_trim audio")
            audio = whisper.pad_or_trim(audio)
        if whisper_prefs['simple_transcribe']:
            installer.set_details("...Whisper transcribe")
            options = {
                "task": "transcribe"
            }
            result = whisper.transcribe(whisper_model, audio, **options)
        else:
            installer.set_details("...log_mel_spectrogram")
            mel = whisper.log_mel_spectrogram(audio).to(whisper_model.device)
            if whisper_prefs['detect_language'] or whisper_prefs['translate']:
                installer.set_details("...detect_language")
                _, probs = whisper_model.detect_language(mel)
                from_language = max(probs, key=probs.get)
                prt(f"Detected language: {from_language}")
            installer.set_details("...DecodingOptions")
            options = whisper.DecodingOptions(fp16 = False)
            result = whisper.decode(whisper_model, mel, options)
        return result.text
    audio_path = whisper_prefs['audio_file'].strip()
    local_path = ""
    if audio_path.startswith("http"):
        if audio_path.startswith("https://youtu") or 'youtube' in audio_path:
            try:
                import ffmpeg
            except ImportError as e:
                installer.set_details("...installing ffmpeg")
                run_sp("pip install -q ffmpeg", realtime=False)
                pass
            try:
                import yt_dlp
            except ImportError as e:
                installer.set_details("...installing yt_dlp")
                run_sp("pip install yt_dlp", realtime=False)
                import yt_dlp
                pass
            f_name = ""
            def cb(d):
                nonlocal f_name
                if d["status"] == "downloading" and "total_bytes_estimate" in d:# and d["total_bytes"] > 0:
                    if d["total_bytes_estimate"] > 0:
                        dl_progress = float(d["downloaded_bytes"]) / float(d["total_bytes_estimate"])
                        progress.value = dl_progress
                        progress.update()
                else:
                    f_name = d['filename']
            installer.set_details("...getting YouTube file")
            prt(progress)
            ydl_opts = {
                'format': 'm4a/bestaudio/best',
                "progress_hooks": [cb],
                'verbose': False,
                'quiet': True,
                'postprocessors': [{
                    'key': 'FFmpegExtractAudio',
                    'preferredcodec': 'mp3',
                }]
            }
            try:
                with yt_dlp.YoutubeDL(ydl_opts) as ydl:
                    error_code = ydl.download(audio_path)
            except Exception as e:
                alert_msg(page, f"ERROR: Couldn't download YouTube video for some reason...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
                return
            clear_last()
            #print(error_code)
            if error_code != 0:
                alert_msg(page, f"ERROR CODE {error_code}: Couldn't download YouTube video for some reason...")
                return
            local_path = f"{f_name.rpartition('.')[0]}.mp3"
            #file_object = yt.formats.filter(only_audio=True)[0].download()
            #f_name = yt.safe_filename()
            #.download(convert='mp3')
            #file_object = yt.download() #b variable stores the filename and meta(if available) as object of Output class.
            #installer.set_details("...converting to mp3")
            #local_path = AudioSegment.from_file(file_object).export(f"{f_name}.mp3", format="mp3")
            #extras.Convert(file_object,'mp3',add_meta=True)
            #local_path = file_object.file_path
        else:
            installer.set_details("...downloading file")
            local_path = download_file(audio_path)
    else:
        local_path = audio_path
    if not (local_path.endswith("mp3") or local_path.endswith("wav")):
        alert_msg(page, f"ERROR: File path must be an mp3 or wav file...")
        return
    if not os.path.exists(local_path):
        alert_msg(page, f"ERROR: File not found...")
        return
    installer.set_details("")
    installer.show_progress(False)
    installer.set_message("Running Whisper-AI on your Recording...")
    prt(progress)
    try:
        #TODO: Split long files into smaller chunks to transcribe queue
        #ffmpeg -hide_banner -i <YOUR AUDIO FILE> -c copy -map 0 -segment_time 1:0:0 -f segment <OUTPUT DIR>/segment_%03d
        transcription = transcribe(local_path)
    except Exception as e:
        alert_msg(page, f"ERROR: Couldn't Transcribe audio for some reason. Possibly out of memory or something wrong with my code...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
        return
    clear_last()
    installer.set_details("")
    installer.set_message("Generated Whisper-AI Transcription:")
    prt(Text(transcription, size=20, selectable=True))
    use_ai = whisper_prefs['reformat'] or whisper_prefs['rewrite'] or whisper_prefs['summarize'] or whisper_prefs['describe'] or whisper_prefs['article'] or whisper_prefs['keypoints'] or whisper_prefs['keywords']
    if use_ai:
        good_key = True
        if whisper_prefs['AI_engine'] == "OpenAI GPT-3" or  whisper_prefs['AI_engine'] == "ChatGPT-3.5 Turbo":
            try:
                if not bool(prefs['OpenAI_api_key']): good_key = False
            except NameError: good_key = False
            if not good_key:
                alert_msg(page, f"Missing OpenAI_api_key... Define your key in Settings.")
                return
            else:
                try:
                    import openai
                except ModuleNotFoundError:
                    installer.set_details("...installing openai")
                    run_sp("pip install --upgrade openai -qq", realtime=False)
                    pass
                finally:
                    import openai
                try:
                    openai.api_key = prefs['OpenAI_api_key']
                except:
                    alert_msg(page, "Invalid OpenAI API Key. Change in Settings...")
                    return
                installer.set_details("")
    def question(request):
        if whisper_prefs['AI_engine'] == "OpenAI GPT-3":
            response = openai.Completion.create(engine="text-davinci-003", prompt=request, max_tokens=2400, temperature=whisper_prefs['AI_temperature'], presence_penalty=1)
            result = response["choices"][0]["text"].strip()
        elif whisper_prefs['AI_engine'] == "ChatGPT-3.5 Turbo":
            response = openai.ChatCompletion.create(model="gpt-3.5-turbo-16k", messages=[{"role": "user", "content": request}])
            result = response["choices"][0]["message"]["content"].strip()
        return result
    if whisper_prefs['reformat']:
        prt("Reformat of transcript:")
        response = question(f"{whisper_requests['reformat']}\n{transcription}")
        prt(Text(response, size=18, selectable=True))
    if whisper_prefs['rewrite']:
        prt("Rewrite edit of transcript:")
        response = question(f"{whisper_requests['rewrite']}\n{transcription}")
        prt(Text(response, size=18, selectable=True))
    if whisper_prefs['summarize']:
        prt("Summary of transcript:")
        response = question(f"{whisper_requests['summarize']}\n{transcription}")
        prt(Text(response, size=18, selectable=True))
    if whisper_prefs['describe']:
        prt("Description of transcript:")
        response = question(f"{whisper_requests['describe']}\n{transcription}")
        prt(Text(response, size=18, selectable=True))
    if whisper_prefs['article']:
        prt("Article of transcript:")
        response = question(f"{whisper_requests['article']}\n{transcription}")
        prt(Text(response, size=18, selectable=True))
    if whisper_prefs['keypoints']:
        prt("Key Points of transcript:")
        response = question(f"{whisper_requests['keypoints']}\n{transcription}")
        prt(Text(response, size=18, selectable=True))
    if whisper_prefs['keywords']:
        prt("SEO Keywords of transcript:")
        response = question(f"{whisper_requests['keywords']}\n{transcription}")
        prt(Text(response, size=18, selectable=True))
    if whisper_prefs['translate']:
        prt("Translation of transcript:")
        response = question(f"Translate the following text from {from_language} into {whisper_prefs['to_language']}.\n{transcription}")
        prt(Text(response, size=18, selectable=True))
    autoscroll(False)
    if prefs['enable_sounds']: page.snd_alert.play()

def run_voice_fixer(page):
    global voice_fixer_prefs, pipe_voice_fixer
    def prt(line):
      if type(line) == str:
        line = Text(line, size=17)
      page.VoiceFixer.controls.append(line)
      page.VoiceFixer.update()
    def clear_last():
      del page.VoiceFixer.controls[-1]
      page.VoiceFixer.update()
    def clear_list():
      page.VoiceFixer.controls = page.VoiceFixer.controls[:1]
    def autoscroll(scroll=True):
      page.VoiceFixer.auto_scroll = scroll
      page.VoiceFixer.update()
    def play_audio(e):
      e.control.data.play()
    if not (voice_fixer_prefs['mode_0'] or voice_fixer_prefs['mode_1'] or voice_fixer_prefs['mode_2']):
        alert_msg(page, f"You must select at least one Fixer Mode to restore...")
        return
    clear_list()
    autoscroll(True)
    progress = ProgressBar(bar_height=8)
    installer = Installing("Installing VoiceFixer Packages...")
    prt(installer)
    clear_pipes("voice_fixer")
    try:
        from voicefixer import VoiceFixer
    except ModuleNotFoundError as e:
        installer.set_details("...installing voicefixer (takes a while)")
        run_sp("pip install voicefixer --upgrade", realtime=False)
        from voicefixer import VoiceFixer
        pass
    if pipe_voice_fixer == None:
        installer.set_details("...initializing voicefixer model")
        pipe_voice_fixer = VoiceFixer()
    audio_path = voice_fixer_prefs['audio_file'].strip()
    local_path = ""
    if audio_path.startswith("http"):
        if audio_path.startswith("https://youtu") or 'youtube' in audio_path:
            try:
                import ffmpeg
            except ImportError as e:
                installer.set_details("...installing ffmpeg")
                run_sp("pip install -q ffmpeg", realtime=False)
                pass
            try:
                import yt_dlp
            except ImportError as e:
                installer.set_details("...installing yt_dlp")
                run_sp("pip install yt_dlp", realtime=False)
                import yt_dlp
                pass
            f_name = ""
            def cb(d):
                nonlocal f_name
                if d["status"] == "downloading" and "total_bytes_estimate" in d:# and d["total_bytes"] > 0:
                    if d["total_bytes_estimate"] > 0:
                        dl_progress = float(d["downloaded_bytes"]) / float(d["total_bytes_estimate"])
                        progress.value = dl_progress
                        progress.update()
                else:
                    f_name = d['filename']
            installer.set_details("...getting YouTube file")
            prt(progress)
            ydl_opts = {
                'format': 'm4a/bestaudio/best',
                "progress_hooks": [cb],
                'verbose': False,
                'quiet': True,
                'postprocessors': [{
                    'key': 'FFmpegExtractAudio',
                    'preferredcodec': 'mp3',
                }]
            }
            try:
                with yt_dlp.YoutubeDL(ydl_opts) as ydl:
                    error_code = ydl.download(audio_path)
            except Exception as e:
                alert_msg(page, f"ERROR: Couldn't download YouTube video for some reason...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
                return
            clear_last()
            #print(error_code)
            if error_code != 0:
                alert_msg(page, f"ERROR CODE {error_code}: Couldn't download YouTube video for some reason...")
                return
            local_path = f"{f_name.rpartition('.')[0]}.mp3"
        else:
            installer.set_details("...downloading file")
            local_path = download_file(audio_path)
    else:
        local_path = audio_path
    if not (local_path.endswith("mp3") or local_path.endswith("wav") or local_path.endswith("flac")):
        alert_msg(page, f"ERROR: File path must be a wav, mp3 or flac file...")
        return
    if not os.path.exists(local_path):
        alert_msg(page, f"ERROR: Audio File not found...")
        return
    if local_path.endswith("mp3"):
        try:
            import ffmpeg
        except ImportError as e:
            installer.set_details("...installing ffmpeg")
            run_sp("pip install -q ffmpeg", realtime=False)
            pass
        try:
            import pydub
        except ImportError:
            installer.set_details("...installing pydub")
            run_sp("pip install -q pydub", realtime=False)
            import pydub
            pass
        from pydub import AudioSegment
        installer.set_details("...converting from mp3")
        sound = AudioSegment.from_mp3(local_path)
        wav_file = local_path.rpartition(slash)[2].rpartition(".")[0]
        local_path = available_file(root_dir, wav_file, 0, ext='wav')
        sound = sound.set_frame_rate(44100)
        installer.set_details("...converting to wav")
        sound.export(local_path, format="wav")
    #save_dir = os.path.join(root_dir, 'audio_out', voice_fixer_prefs['batch_folder_name'])
    #if not os.path.exists(save_dir):
    #    os.makedirs(save_dir, exist_ok=True)
    audio_out = os.path.join(prefs['image_output'].rpartition(slash)[0], 'audio_out')
    if bool(voice_fixer_prefs['batch_folder_name']):
        audio_out = os.path.join(audio_out, voice_fixer_prefs['batch_folder_name'])
    os.makedirs(audio_out, exist_ok=True)
    if bool(voice_fixer_prefs['audio_name']):
        fname = format_filename(voice_fixer_prefs['audio_name'], force_underscore=True)
    else: fname = "output"
    fname = f"{voice_fixer_prefs['file_prefix']}{fname}"
    output_file = available_file(audio_out, fname, 0, ext='wav')
    modes = []
    if voice_fixer_prefs['mode_0']: modes.append(0)
    if voice_fixer_prefs['mode_1']: modes.append(1)
    if voice_fixer_prefs['mode_2']: modes.append(2)
    clear_last()
    try:
        for mode in modes:
            prt(f"Running VoiceFixer mode {mode} on your Recording...")
            prt(progress)
            output_file = available_file(audio_out, f"{fname}-mode{mode}", 0, ext='wav', no_num=True)
            pipe_voice_fixer.restore(input=local_path, output=output_file, cuda=torch_device == "cuda", mode=mode)
            clear_last()
            clear_last()
            prt(AudioPlayer(src=output_file, display=output_file, page=page))
            #a_out = Audio(src=output_file, autoplay=False)
            #page.overlay.append(a_out)
            #page.update()
            #display_name = output_file
            #prt(Row([IconButton(icon=icons.PLAY_CIRCLE_FILLED, icon_size=48, on_click=play_audio, data=a_out), Text(display_name)]))
    except Exception as e:
        alert_msg(page, f"ERROR: Couldn't Restore audio for some reason. Possibly out of memory or something wrong with my code...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
        clear_last()
        return
    autoscroll(False)
    if prefs['enable_sounds']: page.snd_alert.play()


loaded_StableUnCLIP = None
def run_unCLIP(page, from_list=False):
    global unCLIP_prefs, pipe_unCLIP, loaded_StableUnCLIP
    if not status['installed_diffusers']:
      alert_msg(page, "You must Install the HuggingFace Diffusers Library first... ")
      return
    def prt(line, update=True):
      if type(line) == str:
        line = Text(line)
      if from_list:
        page.imageColumn.controls.append(line)
        if update:
          page.imageColumn.update()
      else:
        page.unCLIP.controls.append(line)
        if update:
          page.unCLIP.update()
    def clear_last():
      if from_list:
        del page.imageColumn.controls[-1]
        page.imageColumn.update()
      else:
        del page.unCLIP.controls[-1]
        page.unCLIP.update()
    def clear_list():
      if from_list:
        page.imageColumn.controls.clear()
      else:
        page.unCLIP.controls = page.unCLIP.controls[:1]
    def autoscroll(scroll=True):
      if from_list:
        page.imageColumn.auto_scroll = scroll
        page.imageColumn.update()
        page.unCLIP.auto_scroll = scroll
        page.unCLIP.update()
      else:
        page.unCLIP.auto_scroll = scroll
        page.unCLIP.update()
    progress = ProgressBar(bar_height=8)
    if unCLIP_prefs['use_StableUnCLIP_pipeline']:
      total_steps = unCLIP_prefs['prior_num_inference_steps']
    else:
      total_steps = unCLIP_prefs['prior_num_inference_steps'] + unCLIP_prefs['decoder_num_inference_steps'] + unCLIP_prefs['super_res_num_inference_steps']
    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep}"
      progress.update()
    unCLIP_prompts = []
    if from_list:
      if len(prompts) < 1:
        alert_msg(page, "You need to add Prompts to your List first... ")
        return
      for p in prompts:
        unCLIP_prompts.append(p.prompt)
    else:
      if not bool(unCLIP_prefs['prompt']):
        alert_msg(page, "You need to add a Text Prompt first... ")
        return
      unCLIP_prompts.append(unCLIP_prefs['prompt'])
    autoscroll(True)
    clear_list()
    from PIL.PngImagePlugin import PngInfo
    clear_pipes('unCLIP')
    torch.cuda.empty_cache()
    torch.cuda.reset_max_memory_allocated()
    torch.cuda.reset_peak_memory_stats()
    if from_list:
      page.tabs.selected_index = 4
      page.tabs.update()
    model_id = "kakaobrain/karlo-v1-alpha"
    stable = "Stable " if unCLIP_prefs['use_StableUnCLIP_pipeline'] else ""
    if pipe_unCLIP != None and ((loaded_StableUnCLIP == True and not unCLIP_prefs['use_StableUnCLIP_pipeline']) or (loaded_StableUnCLIP == False and unCLIP_prefs['use_StableUnCLIP_pipeline'])):
        del pipe_unCLIP
        flush()
        pipe_unCLIP = None
    if pipe_unCLIP == None:
        prt(Installing(f"  Downloading {stable}unCLIP Kakaobrain Karlo Pipeline... It's a big one, see console for progress."))
        try:
            if unCLIP_prefs['use_StableUnCLIP_pipeline']:
              from diffusers import UnCLIPScheduler, DDPMScheduler, StableUnCLIPPipeline
              from diffusers.models import PriorTransformer
              from transformers import CLIPTokenizer, CLIPTextModelWithProjection
              prior = PriorTransformer.from_pretrained(model_id, subfolder="prior", torch_dtype=torch.float16)
              prior_text_model_id = "openai/clip-vit-large-patch14"
              prior_tokenizer = CLIPTokenizer.from_pretrained(prior_text_model_id)
              prior_text_model = CLIPTextModelWithProjection.from_pretrained(prior_text_model_id, torch_dtype=torch.float16)
              prior_scheduler = UnCLIPScheduler.from_pretrained(model_id, subfolder="prior_scheduler")
              #prior_scheduler = DDPMScheduler.from_config(prior_scheduler.config)
              prior_scheduler = pipeline_scheduler(prior_scheduler, from_scheduler=False)
              stable_unclip_model_id = "stabilityai/stable-diffusion-2-1-unclip-small"
              pipe_unCLIP = StableUnCLIPPipeline.from_pretrained(
                  stable_unclip_model_id,
                  torch_dtype=torch.float16,
                  variant="fp16",
                  prior_tokenizer=prior_tokenizer,
                  prior_text_encoder=prior_text_model,
                  prior=prior,
                  prior_scheduler=prior_scheduler,
              )
              pipe_unCLIP.to(torch_device)
              pipe_unCLIP.enable_attention_slicing()
              pipe_unCLIP.enable_sequential_cpu_offload()
              #from diffusers import DiffusionPipeline
              #pipe_unCLIP = DiffusionPipeline.from_pretrained(model_id, custom_pipeline="stable_unclip", torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, decoder_pipe_kwargs=dict(image_encoder=None))
              #pipe_unCLIP.to(torch_device)
              #pipe_unCLIP = optimize_pipe(pipe_unCLIP)
              loaded_StableUnCLIP = True
            else:
              from diffusers import UnCLIPPipeline
              pipe_unCLIP = UnCLIPPipeline.from_pretrained(model_id, torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
              #pipe_unCLIP.to(torch_device)
              pipe_unCLIP = optimize_pipe(pipe_unCLIP)
              loaded_StableUnCLIP = False
        except Exception as e:
            clear_last()
            alert_msg(page, f"Error Downloading {stable}unCLIP Pipeline", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            return
        pipe_unCLIP.set_progress_bar_config(disable=True)
        clear_last()
    s = "s" if unCLIP_prefs['num_images'] > 1 else ""
    prt(f" Generating {stable}unCLIP{s} of your Image...")
    batch_output = os.path.join(stable_dir, unCLIP_prefs['batch_folder_name'])
    if not os.path.isdir(batch_output):
      os.makedirs(batch_output)
    batch_output = os.path.join(prefs['image_output'], unCLIP_prefs['batch_folder_name'])
    if not os.path.isdir(batch_output):
      os.makedirs(batch_output)
    for pr in unCLIP_prompts:
        for num in range(unCLIP_prefs['num_images']):
            prt(progress)
            autoscroll(False)
            random_seed = (int(unCLIP_prefs['seed']) + num) if int(unCLIP_prefs['seed']) > 0 else rnd.randint(0,4294967295)
            generator = torch.Generator(device=torch_device).manual_seed(random_seed)
            try:
                if unCLIP_prefs['use_StableUnCLIP_pipeline']:#decoder_num_inference_steps=unCLIP_prefs['decoder_num_inference_steps'], super_res_num_inference_steps=unCLIP_prefs['super_res_num_inference_steps'], decoder_guidance_scale=unCLIP_prefs['decoder_guidance_scale'],
                  images = pipe_unCLIP([pr], prior_num_inference_steps=unCLIP_prefs['prior_num_inference_steps'], prior_guidance_scale=unCLIP_prefs['prior_guidance_scale'], num_images_per_prompt=1, width=512, height=512, generator=generator, callback=callback_fnc, callback_steps=1).images
                else:
                  images = pipe_unCLIP([pr], prior_num_inference_steps=unCLIP_prefs['prior_num_inference_steps'], decoder_num_inference_steps=unCLIP_prefs['decoder_num_inference_steps'], super_res_num_inference_steps=unCLIP_prefs['super_res_num_inference_steps'], prior_guidance_scale=unCLIP_prefs['prior_guidance_scale'], decoder_guidance_scale=unCLIP_prefs['decoder_guidance_scale'], num_images_per_prompt=1, generator=generator, callback=callback_fnc, callback_steps=1).images
            except Exception as e:
                clear_last()
                alert_msg(page, f"Error running {stable}unCLIP Pipeline", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
                return
            autoscroll(True)
            clear_last()
            fname = format_filename(pr)

            if prefs['file_suffix_seed']: fname += f"-{random_seed}"
            for image in images:
                image_path = available_file(os.path.join(stable_dir, unCLIP_prefs['batch_folder_name']), fname, num)
                unscaled_path = image_path
                output_file = image_path.rpartition(slash)[2]
                image.save(image_path)
                out_path = image_path.rpartition(slash)[0]
                upscaled_path = os.path.join(out_path, output_file)
                if not unCLIP_prefs['display_upscaled_image'] or not unCLIP_prefs['apply_ESRGAN_upscale']:
                    prt(Row([ImageButton(src=unscaled_path, width=512, height=512, data=upscaled_path, page=page)], alignment=MainAxisAlignment.CENTER))
                    #prt(Row([Img(src=unscaled_path, fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
                if unCLIP_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
                    w = int(unCLIP_prefs['width'] * unCLIP_prefs["enlarge_scale"])
                    h = int(unCLIP_prefs['height'] * unCLIP_prefs["enlarge_scale"])
                    prt(Row([Text(f'Enlarging {unCLIP_prefs["enlarge_scale"]}X to {w}x{h}')], alignment=MainAxisAlignment.CENTER))
                    os.chdir(os.path.join(dist_dir, 'Real-ESRGAN'))
                    upload_folder = 'upload'
                    result_folder = 'results'
                    if os.path.isdir(upload_folder):
                        shutil.rmtree(upload_folder)
                    if os.path.isdir(result_folder):
                        shutil.rmtree(result_folder)
                    os.mkdir(upload_folder)
                    os.mkdir(result_folder)
                    short_name = f'{fname[:80]}-{num}.png'
                    dst_path = os.path.join(dist_dir, 'Real-ESRGAN', upload_folder, short_name)
                    #print(f'Moving {fpath} to {dst_path}')
                    #shutil.move(fpath, dst_path)
                    shutil.copy(image_path, dst_path)
                    #faceenhance = ' --face_enhance' if unCLIP_prefs["face_enhance"] else ''
                    faceenhance = ''
                    run_sp(f'python inference_realesrgan.py -n realesr-general-x4v3 -i upload --outscale {unCLIP_prefs["enlarge_scale"]}{faceenhance}', cwd=os.path.join(dist_dir, 'Real-ESRGAN'), realtime=False)
                    out_file = short_name.rpartition('.')[0] + '_out.png'
                    upscaled_path = os.path.join(out_path, output_file)
                    shutil.move(os.path.join(dist_dir, 'Real-ESRGAN', result_folder, out_file), upscaled_path)
                    image_path = upscaled_path
                    os.chdir(stable_dir)
                    clear_last()
                if prefs['save_image_metadata']:
                    img = PILImage.open(image_path)
                    metadata = PngInfo()
                    metadata.add_text("artist", prefs['meta_ArtistName'])
                    metadata.add_text("copyright", prefs['meta_Copyright'])
                    metadata.add_text("software", "Stable Diffusion Deluxe" + f", upscaled {unCLIP_prefs['enlarge_scale']}x with ESRGAN" if unCLIP_prefs['apply_ESRGAN_upscale'] else "")
                    metadata.add_text("pipeline", f"{stable}unCLIP")
                    if prefs['save_config_in_metadata']:
                      metadata.add_text("title", pr)
                      config_json = unCLIP_prefs.copy()
                      config_json['model_path'] = model_id
                      config_json['seed'] = random_seed
                      del config_json['num_images']
                      del config_json['display_upscaled_image']
                      del config_json['batch_folder_name']
                      if not config_json['apply_ESRGAN_upscale']:
                        del config_json['enlarge_scale']
                        del config_json['apply_ESRGAN_upscale']
                      metadata.add_text("config_json", json.dumps(config_json, ensure_ascii=True, indent=4))
                    img.save(image_path, pnginfo=metadata)
                #TODO: PyDrive
                if storage_type == "Colab Google Drive":
                    new_file = available_file(os.path.join(prefs['image_output'], unCLIP_prefs['batch_folder_name']), fname, num)
                    out_path = new_file
                    shutil.copy(image_path, new_file)
                elif bool(prefs['image_output']):
                    new_file = available_file(os.path.join(prefs['image_output'], unCLIP_prefs['batch_folder_name']), fname, num)
                    out_path = new_file
                    shutil.copy(image_path, new_file)
                time.sleep(0.2)
                if unCLIP_prefs['display_upscaled_image']:
                    time.sleep(0.6)
                    prt(Row([ImageButton(src=upscaled_path, data=upscaled_path, width=512 * float(unCLIP_prefs["enlarge_scale"]), height=512 * float(unCLIP_prefs["enlarge_scale"]), page=page)], alignment=MainAxisAlignment.CENTER))
                    #prt(Row([Img(src=upscaled_path, fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
                prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))
    autoscroll(False)
    if prefs['enable_sounds']: page.snd_alert.play()

def run_unCLIP_image_variation(page, from_list=False):
    global unCLIP_image_variation_prefs, pipe_unCLIP_image_variation
    if not status['installed_diffusers']:
      alert_msg(page, "You must Install the HuggingFace Diffusers Library first... ")
      return
    def prt(line, update=True):
      if type(line) == str:
        line = Text(line)
      if from_list:
        page.imageColumn.controls.append(line)
        if update:
          page.imageColumn.update()
      else:
        page.UnCLIP_ImageVariation.controls.append(line)
        if update:
          page.UnCLIP_ImageVariation.update()
    def clear_last():
      if from_list:
        del page.imageColumn.controls[-1]
        page.imageColumn.update()
      else:
        del page.UnCLIP_ImageVariation.controls[-1]
        page.UnCLIP_ImageVariation.update()
    def clear_list():
      if from_list:
        page.imageColumn.controls.clear()
      else:
        page.UnCLIP_ImageVariation.controls = page.UnCLIP_ImageVariation.controls[:1]
    def autoscroll(scroll=True):
      if from_list:
        page.imageColumn.auto_scroll = scroll
        page.imageColumn.update()
        page.UnCLIP_ImageVariation.auto_scroll = scroll
        page.UnCLIP_ImageVariation.update()
      else:
        page.UnCLIP_ImageVariation.auto_scroll = scroll
        page.UnCLIP_ImageVariation.update()
    progress = ProgressBar(bar_height=8)
    total_steps = unCLIP_image_variation_prefs['decoder_num_inference_steps'] + unCLIP_image_variation_prefs['super_res_num_inference_steps']
    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep}"
      progress.update()
    unCLIP_image_variation_inits = []
    if from_list:
      if len(prompts) < 1:
        alert_msg(page, "You need to add Prompts to your List first... ")
        return
      for p in prompts:
        if bool(p['init_image']):
          unCLIP_image_variation_inits.append(p.prompt)
    else:
      if not bool(unCLIP_image_variation_prefs['init_image']):
        alert_msg(page, "You need to add a Initial Image first... ")
        return
      unCLIP_image_variation_inits.append(unCLIP_image_variation_prefs['init_image'])
    autoscroll(True)
    clear_list()
    from io import BytesIO
    from PIL.PngImagePlugin import PngInfo
    from PIL import ImageOps
    if from_list:
      page.tabs.selected_index = 4
      page.tabs.update()
    clear_pipes('unCLIP_image_variation')
    torch.cuda.empty_cache()
    torch.cuda.reset_max_memory_allocated()
    torch.cuda.reset_peak_memory_stats()
    if pipe_unCLIP_image_variation == None:
        from diffusers import UnCLIP_ImageVariationPipeline
        prt(Installing(" Downloading unCLIP Image Variation Kakaobrain Karlo Pipeline... It's a big one, see console for progress."))
        try:
            pipe_unCLIP_image_variation = UnCLIP_ImageVariationPipeline.from_pretrained("kakaobrain/karlo-v1-alpha-image-variations", torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
            #pipe_unCLIP_image_variation.to(torch_device)
            pipe_unCLIP_image_variation = optimize_pipe(pipe_unCLIP_image_variation)
        except Exception as e:
            clear_last()
            alert_msg(page, "Error Downloading unCLIP Image Variation Pipeline", content=Text(str(e)))
            return
        pipe_unCLIP_image_variation.set_progress_bar_config(disable=True)
        clear_last()
    s = "s" if unCLIP_image_variation_prefs['num_images'] > 1 else ""
    prt(f"Generating unCLIP Image Variation{s} of your Image...")
    batch_output = os.path.join(stable_dir, unCLIP_image_variation_prefs['batch_folder_name'])
    if not os.path.isdir(batch_output):
      os.makedirs(batch_output)
    batch_output = os.path.join(prefs['image_output'], unCLIP_image_variation_prefs['batch_folder_name'])
    if not os.path.isdir(batch_output):
      os.makedirs(batch_output)
    for init in unCLIP_image_variation_inits:
        if init.startswith('http'):
          init_img = PILImage.open(requests.get(init, stream=True).raw)
        else:
          if os.path.isfile(init):
            init_img = PILImage.open(init)
          else:
            alert_msg(page, f"ERROR: Couldn't find your init_image {init}")
            return
        width, height = init_img.size
        width, height = scale_dimensions(width, height, unCLIP_image_variation_prefs['max_size'])
        init_img = init_img.resize((width, height), resample=PILImage.BICUBIC)
        init_img = ImageOps.exif_transpose(init_img).convert("RGB")
        for num in range(unCLIP_image_variation_prefs['num_images']):
            prt(progress)
            autoscroll(False)
            random_seed = (int(unCLIP_image_variation_prefs['seed']) + num) if int(unCLIP_image_variation_prefs['seed']) > 0 else rnd.randint(0,4294967295)
            generator = torch.Generator(device=torch_device).manual_seed(random_seed)
            try:
                images = pipe_unCLIP_image_variation(image=init_img, decoder_num_inference_steps=unCLIP_image_variation_prefs['decoder_num_inference_steps'], super_res_num_inference_steps=unCLIP_image_variation_prefs['super_res_num_inference_steps'], decoder_guidance_scale=unCLIP_image_variation_prefs['decoder_guidance_scale'], num_images_per_prompt=1, generator=generator).images #, callback=callback_fnc, callback_steps=1
            except Exception as e:
                clear_last()
                alert_msg(page, "Error running unCLIP Image Variation Pipeline", content=Text(str(e)))
                return
            clear_last()
            autoscroll(True)
            #fname = format_filename(unCLIP_image_variation_prefs['file_name'])
            fname = init.rpartition(slash)[2].rpartition('.')[0]
            if prefs['file_suffix_seed']: fname += f"-{random_seed}"
            for image in images:
                image_path = available_file(os.path.join(stable_dir, unCLIP_image_variation_prefs['batch_folder_name']), fname, num)
                unscaled_path = image_path
                output_file = image_path.rpartition(slash)[2]
                image.save(image_path)
                out_path = image_path.rpartition(slash)[0]
                upscaled_path = os.path.join(out_path, output_file)
                if not unCLIP_image_variation_prefs['display_upscaled_image'] or not unCLIP_image_variation_prefs['apply_ESRGAN_upscale']:
                    prt(Row([ImageButton(src=unscaled_path, data=upscaled_path, width=512, height=512, page=page)], alignment=MainAxisAlignment.CENTER))
                    #prt(Row([Img(src=unscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
                if unCLIP_image_variation_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
                    os.chdir(os.path.join(dist_dir, 'Real-ESRGAN'))
                    upload_folder = 'upload'
                    result_folder = 'results'
                    if os.path.isdir(upload_folder):
                        shutil.rmtree(upload_folder)
                    if os.path.isdir(result_folder):
                        shutil.rmtree(result_folder)
                    os.mkdir(upload_folder)
                    os.mkdir(result_folder)
                    short_name = f'{fname[:80]}-{num}.png'
                    dst_path = os.path.join(dist_dir, 'Real-ESRGAN', upload_folder, short_name)
                    #print(f'Moving {fpath} to {dst_path}')
                    #shutil.move(fpath, dst_path)
                    shutil.copy(image_path, dst_path)
                    #faceenhance = ' --face_enhance' if unCLIP_image_variation_prefs["face_enhance"] else ''
                    faceenhance = ''
                    run_sp(f'python inference_realesrgan.py -n realesr-general-x4v3 -i upload --outscale {unCLIP_image_variation_prefs["enlarge_scale"]}{faceenhance}', cwd=os.path.join(dist_dir, 'Real-ESRGAN'), realtime=False)
                    out_file = short_name.rpartition('.')[0] + '_out.png'
                    shutil.move(os.path.join(dist_dir, 'Real-ESRGAN', result_folder, out_file), upscaled_path)
                    image_path = upscaled_path
                    os.chdir(stable_dir)
                    if unCLIP_image_variation_prefs['display_upscaled_image']:
                        time.sleep(0.6)
                        prt(Row([ImageButton(src=upscaled_path, data=upscaled_path, width=512 * float(unCLIP_image_variation_prefs["enlarge_scale"]), height=512 * float(unCLIP_image_variation_prefs["enlarge_scale"]), page=page)], alignment=MainAxisAlignment.CENTER))
                        #prt(Row([Img(src=upscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
                if prefs['save_image_metadata']:
                    img = PILImage.open(image_path)
                    metadata = PngInfo()
                    metadata.add_text("artist", prefs['meta_ArtistName'])
                    metadata.add_text("copyright", prefs['meta_Copyright'])
                    metadata.add_text("software", "Stable Diffusion Deluxe" + f", upscaled {unCLIP_image_variation_prefs['enlarge_scale']}x with ESRGAN" if unCLIP_image_variation_prefs['apply_ESRGAN_upscale'] else "")
                    metadata.add_text("pipeline", "unCLIP_image_variation")
                    if prefs['save_config_in_metadata']:
                      #metadata.add_text("title", unCLIP_image_variation_prefs['file_name'])
                      config_json = unCLIP_image_variation_prefs.copy()
                      config_json['model_path'] = "fusing/karlo-image-variations-diffusers"
                      config_json['seed'] = random_seed
                      del config_json['num_images']
                      del config_json['display_upscaled_image']
                      del config_json['batch_folder_name']
                      del config_json['file_name']
                      if not config_json['apply_ESRGAN_upscale']:
                        del config_json['enlarge_scale']
                        del config_json['apply_ESRGAN_upscale']
                      metadata.add_text("config_json", json.dumps(config_json, ensure_ascii=True, indent=4))
                    img.save(image_path, pnginfo=metadata)
                #TODO: PyDrive
                if storage_type == "Colab Google Drive":
                    new_file = available_file(os.path.join(prefs['image_output'], unCLIP_image_variation_prefs['batch_folder_name']), fname, num)
                    out_path = new_file
                    shutil.copy(image_path, new_file)
                elif bool(prefs['image_output']):
                    new_file = available_file(os.path.join(prefs['image_output'], unCLIP_image_variation_prefs['batch_folder_name']), fname, num)
                    out_path = new_file
                    shutil.copy(image_path, new_file)
                time.sleep(0.2)
                prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))
    autoscroll(False)
    if prefs['enable_sounds']: page.snd_alert.play()

def run_unCLIP_interpolation(page, from_list=False):
    global unCLIP_interpolation_prefs, pipe_unCLIP_interpolation, loaded_StableUnCLIP
    if not status['installed_diffusers']:
      alert_msg(page, "You must Install the HuggingFace Diffusers Library first... ")
      return
    def prt(line, update=True):
      if type(line) == str:
        line = Text(line)
      if from_list:
        page.imageColumn.controls.append(line)
        if update:
          page.imageColumn.update()
      else:
        page.unCLIP_interpolation_output.controls.append(line)
        if update:
          page.unCLIP_interpolation_output.update()
    def clear_last():
      if from_list:
        del page.imageColumn.controls[-1]
        page.imageColumn.update()
      else:
        del page.unCLIP_interpolation_output.controls[-1]
        page.unCLIP_interpolation_output.update()
    def clear_list():
      if from_list:
        page.imageColumn.controls.clear()
      else:
        page.unCLIP_Interpolation.controls = page.unCLIP_Interpolation.controls[:1]
    def autoscroll(scroll=True):
      if from_list:
        page.imageColumn.auto_scroll = scroll
        page.imageColumn.update()
        page.unCLIP_Interpolation.auto_scroll = scroll
        page.unCLIP_Interpolation.update()
      else:
        page.unCLIP_Interpolation.auto_scroll = scroll
        page.unCLIP_Interpolation.update()
    progress = ProgressBar(bar_height=8)
    total_steps = unCLIP_interpolation_prefs['prior_num_inference_steps'] + unCLIP_interpolation_prefs['decoder_num_inference_steps'] + unCLIP_interpolation_prefs['super_res_num_inference_steps']
    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep}"
      progress.update()
    unCLIP_interpolation_prompts = []
    if from_list:
      if len(prompts) < 1:
        alert_msg(page, "You need to add Prompts to your List first... ")
        return
      for p in prompts:
        unCLIP_interpolation_prompts.append({'start_prompt': p.prompt, 'end_prompt':p['prompt2']})
    else:
      if not bool(unCLIP_interpolation_prefs['prompt']):
        alert_msg(page, "You need to add a Text Prompt first... ")
        return
      unCLIP_interpolation_prompts.append({'start_prompt': unCLIP_interpolation_prefs['prompt'], 'end_prompt':unCLIP_interpolation_prefs['end_prompt']})
    autoscroll(True)
    clear_list()
    from PIL.PngImagePlugin import PngInfo
    clear_pipes()#'unCLIP_interpolation')
    torch.cuda.empty_cache()
    torch.cuda.reset_max_memory_allocated()
    torch.cuda.reset_peak_memory_stats()
    if from_list:
      page.tabs.selected_index = 4
      page.tabs.update()
    model_id = "kakaobrain/karlo-v1-alpha"
    stable = "Stable "
    if pipe_unCLIP_interpolation != None:
        del pipe_unCLIP_interpolation
        flush()
        pipe_unCLIP_interpolation = None
    if pipe_unCLIP_interpolation == None:
        prt(Installing(f"Downloading {stable}unCLIP Kakaobrain Karlo Pipeline... It's a big one, see console for progress."))
        try:
            from diffusers import DiffusionPipeline
            pipe_unCLIP_interpolation = DiffusionPipeline.from_pretrained(model_id, custom_pipeline="AlanB/unclip_text_interpolation_mod", torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None) #, decoder_pipe_kwargs=dict(image_encoder=None)
            pipe_unCLIP_interpolation.to(torch_device)
            pipe_unCLIP_interpolation.enable_attention_slicing()
            pipe_unCLIP_interpolation.enable_sequential_cpu_offload()
            loaded_StableUnCLIP = True
        except Exception as e:
            clear_last()
            alert_msg(page, f"Error Downloading {stable}unCLIP Pipeline", content=Text(str(e)))
            return
        #pipe_unCLIP_interpolation.set_progress_bar_config(disable=True)
        clear_last()
    s = "s" if unCLIP_interpolation_prefs['num_images'] > 1 else ""
    prt(f" Generating {stable}unCLIP{s} of your Image...")
    batch_output = os.path.join(stable_dir, unCLIP_interpolation_prefs['batch_folder_name'])
    if not os.path.isdir(batch_output):
      os.makedirs(batch_output)
    batch_output = os.path.join(prefs['image_output'], unCLIP_interpolation_prefs['batch_folder_name'])
    if not os.path.isdir(batch_output):
      os.makedirs(batch_output)
    for pr in unCLIP_interpolation_prompts:
        for num in range(unCLIP_interpolation_prefs['num_images']):
            prt(progress)
            autoscroll(False)
            random_seed = (int(unCLIP_interpolation_prefs['seed']) + num) if int(unCLIP_interpolation_prefs['seed']) > 0 else rnd.randint(0,4294967295)
            generator = torch.Generator(device=torch_device).manual_seed(random_seed)
            try:
                images = pipe_unCLIP_interpolation(start_prompt=pr['start_prompt'], end_prompt=pr['end_prompt'], steps=unCLIP_interpolation_prefs['steps'], prior_num_inference_steps=unCLIP_interpolation_prefs['prior_num_inference_steps'], decoder_num_inference_steps=unCLIP_interpolation_prefs['decoder_num_inference_steps'], super_res_inference_steps=unCLIP_interpolation_prefs['super_res_inference_steps'], prior_guidance_scale=unCLIP_interpolation_prefs['prior_guidance_scale'], decoder_guidance_scale=unCLIP_interpolation_prefs['decoder_guidance_scale'], callback=callback_fnc, generator=generator).images
            except Exception as e:
                clear_last()
                alert_msg(page, f"Error running {stable}unCLIP Interpolation Pipeline", content=Text(str(e)))
                return
            clear_last()
            autoscroll(True)
            file_max_length = int(prefs['file_max_length']) / 2
            fname = f"{format_filename(pr['start_prompt'], max_length=file_max_length)}-to-{format_filename(pr['end_prompt'], max_length=file_max_length)}"

            if prefs['file_suffix_seed']: fname += f"-{random_seed}"
            for image in images:
                image_path = available_file(os.path.join(stable_dir, unCLIP_interpolation_prefs['batch_folder_name']), fname, num)
                unscaled_path = image_path
                output_file = image_path.rpartition(slash)[2]
                image.save(image_path)
                out_path = image_path.rpartition(slash)[0]
                upscaled_path = os.path.join(out_path, output_file)
                if not unCLIP_interpolation_prefs['display_upscaled_image'] or not unCLIP_interpolation_prefs['apply_ESRGAN_upscale']:
                    prt(Row([ImageButton(src=unscaled_path, data=upscaled_path, width=512, height=512, page=page)], alignment=MainAxisAlignment.CENTER))
                    #prt(Row([Img(src=unscaled_path, fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
                if unCLIP_interpolation_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
                    os.chdir(os.path.join(dist_dir, 'Real-ESRGAN'))
                    upload_folder = 'upload'
                    result_folder = 'results'
                    if os.path.isdir(upload_folder):
                        shutil.rmtree(upload_folder)
                    if os.path.isdir(result_folder):
                        shutil.rmtree(result_folder)
                    os.mkdir(upload_folder)
                    os.mkdir(result_folder)
                    short_name = f'{fname[:80]}-{num}.png'
                    dst_path = os.path.join(dist_dir, 'Real-ESRGAN', upload_folder, short_name)
                    #print(f'Moving {fpath} to {dst_path}')
                    #shutil.move(fpath, dst_path)
                    shutil.copy(image_path, dst_path)
                    #faceenhance = ' --face_enhance' if unCLIP_interpolation_prefs["face_enhance"] else ''
                    faceenhance = ''
                    run_sp(f'python inference_realesrgan.py -n realesr-general-x4v3 -i upload --outscale {unCLIP_interpolation_prefs["enlarge_scale"]}{faceenhance}', cwd=os.path.join(dist_dir, 'Real-ESRGAN'), realtime=False)
                    out_file = short_name.rpartition('.')[0] + '_out.png'
                    shutil.move(os.path.join(dist_dir, 'Real-ESRGAN', result_folder, out_file), upscaled_path)
                    image_path = upscaled_path
                    os.chdir(stable_dir)

                if prefs['save_image_metadata']:
                    img = PILImage.open(image_path)
                    metadata = PngInfo()
                    metadata.add_text("artist", prefs['meta_ArtistName'])
                    metadata.add_text("copyright", prefs['meta_Copyright'])
                    metadata.add_text("software", "Stable Diffusion Deluxe" + f", upscaled {unCLIP_interpolation_prefs['enlarge_scale']}x with ESRGAN" if unCLIP_interpolation_prefs['apply_ESRGAN_upscale'] else "")
                    metadata.add_text("pipeline", f"{stable}unCLIP Text Interpolation")
                    if prefs['save_config_in_metadata']:
                      metadata.add_text("title", f"{pr['start_prompt']}-to-{pr['end_prompt']}")
                      config_json = unCLIP_interpolation_prefs.copy()
                      config_json['model_path'] = model_id
                      config_json['seed'] = random_seed
                      del config_json['num_images']
                      del config_json['display_upscaled_image']
                      del config_json['batch_folder_name']
                      if not config_json['apply_ESRGAN_upscale']:
                        del config_json['enlarge_scale']
                        del config_json['apply_ESRGAN_upscale']
                      metadata.add_text("config_json", json.dumps(config_json, ensure_ascii=True, indent=4))
                    img.save(image_path, pnginfo=metadata)
                #TODO: PyDrive
                if storage_type == "Colab Google Drive":
                    new_file = available_file(os.path.join(prefs['image_output'], unCLIP_interpolation_prefs['batch_folder_name']), fname, num)
                    out_path = new_file
                    shutil.copy(image_path, new_file)
                elif bool(prefs['image_output']):
                    new_file = available_file(os.path.join(prefs['image_output'], unCLIP_interpolation_prefs['batch_folder_name']), fname, num)
                    out_path = new_file
                    shutil.copy(image_path, new_file)
                time.sleep(0.2)
                if unCLIP_interpolation_prefs['display_upscaled_image']:
                    time.sleep(0.6)
                    prt(Row([ImageButton(src=upscaled_path, data=upscaled_path, width=512 * float(unCLIP_interpolation_prefs["enlarge_scale"]), height=512 * float(unCLIP_interpolation_prefs["enlarge_scale"]), page=page)], alignment=MainAxisAlignment.CENTER))
                    #prt(Row([Img(src=upscaled_path, fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
                prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))
    autoscroll(False)
    if prefs['enable_sounds']: page.snd_alert.play()

def run_unCLIP_image_interpolation(page, from_list=False):
    global unCLIP_image_interpolation_prefs, pipe_unCLIP_image_interpolation
    if not status['installed_diffusers']:
      alert_msg(page, "You must Install the HuggingFace Diffusers Library first... ")
      return
    def prt(line, update=True):
      if type(line) == str:
        line = Text(line)
      if from_list:
        page.imageColumn.controls.append(line)
        if update:
          page.imageColumn.update()
      else:
        page.UnCLIP_ImageInterpolation.controls.append(line)
        if update:
          page.UnCLIP_ImageInterpolation.update()
    def clear_last():
      if from_list:
        del page.imageColumn.controls[-1]
        page.imageColumn.update()
      else:
        del page.UnCLIP_ImageInterpolation.controls[-1]
        page.UnCLIP_ImageInterpolation.update()
    def clear_list():
      if from_list:
        page.imageColumn.controls.clear()
      else:
        page.UnCLIP_ImageInterpolation.controls = page.UnCLIP_ImageInterpolation.controls[:1]
    def autoscroll(scroll=True):
      if from_list:
        page.imageColumn.auto_scroll = scroll
        page.imageColumn.update()
        page.UnCLIP_ImageInterpolation.auto_scroll = scroll
        page.UnCLIP_ImageInterpolation.update()
      else:
        page.UnCLIP_ImageInterpolation.auto_scroll = scroll
        page.UnCLIP_ImageInterpolation.update()
    progress = ProgressBar(bar_height=8)
    total_steps = unCLIP_image_interpolation_prefs['decoder_num_inference_steps'] + unCLIP_image_interpolation_prefs['super_res_num_inference_steps']
    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep}"
      progress.update()
    unCLIP_image_interpolation_inits = []
    if from_list:
      if len(prompts) < 1:
        alert_msg(page, "You need to add Prompts to your List first... ")
        return
      for p in prompts:
        if bool(p['init_image']) and bool(p['mask_image']):
          unCLIP_image_interpolation_inits.append({'init_image':p['init_image'], 'end_image':p['mask_image']})
    else:
      if not bool(unCLIP_image_interpolation_prefs['init_image']) or not bool(unCLIP_image_interpolation_prefs['end_image']):
        alert_msg(page, "You need to add a Initial and Ending Image first... ")
        return
      unCLIP_image_interpolation_inits.append({'init_image':unCLIP_image_interpolation_prefs['init_image'], 'end_image':unCLIP_image_interpolation_prefs['end_image']})
    autoscroll(True)
    clear_list()
    from io import BytesIO
    from PIL.PngImagePlugin import PngInfo
    from PIL import ImageOps
    if from_list:
      page.tabs.selected_index = 4
      page.tabs.update()
    clear_pipes('unCLIP_image_interpolation')
    torch.cuda.empty_cache()
    torch.cuda.reset_max_memory_allocated()
    torch.cuda.reset_peak_memory_stats()
    dtype = torch.float16 if not prefs['higher_vram_mode'] else torch.float32 if torch.cuda.is_available() else torch.bfloat16
    if pipe_unCLIP_image_interpolation == None:
        from diffusers import DiffusionPipeline
        prt(Installing("Downloading unCLIP Image Interpolation Kakaobrain Karlo Pipeline... It's a big one, see console for progress."))
        try:
            pipe_unCLIP_image_interpolation = DiffusionPipeline.from_pretrained("kakaobrain/karlo-v1-alpha-image-variations", custom_pipeline="unclip_image_interpolation", torch_dtype=dtype, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
            #pipe_unCLIP_image_interpolation.to(torch_device)
            pipe_unCLIP_image_interpolation = optimize_pipe(pipe_unCLIP_image_interpolation)
        except Exception as e:
            clear_last()
            alert_msg(page, "Error Downloading unCLIP Image Interpolation Pipeline", content=Text(str(e)))
            return
        pipe_unCLIP_image_interpolation.set_progress_bar_config(disable=True)
        clear_last()
    s = "s" if unCLIP_image_interpolation_prefs['num_images'] > 1 else ""
    prt(f"Generating unCLIP Image Interpolation{s} of your Image...")
    batch_output = os.path.join(stable_dir, unCLIP_image_interpolation_prefs['batch_folder_name'])
    if not os.path.isdir(batch_output):
      os.makedirs(batch_output)
    batch_output = os.path.join(prefs['image_output'], unCLIP_image_interpolation_prefs['batch_folder_name'])
    if not os.path.isdir(batch_output):
      os.makedirs(batch_output)
    for interpolation_images in unCLIP_image_interpolation_inits:
        if interpolation_images['init_image'].startswith('http'):
          init_img = PILImage.open(requests.get(interpolation_images['init_image'], stream=True).raw)
        else:
          if os.path.isfile(interpolation_images['init_image']):
            init_img = PILImage.open(interpolation_images['init_image'])
          else:
            alert_msg(page, f"ERROR: Couldn't find your init_image {interpolation_images['init_image']}")
            return
        width, height = init_img.size
        width, height = scale_dimensions(width, height, unCLIP_image_interpolation_prefs['max_size'])
        init_img = init_img.resize((width, height), resample=PILImage.BICUBIC)
        init_img = ImageOps.exif_transpose(init_img).convert("RGB")

        if interpolation_images['end_image'].startswith('http'):
          end_img = PILImage.open(requests.get(interpolation_images['end_image'], stream=True).raw)
        else:
          if os.path.isfile(interpolation_images['end_image']):
            end_img = PILImage.open(interpolation_images['end_image'])
          else:
            alert_msg(page, f"ERROR: Couldn't find your init_end_imageimage {interpolation_images['end_image']}")
            return
        width, height = end_img.size
        width, height = scale_dimensions(width, height, unCLIP_image_interpolation_prefs['max_size'])
        end_img = end_img.resize((width, height), resample=PILImage.BICUBIC)
        end_img = ImageOps.exif_transpose(end_img).convert("RGB")
        prt(progress)
        autoscroll(False)
        num = 0
        random_seed = (int(unCLIP_image_interpolation_prefs['seed']) + num) if int(unCLIP_image_interpolation_prefs['seed']) > 0 else rnd.randint(0,4294967295)
        generator = torch.Generator(device=torch_device).manual_seed(random_seed)
        try:
            images = pipe_unCLIP_image_interpolation(image=[init_img, end_img], steps=unCLIP_image_interpolation_prefs['interpolation_steps'], decoder_num_inference_steps=unCLIP_image_interpolation_prefs['decoder_num_inference_steps'], super_res_num_inference_steps=unCLIP_image_interpolation_prefs['super_res_num_inference_steps'], decoder_guidance_scale=unCLIP_image_interpolation_prefs['decoder_guidance_scale'], num_images_per_prompt=unCLIP_image_interpolation_prefs['num_images'], generator=generator).images #, callback=callback_fnc, callback_steps=1
        except Exception as e:
            clear_last()
            alert_msg(page, "Error running unCLIP Image Interpolation Pipeline", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip(), selectable=True)]))
            return
        autoscroll(True)
        clear_last()
        #fname = format_filename(unCLIP_image_interpolation_prefs['file_name'])
        fname_init = interpolation_images['init_image'].rpartition(slash)[2].rpartition('.')[0]
        fname_end = interpolation_images['end_image'].rpartition(slash)[2].rpartition('.')[0]
        fname = f"{fname_init[:int(prefs['file_max_length']/2)]}-to-{fname_end[:int(prefs['file_max_length']/2)]}"
        if prefs['file_suffix_seed']: fname += f"-{random_seed}"
        for image in images:
            random_seed += num
            image_path = available_file(os.path.join(stable_dir, unCLIP_image_interpolation_prefs['batch_folder_name']), fname, num)
            unscaled_path = image_path
            output_file = image_path.rpartition(slash)[2]
            image.save(image_path)
            out_path = image_path.rpartition(slash)[0]
            upscaled_path = os.path.join(out_path, output_file)
            if not unCLIP_image_interpolation_prefs['display_upscaled_image'] or not unCLIP_image_interpolation_prefs['apply_ESRGAN_upscale']:
                prt(Row([ImageButton(src=unscaled_path, data=upscaled_path, width=512, height=512, page=page)], alignment=MainAxisAlignment.CENTER))
                #prt(Row([Img(src=unscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
            if unCLIP_image_interpolation_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
                os.chdir(os.path.join(dist_dir, 'Real-ESRGAN'))
                upload_folder = 'upload'
                result_folder = 'results'
                if os.path.isdir(upload_folder):
                    shutil.rmtree(upload_folder)
                if os.path.isdir(result_folder):
                    shutil.rmtree(result_folder)
                os.mkdir(upload_folder)
                os.mkdir(result_folder)
                short_name = f'{fname[:80]}-{num}.png'
                dst_path = os.path.join(dist_dir, 'Real-ESRGAN', upload_folder, short_name)
                #print(f'Moving {fpath} to {dst_path}')
                #shutil.move(fpath, dst_path)
                shutil.copy(image_path, dst_path)
                #faceenhance = ' --face_enhance' if unCLIP_image_interpolation_prefs["face_enhance"] else ''
                faceenhance = ''
                run_sp(f'python inference_realesrgan.py -n realesr-general-x4v3 -i upload --outscale {unCLIP_image_interpolation_prefs["enlarge_scale"]}{faceenhance}', cwd=os.path.join(dist_dir, 'Real-ESRGAN'), realtime=False)
                out_file = short_name.rpartition('.')[0] + '_out.png'
                shutil.move(os.path.join(dist_dir, 'Real-ESRGAN', result_folder, out_file), upscaled_path)
                image_path = upscaled_path
                os.chdir(stable_dir)
                if unCLIP_image_interpolation_prefs['display_upscaled_image']:
                    time.sleep(0.6)
                    prt(Row([ImageButton(src=upscaled_path, data=upscaled_path, width=512 * float(unCLIP_image_interpolation_prefs["enlarge_scale"]), height=512 * float(unCLIP_image_interpolation_prefs["enlarge_scale"]), page=page)], alignment=MainAxisAlignment.CENTER))
                    #prt(Row([Img(src=upscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
            if prefs['save_image_metadata']:
                img = PILImage.open(image_path)
                metadata = PngInfo()
                metadata.add_text("artist", prefs['meta_ArtistName'])
                metadata.add_text("copyright", prefs['meta_Copyright'])
                metadata.add_text("software", "Stable Diffusion Deluxe" + f", upscaled {unCLIP_image_interpolation_prefs['enlarge_scale']}x with ESRGAN" if unCLIP_image_interpolation_prefs['apply_ESRGAN_upscale'] else "")
                metadata.add_text("pipeline", "unCLIP Image Interpolation")
                if prefs['save_config_in_metadata']:
                    #metadata.add_text("title", unCLIP_image_interpolation_prefs['file_name'])
                    config_json = unCLIP_image_interpolation_prefs.copy()
                    config_json['model_path'] = "kakaobrain/karlo-v1-alpha-image-variations"
                    config_json['seed'] = random_seed
                    del config_json['num_images']
                    del config_json['display_upscaled_image']
                    del config_json['batch_folder_name']
                    del config_json['file_name']
                    if not config_json['apply_ESRGAN_upscale']:
                        del config_json['enlarge_scale']
                        del config_json['apply_ESRGAN_upscale']
                    metadata.add_text("config_json", json.dumps(config_json, ensure_ascii=True, indent=4))
                img.save(image_path, pnginfo=metadata)
            #TODO: PyDrive
            if storage_type == "Colab Google Drive":
                new_file = available_file(os.path.join(prefs['image_output'], unCLIP_image_interpolation_prefs['batch_folder_name']), fname, num)
                out_path = new_file
                shutil.copy(image_path, new_file)
            elif bool(prefs['image_output']):
                new_file = available_file(os.path.join(prefs['image_output'], unCLIP_image_interpolation_prefs['batch_folder_name']), fname, num)
                out_path = new_file
                shutil.copy(image_path, new_file)
            prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))
            num +=1
    autoscroll(False)
    if prefs['enable_sounds']: page.snd_alert.play()


def run_magic_mix(page, from_list=False):
    global magic_mix_prefs, pipe_magic_mix
    if not status['installed_diffusers']:
      alert_msg(page, "You must Install the HuggingFace Diffusers Library first... ")
      return
    def prt(line, update=True):
      if type(line) == str:
        line = Text(line)
      if from_list:
        page.imageColumn.controls.append(line)
        if update:
          page.imageColumn.update()
      else:
        page.MagicMix.controls.append(line)
        if update:
          page.MagicMix.update()
    def clear_last():
      if from_list:
        del page.imageColumn.controls[-1]
        page.imageColumn.update()
      else:
        del page.MagicMix.controls[-1]
        page.MagicMix.update()
    def clear_list():
      if from_list:
        page.imageColumn.controls.clear()
      else:
        page.MagicMix.controls = page.MagicMix.controls[:1]
    def autoscroll(scroll=True):
      if from_list:
        page.imageColumn.auto_scroll = scroll
        page.imageColumn.update()
        page.MagicMix.auto_scroll = scroll
        page.MagicMix.update()
      else:
        page.MagicMix.auto_scroll = scroll
        page.MagicMix.update()
    progress = ProgressBar(bar_height=8)
    total_steps = magic_mix_prefs['num_inference_steps']
    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep}"
      progress.update()
    magic_mix_prompts = []
    if from_list:
      if len(prompts) < 1:
        alert_msg(page, "You need to add Prompts to your List first... ")
        return
      for p in prompts:
        magic_mix_prompts.append(p.prompt)
    else:
      if not bool(magic_mix_prefs['prompt']):
        alert_msg(page, "You need to add a Text Prompt first... ")
        return
      magic_mix_prompts.append(magic_mix_prefs['prompt'])
    if from_list:
      page.tabs.selected_index = 4
      page.tabs.update()
    else:
      clear_list()
    autoscroll(True)
    from io import BytesIO
    from PIL.PngImagePlugin import PngInfo
    from PIL import ImageOps
    if magic_mix_prefs['init_image'].startswith('http'):
      init_img = PILImage.open(requests.get(magic_mix_prefs['init_image'], stream=True).raw)
    else:
      if os.path.isfile(magic_mix_prefs['init_image']):
        init_img = PILImage.open(magic_mix_prefs['init_image'])
      else:
        alert_msg(page, f"ERROR: Couldn't find your init_image {magic_mix_prefs['init_image']}")
        return
    width, height = init_img.size
    width, height = scale_dimensions(width, height, magic_mix_prefs['max_size'])
    init_img = init_img.resize((width, height), resample=PILImage.BICUBIC)
    init_img = ImageOps.exif_transpose(init_img).convert("RGB")
    '''tform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Resize(
            (width, height),
            interpolation=transforms.InterpolationMode.BICUBIC,
            antialias=False,
            ),
        transforms.Normalize(
          [0.48145466, 0.4578275, 0.40821073],
          [0.26862954, 0.26130258, 0.27577711]),
    ])
    init_img = tform(init_img).to(torch_device)'''
    clear_pipes('magic_mix')
    #torch.cuda.empty_cache()
    #torch.cuda.reset_max_memory_allocated()
    #torch.cuda.reset_peak_memory_stats()
    model = get_model(prefs['model_ckpt'])['path']
    scheduler_mode = magic_mix_prefs['scheduler_mode']
    if scheduler_mode == "LMS Discrete":
      from diffusers import LMSDiscreteScheduler
      schedule = LMSDiscreteScheduler.from_pretrained(model, subfolder="scheduler")
    elif scheduler_mode == "PNDM":
      from diffusers import PNDMScheduler
      schedule = PNDMScheduler.from_pretrained(model, subfolder="scheduler")
    elif scheduler_mode == "DDIM":
      from diffusers import DDIMScheduler
      schedule = DDIMScheduler.from_pretrained(model, subfolder="scheduler")
    if pipe_magic_mix == None or magic_mix_prefs['scheduler_mode'] != magic_mix_prefs['scheduler_last']:
        from diffusers import DiffusionPipeline
        prt(Installing("Downloading MagicMix Pipeline... "))
        try:
            pipe_magic_mix = DiffusionPipeline.from_pretrained(model, custom_pipeline="AlanB/magic_mix_mod", scheduler=schedule, safety_checker=None, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
            #pipe_magic_mix.to(torch_device)
            pipe_magic_mix = optimize_pipe(pipe_magic_mix)
            magic_mix_prefs['scheduler_last'] = magic_mix_prefs['scheduler_mode']
        except Exception as e:
            clear_last()
            alert_msg(page, "Error Downloading MagicMix Pipeline", content=Text(str(e)))
            return
        #pipe_magic_mix.set_progress_bar_config(disable=True)
        clear_last()
    s = "es" if magic_mix_prefs['num_images'] > 1 else ""
    prt(f"Generating MagicMix{s} of your Image...")
    batch_output = os.path.join(stable_dir, magic_mix_prefs['batch_folder_name'])
    if not os.path.isdir(batch_output):
      os.makedirs(batch_output)
    batch_output = os.path.join(prefs['image_output'], magic_mix_prefs['batch_folder_name'])
    if not os.path.isdir(batch_output):
      os.makedirs(batch_output)
    for pr in magic_mix_prompts:
        for num in range(magic_mix_prefs['num_images']):
            prt(progress)
            autoscroll(False)
            random_seed = (int(magic_mix_prefs['seed']) + num) if int(magic_mix_prefs['seed']) > 0 else rnd.randint(0,4294967295)
            #generator = torch.Generator(device=torch_device).manual_seed(random_seed)
            try:
                image = pipe_magic_mix(img=init_img, prompt=pr, steps=magic_mix_prefs['num_inference_steps'], kmin=magic_mix_prefs['kmin'], kmax=magic_mix_prefs['kmax'], mix_factor=magic_mix_prefs['mix_factor'], guidance_scale=magic_mix_prefs['guidance_scale'], seed=random_seed, callback=callback_fnc, callback_steps=1)#.images
            except Exception as e:
                clear_last()
                alert_msg(page, "Error running MagicMix Pipeline", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))
                return
            autoscroll(True)
            clear_last()
            fname = format_filename(pr)

            if prefs['file_suffix_seed']: fname += f"-{random_seed}"
            #for image in images:
            image_path = available_file(os.path.join(stable_dir, magic_mix_prefs['batch_folder_name']), fname, num)
            unscaled_path = image_path
            output_file = image_path.rpartition(slash)[2]
            image.save(image_path)
            out_path = image_path.rpartition(slash)[0]
            upscaled_path = os.path.join(out_path, output_file)
            if not magic_mix_prefs['display_upscaled_image'] or not magic_mix_prefs['apply_ESRGAN_upscale']:
                prt(Row([ImageButton(src=unscaled_path, data=upscaled_path, width=width, height=height, page=page)], alignment=MainAxisAlignment.CENTER))
                #prt(Row([Img(src=unscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
            if magic_mix_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
                os.chdir(os.path.join(dist_dir, 'Real-ESRGAN'))
                upload_folder = 'upload'
                result_folder = 'results'
                if os.path.isdir(upload_folder):
                    shutil.rmtree(upload_folder)
                if os.path.isdir(result_folder):
                    shutil.rmtree(result_folder)
                os.mkdir(upload_folder)
                os.mkdir(result_folder)
                short_name = f'{fname[:80]}-{num}.png'
                dst_path = os.path.join(dist_dir, 'Real-ESRGAN', upload_folder, short_name)
                #print(f'Moving {fpath} to {dst_path}')
                #shutil.move(fpath, dst_path)
                shutil.copy(image_path, dst_path)
                #faceenhance = ' --face_enhance' if magic_mix_prefs["face_enhance"] else ''
                faceenhance = ''
                run_sp(f'python inference_realesrgan.py -n realesr-general-x4v3 -i upload --outscale {magic_mix_prefs["enlarge_scale"]}{faceenhance}', cwd=os.path.join(dist_dir, 'Real-ESRGAN'), realtime=False)
                out_file = short_name.rpartition('.')[0] + '_out.png'
                shutil.move(os.path.join(dist_dir, 'Real-ESRGAN', result_folder, out_file), upscaled_path)
                image_path = upscaled_path
                os.chdir(stable_dir)
                if magic_mix_prefs['display_upscaled_image']:
                    time.sleep(0.6)
                    prt(Row([ImageButton(src=upscaled_path, data=upscaled_path, width=width * float(magic_mix_prefs["enlarge_scale"]), height=height * float(magic_mix_prefs["enlarge_scale"]), page=page)], alignment=MainAxisAlignment.CENTER))
                    #prt(Row([Img(src=upscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
            if prefs['save_image_metadata']:
                img = PILImage.open(image_path)
                metadata = PngInfo()
                metadata.add_text("artist", prefs['meta_ArtistName'])
                metadata.add_text("copyright", prefs['meta_Copyright'])
                metadata.add_text("software", "Stable Diffusion Deluxe" + f", upscaled {magic_mix_prefs['enlarge_scale']}x with ESRGAN" if magic_mix_prefs['apply_ESRGAN_upscale'] else "")
                metadata.add_text("pipeline", "magic_mix")
                if prefs['save_config_in_metadata']:
                  metadata.add_text("title", pr)
                  config_json = magic_mix_prefs.copy()
                  config_json['model_path'] = model
                  config_json['seed'] = random_seed
                  del config_json['num_images']
                  del config_json['display_upscaled_image']
                  del config_json['batch_folder_name']
                  del config_json['file_name']
                  del config_json["scheduler_last"]
                  del config_json['max_size']
                  if not config_json['apply_ESRGAN_upscale']:
                    del config_json['enlarge_scale']
                    del config_json['apply_ESRGAN_upscale']
                  metadata.add_text("config_json", json.dumps(config_json, ensure_ascii=True, indent=4))
                img.save(image_path, pnginfo=metadata)
            #TODO: PyDrive
            if storage_type == "Colab Google Drive":
                new_file = available_file(os.path.join(prefs['image_output'], magic_mix_prefs['batch_folder_name']), fname, num)
                out_path = new_file
                shutil.copy(image_path, new_file)
            elif bool(prefs['image_output']):
                new_file = available_file(os.path.join(prefs['image_output'], magic_mix_prefs['batch_folder_name']), fname, num)
                out_path = new_file
                shutil.copy(image_path, new_file)
            time.sleep(0.2)
            prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))
    autoscroll(False)
    if prefs['enable_sounds']: page.snd_alert.play()

def run_paint_by_example(page):
    global paint_by_example_prefs, prefs, status, pipe_paint_by_example
    if not status['installed_diffusers']:
      alert_msg(page, "You need to Install HuggingFace Diffusers before using...")
      return
    if not bool(paint_by_example_prefs['original_image']) or (not bool(paint_by_example_prefs['alpha_mask']) and not bool(paint_by_example_prefs['mask_image'])):
      alert_msg(page, "You must provide the Original Image and the Mask Image to process...")
      return
    if not bool(paint_by_example_prefs['example_image']):
      alert_msg(page, "You must provide an Example Image to Transfer Subject from...")
      return
    def prt(line):
      if type(line) == str:
        line = Text(line, size=17)
      page.PaintByExample.controls.append(line)
      page.PaintByExample.update()
    def clear_last():
      del page.PaintByExample.controls[-1]
      page.PaintByExample.update()
    def autoscroll(scroll=True):
      page.PaintByExample.auto_scroll = scroll
      page.PaintByExample.update()
    def clear_list():
      page.PaintByExample.controls = page.PaintByExample.controls[:1]
    progress = ProgressBar(bar_height=8)
    total_steps = paint_by_example_prefs['num_inference_steps']
    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep}"
      progress.update()
      #print(f'{type(latents)} {len(latents)}- {str(latents)}')
    autoscroll(True)
    clear_list()
    prt(Installing("Installing Paint-by-Example Pipeline..."))
    import requests, random
    from io import BytesIO
    from PIL import ImageOps
    from PIL.PngImagePlugin import PngInfo
    if paint_by_example_prefs['original_image'].startswith('http'):
      #response = requests.get(paint_by_example_prefs['original_image'])
      #original_img = PILImage.open(BytesIO(response.content)).convert("RGB")
      original_img = PILImage.open(requests.get(paint_by_example_prefs['original_image'], stream=True).raw)
    else:
      if os.path.isfile(paint_by_example_prefs['original_image']):
        original_img = PILImage.open(paint_by_example_prefs['original_image'])
      else:
        alert_msg(page, f"ERROR: Couldn't find your original_image {paint_by_example_prefs['original_image']}")
        return
    width, height = original_img.size
    width, height = scale_dimensions(width, height, paint_by_example_prefs['max_size'])
    if bool(paint_by_example_prefs['alpha_mask']):
      original_img = ImageOps.exif_transpose(original_img).convert("RGBA")
    else:
      original_img = ImageOps.exif_transpose(original_img).convert("RGB")
    original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS)
    mask_img = None
    if not bool(paint_by_example_prefs['mask_image']) and bool(paint_by_example_prefs['alpha_mask']):
      red, green, blue, alpha = PILImage.Image.split(original_img)
      #mask_img = ImageOps.invert(alpha.convert('RGB'))
      mask_img = alpha.convert('L')
    else:
      if paint_by_example_prefs['mask_image'].startswith('http'):
        mask_img = PILImage.open(requests.get(paint_by_example_prefs['mask_image'], stream=True).raw)
      else:
        if os.path.isfile(paint_by_example_prefs['mask_image']):
          mask_img = PILImage.open(paint_by_example_prefs['mask_image'])
        else:
          alert_msg(page, f"ERROR: Couldn't find your mask_image {paint_by_example_prefs['mask_image']}")
          return
      if paint_by_example_prefs['invert_mask']:
        mask_img = ImageOps.invert(mask_img.convert('RGB'))
    #mask_img = mask_img.convert("L")
    #mask_img = mask_img.convert("1")
    mask_img = mask_img.resize((width, height), resample=PILImage.NEAREST)
    mask_img = ImageOps.exif_transpose(mask_img).convert("RGB")
    #print(f'Resize to {width}x{height}')
    if paint_by_example_prefs['example_image'].startswith('http'):
      example_img = PILImage.open(requests.get(paint_by_example_prefs['example_image'], stream=True).raw)
    else:
      if os.path.isfile(paint_by_example_prefs['example_image']):
        example_img = PILImage.open(paint_by_example_prefs['example_image'])
      else:
        alert_msg(page, f"ERROR: Couldn't find your Example Image {paint_by_example_prefs['example_image']}")
        return

    clear_pipes('paint_by_example')
    torch.cuda.empty_cache()
    torch.cuda.reset_max_memory_allocated()
    torch.cuda.reset_peak_memory_stats()
    model_id = "Fantasy-Studio/Paint-by-Example"
    if pipe_paint_by_example is None:
      from diffusers import PaintByExamplePipeline, PNDMScheduler
      PNDMscheduler = PNDMScheduler(skip_prk_steps=True)
      pipe_paint_by_example = PaintByExamplePipeline.from_pretrained(model_id, scheduler=PNDMscheduler, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
      pipe_paint_by_example = pipe_paint_by_example.to("cpu")
    clear_last()
    prt("Generating Paint-by-Example of your Image... (slow because it uses CPU instead of GPU)")
    prt(progress)
    autoscroll(False)
    batch_output = os.path.join(stable_dir, paint_by_example_prefs['batch_folder_name'])
    if not os.path.isdir(batch_output):
      os.makedirs(batch_output)
    batch_output = os.path.join(prefs['image_output'], paint_by_example_prefs['batch_folder_name'])
    if not os.path.isdir(batch_output):
      os.makedirs(batch_output)
    random_seed = int(paint_by_example_prefs['seed']) if int(paint_by_example_prefs['seed']) > 0 else random.randint(0,4294967295)
    generator = torch.Generator(device="cpu").manual_seed(random_seed)
    #generator = torch.manual_seed(random_seed)
    try:
      images = pipe_paint_by_example(image=original_img, mask_image=mask_img, example_image=example_img, num_inference_steps=paint_by_example_prefs['num_inference_steps'], eta=paint_by_example_prefs['eta'], guidance_scale=paint_by_example_prefs['guidance_scale'], num_images_per_prompt=paint_by_example_prefs['num_images'], generator=generator, callback=callback_fnc, callback_steps=1).images
    except Exception as e:
      clear_last()
      alert_msg(page, f"ERROR: Couldn't Paint-by-Example your image for some reason.  Possibly out of memory or something wrong with my code...", content=Text(str(e)))
      return
    clear_last()
    clear_last()
    autoscroll(True)
    filename = paint_by_example_prefs['original_image'].rpartition(slash)[2].rpartition('.')[0]
    #if prefs['file_suffix_seed']: fname += f"-{random_seed}"
    num = 0
    for image in images:
        random_seed += num
        fname = filename + (f"-{random_seed}" if prefs['file_suffix_seed'] else "")
        image_path = available_file(os.path.join(stable_dir, paint_by_example_prefs['batch_folder_name']), fname, num)
        unscaled_path = image_path
        output_file = image_path.rpartition(slash)[2]
        image.save(image_path)
        out_path = image_path.rpartition(slash)[0]
        upscaled_path = os.path.join(out_path, output_file)
        if not paint_by_example_prefs['display_upscaled_image'] or not paint_by_example_prefs['apply_ESRGAN_upscale']:
            prt(Row([ImageButton(src=unscaled_path, data=upscaled_path, width=width, height=height, page=page)], alignment=MainAxisAlignment.CENTER))
            #prt(Row([Img(src=unscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
        if paint_by_example_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
            os.chdir(os.path.join(dist_dir, 'Real-ESRGAN'))
            upload_folder = 'upload'
            result_folder = 'results'
            if os.path.isdir(upload_folder):
                shutil.rmtree(upload_folder)
            if os.path.isdir(result_folder):
                shutil.rmtree(result_folder)
            os.mkdir(upload_folder)
            os.mkdir(result_folder)
            short_name = f'{fname[:80]}-{num}.png'
            dst_path = os.path.join(dist_dir, 'Real-ESRGAN', upload_folder, short_name)
            #print(f'Moving {fpath} to {dst_path}')
            #shutil.move(fpath, dst_path)
            shutil.copy(image_path, dst_path)
            #faceenhance = ' --face_enhance' if paint_by_example_prefs["face_enhance"] else ''
            faceenhance = ''
            run_sp(f'python inference_realesrgan.py -n realesr-general-x4v3 -i upload --outscale {paint_by_example_prefs["enlarge_scale"]}{faceenhance}', cwd=os.path.join(dist_dir, 'Real-ESRGAN'), realtime=False)
            out_file = short_name.rpartition('.')[0] + '_out.png'
            shutil.move(os.path.join(dist_dir, 'Real-ESRGAN', result_folder, out_file), upscaled_path)
            image_path = upscaled_path
            os.chdir(stable_dir)
            if paint_by_example_prefs['display_upscaled_image']:
                time.sleep(0.6)
                prt(Row([ImageButton(src=upscaled_path, data=upscaled_path, width=width * float(paint_by_example_prefs["enlarge_scale"]), height=height * float(paint_by_example_prefs["enlarge_scale"]), page=page)], alignment=MainAxisAlignment.CENTER))
                #prt(Row([Img(src=upscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
        if prefs['save_image_metadata']:
            img = PILImage.open(image_path)
            metadata = PngInfo()
            metadata.add_text("artist", prefs['meta_ArtistName'])
            metadata.add_text("copyright", prefs['meta_Copyright'])
            metadata.add_text("software", "Stable Diffusion Deluxe" + f", upscaled {paint_by_example_prefs['enlarge_scale']}x with ESRGAN" if paint_by_example_prefs['apply_ESRGAN_upscale'] else "")
            metadata.add_text("pipeline", "Paint-by-Example")
            if prefs['save_config_in_metadata']:
              config_json = paint_by_example_prefs.copy()
              config_json['model_path'] = model_id
              config_json['seed'] = random_seed
              del config_json['num_images']
              del config_json['max_size']
              del config_json['display_upscaled_image']
              del config_json['batch_folder_name']
              del config_json['invert_mask']
              del config_json['alpha_mask']
              if not config_json['apply_ESRGAN_upscale']:
                del config_json['enlarge_scale']
                del config_json['apply_ESRGAN_upscale']
              metadata.add_text("config_json", json.dumps(config_json, ensure_ascii=True, indent=4))
            img.save(image_path, pnginfo=metadata)
        #TODO: PyDrive
        if storage_type == "Colab Google Drive":
            new_file = available_file(os.path.join(prefs['image_output'], paint_by_example_prefs['batch_folder_name']), fname, num)
            out_path = new_file
            shutil.copy(image_path, new_file)
        elif bool(prefs['image_output']):
            new_file = available_file(os.path.join(prefs['image_output'], paint_by_example_prefs['batch_folder_name']), fname, num)
            out_path = new_file
            shutil.copy(image_path, new_file)
        time.sleep(0.2)
        prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))
        num += 1
    autoscroll(True)
    if prefs['enable_sounds']: page.snd_alert.play()


def run_instruct_pix2pix(page, from_list=False):
    global instruct_pix2pix_prefs, prefs, status, pipe_instruct_pix2pix
    if not status['installed_diffusers']:
      alert_msg(page, "You need to Install HuggingFace Diffusers before using...")
      return
    if not bool(instruct_pix2pix_prefs['original_image']) and not instruct_pix2pix_prefs['use_init_video']:
      alert_msg(page, "You must provide the Original Image and the Mask Image to process...")
      return
    if not bool(instruct_pix2pix_prefs['init_video']) and instruct_pix2pix_prefs['use_init_video']:
      alert_msg(page, "You must provide the Input Initial Video Clip to process...")
      return
    if not bool(instruct_pix2pix_prefs['prompt']):
      alert_msg(page, "You must provide a Instructional Image Editing Prompt...")
      return
    def prt(line, update=True):
      if type(line) == str:
        line = Text(line)
      if from_list:
        page.imageColumn.controls.append(line)
        if update:
          page.imageColumn.update()
      else:
        page.InstructPix2Pix.controls.append(line)
        if update:
          page.InstructPix2Pix.update()
    def clear_last():
      if from_list:
        if len(page.imageColumn.controls) == 0: return
        del page.imageColumn.controls[-1]
        page.imageColumn.update()
      else:
        if len(page.InstructPix2Pix.controls) == 1: return
        del page.InstructPix2Pix.controls[-1]
        page.InstructPix2Pix.update()
    def autoscroll(scroll=True):
      if from_list:
        page.imageColumn.auto_scroll = scroll
        page.imageColumn.update()
      else:
        page.InstructPix2Pix.auto_scroll = scroll
        page.InstructPix2Pix.update()
    progress = ProgressBar(bar_height=8)
    total_steps = instruct_pix2pix_prefs['num_inference_steps']
    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep}"
      progress.update()
      #print(f'{type(latents)} {len(latents)}- {str(latents)}')
    def clear_list():
      if from_list:
        page.imageColumn.controls.clear()
      else:
        page.InstructPix2Pix.controls = page.InstructPix2Pix.controls[:1]
    instruct_pix2pix_prompts = []
    if from_list:
      if len(prompts) < 1:
        alert_msg(page, "You need to add Prompts to your List first... ")
        return
      for p in prompts:
        instruct = {'prompt': p.prompt, 'negative_prompt': p['negative_prompt'], 'original_image': p['init_image'] if bool(p['init_image']) else instruct_pix2pix_prefs['original_image'], 'seed': p['seed']}
        instruct_pix2pix_prompts.append(instruct)
      page.tabs.selected_index = 4
      page.tabs.update()
    else:
      if not bool(instruct_pix2pix_prefs['prompt']):
        alert_msg(page, "You need to add a Text Prompt first... ")
        return
      instruct = {'prompt':instruct_pix2pix_prefs['prompt'], 'negative_prompt': instruct_pix2pix_prefs['negative_prompt'], 'original_image': instruct_pix2pix_prefs['original_image'], 'seed': instruct_pix2pix_prefs['seed']}
      instruct_pix2pix_prompts.append(instruct)
    autoscroll(True)
    clear_list()
    prt(Divider(thickness=2, height=4))
    prt(Installing("Installing Instruct-Pix2Pix Pipeline..."))
    import requests, random
    from io import BytesIO
    from PIL import ImageOps
    from PIL.PngImagePlugin import PngInfo

    clear_pipes('instruct_pix2pix')
    torch.cuda.empty_cache()
    torch.cuda.reset_max_memory_allocated()
    torch.cuda.reset_peak_memory_stats()
    model_id = "timbrooks/instruct-pix2pix"
    model_id_SDXL = "timbrooks/instructpix2pix-clip-filtered"
    if pipe_instruct_pix2pix is None:
      if instruct_pix2pix_prefs['use_SDXL']:
        from diffusers import StableDiffusionXLInstructPix2PixPipeline
        pipe_instruct_pix2pix = StableDiffusionXLInstructPix2PixPipeline.from_pretrained(model_id_SDXL, torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, **safety)
        pipe_instruct_pix2pix = optimize_pipe(pipe_instruct_pix2pix)
      else:
        from diffusers import StableDiffusionInstructPix2PixPipeline
        from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker
        pipe_instruct_pix2pix = StableDiffusionInstructPix2PixPipeline.from_pretrained(model_id, torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, **safety)
        pipe_instruct_pix2pix = optimize_pipe(pipe_instruct_pix2pix)
        #pipe_instruct_pix2pix = pipe_instruct_pix2pix.to(torch_device)
    pipeline_scheduler(pipe_instruct_pix2pix)
    clear_last()
    prt("Generating Instruct-Pix2Pix of your Image...")
    prt(progress)
    max_size = instruct_pix2pix_prefs['max_size']
    batch_output = os.path.join(stable_dir, instruct_pix2pix_prefs['batch_folder_name'])
    output_dir = batch_output
    if not os.path.isdir(batch_output):
      os.makedirs(batch_output)
    batch_output = os.path.join(prefs['image_output'], instruct_pix2pix_prefs['batch_folder_name'])
    if not os.path.isdir(batch_output):
      os.makedirs(batch_output)
    if instruct_pix2pix_prefs['use_init_video']:
        init_vid = instruct_pix2pix_prefs['init_video']
        try:
            start_time = float(instruct_pix2pix_prefs['start_time'])
            end_time = float(instruct_pix2pix_prefs['end_time'])
            fps = int(instruct_pix2pix_prefs['fps'])
        except Exception:
            alert_msg(page, "Make sure your Numbers are actual numbers...")
            return
        if init_vid.startswith('http'):
            init_vid = download_file(init_vid, output_dir)
        else:
            if not os.path.isfile(init_vid):
              alert_msg(page, f"ERROR: Couldn't find your init_video {init_vid}")
              return
        prt("Extracting Frames from Video Clip")
        try:
            import cv2
        except ModuleNotFoundError:
            run_process("pip install -q opencv-contrib-python", page=page)
            import cv2
            pass
        try:
            cap = cv2.VideoCapture(init_vid)
        except Exception as e:
            alert_msg(page, "ERROR Reading Video File. May be Incompatible Format...")
            clear_last()
            return
        count = 0
        video = []
        frames = []
        width = height = 0
        cap.set(cv2.CAP_PROP_FPS, fps)
        video_length = cap.get(cv2.CAP_PROP_FRAME_COUNT)
        start_frame = int(start_time * fps)
        if end_time == 0 or end_time == 0.0:
            end_frame = int(video_length)
        else:
            end_frame = int(end_time * fps)
        total = end_frame - start_frame
        for i in range(start_frame, end_frame):
            cap.set(cv2.CAP_PROP_POS_FRAMES, i)
            success, image = cap.read()
            if success:
                #filename = os.path.join(output_dir, f'{file_prefix}{count}.png')
                if width == 0:
                    shape = image.shape
                    width, height = scale_dimensions(shape[1], shape[0], max=max_size, multiple=16)
                image = cv2.resize(image, (width, height), interpolation = cv2.INTER_AREA)
                #cv2.imwrite(os.path.join(output_dir, filename), image)
                video.append(PILImage.fromarray(image))
                count += 1
        cap.release()
        clear_last()
        #reader = imageio.get_reader(instruct_pix2pix_prefs['init_video'], "ffmpeg")
        #frame_count = instruct_pix2pix_prefs['fps'] #TODO: This isn't frame count, do it right
        #video = [Image.fromarray(reader.get_data(i)) for i in range(frame_count)]
    else: video = None
    for pr in instruct_pix2pix_prompts:
      if not instruct_pix2pix_prefs['use_init_video']:
        if pr['original_image'].startswith('http'):
          #response = requests.get(instruct_pix2pix_prefs['original_image'])
          #original_img = PILImage.open(BytesIO(response.content)).convert("RGB")
          original_img = PILImage.open(requests.get(pr['original_image'], stream=True).raw)
        else:
          if os.path.isfile(pr['original_image']):
            original_img = PILImage.open(pr['original_image'])
          else:
            alert_msg(page, f"ERROR: Couldn't find your original_image {pr['original_image']}")
            return
        width, height = original_img.size
        width, height = scale_dimensions(width, height, instruct_pix2pix_prefs['max_size'])
        original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS)
      for num in range(instruct_pix2pix_prefs['num_images']):
        prt(progress)
        random_seed = (int(pr['seed']) + num) if int(pr['seed']) > 0 else rnd.randint(0,4294967295)
        generator = torch.Generator(device=torch_device).manual_seed(random_seed)
        #generator = torch.manual_seed(random_seed)
        try:
          if instruct_pix2pix_prefs['use_init_video']:
            from diffusers.pipelines.text_to_video_synthesis.pipeline_text_to_video_zero import CrossFrameAttnProcessor
            pipe_instruct_pix2pix.unet.set_attn_processor(CrossFrameAttnProcessor(batch_size=3))
            images = pipe_instruct_pix2pix([pr['prompt']] * len(video), image=video, negative_prompt=pr['negative_prompt'] if bool(pr['negative_prompt']) else None, num_inference_steps=instruct_pix2pix_prefs['num_inference_steps'], eta=instruct_pix2pix_prefs['eta'], image_guidance_scale=instruct_pix2pix_prefs['guidance_scale'], num_images_per_prompt=instruct_pix2pix_prefs['num_images'], generator=generator, callback=callback_fnc, callback_steps=1).images
          else:
            images = pipe_instruct_pix2pix(pr['prompt'], image=original_img, negative_prompt=pr['negative_prompt'] if bool(pr['negative_prompt']) else None, num_inference_steps=instruct_pix2pix_prefs['num_inference_steps'], eta=instruct_pix2pix_prefs['eta'], image_guidance_scale=instruct_pix2pix_prefs['guidance_scale'], num_images_per_prompt=instruct_pix2pix_prefs['num_images'], generator=generator, callback=callback_fnc, callback_steps=1).images
        except Exception as e:
          clear_last()
          alert_msg(page, f"ERROR: Couldn't run Instruct-Pix2Pix on your image for some reason.  Possibly out of memory or something wrong with my code...", content=Text(str(e)))
          gc.collect()
          torch.cuda.empty_cache()
          return
        clear_last()
        clear_last()
        filename = pr['original_image'].rpartition(slash)[2].rpartition('.')[0]
        filename = f"-{format_filename(pr['prompt'])}"
        filename = filename[:int(prefs['file_max_length'])]
        #if prefs['file_suffix_seed']: fname += f"-{random_seed}"
        #num = 0
        for image in images:
            random_seed += num
            fname = filename + (f"-{random_seed}" if prefs['file_suffix_seed'] else "")
            image_path = available_file(os.path.join(stable_dir, instruct_pix2pix_prefs['batch_folder_name']), fname, num)
            unscaled_path = image_path
            output_file = image_path.rpartition(slash)[2]
            image.save(image_path)
            out_path = image_path.rpartition(slash)[0]
            upscaled_path = os.path.join(out_path, output_file)
            if not instruct_pix2pix_prefs['display_upscaled_image'] or not instruct_pix2pix_prefs['apply_ESRGAN_upscale']:
                prt(Row([ImageButton(src=unscaled_path, data=upscaled_path, width=width, height=height, page=page)], alignment=MainAxisAlignment.CENTER))
                #prt(Row([Img(src=unscaled_path, fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
            if instruct_pix2pix_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
                os.chdir(os.path.join(dist_dir, 'Real-ESRGAN'))
                upload_folder = 'upload'
                result_folder = 'results'
                if os.path.isdir(upload_folder):
                    shutil.rmtree(upload_folder)
                if os.path.isdir(result_folder):
                    shutil.rmtree(result_folder)
                os.mkdir(upload_folder)
                os.mkdir(result_folder)
                short_name = f'{fname[:80]}-{num}.png'
                dst_path = os.path.join(dist_dir, 'Real-ESRGAN', upload_folder, short_name)
                #print(f'Moving {fpath} to {dst_path}')
                #shutil.move(fpath, dst_path)
                shutil.copy(image_path, dst_path)
                #faceenhance = ' --face_enhance' if instruct_pix2pix_prefs["face_enhance"] else ''
                faceenhance = ''
                run_sp(f'python inference_realesrgan.py -n realesr-general-x4v3 -i upload --outscale {instruct_pix2pix_prefs["enlarge_scale"]}{faceenhance}', cwd=os.path.join(dist_dir, 'Real-ESRGAN'), realtime=False)
                out_file = short_name.rpartition('.')[0] + '_out.png'
                shutil.move(os.path.join(dist_dir, 'Real-ESRGAN', result_folder, out_file), upscaled_path)
                image_path = upscaled_path
                os.chdir(stable_dir)
                if instruct_pix2pix_prefs['display_upscaled_image']:
                    time.sleep(0.6)
                    prt(Row([ImageButton(src=upscaled_path, data=upscaled_path, width=width * float(instruct_pix2pix_prefs["enlarge_scale"]), height=height * float(instruct_pix2pix_prefs["enlarge_scale"]), page=page)], alignment=MainAxisAlignment.CENTER))
                    #prt(Row([Img(src=upscaled_path, fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
            if prefs['save_image_metadata']:
                img = PILImage.open(image_path)
                metadata = PngInfo()
                metadata.add_text("artist", prefs['meta_ArtistName'])
                metadata.add_text("copyright", prefs['meta_Copyright'])
                metadata.add_text("software", "Stable Diffusion Deluxe" + f", upscaled {instruct_pix2pix_prefs['enlarge_scale']}x with ESRGAN" if instruct_pix2pix_prefs['apply_ESRGAN_upscale'] else "")
                metadata.add_text("pipeline", "Instruct-Pix2Pix")
                if prefs['save_config_in_metadata']:
                  config_json = instruct_pix2pix_prefs.copy()
                  config_json['model_path'] = model_id
                  config_json['seed'] = random_seed
                  config_json['prompt'] = pr['prompt']
                  config_json['negative_prompt'] = pr['negative_prompt']
                  del config_json['num_images']
                  del config_json['max_size']
                  del config_json['display_upscaled_image']
                  del config_json['batch_folder_name']
                  if not config_json['apply_ESRGAN_upscale']:
                    del config_json['enlarge_scale']
                    del config_json['apply_ESRGAN_upscale']
                  metadata.add_text("config_json", json.dumps(config_json, ensure_ascii=True, indent=4))
                img.save(image_path, pnginfo=metadata)
            #TODO: PyDrive
            if storage_type == "Colab Google Drive":
                new_file = available_file(os.path.join(prefs['image_output'], instruct_pix2pix_prefs['batch_folder_name']), fname, num)
                out_path = new_file
                shutil.copy(image_path, new_file)
            elif bool(prefs['image_output']):
                new_file = available_file(os.path.join(prefs['image_output'], instruct_pix2pix_prefs['batch_folder_name']), fname, num)
                out_path = new_file
                shutil.copy(image_path, new_file)
            time.sleep(0.2)
            prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))
            #num += 1
    if prefs['enable_sounds']: page.snd_alert.play()

def run_controlnet(page, from_list=False):
    global controlnet_prefs, prefs, status, pipe_controlnet, controlnet, controlnet_models
    if not status['installed_diffusers']:
      alert_msg(page, "You need to Install HuggingFace Diffusers before using...")
      return
    if not bool(controlnet_prefs['original_image']) and len(controlnet_prefs['multi_controlnets']) == 0:
      alert_msg(page, "You must provide the Original Image to process...")
      return
    if not bool(controlnet_prefs['prompt']) and not from_list:
      alert_msg(page, "You must provide a Prompt to paint in your image...")
      return
    def prt(line, update=True):
      if type(line) == str:
        line = Text(line)
      if from_list:
        page.imageColumn.controls.append(line)
        if update:
          page.imageColumn.update()
      else:
        page.ControlNet.controls.append(line)
        #page.controlnet_output.controls.append(line)
        if update:
          page.ControlNet.update()
          #page.controlnet_output.update()
    def clear_last():
      if from_list:
        if len(page.imageColumn.controls) == 0: return
        del page.imageColumn.controls[-1]
        page.imageColumn.update()
      else:
        if len(page.ControlNet.controls) == 1: return
        del page.ControlNet.controls[-1]
        page.ControlNet.update()
        #if len(page.controlnet_output.controls) == 0: return
        #del page.controlnet_output.controls[-1]
        #page.controlnet_output.update()
    def clear_list():
      if from_list:
        page.imageColumn.controls.clear()
      else:
        page.ControlNet.controls = page.ControlNet.controls[:1]
    def autoscroll(scroll=True):
      if from_list:
        page.imageColumn.auto_scroll = scroll
        page.imageColumn.update()
      else:
        page.ControlNet.auto_scroll = scroll
        page.ControlNet.update()
    progress = ProgressBar(bar_height=8)
    total_steps = controlnet_prefs['steps']
    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep}"
      progress.update()
    controlnet_prompts = []
    if from_list:
      if len(prompts) < 1:
        alert_msg(page, "You need to add Prompts to your List first... ")
        return
      for p in prompts:
        control = {'prompt': p.prompt, 'negative_prompt': p['negative_prompt'] if bool(p['negative_prompt']) else controlnet_prefs['negative_prompt'], 'original_image': p['init_image'] if bool(p['init_image']) else controlnet_prefs['original_image'], 'conditioning_scale': controlnet_prefs['conditioning_scale'], 'control_guidance_start': controlnet_prefs['control_guidance_start'], 'control_guidance_end': controlnet_prefs['control_guidance_end'], 'seed': p['seed']}
        controlnet_prompts.append(control)
      page.tabs.selected_index = 4
      page.tabs.update()
      #page.controlnet_output.controls.clear()
    else:
      if not bool(controlnet_prefs['prompt']):
        alert_msg(page, "You need to add a Text Prompt first... ")
        return
      original = controlnet_prefs['original_image']
      conditioning_scale = controlnet_prefs['conditioning_scale']
      control_guidance_start = controlnet_prefs['control_guidance_start']
      control_guidance_end = controlnet_prefs['control_guidance_end']
      if len(controlnet_prefs['multi_controlnets']) > 0:
        original = []
        conditioning_scale = []
        control_guidance_start = []
        control_guidance_end = []
        for c in controlnet_prefs['multi_controlnets']:
          original.append(c['original_image'])
          conditioning_scale.append(c['conditioning_scale'])
          control_guidance_start.append(c['control_guidance_start'])
          control_guidance_end.append(c['control_guidance_end'])
      control = {'prompt':controlnet_prefs['prompt'], 'negative_prompt': controlnet_prefs['negative_prompt'], 'original_image': original, 'conditioning_scale': conditioning_scale, 'control_guidance_start':control_guidance_start, 'control_guidance_end': control_guidance_end, 'seed': controlnet_prefs['seed']}
      if controlnet_prefs['use_init_video']:
        control['init_video'] = controlnet_prefs['init_video']
        control['start_time'] = controlnet_prefs['start_time']
        control['end_time'] = controlnet_prefs['end_time']
        control['fps'] = controlnet_prefs['fps']
      controlnet_prompts.append(control)
      #page.controlnet_output.controls.clear()
    autoscroll(True)
    clear_list()
    prt(Divider(thickness=2, height=4))
    prt(Installing("Installing ControlNet Packages..."))
    if status['loaded_controlnet'] == controlnet_prefs["control_task"]:
        clear_pipes('controlnet')
    else:
        clear_pipes()
    import requests
    from io import BytesIO
    from PIL import ImageOps
    from PIL.PngImagePlugin import PngInfo
    try:
        try:
          import cv2
        except ModuleNotFoundError:
          run_sp("pip install opencv-contrib-python", realtime=False)
          import cv2
          pass
        try:
          from controlnet_aux import MLSDdetector
        except ModuleNotFoundError:
          run_sp("pip install --upgrade controlnet-aux", realtime=False)
          #run_sp("pip install git+https://github.com/patrickvonplaten/controlnet_aux.git")
          pass
        from controlnet_aux import MLSDdetector
        from controlnet_aux import OpenposeDetector
        from diffusers import StableDiffusionControlNetPipeline, StableDiffusionXLControlNetPipeline, ControlNetModel
        #run_sp("pip install scikit-image", realtime=False)
    except Exception as e:
        clear_last()
        alert_msg(page, f"ERROR Installing Required Packages...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
        flush()
        return
    canny_checkpoint = "lllyasviel/control_v11p_sd15_canny"
    scribble_checkpoint = "lllyasviel/control_v11p_sd15_scribble"
    openpose_checkpoint = "lllyasviel/control_v11p_sd15_openpose"
    depth_checkpoint = "lllyasviel/control_v11p_sd15_depth"
    HED_checkpoint = "lllyasviel/control_v11p_sd15_softedge"
    mlsd_checkpoint = "lllyasviel/control_v11p_sd15_mlsd"
    normal_checkpoint = "lllyasviel/control_v11p_sd15_normalbae"
    seg_checkpoint = "lllyasviel/control_v11p_sd15_seg"
    lineart_checkpoint = "lllyasviel/control_v11p_sd15_lineart"
    ip2p_checkpoint = "lllyasviel/control_v11e_sd15_ip2p"
    shuffle_checkpoint = "lllyasviel/control_v11e_sd15_shuffle"
    tile_checkpoint = "lllyasviel/control_v11f1e_sd15_tile"
    brightness_checkpoint = "ioclab/control_v1p_sd15_brightness"
    hed = None
    openpose = None
    depth_estimator = None
    mlsd = None
    image_processor = None
    image_segmentor = None
    normal = None
    lineart = None
    shuffle = None
    def get_controlnet(task):
        nonlocal hed, openpose, depth_estimator, mlsd, image_processor, image_segmentor, normal, lineart, shuffle
        if controlnet_models[task] != None:
            return controlnet_models[task]
        if task == "Canny Map Edge" or task == "Video Canny Edge":
            task = "Canny Map Edge"
            controlnet_models[task] = ControlNetModel.from_pretrained(canny_checkpoint, torch_dtype=torch.float16).to(torch_device)
        elif task == "Scribble":
            from controlnet_aux import HEDdetector
            hed = HEDdetector.from_pretrained('lllyasviel/Annotators')
            controlnet_models[task] = ControlNetModel.from_pretrained(scribble_checkpoint, torch_dtype=torch.float16).to(torch_device)
        elif task == "OpenPose" or task == "Video OpenPose":
            task = "OpenPose"
            from controlnet_aux import OpenposeDetector
            openpose = OpenposeDetector.from_pretrained('lllyasviel/ControlNet')
            controlnet_models[task] = ControlNetModel.from_pretrained(openpose_checkpoint, torch_dtype=torch.float16).to(torch_device)
        elif task == "Depth":
            from transformers import pipeline
            depth_estimator = pipeline('depth-estimation')
            controlnet_models[task] = ControlNetModel.from_pretrained(depth_checkpoint, torch_dtype=torch.float16).to(torch_device)
        elif task == "Kandinsky Depth":
            from transformers import pipeline
            from diffusers import KandinskyV22PriorPipeline, KandinskyV22ControlnetPipeline
            depth_estimator = pipeline('depth-estimation')
            #controlnet_models[task] = ControlNetModel.from_pretrained(depth_checkpoint, torch_dtype=torch.float16).to(torch_device)
            controlnet_models[task] = KandinskyV22ControlnetPipeline.from_pretrained("kandinsky-community/kandinsky-2-2-controlnet-depth", torch_dtype=torch.float16).to(torch_device)
        elif task == "HED":
            from controlnet_aux import HEDdetector
            hed = HEDdetector.from_pretrained('lllyasviel/Annotators')
            #pidi_net = PidiNetDetector.from_pretrained('lllyasviel/Annotators')
            controlnet_models[task] = ControlNetModel.from_pretrained(HED_checkpoint, torch_dtype=torch.float16).to(torch_device)
        elif task == "M-LSD":
            from controlnet_aux import MLSDdetector
            mlsd = MLSDdetector.from_pretrained('lllyasviel/ControlNet')
            controlnet_models[task] = ControlNetModel.from_pretrained(mlsd_checkpoint, torch_dtype=torch.float16).to(torch_device)
        elif task == "Normal Map":
            #from transformers import pipeline
            #depth_estimator = pipeline("depth-estimation", model ="Intel/dpt-hybrid-midas")
            from controlnet_aux import NormalBaeDetector
            normal = NormalBaeDetector.from_pretrained("lllyasviel/Annotators")
            controlnet_models[task] = ControlNetModel.from_pretrained(normal_checkpoint, torch_dtype=torch.float16).to(torch_device)
        elif task == "Segmented":
            from transformers import AutoImageProcessor, UperNetForSemanticSegmentation
            from controlnet_utils import ade_palette
            image_processor = AutoImageProcessor.from_pretrained("openmmlab/upernet-convnext-small")
            image_segmentor = UperNetForSemanticSegmentation.from_pretrained("openmmlab/upernet-convnext-small")
            controlnet_models[task] = ControlNetModel.from_pretrained(seg_checkpoint, torch_dtype=torch.float16).to(torch_device)
        elif task == "LineArt":
            from controlnet_aux import LineartDetector
            lineart = LineartDetector.from_pretrained("lllyasviel/Annotators")
            controlnet_models[task] = ControlNetModel.from_pretrained(lineart_checkpoint, torch_dtype=torch.float16).to(torch_device)
        elif task == "Shuffle":
            from controlnet_aux import ContentShuffleDetector
            shuffle = ContentShuffleDetector()
            controlnet_models[task] = ControlNetModel.from_pretrained(shuffle_checkpoint, torch_dtype=torch.float16).to(torch_device)
        elif task == "Tile":
            controlnet_models[task] = ControlNetModel.from_pretrained(tile_checkpoint, torch_dtype=torch.float16).to(torch_device)
        elif task == "Brightness":
            controlnet_models[task] = ControlNetModel.from_pretrained(brightness_checkpoint, torch_dtype=torch.float16, use_safetensors=True)
        elif task == "Instruct Pix2Pix":
            controlnet_models[task] = ControlNetModel.from_pretrained(ip2p_checkpoint, torch_dtype=torch.float16).to(torch_device)

        return controlnet_models[task]
    width, height = 0, 0
    def resize_for_condition_image(input_image: PILImage, resolution: int):
        input_image = input_image.convert("RGB")
        W, H = input_image.size
        k = float(resolution) / min(H, W)
        H *= k
        W *= k
        H = int(round(H / 64.0)) * 64
        W = int(round(W / 64.0)) * 64
        img = input_image.resize((W, H), resample=PILImage.LANCZOS)
        return img
    def prep_image(task, img):
        nonlocal hed, openpose, depth_estimator, mlsd, image_processor, image_segmentor, normal, lineart, shuffle
        nonlocal width, height
        if isinstance(img, str):
          if img.startswith('http'):
              #response = requests.get(controlnet_prefs['original_image'])
              #original_img = PILImage.open(BytesIO(response.content)).convert("RGB")
              original_img = PILImage.open(requests.get(img, stream=True).raw)
          else:
              if os.path.isfile(img):
                  original_img = PILImage.open(img)
              else:
                  alert_msg(page, f"ERROR: Couldn't find your original_image {img}")
                  return
          width, height = original_img.size
          width, height = scale_dimensions(width, height, controlnet_prefs['max_size'])
          #print(f"Size: {width}x{height}")
          original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS)
        #return original_img
        try:
            if task == "Canny Map Edge" or task == "Video Canny Edge":
                input_image = np.array(original_img)
                input_image = cv2.Canny(input_image, controlnet_prefs['low_threshold'], controlnet_prefs['high_threshold'])
                input_image = input_image[:, :, None]
                input_image = np.concatenate([input_image, input_image, input_image], axis=2)
                original_img = PILImage.fromarray(input_image)
            elif task == "Scribble":
                original_img = hed(original_img, scribble=True)
            elif task == "OpenPose" or task == "Video OpenPose":
                original_img = openpose(original_img, hand_and_face=True)
            elif task == "Depth":
                original_img = depth_estimator(original_img)['depth']
                input_image = np.array(original_img)
                input_image = input_image[:, :, None]
                input_image = np.concatenate([input_image, input_image, input_image], axis=2)
                original_img = PILImage.fromarray(input_image)
            elif task == "Kandinsky Depth":
                original_img = depth_estimator(original_img)['depth']
                input_image = np.array(original_img)
                input_image = input_image[:, :, None]
                input_image = np.concatenate([input_image, input_image, input_image], axis=2)
                detected_map = torch.from_numpy(input_image).float() / 255.0
                original_img = detected_map.permute(2, 0, 1).unsqueeze(0).half().to("cuda")
                #original_img = PILImage.fromarray(input_image)
            elif task == "HED":
                original_img = hed(original_img, safe=True)
            elif task == "M-LSD":
                original_img = mlsd(original_img)
            elif task == "Normal Map":
                #depth_estimator = pipeline("depth-estimation", model="Intel/dpt-hybrid-midas" )
                '''original_img = depth_estimator(original_img)['predicted_depth'][0]
                input_image = original_img.numpy()
                image_depth = input_image.copy()
                image_depth -= np.min(image_depth)
                image_depth /= np.max(image_depth)
                bg_threhold = 0.4
                x = cv2.Sobel(input_image, cv2.CV_32F, 1, 0, ksize=3)
                x[image_depth < bg_threhold] = 0
                y = cv2.Sobel(input_image, cv2.CV_32F, 0, 1, ksize=3)
                y[image_depth < bg_threhold] = 0
                z = np.ones_like(x) * np.pi * 2.0
                input_image = np.stack([x, y, z], axis=2)
                input_image /= np.sum(input_image ** 2.0, axis=2, keepdims=True) ** 0.5
                input_image = (input_image * 127.5 + 127.5).clip(0, 255).astype(np.uint8)
                original_img = PILImage.fromarray(input_image)'''
                original_img = normal(original_img)
            elif task == "Segmented":
                from controlnet_utils import ade_palette
                pixel_values = image_processor(original_img, return_tensors="pt").pixel_values
                with torch.no_grad():
                  outputs = image_segmentor(pixel_values)
                seg = image_processor.post_process_semantic_segmentation(outputs, target_sizes=[original_img.size[::-1]])[0]
                color_seg = np.zeros((seg.shape[0], seg.shape[1], 3), dtype=np.uint8) # height, width, 3
                palette = np.array(ade_palette())
                for label, color in enumerate(palette):
                    color_seg[seg == label, :] = color
                color_seg = color_seg.astype(np.uint8)
                original_img = PILImage.fromarray(color_seg)
            elif task == "LineArt":
                original_img = lineart(original_img)
            elif task == "Shuffle":
                original_img = shuffle(original_img)
            elif task == "Tile":
                original_img = resize_for_condition_image(original_img, 1024)
            elif task == "Brightness":
                original_img = PILImage.fromarray(original_img).convert('L')
            return original_img
        except Exception as e:
            #clear_last()
            clear_last()
            alert_msg(page, f"ERROR Preparing ControlNet {controlnet_prefs['control_task']} Input Image...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            flush()
            return
    def prep_video(vid):
        nonlocal width, height
        if vid.startswith('http'):
            init_vid = download_file(vid, stable_dir)
        else:
            if os.path.isfile(vid):
                init_vid = vid
            else:
                alert_msg(page, f"ERROR: Couldn't find your init_video {vid}")
                return
        try:
            start_time = float(controlnet_prefs['start_time'])
            end_time = float(controlnet_prefs['end_time'])
            fps = int(controlnet_prefs['fps'])
            max_size = controlnet_prefs['max_size']
        except Exception:
            alert_msg(page, "Make sure your Numbers are actual numbers...")
            return
        prt("Extracting Frames from Video Clip")
        try:
            cap = cv2.VideoCapture(init_vid)
        except Exception as e:
            alert_msg(page, "ERROR Reading Video File. May be Incompatible Format...")
            clear_last()
            return
        count = 0
        video = []
        frames = []
        width = height = 0
        cap.set(cv2.CAP_PROP_FPS, fps)
        video_length = cap.get(cv2.CAP_PROP_FRAME_COUNT)
        start_frame = int(start_time * fps)
        if end_time == 0 or end_time == 0.0:
            end_frame = int(video_length)
        else:
            end_frame = int(end_time * fps)
        total = end_frame - start_frame
        for i in range(start_frame, end_frame):
            cap.set(cv2.CAP_PROP_POS_FRAMES, i)
            success, image = cap.read()
            if success:
                #filename = os.path.join(output_dir, f'{file_prefix}{count}.png')
                if width == 0:
                    shape = image.shape
                    width, height = scale_dimensions(shape[1], shape[0], max=max_size, multiple=16)
                image = cv2.resize(image, (width, height), interpolation = cv2.INTER_AREA)
                #cv2.imwrite(os.path.join(output_dir, filename), image)
                image = prep_image(controlnet_prefs['control_task'], PILImage.fromarray(image))
                video.append(image)
                count += 1
        cap.release()
        clear_last()
        return video
    loaded_controlnet = None
    if len(controlnet_prefs['multi_controlnets']) > 0 and not from_list and not controlnet_prefs['use_init_video']:
        controlnet = []
        loaded_controlnet = []
        for c in controlnet_prefs['multi_controlnets']:
            controlnet.append(get_controlnet(c['control_task']))
            loaded_controlnet.append(c['control_task'])
    else:
        controlnet = get_controlnet(controlnet_prefs['control_task'])
        loaded_controlnet = controlnet_prefs['control_task']
    for k, v in controlnet_models.items():
      if v != None and k in loaded_controlnet:
        del v
        controlnet_models[k] = None
    model = get_model(prefs['model_ckpt'])
    model_path = model['path']
    if pipe_controlnet == None or status['loaded_controlnet'] != controlnet_prefs["control_task"]:
        #if controlnet_prefs["use_SDXL"]:
        #TODO: pipe_controlnet = StableDiffusionXLControlNetPipeline
        pipe_controlnet = StableDiffusionControlNetPipeline.from_pretrained(model_path, controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
        #pipe_controlnet.enable_model_cpu_offload()
        pipe_controlnet = optimize_pipe(pipe_controlnet, vae=True, vae_tiling=True)
        status['loaded_controlnet'] = loaded_controlnet #controlnet_prefs["control_task"]
    #else:
        #pipe_controlnet.controlnet=controlnet
    pipe_controlnet = pipeline_scheduler(pipe_controlnet)
    if controlnet_prefs['use_init_video']:
        from diffusers.pipelines.text_to_video_synthesis.pipeline_text_to_video_zero import CrossFrameAttnProcessor
        pipe_controlnet.unet.set_attn_processor(CrossFrameAttnProcessor(batch_size=2))
        pipe_controlnet.controlnet.set_attn_processor(CrossFrameAttnProcessor(batch_size=2))
    clear_last()
    prt(f"Generating ControlNet {controlnet_prefs['control_task']} of your Image...")
    batch_output = os.path.join(stable_dir, controlnet_prefs['batch_folder_name'])
    if not os.path.isdir(batch_output):
      os.makedirs(batch_output)
    batch_output = os.path.join(prefs['image_output'], controlnet_prefs['batch_folder_name'])
    if not os.path.isdir(batch_output):
      os.makedirs(batch_output)
    for pr in controlnet_prompts:
        prt(progress)
        autoscroll(False)
        filename = f"{controlnet_prefs['file_prefix']}{format_filename(pr['prompt'])}"
        filename = filename[:int(prefs['file_max_length'])]
        if len(controlnet_prefs['multi_controlnets']) > 0 and not from_list and not controlnet_prefs['use_init_video']:
            original_img = []
            for c in controlnet_prefs['multi_controlnets']:
                original_img.append(prep_image(c['control_task'], c['original_image']))
                if controlnet_prefs['show_processed_image']:
                    processed_img = available_file(batch_output, f"{filename}-{c['control_task'].partition(' ')[0]}", 0, no_num=True)
                    w, h = original_img[-1].size
                    original_img[-1].save(processed_img)
                    prt(Row([ImageButton(src=processed_img, data=processed_img, width=w, height=h, page=page)], alignment=MainAxisAlignment.CENTER))
        elif not controlnet_prefs['use_init_video']:
            original_img = prep_image(controlnet_prefs['control_task'], pr['original_image'])
            if controlnet_prefs['show_processed_image']:
                processed_img = available_file(batch_output, f"{filename}-{controlnet_prefs['control_task'].partition(' ')[0]}", 0, no_num=True)
                w, h = original_img.size
                original_img.save(processed_img)
                prt(Row([ImageButton(src=processed_img, data=processed_img, width=w, height=h, page=page)], alignment=MainAxisAlignment.CENTER))
        else:
            video_img = prep_video(pr['original_image'])
            latents = torch.randn((1, 4, 64, 64), device="cuda", dtype=torch.float16).repeat(len(video_img), 1, 1, 1)
        try:
            random_seed = int(pr['seed']) if int(pr['seed']) > 0 else rnd.randint(0,4294967295)
            generator = torch.Generator(device=torch_device).manual_seed(random_seed)
            if not controlnet_prefs['use_init_video']:
                images = pipe_controlnet(pr['prompt'], negative_prompt=pr['negative_prompt'], image=original_img, controlnet_conditioning_scale=pr['conditioning_scale'], control_guidance_start=pr['control_guidance_start'], control_guidance_end=pr['control_guidance_end'], num_inference_steps=controlnet_prefs['steps'], guidance_scale=controlnet_prefs['guidance_scale'], eta=controlnet_prefs['eta'], num_images_per_prompt=controlnet_prefs['batch_size'], height=height, width=width, generator=generator, callback=callback_fnc, callback_steps=1).images
            else:
                images = pipe_controlnet(pr['prompt'] * len(video_img), negative_prompt=pr['negative_prompt'] * len(video_img), image=video_img, latents=latents, controlnet_conditioning_scale=pr['conditioning_scale'], control_guidance_start=pr['control_guidance_start'], control_guidance_end=pr['control_guidance_end'], num_inference_steps=controlnet_prefs['steps'], guidance_scale=controlnet_prefs['guidance_scale'], eta=controlnet_prefs['eta'], height=height, width=width, generator=generator, callback=callback_fnc, callback_steps=1).images
        except Exception as e:
            #clear_last()
            clear_last()
            alert_msg(page, f"ERROR Generating ControlNet {controlnet_prefs['control_task']}...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            flush()
            return
        clear_pipes('controlnet')
        clear_last()
        #clear_last()
        autoscroll(True)
        #filename = pr['original_image'].rpartition(slash)[2].rpartition('.')[0]
        #if prefs['file_suffix_seed']: fname += f"-{random_seed}"
        num = 0
        for image in images:
            random_seed += num
            fname = filename + (f"-{random_seed}" if prefs['file_suffix_seed'] else "")
            image_path = available_file(os.path.join(stable_dir, controlnet_prefs['batch_folder_name']), fname, num)
            unscaled_path = image_path
            output_file = image_path.rpartition(slash)[2]
            #PILImage.fromarray(image).save(image_path)
            image.save(image_path)
            out_path = image_path.rpartition(slash)[0]
            upscaled_path = os.path.join(out_path, output_file)
            new_file = available_file(batch_output, fname, num)
            if not controlnet_prefs['display_upscaled_image'] or not controlnet_prefs['apply_ESRGAN_upscale']:
                prt(Row([ImageButton(src=unscaled_path, data=new_file, width=width, height=height, page=page)], alignment=MainAxisAlignment.CENTER))
                #prt(Row([Img(src=unscaled_path, fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
            if controlnet_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
                os.chdir(os.path.join(dist_dir, 'Real-ESRGAN'))
                upload_folder = 'upload'
                result_folder = 'results'
                if os.path.isdir(upload_folder):
                    shutil.rmtree(upload_folder)
                if os.path.isdir(result_folder):
                    shutil.rmtree(result_folder)
                os.mkdir(upload_folder)
                os.mkdir(result_folder)
                short_name = f'{fname[:80]}-{num}.png'
                dst_path = os.path.join(dist_dir, 'Real-ESRGAN', upload_folder, short_name)
                #print(f'Moving {fpath} to {dst_path}')
                #shutil.move(fpath, dst_path)
                shutil.copy(image_path, dst_path)
                #faceenhance = ' --face_enhance' if controlnet_prefs["face_enhance"] else ''
                faceenhance = ''
                run_sp(f'python inference_realesrgan.py -n realesr-general-x4v3 -i upload --outscale {controlnet_prefs["enlarge_scale"]}{faceenhance}', cwd=os.path.join(dist_dir, 'Real-ESRGAN'), realtime=False)
                out_file = short_name.rpartition('.')[0] + '_out.png'
                shutil.move(os.path.join(dist_dir, 'Real-ESRGAN', result_folder, out_file), upscaled_path)
                image_path = upscaled_path
                os.chdir(stable_dir)

            if prefs['save_image_metadata']:
                task = and_list(controlnet_prefs['control_task']) if isinstance(controlnet_prefs['control_task'], list) else controlnet_prefs['control_task']
                img = PILImage.open(image_path)
                metadata = PngInfo()
                metadata.add_text("artist", prefs['meta_ArtistName'])
                metadata.add_text("copyright", prefs['meta_Copyright'])
                metadata.add_text("software", "Stable Diffusion Deluxe" + f", upscaled {controlnet_prefs['enlarge_scale']}x with ESRGAN" if controlnet_prefs['apply_ESRGAN_upscale'] else "")
                metadata.add_text("pipeline", "ControlNet " + task)
                if prefs['save_config_in_metadata']:
                  config_json = controlnet_prefs.copy()
                  config_json['model_path'] = model_path
                  config_json['seed'] = random_seed
                  config_json['prompt'] = pr['prompt']
                  config_json['negative_prompt'] = pr['negative_prompt']
                  del config_json['batch_size']
                  del config_json['max_size']
                  del config_json['display_upscaled_image']
                  del config_json['batch_folder_name']
                  if not config_json['apply_ESRGAN_upscale']:
                    del config_json['enlarge_scale']
                    del config_json['apply_ESRGAN_upscale']
                  metadata.add_text("config_json", json.dumps(config_json, ensure_ascii=True, indent=4))
                img.save(image_path, pnginfo=metadata)
            #TODO: PyDrive
            if storage_type == "Colab Google Drive":
                #new_file = available_file(output_path, fname, num)
                #out_path = new_file
                shutil.copy(image_path, new_file)
            elif bool(prefs['image_output']):
                #new_file = available_file(output_path, fname, num)
                #out_path = new_file
                shutil.copy(image_path, new_file)
            if controlnet_prefs['display_upscaled_image']:
                prt(Row([ImageButton(src=new_file, data=new_file, width=width * float(controlnet_prefs["enlarge_scale"]), height=height * float(controlnet_prefs["enlarge_scale"]), page=page)], alignment=MainAxisAlignment.CENTER))
                #prt(Row([Img(src=upscaled_path, fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
            prt(Row([Text(new_file)], alignment=MainAxisAlignment.CENTER))
            num += 1
    autoscroll(False)
    del hed, openpose, depth_estimator, mlsd, image_processor, image_segmentor, normal, lineart, shuffle
    if prefs['enable_sounds']: page.snd_alert.play()

def run_controlnet_xl(page, from_list=False):
    global controlnet_xl_prefs, prefs, status, pipe_controlnet, controlnet, controlnet_xl_models
    if not status['installed_diffusers']:
      alert_msg(page, "You need to Install HuggingFace Diffusers before using...")
      return
    if not bool(controlnet_xl_prefs['original_image']) and len(controlnet_xl_prefs['multi_controlnets']) == 0:
      alert_msg(page, "You must provide the Original Image to process...")
      return
    if not bool(controlnet_xl_prefs['prompt']) and not from_list:
      alert_msg(page, "You must provide a Prompt to paint in your image...")
      return
    def prt(line, update=True):
      if type(line) == str:
        line = Text(line)
      if from_list:
        page.imageColumn.controls.append(line)
        if update:
          page.imageColumn.update()
      else:
        page.ControlNetXL.controls.append(line)
        #page.controlnet_xl_output.controls.append(line)
        if update:
          page.ControlNetXL.update()
          #page.controlnet_xl_output.update()
    def clear_last():
      if from_list:
        if len(page.imageColumn.controls) == 0: return
        del page.imageColumn.controls[-1]
        page.imageColumn.update()
      else:
        if len(page.ControlNetXL.controls) == 1: return
        del page.ControlNetXL.controls[-1]
        page.ControlNetXL.update()
        #if len(page.controlnet_xl_output.controls) == 0: return
        #del page.controlnet_xl_output.controls[-1]
        #page.controlnet_xl_output.update()
    def clear_list():
      if from_list:
        page.imageColumn.controls.clear()
      else:
        page.ControlNetXL.controls = page.ControlNetXL.controls[:1]
    def autoscroll(scroll=True):
      if from_list:
        page.imageColumn.auto_scroll = scroll
        page.imageColumn.update()
      else:
        page.ControlNetXL.auto_scroll = scroll
        page.ControlNetXL.update()
    progress = ProgressBar(bar_height=8)
    total_steps = controlnet_xl_prefs['steps']
    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep}"
      progress.update()
    controlnet_xl_prompts = []
    if from_list:
      if len(prompts) < 1:
        alert_msg(page, "You need to add Prompts to your List first... ")
        return
      for p in prompts:
        control = {'prompt': p.prompt, 'negative_prompt': p['negative_prompt'] if bool(p['negative_prompt']) else controlnet_xl_prefs['negative_prompt'], 'original_image': p['init_image'] if bool(p['init_image']) else controlnet_xl_prefs['original_image'], 'conditioning_scale': controlnet_xl_prefs['conditioning_scale'], 'control_guidance_start': controlnet_xl_prefs['control_guidance_start'], 'control_guidance_end': controlnet_xl_prefs['control_guidance_end'], 'seed': p['seed']}
        controlnet_xl_prompts.append(control)
      page.tabs.selected_index = 4
      page.tabs.update()
      #page.controlnet_xl_output.controls.clear()
    else:
      if not bool(controlnet_xl_prefs['prompt']):
        alert_msg(page, "You need to add a Text Prompt first... ")
        return
      original = controlnet_xl_prefs['original_image']
      conditioning_scale = controlnet_xl_prefs['conditioning_scale']
      control_guidance_start = controlnet_xl_prefs['control_guidance_start']
      control_guidance_end = controlnet_xl_prefs['control_guidance_end']
      if len(controlnet_xl_prefs['multi_controlnets']) > 0:
        original = []
        conditioning_scale = []
        control_guidance_start = []
        control_guidance_end = []
        for c in controlnet_xl_prefs['multi_controlnets']:
          original.append(c['original_image'])
          conditioning_scale.append(c['conditioning_scale'])
          control_guidance_start.append(c['control_guidance_start'])
          control_guidance_end.append(c['control_guidance_end'])
      control = {'prompt':controlnet_xl_prefs['prompt'], 'negative_prompt': controlnet_xl_prefs['negative_prompt'], 'original_image': original, 'conditioning_scale': conditioning_scale, 'control_guidance_start':control_guidance_start, 'control_guidance_end': control_guidance_end, 'seed': controlnet_xl_prefs['seed']}
      if controlnet_xl_prefs['use_init_video']:
        control['init_video'] = controlnet_xl_prefs['init_video']
        control['start_time'] = controlnet_xl_prefs['start_time']
        control['end_time'] = controlnet_xl_prefs['end_time']
        control['fps'] = controlnet_xl_prefs['fps']
      controlnet_xl_prompts.append(control)
      #page.controlnet_xl_output.controls.clear()
    autoscroll(True)
    clear_list()
    prt(Divider(thickness=2, height=4))
    prt(Installing("Installing ControlNetXL Packages..."))
    if status['loaded_controlnet'] == controlnet_xl_prefs["control_task"]:
        clear_pipes('controlnet')
    else:
        clear_pipes()
    import requests
    from io import BytesIO
    from PIL import ImageOps
    from PIL.PngImagePlugin import PngInfo
    try:
        try:
          import cv2
        except ModuleNotFoundError:
          run_sp("pip install opencv-contrib-python", realtime=False)
          import cv2
          pass
        try:
          from controlnet_aux import MLSDdetector
        except ModuleNotFoundError:
          run_sp("pip install --upgrade controlnet-aux", realtime=False)
          #run_sp("pip install git+https://github.com/patrickvonplaten/controlnet_aux.git")
          pass
        from controlnet_aux import MLSDdetector
        from controlnet_aux import OpenposeDetector
        from diffusers import StableDiffusionXLControlNetPipeline, ControlNetModel, AutoencoderKL
        #run_sp("pip install scikit-image", realtime=False)
    except Exception as e:
        clear_last()
        alert_msg(page, f"ERROR Installing Required Packages...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
        flush()
        return
    canny_checkpoint = "diffusers/controlnet-canny-sdxl-1.0"
    canny_small_checkpoint = "diffusers/controlnet-canny-sdxl-1.0-small"
    canny_mid_checkpoint = "diffusers/controlnet-canny-sdxl-1.0-mid"
    depth_checkpoint = "diffusers/controlnet-depth-sdxl-1.0"
    depth_small_checkpoint = "diffusers/controlnet-depth-sdxl-1.0-small"
    depth_mid_checkpoint = "diffusers/controlnet-depth-sdxl-1.0-mid"
    seg_checkpoint = "SargeZT/sdxl-controlnet-seg"
    softedge_checkpoint = "SargeZT/controlnet-sd-xl-1.0-softedge-dexined"#"SargeZT/sdxl-controlnet-softedge"
    lineart_checkpoint = "zbulrush/controlnet-sd-xl-1.0-lineart"
    openpose_checkpoint = "thibaud/controlnet-openpose-sdxl-1.0"
    scribble_checkpoint = "lllyasviel/control_v11p_sd15_scribble"
    HED_checkpoint = "lllyasviel/control_v11p_sd15_softedge"
    mlsd_checkpoint = "lllyasviel/control_v11p_sd15_mlsd"
    normal_checkpoint = "lllyasviel/control_v11p_sd15_normalbae"
    ip2p_checkpoint = "lllyasviel/control_v11e_sd15_ip2p"
    shuffle_checkpoint = "lllyasviel/control_v11e_sd15_shuffle"
    tile_checkpoint = "lllyasviel/control_v11f1e_sd15_tile"
    brightness_checkpoint = "ioclab/control_v1p_sd15_brightness"
    hed = None
    openpose = None
    depth_estimator = None
    feature_extractor = None
    mlsd = None
    image_processor = None
    image_segmentor = None
    normal = None
    lineart = None
    shuffle = None
    original_img = None
    def get_controlnet(task):
        nonlocal hed, openpose, depth_estimator, feature_extractor, mlsd, image_processor, image_segmentor, normal, lineart, shuffle
        if controlnet_xl_models[task] != None:
            return controlnet_xl_models[task]
        if "Canny Map" in task or task == "Video Canny Edge":
            controlnet_xl_models[task] = ControlNetModel.from_pretrained(canny_mid_checkpoint if 'mid' in task else canny_small_checkpoint if 'small' in task else canny_checkpoint, torch_dtype=torch.float16).to(torch_device)
            task = "Canny Map Edge"
        elif task == "Scribble":
            from controlnet_aux import HEDdetector
            hed = HEDdetector.from_pretrained('lllyasviel/Annotators')
            controlnet_xl_models[task] = ControlNetModel.from_pretrained(scribble_checkpoint, torch_dtype=torch.float16).to(torch_device)
        elif task == "OpenPose" or task == "Video OpenPose":
            task = "OpenPose"
            from controlnet_aux import OpenposeDetector
            openpose = OpenposeDetector.from_pretrained('lllyasviel/ControlNet')
            controlnet_xl_models[task] = ControlNetModel.from_pretrained(openpose_checkpoint, torch_dtype=torch.float16).to(torch_device)
        elif task == "Kandinsky Depth":
            from transformers import pipeline
            from diffusers import KandinskyV22PriorPipeline, KandinskyV22ControlnetPipeline
            depth_estimator = pipeline('depth-estimation')
            #controlnet_xl_models[task] = ControlNetModel.from_pretrained(depth_checkpoint, torch_dtype=torch.float16).to(torch_device)
            controlnet_xl_models[task] = KandinskyV22ControlnetPipeline.from_pretrained("kandinsky-community/kandinsky-2-2-controlnet-depth", torch_dtype=torch.float16).to(torch_device)
        elif "Depth" in task:
            from transformers import DPTFeatureExtractor, DPTForDepthEstimation
            depth_estimator = DPTForDepthEstimation.from_pretrained("Intel/dpt-hybrid-midas").to("cuda")
            feature_extractor = DPTFeatureExtractor.from_pretrained("Intel/dpt-hybrid-midas")
            controlnet_xl_models[task] = ControlNetModel.from_pretrained(depth_mid_checkpoint if 'mid' in task else depth_small_checkpoint if 'small' in task else depth_checkpoint, variant="fp16", use_safetensors=True, torch_dtype=torch.float16).to(torch_device)
        elif task == "Softedge":
            from controlnet_aux import HEDdetector, PidiNetDetector
            hed = HEDdetector.from_pretrained('lllyasviel/Annotators')
            hed = PidiNetDetector.from_pretrained('lllyasviel/Annotators') #pidi_net
            controlnet_xl_models[task] = ControlNetModel.from_pretrained(softedge_checkpoint, torch_dtype=torch.float16).to(torch_device)
        elif task == "HED":
            from controlnet_aux import HEDdetector
            hed = HEDdetector.from_pretrained('lllyasviel/Annotators')
            #pidi_net = PidiNetDetector.from_pretrained('lllyasviel/Annotators')
            controlnet_xl_models[task] = ControlNetModel.from_pretrained(HED_checkpoint, torch_dtype=torch.float16).to(torch_device)
        elif task == "M-LSD":
            from controlnet_aux import MLSDdetector
            mlsd = MLSDdetector.from_pretrained('lllyasviel/ControlNetXL')
            controlnet_xl_models[task] = ControlNetModel.from_pretrained(mlsd_checkpoint, torch_dtype=torch.float16).to(torch_device)
        elif task == "Normal Map":
            #from transformers import pipeline
            #depth_estimator = pipeline("depth-estimation", model ="Intel/dpt-hybrid-midas")
            from controlnet_aux import NormalBaeDetector
            normal = NormalBaeDetector.from_pretrained("lllyasviel/Annotators")
            controlnet_xl_models[task] = ControlNetModel.from_pretrained(normal_checkpoint, torch_dtype=torch.float16).to(torch_device)
        elif task == "Segmented":
            from transformers import AutoImageProcessor, UperNetForSemanticSegmentation
            from controlnet_utils import ade_palette
            image_processor = AutoImageProcessor.from_pretrained("openmmlab/upernet-convnext-small")
            image_segmentor = UperNetForSemanticSegmentation.from_pretrained("openmmlab/upernet-convnext-small")
            controlnet_xl_models[task] = ControlNetModel.from_pretrained(seg_checkpoint, torch_dtype=torch.float16).to(torch_device)
        elif task == "LineArt":
            from controlnet_aux import LineartDetector
            lineart = LineartDetector.from_pretrained("lllyasviel/Annotators")
            controlnet_xl_models[task] = ControlNetModel.from_pretrained(lineart_checkpoint, torch_dtype=torch.float16).to(torch_device)
        elif task == "Shuffle":
            from controlnet_aux import ContentShuffleDetector
            shuffle = ContentShuffleDetector()
            controlnet_xl_models[task] = ControlNetModel.from_pretrained(shuffle_checkpoint, torch_dtype=torch.float16).to(torch_device)
        elif task == "Tile":
            controlnet_xl_models[task] = ControlNetModel.from_pretrained(tile_checkpoint, torch_dtype=torch.float16).to(torch_device)
        elif task == "Brightness":
            controlnet_xl_models[task] = ControlNetModel.from_pretrained(brightness_checkpoint, torch_dtype=torch.float16, use_safetensors=True)
        elif task == "Instruct Pix2Pix":
            controlnet_xl_models[task] = ControlNetModel.from_pretrained(ip2p_checkpoint, torch_dtype=torch.float16).to(torch_device)

        return controlnet_xl_models[task]
    width, height = 0, 0
    def resize_for_condition_image(input_image: PILImage, resolution: int):
        input_image = input_image.convert("RGB")
        W, H = input_image.size
        k = float(resolution) / min(H, W)
        H *= k
        W *= k
        H = int(round(H / 64.0)) * 64
        W = int(round(W / 64.0)) * 64
        img = input_image.resize((W, H), resample=PILImage.LANCZOS)
        return img
    def prep_image(task, img):
        nonlocal hed, openpose, depth_estimator, feature_extractor, mlsd, image_processor, image_segmentor, normal, lineart, shuffle
        nonlocal width, height
        if isinstance(img, str):
          if img.startswith('http'):
              #response = requests.get(controlnet_xl_prefs['original_image'])
              #original_img = PILImage.open(BytesIO(response.content)).convert("RGB")
              original_img = PILImage.open(requests.get(img, stream=True).raw)
          else:
              if os.path.isfile(img):
                  original_img = PILImage.open(img)
              else:
                  alert_msg(page, f"ERROR: Couldn't find your original_image {img}")
                  return
          width, height = original_img.size
          width, height = scale_dimensions(width, height, controlnet_xl_prefs['max_size'])
          #print(f"Size: {width}x{height}")
          original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS)
        #return original_img
        try:
            if 'Canny' in task: # == "Canny Map Edge" or task == "Video Canny Edge":
                input_image = np.array(original_img)
                input_image = cv2.Canny(input_image, controlnet_xl_prefs['low_threshold'], controlnet_xl_prefs['high_threshold'])
                input_image = input_image[:, :, None]
                input_image = np.concatenate([input_image, input_image, input_image], axis=2)
                original_img = PILImage.fromarray(input_image)
            elif task == "Scribble":
                original_img = hed(original_img, scribble=True)
            elif task == "OpenPose" or task == "Video OpenPose":
                original_img = openpose(original_img, hand_and_face=True)
            elif task == "Kandinsky Depth":
                original_img = depth_estimator(original_img)['depth']
                input_image = np.array(original_img)
                input_image = input_image[:, :, None]
                input_image = np.concatenate([input_image, input_image, input_image], axis=2)
                detected_map = torch.from_numpy(input_image).float() / 255.0
                original_img = detected_map.permute(2, 0, 1).unsqueeze(0).half().to("cuda")
                #original_img = PILImage.fromarray(input_image)
            elif "Depth" in task:
                original_img = feature_extractor(images=original_img, return_tensors="pt").pixel_values.to("cuda")
                with torch.no_grad(), torch.autocast("cuda"):
                    depth_map = depth_estimator(original_img).predicted_depth
                depth_map = torch.nn.functional.interpolate(
                    depth_map.unsqueeze(1),
                    size=(1024, 1024),
                    mode="bicubic",
                    align_corners=False,
                )
                depth_min = torch.amin(depth_map, dim=[1, 2, 3], keepdim=True)
                depth_max = torch.amax(depth_map, dim=[1, 2, 3], keepdim=True)
                depth_map = (depth_map - depth_min) / (depth_max - depth_min)
                original_img = torch.cat([depth_map] * 3, dim=1)
                original_img = original_img.permute(0, 2, 3, 1).cpu().numpy()[0]
                original_img = PILImage.fromarray((original_img * 255.0).clip(0, 255).astype(np.uint8))
            elif task == "Softedge":
                original_img = hed(original_img, safe=True)
            elif task == "HED":
                original_img = hed(original_img, safe=True)
            elif task == "M-LSD":
                original_img = mlsd(original_img)
            elif task == "Normal Map":
                #depth_estimator = pipeline("depth-estimation", model="Intel/dpt-hybrid-midas" )
                '''original_img = depth_estimator(original_img)['predicted_depth'][0]
                input_image = original_img.numpy()
                image_depth = input_image.copy()
                image_depth -= np.min(image_depth)
                image_depth /= np.max(image_depth)
                bg_threhold = 0.4
                x = cv2.Sobel(input_image, cv2.CV_32F, 1, 0, ksize=3)
                x[image_depth < bg_threhold] = 0
                y = cv2.Sobel(input_image, cv2.CV_32F, 0, 1, ksize=3)
                y[image_depth < bg_threhold] = 0
                z = np.ones_like(x) * np.pi * 2.0
                input_image = np.stack([x, y, z], axis=2)
                input_image /= np.sum(input_image ** 2.0, axis=2, keepdims=True) ** 0.5
                input_image = (input_image * 127.5 + 127.5).clip(0, 255).astype(np.uint8)
                original_img = PILImage.fromarray(input_image)'''
                original_img = normal(original_img)
            elif task == "Segmented":
                from controlnet_utils import ade_palette
                pixel_values = image_processor(original_img, return_tensors="pt").pixel_values
                with torch.no_grad():
                  outputs = image_segmentor(pixel_values)
                seg = image_processor.post_process_semantic_segmentation(outputs, target_sizes=[original_img.size[::-1]])[0]
                color_seg = np.zeros((seg.shape[0], seg.shape[1], 3), dtype=np.uint8) # height, width, 3
                palette = np.array(ade_palette())
                for label, color in enumerate(palette):
                    color_seg[seg == label, :] = color
                color_seg = color_seg.astype(np.uint8)
                original_img = PILImage.fromarray(color_seg)
            elif task == "LineArt":
                original_img = lineart(original_img)
            elif task == "Shuffle":
                original_img = shuffle(original_img)
            elif task == "Tile":
                original_img = resize_for_condition_image(original_img, 1024)
            elif task == "Brightness":
                original_img = PILImage.fromarray(original_img).convert('L')
            return original_img
        except Exception as e:
            #clear_last()
            clear_last()
            alert_msg(page, f"ERROR Preparing ControlNet-XL {controlnet_xl_prefs['control_task']} Input Image...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            flush()
            return
    def prep_video(vid):
        nonlocal width, height
        if vid.startswith('http'):
            init_vid = download_file(vid, stable_dir)
        else:
            if os.path.isfile(vid):
                init_vid = vid
            else:
                alert_msg(page, f"ERROR: Couldn't find your init_video {vid}")
                return
        try:
            start_time = float(controlnet_xl_prefs['start_time'])
            end_time = float(controlnet_xl_prefs['end_time'])
            fps = int(controlnet_xl_prefs['fps'])
            max_size = controlnet_xl_prefs['max_size']
        except Exception:
            alert_msg(page, "Make sure your Numbers are actual numbers...")
            return
        prt("Extracting Frames from Video Clip")
        try:
            cap = cv2.VideoCapture(init_vid)
        except Exception as e:
            alert_msg(page, "ERROR Reading Video File. May be Incompatible Format...")
            clear_last()
            return
        count = 0
        video = []
        frames = []
        width = height = 0
        cap.set(cv2.CAP_PROP_FPS, fps)
        video_length = cap.get(cv2.CAP_PROP_FRAME_COUNT)
        start_frame = int(start_time * fps)
        if end_time == 0 or end_time == 0.0:
            end_frame = int(video_length)
        else:
            end_frame = int(end_time * fps)
        total = end_frame - start_frame
        for i in range(start_frame, end_frame):
            cap.set(cv2.CAP_PROP_POS_FRAMES, i)
            success, image = cap.read()
            if success:
                #filename = os.path.join(output_dir, f'{file_prefix}{count}.png')
                if width == 0:
                    shape = image.shape
                    width, height = scale_dimensions(shape[1], shape[0], max=max_size, multiple=16)
                image = cv2.resize(image, (width, height), interpolation = cv2.INTER_AREA)
                #cv2.imwrite(os.path.join(output_dir, filename), image)
                image = prep_image(controlnet_xl_prefs['control_task'], PILImage.fromarray(image))
                video.append(image)
                count += 1
        cap.release()
        clear_last()
        return video
    loaded_controlnet = None
    if len(controlnet_xl_prefs['multi_controlnets']) > 0 and not from_list and not controlnet_xl_prefs['use_init_video']:
        controlnet = []
        loaded_controlnet = []
        for c in controlnet_xl_prefs['multi_controlnets']:
            controlnet.append(get_controlnet(c['control_task']))
            loaded_controlnet.append(c['control_task'])
    else:
        controlnet = get_controlnet(controlnet_xl_prefs['control_task'])
        loaded_controlnet = controlnet_xl_prefs['control_task']
    for k, v in controlnet_xl_models.items():
      if v != None and k in loaded_controlnet:
        del v
        controlnet_xl_models[k] = None
    #model = get_model(prefs['model_ckpt'])
    model_path = "stabilityai/stable-diffusion-xl-base-1.0"
    if pipe_controlnet == None or status['loaded_controlnet'] != controlnet_xl_prefs["control_task"]:
        #if controlnet_xl_prefs["use_SDXL"]:
        #TODO: pipe_controlnet = StableDiffusionXLControlNetPipeline
        vae = AutoencoderKL.from_pretrained("madebyollin/sdxl-vae-fp16-fix", torch_dtype=torch.float16)
        pipe_controlnet = StableDiffusionXLControlNetPipeline.from_pretrained(model_path, controlnet=controlnet, vae=vae, safety_checker=None, torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
        #pipe_controlnet.enable_model_cpu_offload()
        pipe_controlnet = optimize_pipe(pipe_controlnet, vae=True, vae_tiling=True)
        status['loaded_controlnet'] = loaded_controlnet #controlnet_xl_prefs["control_task"]
    #else:
        #pipe_controlnet.controlnet=controlnet
    pipe_controlnet = pipeline_scheduler(pipe_controlnet)
    if controlnet_xl_prefs['use_init_video']:
        from diffusers.pipelines.text_to_video_synthesis.pipeline_text_to_video_zero import CrossFrameAttnProcessor
        pipe_controlnet.unet.set_attn_processor(CrossFrameAttnProcessor(batch_size=2))
        pipe_controlnet.controlnet.set_attn_processor(CrossFrameAttnProcessor(batch_size=2))
    clear_last()
    prt(f"Generating ControlNet-XL {controlnet_xl_prefs['control_task']} of your Image...")
    batch_output = os.path.join(stable_dir, controlnet_xl_prefs['batch_folder_name'])
    if not os.path.isdir(batch_output):
      os.makedirs(batch_output)
    batch_output = os.path.join(prefs['image_output'], controlnet_xl_prefs['batch_folder_name'])
    if not os.path.isdir(batch_output):
      os.makedirs(batch_output)
    for pr in controlnet_xl_prompts:
        prt(progress)
        autoscroll(False)
        filename = f"{controlnet_xl_prefs['file_prefix']}{format_filename(pr['prompt'])}"
        filename = filename[:int(prefs['file_max_length'])]
        if len(controlnet_xl_prefs['multi_controlnets']) > 0 and not from_list and not controlnet_xl_prefs['use_init_video']:
            original_img = []
            for c in controlnet_xl_prefs['multi_controlnets']:
                original_img.append(prep_image(c['control_task'], c['original_image']))
                if controlnet_xl_prefs['show_processed_image']:
                    processed_img = available_file(batch_output, f"{filename}-{c['control_task'].partition(' ')[0]}", 0, no_num=True)
                    w, h = original_img[-1].size
                    original_img[-1].save(processed_img)
                    prt(Row([ImageButton(src=processed_img, data=processed_img, width=w, height=h, page=page)], alignment=MainAxisAlignment.CENTER))
        elif not controlnet_xl_prefs['use_init_video']:
            original_img = prep_image(controlnet_xl_prefs['control_task'], pr['original_image'])
            if controlnet_xl_prefs['show_processed_image']:
                processed_img = available_file(batch_output, f"{filename}-{controlnet_xl_prefs['control_task'].partition(' ')[0]}", 0, no_num=True)
                w, h = original_img.size
                original_img.save(processed_img)
                prt(Row([ImageButton(src=processed_img, data=processed_img, width=w, height=h, page=page)], alignment=MainAxisAlignment.CENTER))
        else:
            video_img = prep_video(pr['original_image'])
            latents = torch.randn((1, 4, 64, 64), device="cuda", dtype=torch.float16).repeat(len(video_img), 1, 1, 1)
   
        try:
            random_seed = int(pr['seed']) if int(pr['seed']) > 0 else rnd.randint(0,4294967295)
            generator = torch.Generator(device=torch_device).manual_seed(random_seed)
            if not controlnet_xl_prefs['use_init_video']:
                images = pipe_controlnet(pr['prompt'], negative_prompt=pr['negative_prompt'], image=original_img, controlnet_xl_conditioning_scale=pr['conditioning_scale'], control_guidance_start=pr['control_guidance_start'], control_guidance_end=pr['control_guidance_end'], num_inference_steps=controlnet_xl_prefs['steps'], guidance_scale=controlnet_xl_prefs['guidance_scale'], eta=controlnet_xl_prefs['eta'], num_images_per_prompt=controlnet_xl_prefs['batch_size'], height=height, width=width, generator=generator, callback=callback_fnc, callback_steps=1).images
            else:
                images = pipe_controlnet(pr['prompt'] * len(video_img), negative_prompt=pr['negative_prompt'] * len(video_img), image=video_img, latents=latents, controlnet_xl_conditioning_scale=pr['conditioning_scale'], control_guidance_start=pr['control_guidance_start'], control_guidance_end=pr['control_guidance_end'], num_inference_steps=controlnet_xl_prefs['steps'], guidance_scale=controlnet_xl_prefs['guidance_scale'], eta=controlnet_xl_prefs['eta'], height=height, width=width, generator=generator, callback=callback_fnc, callback_steps=1).images
        except Exception as e:
            #clear_last()
            clear_last()
            alert_msg(page, f"ERROR Generating ControlNet-XL {controlnet_xl_prefs['control_task']}...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            flush()
            return
        clear_pipes('controlnet')
        clear_last()
        #clear_last()
        autoscroll(True)
        #filename = pr['original_image'].rpartition(slash)[2].rpartition('.')[0]
        
        #if prefs['file_suffix_seed']: fname += f"-{random_seed}"
        num = 0
        for image in images:
            random_seed += num
            fname = filename + (f"-{random_seed}" if prefs['file_suffix_seed'] else "")
            image_path = available_file(os.path.join(stable_dir, controlnet_xl_prefs['batch_folder_name']), fname, num)
            unscaled_path = image_path
            output_file = image_path.rpartition(slash)[2]
            #PILImage.fromarray(image).save(image_path)
            image.save(image_path)
            out_path = image_path.rpartition(slash)[0]
            upscaled_path = os.path.join(out_path, output_file)
            new_file = available_file(batch_output, fname, num)
            if not controlnet_xl_prefs['display_upscaled_image'] or not controlnet_xl_prefs['apply_ESRGAN_upscale']:
                prt(Row([ImageButton(src=unscaled_path, data=new_file, width=width, height=height, page=page)], alignment=MainAxisAlignment.CENTER))
                #prt(Row([Img(src=unscaled_path, fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
            if controlnet_xl_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
                os.chdir(os.path.join(dist_dir, 'Real-ESRGAN'))
                upload_folder = 'upload'
                result_folder = 'results'
                if os.path.isdir(upload_folder):
                    shutil.rmtree(upload_folder)
                if os.path.isdir(result_folder):
                    shutil.rmtree(result_folder)
                os.mkdir(upload_folder)
                os.mkdir(result_folder)
                short_name = f'{fname[:80]}-{num}.png'
                dst_path = os.path.join(dist_dir, 'Real-ESRGAN', upload_folder, short_name)
                #print(f'Moving {fpath} to {dst_path}')
                #shutil.move(fpath, dst_path)
                shutil.copy(image_path, dst_path)
                #faceenhance = ' --face_enhance' if controlnet_xl_prefs["face_enhance"] else ''
                faceenhance = ''
                run_sp(f'python inference_realesrgan.py -n realesr-general-x4v3 -i upload --outscale {controlnet_xl_prefs["enlarge_scale"]}{faceenhance}', cwd=os.path.join(dist_dir, 'Real-ESRGAN'), realtime=False)
                out_file = short_name.rpartition('.')[0] + '_out.png'
                shutil.move(os.path.join(dist_dir, 'Real-ESRGAN', result_folder, out_file), upscaled_path)
                image_path = upscaled_path
                os.chdir(stable_dir)

            if prefs['save_image_metadata']:
                task = and_list(controlnet_xl_prefs['control_task']) if isinstance(controlnet_xl_prefs['control_task'], list) else controlnet_xl_prefs['control_task']
                img = PILImage.open(image_path)
                metadata = PngInfo()
                metadata.add_text("artist", prefs['meta_ArtistName'])
                metadata.add_text("copyright", prefs['meta_Copyright'])
                metadata.add_text("software", "Stable Diffusion Deluxe" + f", upscaled {controlnet_xl_prefs['enlarge_scale']}x with ESRGAN" if controlnet_xl_prefs['apply_ESRGAN_upscale'] else "")
                metadata.add_text("pipeline", "ControlNetXL " + task)
                if prefs['save_config_in_metadata']:
                  config_json = controlnet_xl_prefs.copy()
                  config_json['model_path'] = model_path
                  config_json['seed'] = random_seed
                  config_json['prompt'] = pr['prompt']
                  config_json['negative_prompt'] = pr['negative_prompt']
                  del config_json['batch_size']
                  del config_json['max_size']
                  del config_json['display_upscaled_image']
                  del config_json['batch_folder_name']
                  if not config_json['apply_ESRGAN_upscale']:
                    del config_json['enlarge_scale']
                    del config_json['apply_ESRGAN_upscale']
                  metadata.add_text("config_json", json.dumps(config_json, ensure_ascii=True, indent=4))
                img.save(image_path, pnginfo=metadata)
            #TODO: PyDrive
            if storage_type == "Colab Google Drive":
                #new_file = available_file(output_path, fname, num)
                #out_path = new_file
                shutil.copy(image_path, new_file)
            elif bool(prefs['image_output']):
                #new_file = available_file(output_path, fname, num)
                #out_path = new_file
                shutil.copy(image_path, new_file)
            if controlnet_xl_prefs['display_upscaled_image']:
                prt(Row([ImageButton(src=new_file, data=new_file, width=width * float(controlnet_xl_prefs["enlarge_scale"]), height=height * float(controlnet_xl_prefs["enlarge_scale"]), page=page)], alignment=MainAxisAlignment.CENTER))
                #prt(Row([Img(src=upscaled_path, fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
            prt(Row([Text(new_file)], alignment=MainAxisAlignment.CENTER))
            num += 1
    autoscroll(False)
    del hed, openpose, depth_estimator, feature_extractor, mlsd, image_processor, image_segmentor, normal, lineart, shuffle
    if prefs['enable_sounds']: page.snd_alert.play()


def run_controlnet_tile_upscale(page, source_image, prompt="best quality", scale_factor=2.5):
    global controlnet_prefs, prefs, status, pipe_controlnet, controlnet, controlnet_models
    if not status['installed_diffusers']:
      alert_msg(page, "You need to Install HuggingFace Diffusers before using ControlNet Tile Upscale...")
      return
    from diffusers import ControlNetModel, DiffusionPipeline

    def resize_for_condition_image(input_image: PILImage, resolution: int):
        input_image = input_image.convert("RGB")
        W, H = input_image.size
        k = float(resolution) / min(H, W)
        H *= k
        W *= k
        H = int(round(H / 64.0)) * 64
        W = int(round(W / 64.0)) * 64
        img = input_image.resize((W, H), resample=Image.LANCZOS)
        return img

    controlnet = ControlNetModel.from_pretrained('lllyasviel/control_v11f1e_sd15_tile',
                                                torch_dtype=torch.float16)
    pipe_controlnet = DiffusionPipeline.from_pretrained("runwayml/stable-diffusion-v1-5",
                                            custom_pipeline="stable_diffusion_controlnet_img2img",
                                            controlnet=controlnet,
                                            torch_dtype=torch.float16).to('cuda')
    pipe_controlnet.enable_xformers_memory_efficient_attention()
    if isinstance(source_image, str):
        from diffusers.utils import load_image
        source_image = load_image(source_image)

    condition_image = resize_for_condition_image(source_image, 1024)
    image = pipe_controlnet(prompt=prompt,
                negative_prompt="blur, lowres, bad anatomy, bad hands, cropped, worst quality",
                image=condition_image,
                controlnet_conditioning_image=condition_image,
                width=condition_image.size[0],
                height=condition_image.size[1],
                strength=1.0,
                generator=torch.manual_seed(0),
                num_inference_steps=32,
                ).images[0]
    return image

def run_controlnet_video2video(page):
    global controlnet_video2video_prefs, status
    if not status['installed_diffusers']:
      alert_msg(page, "You need to Install HuggingFace Diffusers before using...")
      return
    def prt(line):
      if type(line) == str:
        line = Text(line)
      page.ControlNet_Video2Video.controls.append(line)
      page.ControlNet_Video2Video.update()
    def clear_last():
      del page.ControlNet_Video2Video.controls[-1]
      page.ControlNet_Video2Video.update()
    def autoscroll(scroll=True):
        page.ControlNet_Video2Video.auto_scroll = scroll
        page.ControlNet_Video2Video.update()
    if not bool(controlnet_video2video_prefs['init_video']):
        alert_msg(page, "You must provide a target init video...")
        return
    if not bool(controlnet_video2video_prefs['prompt']):
        alert_msg(page, "You must provide an interesting prompt to guide the video...")
        return
    page.ControlNet_Video2Video.controls = page.ControlNet_Video2Video.controls[:1]
    autoscroll()
    installer = Installing("Installing ControlNet Video2Video Libraries...")
    prt(installer)
    controlnet_video2video_dir = os.path.join(root_dir, "controlnetvideo")
    if not os.path.exists(controlnet_video2video_dir):
        try:
            installer.set_details("...cloning un1tz3r0/controlnetvideo.git")
            run_sp("git clone https://github.com/un1tz3r0/controlnetvideo.git", cwd=root_dir, realtime=False)
        except Exception as e:
            clear_last()
            alert_msg(page, "Error Installing ControlNet Video Requirements", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            return
    try:
        import ffmpeg
    except ImportError as e:
        installer.set_details("...installing ffmpeg")
        run_sp("pip install -q ffmpeg", realtime=False)
        pass
    try:
        import cv2
    except ModuleNotFoundError:
        run_sp("pip install opencv-python", realtime=False)
        import cv2
        pass
    try:
        import click
    except ImportError as e:
        installer.set_details("...installing click")
        run_sp("pip install -q click", realtime=False)
        pass
    try:
        import moviepy
    except ImportError as e:
        installer.set_details("...installing moviepy")
        run_sp("pip install -q moviepy", realtime=False)
        pass
    try:
        import xformers
    except ModuleNotFoundError:
        installer.set_details("...installing FaceBook's Xformers")
        #run_sp("pip install --pre -U triton", realtime=False)
        run_sp("pip install -U xformers", realtime=False)
        status['installed_xformers'] = True
        pass
    try:
        import mediapipe
    except ImportError as e:
        installer.set_details("...installing mediapipe")
        run_sp("pip install -q mediapipe", realtime=False)
        pass
    try:
        import controlnet_aux
    except ImportError as e:
        installer.set_details("...installing controlnet-aux")
        run_sp("pip install -q controlnet-aux", realtime=False)
        pass
    try:
        import watchdog
    except ImportError as e:
        installer.set_details("...installing watchdog")
        run_sp("pip install -q watchdog", realtime=False)
        pass
    clear_pipes()

    from PIL import ImageOps
    if bool(controlnet_video2video_prefs['output_name']):
        fname = format_filename(controlnet_video2video_prefs['output_name'], force_underscore=True)
    elif bool(controlnet_video2video_prefs['prompt']):
        fname = format_filename(controlnet_video2video_prefs['prompt'], force_underscore=True)
    elif bool(controlnet_video2video_prefs['batch_folder_name']):
        fname = format_filename(controlnet_video2video_prefs['batch_folder_name'], force_underscore=True)
    else: fname = "output"
    if bool(controlnet_video2video_prefs['file_prefix']):
        fname = f"{controlnet_video2video_prefs['file_prefix']}{fname}"
    if bool(controlnet_video2video_prefs['batch_folder_name']):
        batch_output = os.path.join(stable_dir, controlnet_video2video_prefs['batch_folder_name'])
    else: batch_output = stable_dir
    if not os.path.exists(batch_output):
        os.makedirs(batch_output)
    output_path = os.path.join(prefs['image_output'], controlnet_video2video_prefs['batch_folder_name'])
    if not os.path.exists(output_path):
        os.makedirs(output_path)
    init_vid = controlnet_video2video_prefs['init_video']
    if init_vid.startswith('http'):
        init_vid = download_file(init_vid, batch_output)
    else:
        if not os.path.isfile(init_vid):
            alert_msg(page, f"ERROR: Couldn't find your init_video {init_vid}")
            return
    clear_last()
    progress = ProgressBar(bar_height=8)
    prt(f"Generating your ControlNet Video...")
    #prt(progress)
    autoscroll(False)
    total_steps = controlnet_video2video_prefs['steps']
    def callback_fnc(step: int) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}"
      progress.update()
    output_file = available_file(output_path, fname, 0, ext='mp4', no_num=True)
    #output_file = os.path.join(output_path, f"{fname}{'.mp4' if is_video else '.png'}")
    cmd = f'python3 controlnetvideo.py "{init_vid}" --controlnet {controlnet_video2video_prefs["control_task"].lower()}'
    cmd += f' --prompt "{controlnet_video2video_prefs["prompt"]}"'
    if bool(controlnet_video2video_prefs["negative_prompt"]):
      cmd += f' --negative-prompt "{controlnet_video2video_prefs["negative_prompt"]}"'
    cmd += f" --prompt-strength {controlnet_video2video_prefs['prompt_strength']}"
    cmd += f" --num-inference-steps {controlnet_video2video_prefs['steps']}"
    cmd += f" --init-image-strength {controlnet_video2video_prefs['init_image_strength']}"
    cmd += f" --controlnet-strength {controlnet_video2video_prefs['controlnet_strength']}"
    cmd += f" --feedthrough-strength {controlnet_video2video_prefs['feedthrough_strength']}"
    cmd += f" --motion-alpha {controlnet_video2video_prefs['motion_alpha']} --motion-sigma {controlnet_video2video_prefs['motion_sigma']}"
    cmd += f" --color-amount {controlnet_video2video_prefs['color_amount']}"
    cmd += f" --color-fix {controlnet_video2video_prefs['color_fix'].lower()}"
    cmd += f" --start-time {controlnet_video2video_prefs['start_time']}"
    if controlnet_video2video_prefs["end_time"] != 0.0:
      cmd += f" --end-time {controlnet_video2video_prefs['end_time']}"
    if controlnet_video2video_prefs["duration"] != 0.0:
      cmd += f" --duration {controlnet_video2video_prefs['duration']}"
    if not controlnet_video2video_prefs["fix_orientation"]:
      cmd += f" --no-fix-orientation"
    cmd += f" --canny-low-thr {controlnet_video2video_prefs['low_threshold']} --canny-high-thr {controlnet_video2video_prefs['high_threshold']}"
    cmd += f" --mlsd-score-thr {controlnet_video2video_prefs['mlsd_score_thr']} --mlsd-dist-thr {controlnet_video2video_prefs['mlsd_dist_thr']}"
    cmd += f" --max-dimension {controlnet_video2video_prefs['max_dimension']} --min-dimension {controlnet_video2video_prefs['min_dimension']}"
    cmd += f" --round-dims-to {controlnet_video2video_prefs['round_dims_to']}"
    if controlnet_video2video_prefs['no_audio']: cmd += " --no-audio"
    if controlnet_video2video_prefs['skip_dumped_frames']: cmd += " --skip-dumped-frames"
    frames = os.path.join(batch_output, '{n:08d}.png') #TODO Add fname
    cmd += f" --dump-frames '{frames}'"
    cmd += f' "{output_file}"'
    w = 0
    h = 0
    img_idx = 0
    from watchdog.observers import Observer
    from watchdog.events import LoggingEventHandler, FileSystemEventHandler
    class Handler(FileSystemEventHandler):
      def __init__(self):
        super().__init__()
      def on_created(self, event):
        nonlocal img_idx, w, h
        if event.is_directory:
          return None
        elif event.event_type == 'created' and event.src_path.endswith("png"):
          autoscroll(True)
          if w == 0:
            time.sleep(0.8)
            try:
              frame = PILImage.open(event.src_path)
              w, h = frame.size
              clear_last()
            except Exception:
              pass
          clear_last()
          if controlnet_video2video_prefs['save_frames']:
            fpath = os.path.join(output_path, event.src_path.rpartition(slash)[2])
          else:
            fpath = event.src_path
          #prt(Divider(height=6, thickness=2))
          prt(Row([ImageButton(src=event.src_path, data=fpath, width=w, height=h, subtitle=f"Frame {img_idx} - {event.src_path}", center=True, page=page)], alignment=MainAxisAlignment.CENTER))
          prt(Row([Text(f'{event.src_path}')], alignment=MainAxisAlignment.CENTER))
          page.update()
          prt(progress)
          if controlnet_video2video_prefs['save_frames']:
            fpath = os.path.join(output_path, event.src_path.rpartition(slash)[2])
            shutil.copy(event.src_path, fpath)
          time.sleep(0.2)
          autoscroll(False)
          img_idx += 1
    image_handler = Handler()
    observer = Observer()
    observer.schedule(image_handler, batch_output, recursive=True)
    observer.start()
    prt(f"Running {cmd}")
    prt(progress)
    try:
        #os.system(f'cd {controlnet_video2video_dir};{cmd}')
        #if is_Colab:
        #  os.chdir(controlnet_video2video_dir)
        #  $cmd
        #  os.chdir(root_dir)
        #else:
        #TODO: Parse output to get percent current for progress callback_fnc
        run_sp(cmd, cwd=controlnet_video2video_dir, realtime=controlnet_video2video_prefs['show_console'])
    except Exception as e:
        clear_last()
        observer.stop()
        alert_msg(page, "Error running controlnetvideo.py!", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))
        return
    clear_last()
    observer.stop()
    #clear_last()
    autoscroll(True)
    #TODO: Upscale Image
    if os.path.isfile(output_file):
        prt(Row([VideoContainer(output_file)], alignment=MainAxisAlignment.CENTER))
        #prt(Row([VideoPlayer(video_file=output_file, width=width, height=height)], alignment=MainAxisAlignment.CENTER))
    else:
        prt("Error Generating Output File! Maybe NSFW Image detected?")
    prt(Row([Text(output_file)], alignment=MainAxisAlignment.CENTER))
    autoscroll(False)
    if prefs['enable_sounds']: page.snd_alert.play()


def run_deepfloyd(page, from_list=False):
    global deepfloyd_prefs, prefs, status, pipe_deepfloyd, pipe_deepfloyd2, pipe_deepfloyd3
    import torch
    #if not status['installed_diffusers']:
    #  alert_msg(page, "You need to Install HuggingFace Diffusers before using...")
    #  return
    #if not bool(deepfloyd_prefs['init_image']):
    #  alert_msg(page, "You must provide the Original Image and the Mask Image to process...")
    #  return
    if not bool(deepfloyd_prefs['prompt']):
      alert_msg(page, "You must provide a Text-to-Image Prompt...")
      return
    def prt(line, update=True):
      if type(line) == str:
        line = Text(line)
      if from_list:
        page.imageColumn.controls.append(line)
        if update:
          page.imageColumn.update()
      else:
        page.DeepFloyd.controls.append(line)
        if update:
          page.DeepFloyd.update()
    def clear_last(update=True):
      if from_list:
        if len(page.imageColumn.controls) == 0: return
        del page.imageColumn.controls[-1]
        if update: page.imageColumn.update()
      else:
        if len(page.DeepFloyd.controls) == 1: return
        del page.DeepFloyd.controls[-1]
        if update: page.DeepFloyd.update()
    def autoscroll(scroll=True):
      if from_list:
        page.imageColumn.auto_scroll = scroll
        page.imageColumn.update()
      else:
        page.DeepFloyd.auto_scroll = scroll
        page.DeepFloyd.update()
    progress = ProgressBar(bar_height=8)
    total_steps = deepfloyd_prefs['num_inference_steps']
    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep}"
      progress.update()
      #print(f'{type(latents)} {len(latents)}- {str(latents)}')
    def clear_list():
      if from_list:
        page.imageColumn.controls.clear()
      else:
        page.DeepFloyd.controls = page.DeepFloyd.controls[:1]
    deepfloyd_prompts = []
    if from_list:
      if len(prompts) < 1:
        alert_msg(page, "You need to add Prompts to your List first... ")
        return
      for p in prompts:
        df_prompt = {'prompt': p.prompt, 'negative_prompt': p['negative_prompt'], 'init_image': p['init_image'] if bool(p['init_image']) else deepfloyd_prefs['init_image'], 'mask_image': p['mask_image'] if bool(p['mask_image']) else deepfloyd_prefs['mask_image'], 'image_strength': p['init_image_strength'], 'guidance_scale':p['guidance_scale'], 'num_inference_steps': p['steps'], 'seed': p['seed']}
        deepfloyd_prompts.append(df_prompt)
      page.tabs.selected_index = 4
      page.tabs.update()
    else:
      if not bool(deepfloyd_prefs['prompt']):
        alert_msg(page, "You need to add a Text Prompt first... ")
        return
      df_prompt = {'prompt':deepfloyd_prefs['prompt'], 'negative_prompt': deepfloyd_prefs['negative_prompt'], 'init_image': deepfloyd_prefs['init_image'], 'mask_image': deepfloyd_prefs['mask_image'], 'image_strength': deepfloyd_prefs['image_strength'], 'guidance_scale':deepfloyd_prefs['guidance_scale'], 'num_inference_steps': deepfloyd_prefs['num_inference_steps'], 'seed': deepfloyd_prefs['seed']}
      deepfloyd_prompts.append(df_prompt)
    autoscroll(True)
    clear_list()
    prt(Divider(thickness=2, height=4))
    #if not status['installed_diffusers']:
    installer = Installing("Installing Diffusers & Required Packages...")
    prt(installer)
    try:
        import diffusers
        if force_updates: raise ModuleNotFoundError("Forcing update")
    except ModuleNotFoundError:
        installer.set_details("...HuggingFace Diffusers")
        #run_process("pip install --upgrade diffusers~=0.16", page=page)
        #run_process("pip install --upgrade git+https://github.com/Skquark/diffusers.git", page=page)
        run_process("pip install --upgrade git+https://github.com/Skquark/diffusers.git@main#egg=diffusers[torch]", page=page)
        pass
    try:
        import transformers
        if force_updates: raise ModuleNotFoundError("Forcing update")
    except ModuleNotFoundError:
        installer.set_details("...Transformers v4.28")
        #run_process("pip install -qq --upgrade git+https://github.com/huggingface/transformers", page=page)
        run_process("pip install --upgrade transformers~=4.28", page=page)
        pass
    try:
        import safetensors
        from safetensors import safe_open
    except ModuleNotFoundError:
        installer.set_details("...SafeTensors v0.3")
        run_process("pip install --upgrade safetensors~=0.3", page=page)
        import safetensors
        from safetensors import safe_open
        pass
    try:
        import sentencepiece
    except ImportError:
        installer.set_details("...SentencePiece")
        run_sp("pip install --upgrade sentencepiece", realtime=False) #~=0.1
        import sentencepiece
        pass
    try:
        import accelerate
        #TODO: Uninstall other version first
    except ImportError:
        installer.set_details("...Accelerate v0.18")
        run_process("pip install --upgrade accelerate~=0.18.0", page=page)
        pass
    installer.set_message("Installing DeepFloyd IF Required Packages...")
    if deepfloyd_prefs['low_memory']:
        #clear_last(update=False)
        #prt(Installing("Installing DeepFloyd IF Required Packages..."))
        try:
          import bitsandbytes
        except ImportError:
          installer.set_details("...BitsandBytes")
          os.environ['LD_LIBRARY_PATH'] += "/usr/lib/wsl/lib:$LD_LIBRARY_PATH"
          #run_sp("export LD_LIBRARY_PATH=/usr/lib/wsl/lib:$LD_LIBRARY_PATH", realtime=False)
          if sys.platform.startswith("win"):
              run_sp("pip install bitsandbytes-windows", realtime=False)
          else:
              run_sp("pip install --upgrade bitsandbytes", realtime=False) #~=0.38
          import bitsandbytes
          pass
    try:
        import torch
    except ModuleNotFoundError:
        installer.set_details("...Torch v2.0")
        run_process("pip install --upgrade torch~=2.0", page=page)
        import torch
        pass
    try:
        from huggingface_hub import notebook_login, HfFolder, login
    except ModuleNotFoundError:
        installer.set_details("...HuggingFace Hub")
        run_process("pip install huggingface_hub --upgrade", page=page)
        import torch
        pass
    if not os.path.exists(HfFolder.path_token):
        try:
          login(token=prefs['HuggingFace_api_key'], add_to_git_credential=True)
        except Exception:
          alert_msg(page, "ERROR Logging into HuggingFace... Check your API Key or Internet conenction.")
          return

    import requests, random
    from io import BytesIO
    from PIL import ImageOps
    from PIL.PngImagePlugin import PngInfo
    from diffusers import DiffusionPipeline
    from diffusers.utils import pt_to_pil
    from diffusers import IFPipeline, IFImg2ImgPipeline, IFInpaintingPipeline, IFSuperResolutionPipeline
    #from diffusers.pipelines.deepfloyd_if.safety_checker IFSafetyChecker
    from transformers import T5EncoderModel, T5Tokenizer
    #run_sp("accelerate config default", realtime=False)
    clear_pipes('deepfloyd')
    torch.cuda.empty_cache()
    #torch.cuda.reset_max_memory_allocated()
    #torch.cuda.reset_peak_memory_stats()
    if deepfloyd_prefs['model_size'].startswith("X"):
        model_id = "DeepFloyd/IF-I-XL-v1.0"
        model_id_II = "DeepFloyd/IF-II-XL-v1.0"
    elif deepfloyd_prefs['model_size'].startswith("L"):
        model_id = "DeepFloyd/IF-I-L-v1.0"
        model_id_II = "DeepFloyd/IF-II-L-v1.0"
    elif deepfloyd_prefs['model_size'].startswith("M"):
        model_id = "DeepFloyd/IF-I-M-v1.0"
        model_id_II = "DeepFloyd/IF-II-M-v1.0"
    clear_last(update=False)
    if 'last_deepfloyd_mode' not in status:
      status['last_deepfloyd_mode'] = ""
    max_size = deepfloyd_prefs['max_size']
    batch_output = os.path.join(stable_dir, deepfloyd_prefs['batch_folder_name'])
    output_dir = batch_output
    if not os.path.isdir(batch_output):
      os.makedirs(batch_output)
    batch_output = os.path.join(prefs['image_output'], deepfloyd_prefs['batch_folder_name'])
    if not os.path.isdir(batch_output):
      os.makedirs(batch_output)

    for pr in deepfloyd_prompts:
        init_img = None
        mask_img = None
        if bool(pr['init_image']):
            if pr['init_image'].startswith('http'):
                init_img = PILImage.open(requests.get(pr['init_image'], stream=True).raw)
            else:
                if os.path.isfile(pr['init_image']):
                    init_img = PILImage.open(pr['init_image'])
                else:
                    alert_msg(page, f"ERROR: Couldn't find your init_image {pr['init_image']}")
                    return
            width, height = init_img.size
            width, height = scale_dimensions(width, height, deepfloyd_prefs['max_size'])
            if bool(deepfloyd_prefs['alpha_mask']):
                init_img = init_img.convert("RGBA")
            else:
                init_img = init_img.convert("RGB")
            init_img = init_img.resize((width, height), resample=PILImage.Resampling.LANCZOS)
            if not bool(pr['mask_image']) and bool(deepfloyd_prefs['alpha_mask']):
                mask_img = init_img.convert('RGBA')
                red, green, blue, alpha = PILImage.Image.split(init_img)
                mask_img = alpha.convert('L')
            elif bool(pr['mask_image']):
                if pr['mask_image'].startswith('http'):
                    #response = requests.get(deepfloyd_prefs['init_image'])
                    #init_img = PILImage.open(BytesIO(response.content)).convert("RGB")
                    mask_img = PILImage.open(requests.get(pr['mask_image'], stream=True).raw)
                else:
                    if os.path.isfile(pr['mask_image']):
                        mask_img = PILImage.open(pr['mask_image'])
                    else:
                        alert_msg(page, f"ERROR: Couldn't find your mask_image {pr['mask_image']}")
                        return
                width, height = mask_img.size
                width, height = scale_dimensions(width, height, deepfloyd_prefs['max_size'])
                mask_img = mask_img.resize((width, height), resample=PILImage.Resampling.LANCZOS)
        if deepfloyd_prefs['invert_mask'] and not deepfloyd_prefs['alpha_mask']:
            from PIL import ImageOps
            mask_img = ImageOps.invert(mask_img.convert('RGB'))
        for num in range(deepfloyd_prefs['num_images']):
            random_seed = (int(pr['seed']) + num) if int(pr['seed']) > 0 else rnd.randint(0,4294967295)
            generator = torch.Generator().manual_seed(random_seed)
            try:
                installer = Installing("Running DeepFloyd-IF Text Encoder...")
                prt(installer)
                if deepfloyd_prefs['low_memory']:
                    #, load_in_8bit=True
                    installer.set_details("...text_encoder T5EncoderModel") #, variant="8bit"
                    text_encoder = T5EncoderModel.from_pretrained(model_id, subfolder="text_encoder", device_map="auto", load_in_8bit=True, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
                    installer.set_details("...DiffusionPipeline")
                    pipe_deepfloyd = DiffusionPipeline.from_pretrained(model_id, text_encoder=text_encoder, unet=None, use_safetensors=True, device_map=None, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
                    # Still getting errors here! WTF?
                    #images = pipe_deepfloyd(pr['prompt'], image=init_img, negative_prompt=pr['negative_prompt'] if bool(pr['negative_prompt']) else None, num_inference_steps=deepfloyd_prefs['num_inference_steps'], eta=deepfloyd_prefs['eta'], image_guidance_scale=deepfloyd_prefs['guidance_scale'], num_images_per_prompt=deepfloyd_prefs['num_images'], generator=generator, callback=callback_fnc, callback_steps=1).images
                    installer.set_details("...encode_prompts")
                    prompt_embeds, negative_embeds = pipe_deepfloyd.encode_prompt(pr['prompt'], negative_prompt=pr['negative_prompt'] if bool(pr['negative_prompt']) else None)
                    del text_encoder
                    del pipe_deepfloyd
                    pipe_deepfloyd = None
                installer.set_details("...clearing pipes")
                flush()
                clear_last(update=False)
                safety_modules = {}
                if init_img == None:
                    prt(Installing("Stage 1: Installing DeepFloyd-IF Pipeline..."))
                    # if prefs['disable_nsfw_filter'] else IFSafetyChecker.from_pretrained("CompVis/stable-diffusion-safety-checker").to(torch_device)
                    total_steps = pr['num_inference_steps']
                    #clear_last()
                    if deepfloyd_prefs['low_memory']:
                        pipe_deepfloyd = IFPipeline.from_pretrained(model_id, text_encoder=None, device_map="sequential", use_safetensors=True, variant="fp16", torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, **safety)
                        #pipe_deepfloyd.enable_model_cpu_offload()
                        #pipe_deepfloyd.unet.to(memory_format=torch.channels_last)
                        #pipe_deepfloyd.unet = torch.compile(pipe_deepfloyd.unet, mode="reduce-overhead", fullgraph=True)
                    else:
                        if not (deepfloyd_prefs['keep_pipelines'] and pipe_deepfloyd != None and status['last_deepfloyd_mode'] != "text2image"):
                            #install.set_details("...DiffusionPipeline")
                            pipe_deepfloyd = DiffusionPipeline.from_pretrained(model_id, variant="fp16", torch_dtype=torch.float16, use_safetensors=True, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, **safety)
                            pipe_deepfloyd.to(torch_device)
                            #pipe_deepfloyd.enable_model_cpu_offload()
                            if prefs['enable_torch_compile']:
                                pipe_deepfloyd.unet.to(memory_format=torch.channels_last)
                                pipe_deepfloyd.unet = torch.compile(pipe_deepfloyd.unet, mode="reduce-overhead", fullgraph=True)
                        #install.set_details("...encode_prompts")
                        prompt_embeds, negative_embeds = pipe_deepfloyd.encode_prompt(pr['prompt'], negative_prompt=pr['negative_prompt'] if bool(pr['negative_prompt']) else None)
                    clear_last()
                    prt(progress)
                    images = pipe_deepfloyd(
                        prompt_embeds=prompt_embeds,
                        negative_prompt_embeds=negative_embeds,
                        num_inference_steps = pr['num_inference_steps'],
                        guidance_scale = pr['guidance_scale'],
                        output_type="pt",
                        generator=generator,
                        callback=callback_fnc, callback_steps=1,
                    ).images
                    safety_modules = {
                        "feature_extractor": pipe_deepfloyd.feature_extractor,
                        "safety_checker": pipe_deepfloyd.safety_checker,
                        "watermarker": pipe_deepfloyd.watermarker,
                    }
                    if not deepfloyd_prefs['keep_pipelines']:
                        del pipe_deepfloyd
                        flush()
                        pipe_deepfloyd = None
                    total_steps = deepfloyd_prefs['superres_num_inference_steps']
                    clear_last()
                    prt(Installing("Stage 2: Installing DeepFloyd Super Resolution Pipeline..."))
                    #IFSuperResolutionPipeline
                    if pipe_deepfloyd2 == None or status['last_deepfloyd_mode'] != "text2image":
                        pipe_deepfloyd2 = DiffusionPipeline.from_pretrained(model_id_II, text_encoder=None, variant="fp16", use_safetensors=True, torch_dtype=torch.float16)
                        if not deepfloyd_prefs['low_memory']:
                            pipe_deepfloyd2.enable_model_cpu_offload()
                    clear_last()
                    prt(progress)
                    images = pipe_deepfloyd2(
                        image=images,
                        prompt_embeds=prompt_embeds,
                        negative_prompt_embeds=negative_embeds,
                        num_inference_steps = deepfloyd_prefs['superres_num_inference_steps'],
                        guidance_scale = deepfloyd_prefs['superres_guidance_scale'],
                        output_type="pt",
                        generator=generator,
                        callback=callback_fnc, callback_steps=1,
                    ).images
                    status['last_deepfloyd_mode'] = "text2image"
                elif init_img != None and mask_img == None:
                    prt(Installing("Stage 1: Installing DeepFloyd-IF Image2Image Pipeline..."))
                    if deepfloyd_prefs['low_memory']:
                        pipe_deepfloyd = IFImg2ImgPipeline.from_pretrained(model_id, variant="fp16", torch_dtype=torch.float16, use_safetensors=True, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, **safety)
                        #pipe_deepfloyd.enable_model_cpu_offload()
                    else:
                        if not (deepfloyd_prefs['keep_pipelines'] and pipe_deepfloyd != None and status['last_deepfloyd_mode'] != "image2image"):
                            #install.set_details("...DiffusionPipeline")
                            pipe_deepfloyd = IFImg2ImgPipeline.from_pretrained(model_id, variant="fp16", torch_dtype=torch.float16, use_safetensors=True, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, **safety)
                            pipe_deepfloyd.to(torch_device)
                            #pipe_deepfloyd.enable_model_cpu_offload()
                            if prefs['enable_torch_compile']:
                                pipe_deepfloyd.unet.to(memory_format=torch.channels_last)
                                pipe_deepfloyd.unet = torch.compile(pipe_deepfloyd.unet, mode="reduce-overhead", fullgraph=True)
                        #install.set_details("...encode_prompts")
                        prompt_embeds, negative_embeds = pipe_deepfloyd.encode_prompt(pr['prompt'], negative_prompt=pr['negative_prompt'] if bool(pr['negative_prompt']) else None)
                    total_steps = pr['num_inference_steps']
                    clear_last()
                    prt(progress)
                    images = pipe_deepfloyd(
                        image=init_img,
                        strength=pr['image_strength'],
                        prompt_embeds=prompt_embeds,
                        negative_prompt_embeds=negative_embeds,
                        num_inference_steps = pr['num_inference_steps'],
                        guidance_scale = pr['guidance_scale'],
                        output_type="pt",
                        generator=generator,
                        callback=callback_fnc, callback_steps=1,
                    ).images
                    safety_modules = {
                        "feature_extractor": pipe_deepfloyd.feature_extractor,
                        "safety_checker": pipe_deepfloyd.safety_checker,
                        "watermarker": pipe_deepfloyd.watermarker,
                    }
                    if not deepfloyd_prefs['keep_pipelines']:
                        del pipe_deepfloyd
                        flush()
                        pipe_deepfloyd = None
                    total_steps = deepfloyd_prefs['superres_num_inference_steps']
                    clear_last()
                    prt(Installing("Stage 2: Installing DeepFloyd Img2Img Super Resolution Pipeline..."))
                    from diffusers import IFImg2ImgSuperResolutionPipeline
                    if pipe_deepfloyd2 == None or status['last_deepfloyd_mode'] != "image2image":
                        pipe_deepfloyd2 = IFImg2ImgSuperResolutionPipeline.from_pretrained("DeepFloyd/IF-II-L-v1.0", text_encoder=None, variant="fp16", use_safetensors=True, torch_dtype=torch.float16, device_map="auto")
                        if not deepfloyd_prefs['low_memory']:
                            pipe_deepfloyd2.enable_model_cpu_offload()
                    clear_last()
                    prt(progress)
                    images = pipe_deepfloyd2(
                        image=images,
                        origional_image=init_img,
                        strength=pr['image_strength'],
                        prompt_embeds=prompt_embeds,
                        negative_prompt_embeds=negative_embeds,
                        num_inference_steps = deepfloyd_prefs['superres_num_inference_steps'],
                        guidance_scale = deepfloyd_prefs['superres_guidance_scale'],
                        output_type="pt",
                        generator=generator,
                        callback=callback_fnc, callback_steps=1,
                    ).images
                    status['last_deepfloyd_mode'] = "image2image"
                elif init_img != None and mask_img != None:
                    prt(Installing("Stage 1: Installing DeepFloyd-IF Inpainting Pipeline..."))
                    # if prefs['disable_nsfw_filter'] else IFSafetyChecker.from_pretrained("CompVis/stable-diffusion-safety-checker").to(torch_device)
                    if deepfloyd_prefs['low_memory']:
                        pipe_deepfloyd = IFInpaintingPipeline.from_pretrained(model_id, variant="fp16", torch_dtype=torch.float16, use_safetensors=True, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, **safety)
                        pipe_deepfloyd.enable_model_cpu_offload()
                    else:
                        if not (deepfloyd_prefs['keep_pipelines'] and pipe_deepfloyd != None and status['last_deepfloyd_mode'] != "inpainting"):
                            #install.set_details("...DiffusionPipeline")
                            pipe_deepfloyd = IFInpaintingPipeline.from_pretrained(model_id, variant="fp16", torch_dtype=torch.float16, use_safetensors=True, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None, **safety)
                            pipe_deepfloyd.to(torch_device)
                            #pipe_deepfloyd.enable_model_cpu_offload()
                            if prefs['enable_torch_compile']:
                                pipe_deepfloyd.unet.to(memory_format=torch.channels_last)
                                pipe_deepfloyd.unet = torch.compile(pipe_deepfloyd.unet, mode="reduce-overhead", fullgraph=True)
                        #install.set_details("...encode_prompts")
                        prompt_embeds, negative_embeds = pipe_deepfloyd.encode_prompt(pr['prompt'], negative_prompt=pr['negative_prompt'] if bool(pr['negative_prompt']) else None)
                    total_steps = pr['num_inference_steps']
                    clear_last()
                    prt(progress)
                    images = pipe_deepfloyd(
                        image=init_img,
                        mask_image=mask_img,
                        strength=pr['image_strength'],
                        prompt_embeds=prompt_embeds,
                        negative_prompt_embeds=negative_embeds,
                        num_inference_steps = pr['num_inference_steps'],
                        guidance_scale = pr['guidance_scale'],
                        output_type="pt",
                        generator=generator,
                        callback=callback_fnc, callback_steps=1,
                    ).images
                    safety_modules = {
                        "feature_extractor": pipe_deepfloyd.feature_extractor,
                        "safety_checker": pipe_deepfloyd.safety_checker,
                        "watermarker": pipe_deepfloyd.watermarker,
                    }
                    if not deepfloyd_prefs['keep_pipelines']:
                        del pipe_deepfloyd
                        flush()
                        pipe_deepfloyd = None
                    clear_last()
                    prt(Installing("Stage 2: Installing DeepFloyd Inpainting Super Resolution Pipeline..."))
                    from diffusers import IFInpaintingSuperResolutionPipeline
                    if pipe_deepfloyd2 == None or status['last_deepfloyd_mode'] != "inpainting":
                        pipe_deepfloyd2 = IFInpaintingSuperResolutionPipeline.from_pretrained("DeepFloyd/IF-II-L-v1.0", text_encoder=None, use_safetensors=True, variant="fp16", torch_dtype=torch.float16, device_map=None)
                        if not deepfloyd_prefs['low_memory']:
                            pipe_deepfloyd2.enable_model_cpu_offload()
                    total_steps = deepfloyd_prefs['superres_num_inference_steps']
                    clear_last()
                    prt(progress)
                    images = pipe_deepfloyd2(
                        image=images,
                        origional_image=init_img,
                        mask_image=mask_img,
                        strength=pr['image_strength'],
                        prompt_embeds=prompt_embeds,
                        negative_prompt_embeds=negative_embeds,
                        num_inference_steps = deepfloyd_prefs['superres_num_inference_steps'],
                        guidance_scale = deepfloyd_prefs['superres_guidance_scale'],
                        output_type="pt",
                        generator=generator,
                        callback=callback_fnc, callback_steps=1,
                    ).images
                    status['last_deepfloyd_mode'] = "inpainting"
                if not deepfloyd_prefs['keep_pipelines']:
                    del pipe_deepfloyd2
                    flush()
                    pipe_deepfloyd2 = None
                prt(Installing("Stage 3: Installing Stable Diffusion X4 Upscaler Pipeline..."))
                if pipe_deepfloyd3 == None:
                    pipe_deepfloyd3 = DiffusionPipeline.from_pretrained("stabilityai/stable-diffusion-x4-upscaler", **safety_modules, use_safetensors=True, torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
                    pipe_deepfloyd3.enable_model_cpu_offload()
                total_steps = deepfloyd_prefs['upscale_num_inference_steps']
                clear_last()
                prt(progress)
                images = pipe_deepfloyd3(prompt=pr['prompt'], negative_prompt=pr['negative_prompt'] if bool(pr['negative_prompt']) else None, image=images, noise_level=100, num_inference_steps=deepfloyd_prefs['upscale_num_inference_steps'], guidance_scale=deepfloyd_prefs['upscale_guidance_scale'], generator=generator, callback=callback_fnc, callback_steps=1).images
                if deepfloyd_prefs['apply_watermark']:
                    from diffusers.pipelines.deepfloyd_if import IFWatermarker
                    watermarker = IFWatermarker.from_pretrained(model_id, subfolder="watermarker")
                    watermarker.apply_watermark(images, pipe_deepfloyd3.unet.config.sample_size)
                    del watermarker
                if not deepfloyd_prefs['keep_pipelines']:
                    del pipe_deepfloyd3
                    flush()
                    pipe_deepfloyd3 = None

            except EnvironmentError as e:
                clear_last()
                alert_msg(page, f"ERROR: You must accept the license on the DeepFloyd model card first.", content=Text(str(e)))
                #del pipe_deepfloyd
                flush()
                return
            except Exception as e:
                clear_last()
                alert_msg(page, f"ERROR: Couldn't run IF-DeepFloyd on your image for some reason.  Possibly out of memory or something wrong with my code...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
                clear_pipes()
                return

            #clear_last()
            clear_last()
            filename = f"{deepfloyd_prefs['file_prefix']}{format_filename(pr['prompt'])}"
            #if prefs['file_suffix_seed']: fname += f"-{random_seed}"
            #num = 0
            for image in images:
                random_seed += num
                fname = filename + (f"-{random_seed}" if prefs['file_suffix_seed'] else "")
                image_path = available_file(os.path.join(stable_dir, deepfloyd_prefs['batch_folder_name']), fname, num)
                unscaled_path = image_path
                output_file = image_path.rpartition(slash)[2]
                image = pt_to_pil(image)
                image.save(image_path)
                out_path = image_path.rpartition(slash)[0]
                upscaled_path = os.path.join(out_path, output_file)
                if not deepfloyd_prefs['display_upscaled_image'] or not deepfloyd_prefs['apply_ESRGAN_upscale']:
                    prt(Row([ImageButton(src=unscaled_path, data=upscaled_path, width=width, height=height, page=page)], alignment=MainAxisAlignment.CENTER))
                    #prt(Row([Img(src=unscaled_path, fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
                if deepfloyd_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
                    #w = int(arg['width'] * prefs["enlarge_scale"])
                    #h = int(arg['height'] * prefs["enlarge_scale"])
                    #prt(Row([Text(f'Enlarging {prefs["enlarge_scale"]}X to {w}x{h}')], alignment=MainAxisAlignment.CENTER))
                    prt(Row([Text(f'Enlarging Real-ESRGAN {prefs["enlarge_scale"]}X')], alignment=MainAxisAlignment.CENTER))
                    os.chdir(os.path.join(dist_dir, 'Real-ESRGAN'))
                    upload_folder = 'upload'
                    result_folder = 'results'
                    if os.path.isdir(upload_folder):
                        shutil.rmtree(upload_folder)
                    if os.path.isdir(result_folder):
                        shutil.rmtree(result_folder)
                    os.mkdir(upload_folder)
                    os.mkdir(result_folder)
                    short_name = f'{fname[:80]}-{num}.png'
                    dst_path = os.path.join(dist_dir, 'Real-ESRGAN', upload_folder, short_name)
                    #shutil.move(fpath, dst_path)
                    shutil.copy(image_path, dst_path)
                    #faceenhance = ' --face_enhance' if deepfloyd_prefs["face_enhance"] else ''
                    faceenhance = ''
                    run_sp(f'python inference_realesrgan.py -n realesr-general-x4v3 -i upload --outscale {deepfloyd_prefs["enlarge_scale"]}{faceenhance}', cwd=os.path.join(dist_dir, 'Real-ESRGAN'), realtime=False)
                    out_file = short_name.rpartition('.')[0] + '_out.png'
                    shutil.move(os.path.join(dist_dir, 'Real-ESRGAN', result_folder, out_file), upscaled_path)
                    image_path = upscaled_path
                    os.chdir(stable_dir)
                    clear_last()
                    if deepfloyd_prefs['display_upscaled_image']:
                        time.sleep(0.6)
                        prt(Row([ImageButton(src=upscaled_path, data=upscaled_path, width=width * float(deepfloyd_prefs["enlarge_scale"]), height=height * float(deepfloyd_prefs["enlarge_scale"]), page=page)], alignment=MainAxisAlignment.CENTER))
                        #prt(Row([Img(src=upscaled_path, fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
                else:
                    time.sleep(0.8)
                if prefs['save_image_metadata']:
                    img = PILImage.open(image_path)
                    metadata = PngInfo()
                    metadata.add_text("artist", prefs['meta_ArtistName'])
                    metadata.add_text("copyright", prefs['meta_Copyright'])
                    metadata.add_text("software", "Stable Diffusion Deluxe" + f", upscaled {deepfloyd_prefs['enlarge_scale']}x with ESRGAN" if deepfloyd_prefs['apply_ESRGAN_upscale'] else "")
                    metadata.add_text("pipeline", "DeepFloyd-IF") #TODO: Img2Img or Inpainting
                    if prefs['save_config_in_metadata']:
                        config_json = deepfloyd_prefs.copy()
                        config_json['model_path'] = model_id
                        config_json['seed'] = random_seed
                        config_json['prompt'] = pr['prompt']
                        config_json['negative_prompt'] = pr['negative_prompt']
                        del config_json['num_images']
                        del config_json['max_size']
                        del config_json['display_upscaled_image']
                        del config_json['batch_folder_name']
                        if not config_json['apply_ESRGAN_upscale']:
                            del config_json['enlarge_scale']
                            del config_json['apply_ESRGAN_upscale']
                        metadata.add_text("config_json", json.dumps(config_json, ensure_ascii=True, indent=4))
                    img.save(image_path, pnginfo=metadata)
                #TODO: PyDrive
                if storage_type == "Colab Google Drive":
                    new_file = available_file(os.path.join(prefs['image_output'], deepfloyd_prefs['batch_folder_name']), fname, num)
                    out_path = new_file
                    shutil.copy(image_path, new_file)
                elif bool(prefs['image_output']):
                    new_file = available_file(os.path.join(prefs['image_output'], deepfloyd_prefs['batch_folder_name']), fname, num)
                    out_path = new_file
                    shutil.copy(image_path, new_file)
                prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))
                #num += 1
    if not deepfloyd_prefs['keep_pipelines']:
        clear_pipes()
    if prefs['enable_sounds']: page.snd_alert.play()


def run_text_to_video(page):
    global text_to_video_prefs, prefs, status, pipe_text_to_video, model_path
    if not status['installed_diffusers']:
      alert_msg(page, "You need to Install HuggingFace Diffusers before using...")
      return
    def prt(line):
      if type(line) == str:
        line = Text(line, size=17)
      page.TextToVideo.controls.append(line)
      page.TextToVideo.update()
    def clear_last():
      del page.TextToVideo.controls[-1]
      page.TextToVideo.update()
    def clear_list():
      page.TextToVideo.controls = page.TextToVideo.controls[:1]
    def autoscroll(scroll=True):
      page.TextToVideo.auto_scroll = scroll
      page.TextToVideo.update()
    progress = ProgressBar(bar_height=8)
    total_steps = text_to_video_prefs['num_inference_steps']
    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep}"
      progress.update()
      #print(f'{type(latents)} {len(latents)}- {str(latents)}')
    clear_list()
    autoscroll(True)
    prt(Installing("Installing Text-To-Video Pipeline..."))
    #), dropdown.Option(), dropdown.Option(), dropdown.Option(), dropdown.Option(
    if text_to_video_prefs['model'] == "damo-vilab/text-to-video-ms-1.7b":
        model_id = "damo-vilab/text-to-video-ms-1.7b"
    elif text_to_video_prefs['model'] == "modelscope-damo-text2video-synthesis":
        model_id = "damo-vilab/modelscope-damo-text2video-synthesis"
    elif text_to_video_prefs['model'] == "modelscope-damo-text2video-pruned-weights":
        model_id = "kabachuha/modelscope-damo-text2video-pruned-weights"
    elif text_to_video_prefs['model'] == "cerspense/zeroscope_v2_576w":
        model_id = "cerspense/zeroscope_v2_576w"
    elif text_to_video_prefs['model'] == "cerspense/zeroscope_v2_XL":
        model_id = "cerspense/zeroscope_v2_XL"
    clear_pipes()
    #clear_pipes('text_to_video')
    if pipe_text_to_video is None:
        from diffusers import TextToVideoSDPipeline, DPMSolverMultistepScheduler
        pipe_text_to_video = TextToVideoSDPipeline.from_pretrained(model_id, torch_dtype=torch.float16, variant="fp16", cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
        #pipe_text_to_video = pipeline_scheduler(pipe_text_to_video)
        pipe_text_to_video.scheduler = DPMSolverMultistepScheduler.from_config(pipe_text_to_video.scheduler.config)
        if text_to_video_prefs['lower_memory']:
            pipe_text_to_video.enable_sequential_cpu_offload()
            #pipe_text_to_video.enable_model_cpu_offload()
            pipe_text_to_video.unet.enable_forward_chunking(chunk_size=1, dim=1)
            pipe_text_to_video.enable_vae_tiling()
            pipe_text_to_video.enable_vae_slicing()
        else:
            pipe_text_to_video = pipe_text_to_video.to(torch_device)
        #pipe_text_to_video = optimize_pipe(pipe_text_to_video, vae_tiling=True, vae=True, to_gpu=False)
        pipe_text_to_video.set_progress_bar_config(disable=True)
    else:
        pipe_text_to_video = pipeline_scheduler(pipe_text_to_video)
    clear_last()
    prt("Generating Text-To-Video of your Prompt...")
    prt(progress)
    autoscroll(False)
    batch_output = os.path.join(stable_dir, text_to_video_prefs['batch_folder_name'])
    if not os.path.isdir(batch_output):
      os.makedirs(batch_output)
    local_output = batch_output
    batch_output = os.path.join(prefs['image_output'], text_to_video_prefs['batch_folder_name'])
    if not os.path.isdir(batch_output):
      os.makedirs(batch_output)
    random_seed = int(text_to_video_prefs['seed']) if int(text_to_video_prefs['seed']) > 0 else rnd.randint(0,4294967295)
    generator = torch.Generator(device="cpu").manual_seed(random_seed)
    #generator = torch.manual_seed(random_seed)
    width = text_to_video_prefs['width']
    height = text_to_video_prefs['height']
    try:
      #print(f"prompt={text_to_video_prefs['prompt']}, negative_prompt={text_to_video_prefs['negative_prompt']}, editing_prompt={editing_prompt}, edit_warmup_steps={edit_warmup_steps}, edit_guidance_scale={edit_guidance_scale}, edit_threshold={edit_threshold}, edit_weights={edit_weights}, reverse_editing_direction={reverse_editing_direction}, edit_momentum_scale={text_to_video_prefs['edit_momentum_scale']}, edit_mom_beta={text_to_video_prefs['edit_mom_beta']}, num_inference_steps={text_to_video_prefs['num_inference_steps']}, eta={text_to_video_prefs['eta']}, guidance_scale={text_to_video_prefs['guidance_scale']}")
      #, output_type = "pt", width=width, height=height
      frames = pipe_text_to_video(prompt=text_to_video_prefs['prompt'], negative_prompt=text_to_video_prefs['negative_prompt'], num_frames=text_to_video_prefs['num_frames'], num_inference_steps=text_to_video_prefs['num_inference_steps'], eta=text_to_video_prefs['eta'], guidance_scale=text_to_video_prefs['guidance_scale'], generator=generator, callback=callback_fnc, callback_steps=1).frames
    except Exception as e:
      clear_last()
      clear_last()
      alert_msg(page, f"ERROR: Couldn't Text-To-Video your image for some reason. Possibly out of memory or something wrong with my code...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
      return
    clear_last()
    clear_last()
    autoscroll(True)
    save_path = os.path.join(prefs['image_output'], text_to_video_prefs['batch_folder_name'])
    filename = f"{prefs['file_prefix']}{format_filename(text_to_video_prefs['prompt'])}"
    filename = filename[:int(prefs['file_max_length'])]
    #if prefs['file_suffix_seed']: filename += f"-{random_seed}"
    autoscroll(True)
    video_path = ""
    if text_to_video_prefs['export_to_video']:
        from diffusers.utils import export_to_video
        video_path = export_to_video(frames)
        shutil.copy(video_path, available_file(local_output, filename, 0, ext="mp4", no_num=True))
        shutil.copy(video_path, available_file(batch_output, filename, 0, ext="mp4", no_num=True))
        #print(f"video_path: {video_path}")
    #video = frames.cpu().numpy()
    #print(f"video: {video}")
    import cv2
    from PIL.PngImagePlugin import PngInfo
    num = 0
    for image in frames:
        random_seed += num
        fname = filename + (f"-{random_seed}" if prefs['file_suffix_seed'] else "")
        image_path = available_file(batch_output, fname, num)
        unscaled_path = image_path
        output_file = image_path.rpartition(slash)[2]
        #uint8_image = (image * 255).round().astype("uint8")
        #np_image = image.cpu().numpy()
        #print(f"image: {type(image)}, np_image: {type(np_image)}")
        #print(f"image: {type(image)} to {image_path}")
        cv2.imwrite(image_path, image)
        #PILImage.fromarray(np_image).save(image_path)
        out_path = image_path.rpartition(slash)[0]
        upscaled_path = os.path.join(out_path, output_file)
        if not text_to_video_prefs['display_upscaled_image'] or not text_to_video_prefs['apply_ESRGAN_upscale']:
            prt(Row([ImageButton(src=unscaled_path, data=upscaled_path, width=width, height=height, page=page)], alignment=MainAxisAlignment.CENTER))
        if text_to_video_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
            os.chdir(os.path.join(dist_dir, 'Real-ESRGAN'))
            upload_folder = 'upload'
            result_folder = 'results'
            if os.path.isdir(upload_folder):
                shutil.rmtree(upload_folder)
            if os.path.isdir(result_folder):
                shutil.rmtree(result_folder)
            os.mkdir(upload_folder)
            os.mkdir(result_folder)
            short_name = f'{fname[:80]}-{num}.png'
            dst_path = os.path.join(dist_dir, 'Real-ESRGAN', upload_folder, short_name)
            #print(f'Moving {fpath} to {dst_path}')
            #shutil.move(fpath, dst_path)
            shutil.copy(image_path, dst_path)
            #faceenhance = ' --face_enhance' if text_to_video_prefs["face_enhance"] else ''
            faceenhance = ''
            run_sp(f'python inference_realesrgan.py -n realesr-general-x4v3 -i upload --outscale {text_to_video_prefs["enlarge_scale"]}{faceenhance}', cwd=os.path.join(dist_dir, 'Real-ESRGAN'), realtime=False)
            out_file = short_name.rpartition('.')[0] + '_out.png'
            shutil.move(os.path.join(dist_dir, 'Real-ESRGAN', result_folder, out_file), upscaled_path)
            image_path = upscaled_path
            os.chdir(stable_dir)
            if text_to_video_prefs['display_upscaled_image']:
                time.sleep(0.6)
                prt(Row([ImageButton(src=upscaled_path, data=upscaled_path, width=width * float(text_to_video_prefs["enlarge_scale"]), height=height * float(text_to_video_prefs["enlarge_scale"]), page=page)], alignment=MainAxisAlignment.CENTER))
                #prt(Row([Img(src=upscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
        if prefs['save_image_metadata']:
            img = PILImage.open(image_path)
            metadata = PngInfo()
            metadata.add_text("artist", prefs['meta_ArtistName'])
            metadata.add_text("copyright", prefs['meta_Copyright'])
            metadata.add_text("software", "Stable Diffusion Deluxe" + f", upscaled {text_to_video_prefs['enlarge_scale']}x with ESRGAN" if text_to_video_prefs['apply_ESRGAN_upscale'] else "")
            metadata.add_text("pipeline", "Text-To-Video")
            if prefs['save_config_in_metadata']:
              config_json = text_to_video_prefs.copy()
              config_json['model_path'] = model_id
              config_json['scheduler_mode'] = prefs['scheduler_mode']
              config_json['seed'] = random_seed
              del config_json['num_frames']
              del config_json['width']
              del config_json['height']
              del config_json['display_upscaled_image']
              del config_json['batch_folder_name']
              del config_json['lower_memory']
              if not config_json['apply_ESRGAN_upscale']:
                del config_json['enlarge_scale']
                del config_json['apply_ESRGAN_upscale']
              metadata.add_text("config_json", json.dumps(config_json, ensure_ascii=True, indent=4))
            img.save(image_path, pnginfo=metadata)
        #TODO: PyDrive
        if storage_type == "Colab Google Drive":
            new_file = available_file(os.path.join(prefs['image_output'], text_to_video_prefs['batch_folder_name']), fname, num)
            out_path = new_file
            shutil.copy(image_path, new_file)
        elif bool(prefs['image_output']):
            new_file = available_file(os.path.join(prefs['image_output'], text_to_video_prefs['batch_folder_name']), fname, num)
            out_path = new_file
            shutil.copy(image_path, new_file)
        prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))
        num += 1
    if bool(video_path):
        prt(Row([VideoContainer(video_path)], alignment=MainAxisAlignment.CENTER))
    autoscroll(False)
    if prefs['enable_sounds']: page.snd_alert.play()

def run_text_to_video_zero(page):
    global text_to_video_zero_prefs, prefs, status, pipe_text_to_video_zero, model_path
    if not status['installed_diffusers']:
      alert_msg(page, "You need to Install HuggingFace Diffusers before using...")
      return
    def prt(line):
      if type(line) == str:
        line = Text(line, size=17)
      page.TextToVideo.controls.append(line)
      page.TextToVideo.update()
    def clear_last():
      del page.TextToVideo.controls[-1]
      page.TextToVideo.update()
    def clear_list():
      page.TextToVideo.controls = page.TextToVideo.controls[:1]
    def autoscroll(scroll=True):
      page.TextToVideo.auto_scroll = scroll
      page.TextToVideo.update()
      page.TextToVideo.auto_scroll = scroll
      page.TextToVideo.update()
    progress = ProgressBar(bar_height=8)
    total_steps = text_to_video_zero_prefs['num_inference_steps']
    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep}"
      progress.update()
      #print(f'{type(latents)} {len(latents)}- {str(latents)}')
    clear_list()
    autoscroll(True)
    prt(Installing("Installing Text-To-Video Zero Pipeline..."))
    import cv2
    #model_id = "damo-vilab/text-to-video-ms-1.7b"
    clear_pipes()
    #clear_pipes('text_to_video_zero')
    torch.cuda.empty_cache()
    torch.cuda.reset_max_memory_allocated()
    torch.cuda.reset_peak_memory_stats()
    if pipe_text_to_video_zero is None:
        from diffusers import TextToVideoZeroPipeline, DPMSolverMultistepScheduler
        pipe_text_to_video_zero = TextToVideoZeroPipeline.from_pretrained(model_path, torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
        #pipe_text_to_video_zero = pipeline_scheduler(pipe_text_to_video_zero)
        pipe_text_to_video_zero.scheduler = DPMSolverMultistepScheduler.from_config(pipe_text_to_video_zero.scheduler.config)
        pipe_text_to_video_zero = pipe_text_to_video_zero.to(torch_device)
        #pipe_text_to_video_zero = optimize_pipe(pipe_text_to_video_zero, vae_tiling=True, vae=True, to_gpu=False)
        pipe_text_to_video_zero.set_progress_bar_config(disable=True)
    else:
        pipe_text_to_video_zero = pipeline_scheduler(pipe_text_to_video_zero)
    try:
        import imageio
    except ModuleNotFoundError:
        run_sp("pip install imageio", realtime=False)
        import imageio
        pass
    clear_last()
    prt("Generating Text-To-Video Zero from your Prompt...")
    prt(progress)
    autoscroll(False)
    batch_output = os.path.join(stable_dir, text_to_video_zero_prefs['batch_folder_name'])
    if not os.path.isdir(batch_output):
      os.makedirs(batch_output)
    local_output = batch_output
    batch_output = os.path.join(prefs['image_output'], text_to_video_zero_prefs['batch_folder_name'])
    if not os.path.isdir(batch_output):
      os.makedirs(batch_output)
    random_seed = int(text_to_video_zero_prefs['seed']) if int(text_to_video_zero_prefs['seed']) > 0 else rnd.randint(0,4294967295)
    generator = torch.Generator(device="cuda").manual_seed(random_seed)
    #generator = torch.manual_seed(random_seed)
    width = text_to_video_zero_prefs['width']
    height = text_to_video_zero_prefs['height']
    try:
      #print(f"prompt={text_to_video_zero_prefs['prompt']}, negative_prompt={text_to_video_zero_prefs['negative_prompt']}, editing_prompt={editing_prompt}, edit_warmup_steps={edit_warmup_steps}, edit_guidance_scale={edit_guidance_scale}, edit_threshold={edit_threshold}, edit_weights={edit_weights}, reverse_editing_direction={reverse_editing_direction}, edit_momentum_scale={text_to_video_zero_prefs['edit_momentum_scale']}, edit_mom_beta={text_to_video_zero_prefs['edit_mom_beta']}, num_inference_steps={text_to_video_zero_prefs['num_inference_steps']}, eta={text_to_video_zero_prefs['eta']}, guidance_scale={text_to_video_zero_prefs['guidance_scale']}")
      #, output_type = "pt", width=width, height=height
      frames = pipe_text_to_video_zero(prompt=text_to_video_zero_prefs['prompt'], negative_prompt=text_to_video_zero_prefs['negative_prompt'], video_length=text_to_video_zero_prefs['num_frames'], num_inference_steps=text_to_video_zero_prefs['num_inference_steps'], eta=text_to_video_zero_prefs['eta'], guidance_scale=text_to_video_zero_prefs['guidance_scale'], motion_field_strength_x=text_to_video_zero_prefs['motion_field_strength_x'], motion_field_strength_y=text_to_video_zero_prefs['motion_field_strength_y'], t0=text_to_video_zero_prefs['t0'], t1=text_to_video_zero_prefs['t1'], generator=generator, callback=callback_fnc, callback_steps=1).images
    except Exception as e:
      clear_last()
      clear_last()
      alert_msg(page, f"ERROR: Couldn't Text-To-Video Zero your image for some reason. Possibly out of memory or something wrong with my code...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
      return
    clear_last()
    clear_last()
    save_path = os.path.join(prefs['image_output'], text_to_video_zero_prefs['batch_folder_name'])
    filename = f"{prefs['file_prefix']}{format_filename(text_to_video_zero_prefs['prompt'])}"
    filename = filename[:int(prefs['file_max_length'])]
    #if prefs['file_suffix_seed']: filename += f"-{random_seed}"
    autoscroll(True)
    video_path = ""
    if text_to_video_zero_prefs['export_to_video']:
        #from diffusers.utils import export_to_video
        #video_path = export_to_video(frames)
        local_file = available_file(local_output, filename, 0, ext="mp4", no_num=True)
        save_file = available_file(batch_output, filename, 0, ext="mp4", no_num=True)
        imageio.mimsave(local_file, frames, fps=4)
        shutil.copy(local_file, save_file)
        #print(f"video_path: {video_path}")
    #video = frames.cpu().numpy()
    #print(f"video: {video}")
    #print(f"frames type: {type(frames)} len: {len(frames)}")
    #result = [(r * 255).astype("uint8") for r in result]
    from PIL.PngImagePlugin import PngInfo
    num = 0
    for image in frames:
        random_seed += num
        fname = filename + (f"-{random_seed}" if prefs['file_suffix_seed'] else "")
        image_path = available_file(batch_output, fname, num)
        unscaled_path = image_path
        output_file = image_path.rpartition(slash)[2]
        #uint8_image = (image * 255).round().astype("uint8")
        img = PILImage.fromarray((image * 255).astype("uint8"))
        #print(f"img type: {type(img)}")
        #np_image = image.cpu().numpy()
        #print(f"image: {type(image)}, np_image: {type(np_image)}")
        #print(f"image type: {type(image)} to {image_path}")
        #cv2.imwrite(image_path, image)
        #PILImage.fromarray(image).save(image_path)
        img.save(image_path)
        #image.save(image_path)
        #imageio.imwrite(local_file, image, extension=".png")
        #img = pipe_text_to_video_zero.numpy_to_pil(image)
        #img.save(image_path)
        #imageio.imsave(local_file, img, extension=".png")
        #PILImage.fromarray(img).save(image_path)
        out_path = image_path.rpartition(slash)[0]
        upscaled_path = os.path.join(out_path, output_file)
        if not text_to_video_zero_prefs['display_upscaled_image'] or not text_to_video_zero_prefs['apply_ESRGAN_upscale']:
            prt(Row([ImageButton(src=unscaled_path, data=upscaled_path, width=width, height=height, page=page)], alignment=MainAxisAlignment.CENTER))
        if text_to_video_zero_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
            os.chdir(os.path.join(dist_dir, 'Real-ESRGAN'))
            upload_folder = 'upload'
            result_folder = 'results'
            if os.path.isdir(upload_folder):
                shutil.rmtree(upload_folder)
            if os.path.isdir(result_folder):
                shutil.rmtree(result_folder)
            os.mkdir(upload_folder)
            os.mkdir(result_folder)
            short_name = f'{fname[:80]}-{num}.png'
            dst_path = os.path.join(dist_dir, 'Real-ESRGAN', upload_folder, short_name)
            #print(f'Moving {fpath} to {dst_path}')
            #shutil.move(fpath, dst_path)
            shutil.copy(image_path, dst_path)
            #faceenhance = ' --face_enhance' if text_to_video_zero_prefs["face_enhance"] else ''
            faceenhance = ''
            run_sp(f'python inference_realesrgan.py -n realesr-general-x4v3 -i upload --outscale {text_to_video_zero_prefs["enlarge_scale"]}{faceenhance}', cwd=os.path.join(dist_dir, 'Real-ESRGAN'), realtime=False)
            out_file = short_name.rpartition('.')[0] + '_out.png'
            shutil.move(os.path.join(dist_dir, 'Real-ESRGAN', result_folder, out_file), upscaled_path)
            image_path = upscaled_path
            os.chdir(stable_dir)
            if text_to_video_zero_prefs['display_upscaled_image']:
                time.sleep(0.2)
                prt(Row([ImageButton(src=upscaled_path, data=upscaled_path, width=width * float(text_to_video_zero_prefs["enlarge_scale"]), height=height * float(text_to_video_zero_prefs["enlarge_scale"]), page=page)], alignment=MainAxisAlignment.CENTER))
                #prt(Row([Img(src=upscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
        if prefs['save_image_metadata']:
            img = PILImage.open(image_path)
            metadata = PngInfo()
            metadata.add_text("artist", prefs['meta_ArtistName'])
            metadata.add_text("copyright", prefs['meta_Copyright'])
            metadata.add_text("software", "Stable Diffusion Deluxe" + f", upscaled {text_to_video_zero_prefs['enlarge_scale']}x with ESRGAN" if text_to_video_zero_prefs['apply_ESRGAN_upscale'] else "")
            metadata.add_text("pipeline", "Text-To-Video Zero")
            if prefs['save_config_in_metadata']:
              config_json = text_to_video_zero_prefs.copy()
              config_json['model_path'] = model_path
              config_json['scheduler_mode'] = prefs['scheduler_mode']
              config_json['seed'] = random_seed
              del config_json['num_frames']
              del config_json['width']
              del config_json['height']
              del config_json['display_upscaled_image']
              del config_json['batch_folder_name']
              if not config_json['apply_ESRGAN_upscale']:
                del config_json['enlarge_scale']
                del config_json['apply_ESRGAN_upscale']
              metadata.add_text("config_json", json.dumps(config_json, ensure_ascii=True, indent=4))
            img.save(image_path, pnginfo=metadata)
        #TODO: PyDrive
        if storage_type == "Colab Google Drive":
            new_file = available_file(os.path.join(prefs['image_output'], text_to_video_zero_prefs['batch_folder_name']), fname, num)
            out_path = new_file
            shutil.copy(image_path, new_file)
        elif bool(prefs['image_output']):
            new_file = available_file(os.path.join(prefs['image_output'], text_to_video_zero_prefs['batch_folder_name']), fname, num)
            out_path = new_file
            shutil.copy(image_path, new_file)
        prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))
        num += 1
    autoscroll(False)
    if prefs['enable_sounds']: page.snd_alert.play()

def run_video_to_video(page):
    global video_to_video_prefs, prefs, status, pipe_video_to_video, model_path
    if not status['installed_diffusers']:
      alert_msg(page, "You need to Install HuggingFace Diffusers before using...")
      return
    def prt(line):
      if type(line) == str:
        line = Text(line, size=17)
      page.VideoToVideo.controls.append(line)
      page.VideoToVideo.update()
    def clear_last():
      del page.VideoToVideo.controls[-1]
      page.VideoToVideo.update()
    def clear_list():
      page.TextToVideo.controls = page.VideoToVideo.controls[:1]
    def autoscroll(scroll=True):
      page.VideoToVideo.auto_scroll = scroll
      page.VideoToVideo.update()
    if not bool(video_to_video_prefs['init_video']):
      alert_msg(page, "You must provide the Input Initial Video Clip to process...")
      return
    progress = ProgressBar(bar_height=8)
    total_steps = video_to_video_prefs['num_inference_steps']
    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep}"
      progress.update()
      #print(f'{type(latents)} {len(latents)}- {str(latents)}')
    clear_list()
    autoscroll(True)
    installer = Installing("Installing Video-To-Video Pipeline...")
    prt(installer)
    #), dropdown.Option(), dropdown.Option(), dropdown.Option(), dropdown.Option(
    try:
        import cv2
    except ModuleNotFoundError:
        run_sp("pip install opencv-contrib-python", realtime=False)
        import cv2
        pass
    #model_id = "cerspense/zeroscope_v2_XL"
    if video_to_video_prefs['model'] == "damo-vilab/text-to-video-ms-1.7b":
        model_id = "damo-vilab/text-to-video-ms-1.7b"
    elif video_to_video_prefs['model'] == "modelscope-damo-text2video-synthesis":
        model_id = "damo-vilab/modelscope-damo-text2video-synthesis"
    elif video_to_video_prefs['model'] == "modelscope-damo-text2video-pruned-weights":
        model_id = "kabachuha/modelscope-damo-text2video-pruned-weights"
    elif video_to_video_prefs['model'] == "cerspense/zeroscope_v2_576w":
        model_id = "cerspense/zeroscope_v2_576w"
    elif video_to_video_prefs['model'] == "cerspense/zeroscope_v2_XL":
        model_id = "cerspense/zeroscope_v2_XL"
    clear_pipes('video_to_video')
    if pipe_video_to_video is None:
        installer.set_details("...initializing VideoToVideo SDPipeline")
        from diffusers import VideoToVideoSDPipeline, DPMSolverMultistepScheduler
        pipe_video_to_video = VideoToVideoSDPipeline.from_pretrained(model_id, torch_dtype=torch.float16, variant="fp16", cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
        #pipe_video_to_video = pipeline_scheduler(pipe_video_to_video)
        pipe_video_to_video.scheduler = DPMSolverMultistepScheduler.from_config(pipe_video_to_video.scheduler.config)
        if video_to_video_prefs['lower_memory']:
            #pipe_video_to_video.enable_sequential_cpu_offload()
            pipe_video_to_video.enable_model_cpu_offload()
            pipe_video_to_video.unet.enable_forward_chunking(chunk_size=1, dim=1)
            #pipe_video_to_video.enable_vae_tiling()
            pipe_video_to_video.enable_vae_slicing()
        else:
            pipe_video_to_video = pipe_video_to_video.to(torch_device)
        #pipe_video_to_video = optimize_pipe(pipe_video_to_video, vae_tiling=True, vae=True, to_gpu=False)
        pipe_video_to_video.set_progress_bar_config(disable=True)
    else:
        pipe_video_to_video = pipeline_scheduler(pipe_video_to_video)
    #clear_last()
    def prep_video(vid):
        nonlocal width, height
        if vid.startswith('http'):
            init_vid = download_file(vid, stable_dir)
            installer.set_details("...extracting frames from video")
        else:
            if os.path.isfile(vid):
                init_vid = vid
            else:
                alert_msg(page, f"ERROR: Couldn't find your init_video {vid}")
                return
        try:
            start_time = float(controlnet_prefs['start_time'])
            end_time = float(controlnet_prefs['end_time'])
            fps = int(controlnet_prefs['fps'])
            max_size = controlnet_prefs['max_size']
        except Exception:
            alert_msg(page, "Make sure your Numbers are actual numbers...")
            return
        installer.set_details("Extracting Frames from Video Clip")
        try:
            cap = cv2.VideoCapture(init_vid)
        except Exception as e:
            alert_msg(page, "ERROR Reading Video File. May be Incompatible Format...")
            clear_last()
            return
        count = 0
        video = []
        frames = []
        width = height = 0
        cap.set(cv2.CAP_PROP_FPS, fps)
        video_length = cap.get(cv2.CAP_PROP_FRAME_COUNT)
        start_frame = int(start_time * fps)
        if end_time == 0 or end_time == 0.0:
            end_frame = int(video_length)
        else:
            end_frame = int(end_time * fps)
        total = end_frame - start_frame
        for i in range(start_frame, end_frame):
            cap.set(cv2.CAP_PROP_POS_FRAMES, i)
            success, image = cap.read()
            if success:
                #filename = os.path.join(output_dir, f'{file_prefix}{count}.png')
                if width == 0:
                    shape = image.shape
                    width, height = scale_dimensions(shape[1], shape[0], max=max_size, multiple=16)
                image = cv2.resize(image, (width, height), interpolation = cv2.INTER_AREA)
                #cv2.imwrite(os.path.join(output_dir, filename), image)
                #image = prep_image(controlnet_prefs['control_task'], PILImage.fromarray(image))
                video.append(image)
                count += 1
        cap.release()
        #clear_last()
        return video
    init_frames = prep_video(video_to_video_prefs['init_video'])
    clear_last()
    prt(f"Generating Video-To-Video on {len(frames)} Frames with your Prompt...")
    prt(progress)
    autoscroll(False)
    batch_output = os.path.join(stable_dir, video_to_video_prefs['batch_folder_name'])
    if not os.path.isdir(batch_output):
      os.makedirs(batch_output)
    local_output = batch_output
    batch_output = os.path.join(prefs['image_output'], video_to_video_prefs['batch_folder_name'])
    if not os.path.isdir(batch_output):
      os.makedirs(batch_output)
    random_seed = int(video_to_video_prefs['seed']) if int(video_to_video_prefs['seed']) > 0 else rnd.randint(0,4294967295)
    generator = torch.Generator(device="cpu").manual_seed(random_seed)
    #generator = torch.manual_seed(random_seed)
    width = video_to_video_prefs['width']
    height = video_to_video_prefs['height']
    try:
      #print(f"prompt={video_to_video_prefs['prompt']}, negative_prompt={video_to_video_prefs['negative_prompt']}, editing_prompt={editing_prompt}, edit_warmup_steps={edit_warmup_steps}, edit_guidance_scale={edit_guidance_scale}, edit_threshold={edit_threshold}, edit_weights={edit_weights}, reverse_editing_direction={reverse_editing_direction}, edit_momentum_scale={video_to_video_prefs['edit_momentum_scale']}, edit_mom_beta={video_to_video_prefs['edit_mom_beta']}, num_inference_steps={video_to_video_prefs['num_inference_steps']}, eta={video_to_video_prefs['eta']}, guidance_scale={video_to_video_prefs['guidance_scale']}")
      #, output_type = "pt", width=width, height=height
      frames = pipe_video_to_video(prompt=video_to_video_prefs['prompt'], negative_prompt=video_to_video_prefs['negative_prompt'], video=init_frames, num_inference_steps=video_to_video_prefs['num_inference_steps'], eta=video_to_video_prefs['eta'], guidance_scale=video_to_video_prefs['guidance_scale'], strength=video_to_video_prefs['strength'], generator=generator, callback=callback_fnc, callback_steps=1).frames
    except Exception as e:
      clear_last()
      clear_last()
      alert_msg(page, f"ERROR: Couldn't Video-To-Video your image for some reason. Possibly out of memory or something wrong with my code...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
      return
    clear_last()
    clear_last()
    autoscroll(True)
    save_path = os.path.join(prefs['image_output'], video_to_video_prefs['batch_folder_name'])
    filename = f"{prefs['file_prefix']}{format_filename(video_to_video_prefs['prompt'])}"
    filename = filename[:int(prefs['file_max_length'])]
    #if prefs['file_suffix_seed']: filename += f"-{random_seed}"
    autoscroll(True)
    video_path = ""
    if video_to_video_prefs['export_to_video']:
        from diffusers.utils import export_to_video
        video_path = export_to_video(frames)
        shutil.copy(video_path, available_file(local_output, filename, 0, ext="mp4", no_num=True))
        shutil.copy(video_path, available_file(batch_output, filename, 0, ext="mp4", no_num=True))
        #print(f"video_path: {video_path}")
    #video = frames.cpu().numpy()
    #print(f"video: {video}")
    import cv2
    from PIL.PngImagePlugin import PngInfo
    num = 0
    for image in frames:
        random_seed += num
        fname = filename + (f"-{random_seed}" if prefs['file_suffix_seed'] else "")
        image_path = available_file(batch_output, fname, num)
        unscaled_path = image_path
        output_file = image_path.rpartition(slash)[2]
        #uint8_image = (image * 255).round().astype("uint8")
        #np_image = image.cpu().numpy()
        #print(f"image: {type(image)}, np_image: {type(np_image)}")
        #print(f"image: {type(image)} to {image_path}")
        cv2.imwrite(image_path, image)
        #PILImage.fromarray(np_image).save(image_path)
        out_path = image_path.rpartition(slash)[0]
        upscaled_path = os.path.join(out_path, output_file)
        if not video_to_video_prefs['display_upscaled_image'] or not video_to_video_prefs['apply_ESRGAN_upscale']:
            prt(Row([ImageButton(src=unscaled_path, data=upscaled_path, width=width, height=height, page=page)], alignment=MainAxisAlignment.CENTER))
        if video_to_video_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
            os.chdir(os.path.join(dist_dir, 'Real-ESRGAN'))
            upload_folder = 'upload'
            result_folder = 'results'
            if os.path.isdir(upload_folder):
                shutil.rmtree(upload_folder)
            if os.path.isdir(result_folder):
                shutil.rmtree(result_folder)
            os.mkdir(upload_folder)
            os.mkdir(result_folder)
            short_name = f'{fname[:80]}-{num}.png'
            dst_path = os.path.join(dist_dir, 'Real-ESRGAN', upload_folder, short_name)
            #print(f'Moving {fpath} to {dst_path}')
            #shutil.move(fpath, dst_path)
            shutil.copy(image_path, dst_path)
            #faceenhance = ' --face_enhance' if video_to_video_prefs["face_enhance"] else ''
            faceenhance = ''
            run_sp(f'python inference_realesrgan.py -n realesr-general-x4v3 -i upload --outscale {video_to_video_prefs["enlarge_scale"]}{faceenhance}', cwd=os.path.join(dist_dir, 'Real-ESRGAN'), realtime=False)
            out_file = short_name.rpartition('.')[0] + '_out.png'
            shutil.move(os.path.join(dist_dir, 'Real-ESRGAN', result_folder, out_file), upscaled_path)
            image_path = upscaled_path
            os.chdir(stable_dir)
            if video_to_video_prefs['display_upscaled_image']:
                time.sleep(0.6)
                prt(Row([ImageButton(src=upscaled_path, data=upscaled_path, width=width * float(video_to_video_prefs["enlarge_scale"]), height=height * float(video_to_video_prefs["enlarge_scale"]), page=page)], alignment=MainAxisAlignment.CENTER))
                #prt(Row([Img(src=upscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
        if prefs['save_image_metadata']:
            img = PILImage.open(image_path)
            metadata = PngInfo()
            metadata.add_text("artist", prefs['meta_ArtistName'])
            metadata.add_text("copyright", prefs['meta_Copyright'])
            metadata.add_text("software", "Stable Diffusion Deluxe" + f", upscaled {video_to_video_prefs['enlarge_scale']}x with ESRGAN" if video_to_video_prefs['apply_ESRGAN_upscale'] else "")
            metadata.add_text("pipeline", "Video-To-Video")
            if prefs['save_config_in_metadata']:
              config_json = video_to_video_prefs.copy()
              config_json['model_path'] = model_id
              config_json['scheduler_mode'] = prefs['scheduler_mode']
              config_json['seed'] = random_seed
              del config_json['num_frames']
              del config_json['width']
              del config_json['height']
              del config_json['display_upscaled_image']
              del config_json['batch_folder_name']
              del config_json['lower_memory']
              if not config_json['apply_ESRGAN_upscale']:
                del config_json['enlarge_scale']
                del config_json['apply_ESRGAN_upscale']
              metadata.add_text("config_json", json.dumps(config_json, ensure_ascii=True, indent=4))
            img.save(image_path, pnginfo=metadata)
        #TODO: PyDrive
        if storage_type == "Colab Google Drive":
            new_file = available_file(os.path.join(prefs['image_output'], video_to_video_prefs['batch_folder_name']), fname, num)
            out_path = new_file
            shutil.copy(image_path, new_file)
        elif bool(prefs['image_output']):
            new_file = available_file(os.path.join(prefs['image_output'], video_to_video_prefs['batch_folder_name']), fname, num)
            out_path = new_file
            shutil.copy(image_path, new_file)
        prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))
        num += 1
    if bool(video_path):
        prt(Row([VideoContainer(video_path)], alignment=MainAxisAlignment.CENTER))
    autoscroll(False)
    if prefs['enable_sounds']: page.snd_alert.play()


def run_potat1(page):
    global potat1_prefs, prefs, status, pipe_potat1, model_path
    if not status['installed_diffusers']:
      alert_msg(page, "You need to Install HuggingFace Diffusers before using...")
      return
    def prt(line):
      if type(line) == str:
        line = Text(line, size=17)
      page.Potat1.controls.append(line)
      page.Potat1.update()
    def clear_last():
      del page.Potat1.controls[-1]
      page.Potat1.update()
    def clear_list():
      page.Potat1.controls = page.Potat1.controls[:1]
    def autoscroll(scroll=True):
      page.Potat1.auto_scroll = scroll
      page.Potat1.update()
    progress = ProgressBar(bar_height=8)
    total_steps = potat1_prefs['num_inference_steps']
    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep}"
      progress.update()
      #print(f'{type(latents)} {len(latents)}- {str(latents)}')
    clear_list()
    autoscroll(True)
    installer = Installing("Installing Potat1 Text-To-Video Pipeline...")
    prt(installer)
    #model_id = "damo-vilab/text-to-video-ms-1.7b"
    clear_pipes()
    potat1_dir = os.path.join(root_dir, "potat1")
    finetuning_dir = os.path.join(root_dir, "Text-To-Video-Finetuning")
    torch_installed = False
    try:
        import torch
        torch_installed = True
    except:
        pass
    if torch_installed:
        if version.parse(torch.__version__) >= version.parse("2.0.0"):
            torch_installed = False
    if not torch_installed:
        installer.set_details("...installing Torch 1.13.1")
        run_sp("pip install -q torch==1.13.1+cu116 torchvision==0.14.1+cu116 torchaudio==0.13.1 torchtext==0.14.1 torchdata==0.5.1 --extra-index-url https://download.pytorch.org/whl/cu116 -U", realtime=False)
    run_sp("pip install imageio[ffmpeg] -U einops omegaconf decord", realtime=False)
    if not os.path.exists(finetuning_dir):
        installer.set_details("...camenduru/Text-To-Video-Finetuning")
        run_sp("git clone -b dev https://github.com/camenduru/Text-To-Video-Finetuning", realtime=False, cwd=root_dir)
    if not os.path.exists(potat1_dir):
        installer.set_details("...camenduru/potat1")
        run_sp("git clone https://huggingface.co/camenduru/potat1", realtime=False, cwd=root_dir)
    #clear_pipes('potat1')
    clear_last()
    prt("Generating Potat1 of your Prompt...")
    prt(progress)
    autoscroll(False)
    batch_output = os.path.join(stable_dir, potat1_prefs['batch_folder_name'])
    if not os.path.isdir(batch_output):
      os.makedirs(batch_output)
    local_output = batch_output
    batch_output = os.path.join(prefs['image_output'], potat1_prefs['batch_folder_name'])
    if not os.path.isdir(batch_output):
      os.makedirs(batch_output)
    random_seed = int(potat1_prefs['seed']) if int(potat1_prefs['seed']) > 0 else rnd.randint(0,4294967295)
    #generator = torch.Generator(device="cpu").manual_seed(random_seed)
    #generator = torch.manual_seed(random_seed)
    width = potat1_prefs['width']
    height = potat1_prefs['height']
    x = " -x" if status['installed_xformers'] else ""
    rw = " -rw" if potat1_prefs['remove_watermark'] else ""
    try:
        run_sp(f'python inference.py -m "{potat1_dir}" -p "{potat1_prefs["prompt"]}" -n "{potat1_prefs["negative_prompt"]}" -W {width} -H {height} -o {batch_output} -d cuda{x}{rw} -s {potat1_prefs["num_inference_steps"]} -g {potat1_prefs["guidance_scale"]} -f {potat1_prefs["fps"]} -T {potat1_prefs["num_frames"]}', cwd=finetuning_dir)
      #print(f"prompt={potat1_prefs['prompt']}, negative_prompt={potat1_prefs['negative_prompt']}, editing_prompt={editing_prompt}, edit_warmup_steps={edit_warmup_steps}, edit_guidance_scale={edit_guidance_scale}, edit_threshold={edit_threshold}, edit_weights={edit_weights}, reverse_editing_direction={reverse_editing_direction}, edit_momentum_scale={potat1_prefs['edit_momentum_scale']}, edit_mom_beta={potat1_prefs['edit_mom_beta']}, num_inference_steps={potat1_prefs['num_inference_steps']}, eta={potat1_prefs['eta']}, guidance_scale={potat1_prefs['guidance_scale']}")
      #, output_type = "pt", width=width, height=height
      #frames = pipe_potat1(prompt=potat1_prefs['prompt'], negative_prompt=potat1_prefs['negative_prompt'], num_frames=potat1_prefs['num_frames'], num_inference_steps=potat1_prefs['num_inference_steps'], eta=potat1_prefs['eta'], guidance_scale=potat1_prefs['guidance_scale'], generator=generator, callback=callback_fnc, callback_steps=1).frames
    except Exception as e:
      clear_last()
      clear_last()
      alert_msg(page, f"ERROR: Potat1 Text-To-Video failed for some reason. Possibly out of memory or something wrong with the code...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
      return
    clear_last()
    clear_last()
    autoscroll(True)
    filename = f"{format_filename(potat1_prefs['prompt'])}"
    #filename = filename[:int(prefs['file_max_length'])]
    #if prefs['file_suffix_seed']: filename += f"-{random_seed}"
    autoscroll(True)
    video_path = ""
    '''if potat1_prefs['export_to_video']:
        from diffusers.utils import export_to_video
        video_path = export_to_video(frames)
        shutil.copy(video_path, available_file(local_output, filename, 0, ext="mp4", no_num=True))
        shutil.copy(video_path, available_file(batch_output, filename, 0, ext="mp4", no_num=True))
    import cv2
    from PIL.PngImagePlugin import PngInfo
    num = 0
    for image in frames:
        random_seed += num
        fname = filename + (f"-{random_seed}" if prefs['file_suffix_seed'] else "")
        image_path = available_file(batch_output, fname, num)
        unscaled_path = image_path
        output_file = image_path.rpartition(slash)[2]
        #uint8_image = (image * 255).round().astype("uint8")
        #np_image = image.cpu().numpy()
        #print(f"image: {type(image)}, np_image: {type(np_image)}")
        #print(f"image: {type(image)} to {image_path}")
        cv2.imwrite(image_path, image)
        #PILImage.fromarray(np_image).save(image_path)
        out_path = image_path.rpartition(slash)[0]
        upscaled_path = os.path.join(out_path, output_file)
        if not potat1_prefs['display_upscaled_image'] or not potat1_prefs['apply_ESRGAN_upscale']:
            prt(Row([ImageButton(src=unscaled_path, data=upscaled_path, width=width, height=height, page=page)], alignment=MainAxisAlignment.CENTER))
        if potat1_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
            os.chdir(os.path.join(dist_dir, 'Real-ESRGAN'))
            upload_folder = 'upload'
            result_folder = 'results'
            if os.path.isdir(upload_folder):
                shutil.rmtree(upload_folder)
            if os.path.isdir(result_folder):
                shutil.rmtree(result_folder)
            os.mkdir(upload_folder)
            os.mkdir(result_folder)
            short_name = f'{fname[:80]}-{num}.png'
            dst_path = os.path.join(dist_dir, 'Real-ESRGAN', upload_folder, short_name)
            #print(f'Moving {fpath} to {dst_path}')
            #shutil.move(fpath, dst_path)
            shutil.copy(image_path, dst_path)
            #faceenhance = ' --face_enhance' if potat1_prefs["face_enhance"] else ''
            faceenhance = ''
            run_sp(f'python inference_realesrgan.py -n realesr-general-x4v3 -i upload --outscale {potat1_prefs["enlarge_scale"]}{faceenhance}', cwd=os.path.join(dist_dir, 'Real-ESRGAN'), realtime=False)
            out_file = short_name.rpartition('.')[0] + '_out.png'
            shutil.move(os.path.join(dist_dir, 'Real-ESRGAN', result_folder, out_file), upscaled_path)
            image_path = upscaled_path
            os.chdir(stable_dir)
            if potat1_prefs['display_upscaled_image']:
                time.sleep(0.6)
                prt(Row([ImageButton(src=upscaled_path, data=upscaled_path, width=width * float(potat1_prefs["enlarge_scale"]), height=height * float(potat1_prefs["enlarge_scale"]), page=page)], alignment=MainAxisAlignment.CENTER))
                #prt(Row([Img(src=upscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
        if prefs['save_image_metadata']:
            img = PILImage.open(image_path)
            metadata = PngInfo()
            metadata.add_text("artist", prefs['meta_ArtistName'])
            metadata.add_text("copyright", prefs['meta_Copyright'])
            metadata.add_text("software", "Stable Diffusion Deluxe" + f", upscaled {potat1_prefs['enlarge_scale']}x with ESRGAN" if potat1_prefs['apply_ESRGAN_upscale'] else "")
            metadata.add_text("pipeline", "Text-To-Video")
            if prefs['save_config_in_metadata']:
              config_json = potat1_prefs.copy()
              config_json['model_path'] = model_id
              config_json['scheduler_mode'] = prefs['scheduler_mode']
              config_json['seed'] = random_seed
              del config_json['num_frames']
              del config_json['width']
              del config_json['height']
              del config_json['display_upscaled_image']
              del config_json['batch_folder_name']
              del config_json['lower_memory']
              if not config_json['apply_ESRGAN_upscale']:
                del config_json['enlarge_scale']
                del config_json['apply_ESRGAN_upscale']
              metadata.add_text("config_json", json.dumps(config_json, ensure_ascii=True, indent=4))
            img.save(image_path, pnginfo=metadata)
        #TODO: PyDrive
        if storage_type == "Colab Google Drive":
            new_file = available_file(os.path.join(prefs['image_output'], potat1_prefs['batch_folder_name']), fname, num)
            out_path = new_file
            shutil.copy(image_path, new_file)
        elif bool(prefs['image_output']):
            new_file = available_file(os.path.join(prefs['image_output'], potat1_prefs['batch_folder_name']), fname, num)
            out_path = new_file
            shutil.copy(image_path, new_file)
        prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))
        num += 1'''
    #prt(Row([VideoContainer(video_path)], alignment=MainAxisAlignment.CENTER))
    prt(f"Done creating video... Check {batch_output}")
    autoscroll(False)
    if prefs['enable_sounds']: page.snd_alert.play()


def run_stable_animation(page):
    global stable_animation_prefs, prefs, status
    if not bool(prefs['Stability_api_key']):
        alert_msg(e.page, "You must have your DreamStudio.ai Stability-API Key to use Stability.  Note that it will cost you tokens.")
        return
    def prt(line):
      if type(line) == str:
        line = Text(line, size=17)
      page.StableAnimation.controls.append(line)
      page.StableAnimation.update()
    def clear_last():
      del page.StableAnimation.controls[-1]
      page.StableAnimation.update()
    def clear_list():
      page.StableAnimation.controls = page.StableAnimation.controls[:1]
    def autoscroll(scroll=True):
      page.StableAnimation.auto_scroll = scroll
      page.StableAnimation.update()
    abort_run = False
    def abort_diffusion(e):
      nonlocal abort_run
      abort_run = True
      page.snd_error.play()
      page.snd_delete.play()
    progress = ProgressBar(bar_height=8)
    total_steps = stable_animation_prefs['num_inference_steps']
    def callback_fnc(step: int) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}"
      progress.update()
    clear_list()
    autoscroll(True)
    prt(Installing("Installing Stable Animation Stability.ai Pipeline..."))
    #import cv2
    #run_sp('pip install "stability_sdk[anim_ui]"', realtime=True)
    try:
        from stability_sdk import api
    except Exception:
        run_sp('pip install "stability_sdk[anim_ui]"', realtime=True) #
        from stability_sdk import api
        pass
    from tqdm import tqdm
    from PIL.PngImagePlugin import PngInfo

    #batch_output = os.path.join(stable_dir, stable_animation_prefs['batch_folder_name'])
    #if not os.path.isdir(batch_output):
    #  os.makedirs(batch_output)
    #local_output = batch_output
    batch_output = os.path.join(prefs['image_output'], stable_animation_prefs['batch_folder_name'])
    if not os.path.isdir(batch_output):
      os.makedirs(batch_output)
    random_seed = int(stable_animation_prefs['seed']) if int(stable_animation_prefs['seed']) > 0 else rnd.randint(0,4294967295)
    width = stable_animation_prefs['width']
    height = stable_animation_prefs['height']
    animation_prompts = stable_animation_prefs['animation_prompts']
    '''try:
        prompts = json.loads(animation_prompts)
    except json.JSONDecodeError:
        try:
            prompts = eval(animation_prompts)
        except Exception as e:
            alert_msg(page, "Invalid JSON or Python code for animation_prompts.")
            return'''
    prompts = {int(k): v for k, v in animation_prompts.items()}
    from stability_sdk.api import (ClassifierException, Context, OutOfCreditsException)
    context = Context("grpc.stability.ai:443", prefs['Stability_api_key'])
    try:
        balance, profile_picture = context.get_user_info()
    except:
        alert_msg(page, "Error getting Stability.ai User Info")
        return
    from stability_sdk.animation import (
        AnimationArgs,
        Animator,
        interpolate_frames
    )
    from stability_sdk.utils import (create_video_from_frames, extract_frames_from_video, interpolate_mode_from_string)
    args = AnimationArgs()
    args.width = width
    args.height = height
    args.sampler = stable_animation_prefs['sampler']
    #args_generation.custom_model = stable_animation_prefs['custom_model']
    args.model = stable_animation_prefs['model'].lower()
    args.seed = random_seed
    args.cfg_scale = stable_animation_prefs['guidance_scale']
    args.clip_guidance = stable_animation_prefs['clip_guidance']
    args.init_image = stable_animation_prefs['init_image']
    args.init_sizing = stable_animation_prefs['init_sizing'].lower()
    args.mask_path = stable_animation_prefs['mask_image']
    args.mask_invert = stable_animation_prefs['mask_invert']
    args.preset = stable_animation_prefs['style_preset']
    args.animation_mode = stable_animation_prefs['animation_mode']
    args.max_frames = stable_animation_prefs['max_frames']
    args.border = stable_animation_prefs['border'].lower()
    args.noise_add_curve = stable_animation_prefs['noise_add_curve']
    args.noise_scale_curve = stable_animation_prefs['noise_scale_curve']
    args.strength_curve = stable_animation_prefs['strength_curve']
    args.steps_curve = stable_animation_prefs['steps_curve']
    args.steps_strength_adj = stable_animation_prefs['steps_strength_adj']
    args.interpolate_prompts = stable_animation_prefs['interpolate_prompts']
    args.locked_seed = stable_animation_prefs['locked_seed']
    args.angle = stable_animation_prefs['angle']
    args.zoom = stable_animation_prefs['zoom']
    args.translation_x = stable_animation_prefs['translation_x']
    args.translation_y = stable_animation_prefs['translation_y']
    args.translation_z = stable_animation_prefs['translation_z']
    args.rotation_x = stable_animation_prefs['rotation_x']
    args.rotation_y = stable_animation_prefs['rotation_y']
    args.rotation_z = stable_animation_prefs['rotation_z']
    args.diffusion_cadence_curve = stable_animation_prefs['diffusion_cadence_curve']
    args.cadence_interp = stable_animation_prefs['cadence_interp'].lower()
    args.cadence_spans = stable_animation_prefs['cadence_spans']
    args.color_coherence = stable_animation_prefs['color_coherence']
    args.brightness_curve = stable_animation_prefs['brightness_curve']
    args.contrast_curve = stable_animation_prefs['contrast_curve']
    args.hue_curve = stable_animation_prefs['hue_curve']
    args.saturation_curve = stable_animation_prefs['saturation_curve']
    args.lightness_curve = stable_animation_prefs['lightness_curve']
    args.color_match_animate = stable_animation_prefs['color_match_animate']
    args.depth_model_weight = stable_animation_prefs['depth_model_weight']
    args.near_plane = stable_animation_prefs['near_plane']
    args.far_plane = stable_animation_prefs['far_plane']
    args.fov_curve = stable_animation_prefs['fov_curve']
    args.depth_blur_curve = stable_animation_prefs['depth_blur_curve']
    args.depth_warp_curve = stable_animation_prefs['depth_warp_curve']
    #args_depth.save_depth_maps = stable_animation_prefs['save_depth_maps']
    args.camera_type = stable_animation_prefs['camera_type'].lower()
    args.render_mode = stable_animation_prefs['render_mode'].lower()
    args.mask_power = stable_animation_prefs['mask_power']
    args.use_inpainting_model = stable_animation_prefs['use_inpainting_model']
    args.inpaint_border = stable_animation_prefs['inpaint_border']
    args.mask_min_value = stable_animation_prefs['mask_min_value']
    args.mask_binarization_thr = stable_animation_prefs['mask_binarization_thr']
    #args_inpaint.save_inpaint_masks = False
    args.video_init_path = stable_animation_prefs['video_init_path']
    args.extract_nth_frame = int(stable_animation_prefs['extract_nth_frame'])
    args.video_mix_in_curve = stable_animation_prefs['video_mix_in_curve']
    args.video_flow_warp = stable_animation_prefs['video_flow_warp']
    args.fps = stable_animation_prefs['output_fps']
    args.resume = stable_animation_prefs['resume']
    args.resume_from = int(stable_animation_prefs['resume_from'])
    args.reverse = False
    #arg_objs = AnimationArgs(args_generation, args_animation, args_camera, args_coherence, args_color, args_depth, args_render_3d, args_inpaint, args_vid_in, args_vid_out)
    try:
        animator = Animator(
            api_context=context,
            animation_prompts=prompts,
            negative_prompt=stable_animation_prefs['negative_prompt'],
            negative_prompt_weight=stable_animation_prefs['negative_prompt_weight'],
            args=args,
            #out_dir=batch_output
            #negative_prompt_weight=negative_prompt_weight,
            #resume=resume,
        )
    except ClassifierException as e:
        alert_msg(page, "Animation terminated early due to NSFW classifier. Sorry for the Censorship...")
        return
    except OutOfCreditsException as e:
        alert_msg(page, f"Animation terminated early, out of credits. Refill them Tokens...", content=Text(f"{e.details}"))
        return
    except Exception as e:
        alert_msg(page, f"Animation terminated early due to exception:", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
        return
    total_steps = args.max_frames
    filename = f"{prefs['file_prefix']}{format_filename(prompts[0])}"
    filename = filename[:int(prefs['file_max_length'])]
    #fname = filename + (f"-{random_seed}" if prefs['file_suffix_seed'] else "")
    clear_last()
    prt(Row([Text(f"Generating Stable Animation from your Prompts... Available Credits: {round(balance, 1)}"), Container(content=None, expand=True), IconButton(icon=icons.CANCEL, tooltip="Abort Current Run", on_click=abort_diffusion)]))
    prt(progress)
    autoscroll(True)
    try:
      #frames = pipe_stable_animation(prompt=stable_animation_prefs['prompt'], negative_prompt=stable_animation_prefs['negative_prompt'], video_length=stable_animation_prefs['max_frames'], num_inference_steps=stable_animation_prefs['num_inference_steps'], eta=stable_animation_prefs['eta'], guidance_scale=stable_animation_prefs['guidance_scale'], motion_field_strength_x=stable_animation_prefs['motion_field_strength_x'], motion_field_strength_y=stable_animation_prefs['motion_field_strength_y'], t0=stable_animation_prefs['t0'], t1=stable_animation_prefs['t1'], generator=generator, callback=callback_fnc, callback_steps=1).images
      for num, image in enumerate(tqdm(animator.render(), initial=animator.start_frame_idx, total=args.max_frames), start=animator.start_frame_idx):
        if abort_run:
            #clear_last()
            #clear_last()
            prt("🛑  Aborted Current Animation Run")
            return
        callback_fnc(num)
        fname = f"frame_{num:05d}"
        #image_path = available_file(batch_output, fname, num, no_num=True)
        image_path = os.path.join(batch_output, f"{fname}.png")
        unscaled_path = image_path
        output_file = image_path.rpartition(slash)[2]
        image.save(image_path)
        out_path = image_path.rpartition(slash)[0]
        upscaled_path = os.path.join(out_path, output_file)

        if stable_animation_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
            os.chdir(os.path.join(dist_dir, 'Real-ESRGAN'))
            upload_folder = 'upload'
            result_folder = 'results'
            if os.path.isdir(upload_folder):
                shutil.rmtree(upload_folder)
            if os.path.isdir(result_folder):
                shutil.rmtree(result_folder)
            os.mkdir(upload_folder)
            os.mkdir(result_folder)
            short_name = f'{fname[:80]}-{num}.png'
            dst_path = os.path.join(dist_dir, 'Real-ESRGAN', upload_folder, short_name)
            shutil.copy(image_path, dst_path)
            faceenhance = ''
            run_sp(f'python inference_realesrgan.py -n realesr-general-x4v3 -i upload --outscale {stable_animation_prefs["enlarge_scale"]}{faceenhance}', cwd=os.path.join(dist_dir, 'Real-ESRGAN'), realtime=False)
            out_file = short_name.rpartition('.')[0] + '_out.png'
            shutil.move(os.path.join(dist_dir, 'Real-ESRGAN', result_folder, out_file), upscaled_path)
            image_path = upscaled_path
            os.chdir(stable_dir)
            if stable_animation_prefs['display_upscaled_image']:
                prt(Row([ImageButton(src=upscaled_path, data=upscaled_path, width=width * float(stable_animation_prefs["enlarge_scale"]), height=height * float(stable_animation_prefs["enlarge_scale"]), page=page)], alignment=MainAxisAlignment.CENTER))
                #prt(Row([Img(src=upscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
                time.sleep(0.2)
        if prefs['save_image_metadata']:
            img = PILImage.open(image_path)
            metadata = PngInfo()
            metadata.add_text("artist", prefs['meta_ArtistName'])
            metadata.add_text("copyright", prefs['meta_Copyright'])
            metadata.add_text("software", "Stable Diffusion Deluxe" + f", upscaled {stable_animation_prefs['enlarge_scale']}x with ESRGAN" if stable_animation_prefs['apply_ESRGAN_upscale'] else "")
            metadata.add_text("pipeline", "Stable Animation")
            if prefs['save_config_in_metadata']:
                config_json = stable_animation_prefs.copy()
                config_json['model_path'] = model_path
                config_json['scheduler_mode'] = prefs['scheduler_mode']
                config_json['seed'] = random_seed
                del config_json['max_frames']
                del config_json['width']
                del config_json['height']
                del config_json['display_upscaled_image']
                del config_json['batch_folder_name']
                if not config_json['apply_ESRGAN_upscale']:
                    del config_json['enlarge_scale']
                    del config_json['apply_ESRGAN_upscale']
                metadata.add_text("config_json", json.dumps(config_json, ensure_ascii=True, indent=4))
            img.save(image_path, pnginfo=metadata)
        if not stable_animation_prefs['display_upscaled_image'] or not stable_animation_prefs['apply_ESRGAN_upscale']:
            prt(Row([ImageButton(src=unscaled_path, data=upscaled_path, width=width, height=height, page=page)], alignment=MainAxisAlignment.CENTER))
        '''if storage_type == "Colab Google Drive":
            new_file = available_file(os.path.join(prefs['image_output'], stable_animation_prefs['batch_folder_name']), fname, num, no_num=True)
            out_path = new_file
            shutil.copy(image_path, new_file)
        elif bool(prefs['image_output']):
            new_file = available_file(os.path.join(prefs['image_output'], stable_animation_prefs['batch_folder_name']), fname, num, no_num=True)
            out_path = new_file
            shutil.copy(image_path, new_file)'''
        prt(Row([Text(image_path)], alignment=MainAxisAlignment.CENTER))
    except Exception as e:
      #clear_last()
      #clear_last()
      alert_msg(page, f"ERROR: Couldn't Stable Animation your image for some reason. Possibly out of memory or something wrong with my code...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
      return
    #clear_last()
    #clear_last()
    save_path = os.path.join(prefs['image_output'], stable_animation_prefs['batch_folder_name'])
    #filename = f"{prefs['file_prefix']}{format_filename(prompts[0])}"
    #filename = filename[:int(prefs['file_max_length'])]
    #autoscroll(True)
    progress = Container(content=None)
    try:
        new_balance, profile_picture = context.get_user_info()
    except:
        alert_msg(page, "Error getting User Info")
        return
    used_balance = balance - new_balance
    cost = f"${round(used_balance * 0.01, 2)}"
    prt(f"🫰  Done Generating Animation... Used {round(used_balance, 2)} Credits Total ~ {cost}")
    if stable_animation_prefs['export_to_video']:
        prt("Exporting Frames to Video")
        #from diffusers.utils import export_to_video
        #video_path = export_to_video(frames)
        #local_file = available_file(local_output, filename, 0, ext="mp4", no_num=True)
        save_file = available_file(batch_output, filename, 0, ext="mp4", no_num=True)
        create_video_from_frames(batch_output, save_file, fps=stable_animation_prefs['output_fps'])
        #imageio.mimsave(local_file, frames, fps=4)
        #shutil.copy(local_file, save_file)
        clear_last()
        try:
            prt(Row([VideoContainer(save_file)], alignment=MainAxisAlignment.CENTER))
            #prt(VideoPlayer(save_file, width, height))
        except Exception as e:
            print(f"Error showing VideoPlayer: {e}")
            pass
    autoscroll(False)
    if prefs['enable_sounds']: page.snd_alert.play()

def run_roop(page):
    global roop_prefs, status
    def prt(line):
      if type(line) == str:
        line = Text(line)
      page.Roop.controls.append(line)
      page.Roop.update()
    def clear_last():
      del page.Roop.controls[-1]
      page.Roop.update()
    def autoscroll(scroll=True):
        page.Roop.auto_scroll = scroll
        page.Roop.update()
    if not bool(roop_prefs['source_image']) or not bool(roop_prefs['target_image']):
        alert_msg(page, "You must provide a source image and target video or image...")
        return
    page.Roop.controls = page.Roop.controls[:1]
    autoscroll()
    installer = Installing("Installing ROOP Libraries...")
    prt(installer)
    roop_dir = os.path.join(root_dir, "roop")
    if not os.path.exists(roop_dir):
        try:
            installer.set_details("...cloning s0md3v/roop.git")
            run_process("git clone https://github.com/s0md3v/roop.git", cwd=root_dir)
            installer.set_details("...installing requirements")
            #run_process("pip install -r requirements.txt", cwd=roop_dir)
            installer.set_details("...downloading roop inswapper")
            download_file("https://huggingface.co/camenduru/roop/resolve/main/inswapper_128.onnx", to=roop_dir)
        except Exception as e:
            clear_last()
            alert_msg(page, "Error Installing Point-E Requirements", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            return
    try:
        import ffmpeg
    except ImportError as e:
        installer.set_details("...installing ffmpeg")
        run_sp("pip install -q ffmpeg", realtime=False)
        pass
    try:
        import cv2
    except ModuleNotFoundError:
        run_sp("pip install opencv-python", realtime=False)
        import cv2
        pass
    try:
        import onnx
    except ImportError as e:
        installer.set_details("...installing onnx")
        run_sp("pip install -q onnx==1.14.0", realtime=False)
        pass
    try:
        import insightface
    except ImportError as e:
        installer.set_details("...installing insightface")
        run_sp("pip install -q insightface==0.7.3", realtime=False)
        pass
    try:
        import tk
    except ImportError as e:
        installer.set_details("...installing tk")
        run_sp("pip install -q tk==0.1.0", realtime=False)
        pass
    try:
        import customtkinter
    except ImportError as e:
        installer.set_details("...installing customtkinter")
        run_sp("pip install -q customtkinter==5.1.3", realtime=False)
        pass
    try:
        import onnxruntime
    except ImportError as e:
        installer.set_details("...installing onnxruntime")
        run_sp("pip install -q onnxruntime-gpu==1.15.0", realtime=False)
        pass
    try:
        import opennsfw2
    except ImportError as e:
        installer.set_details("...installing opennsfw2")
        run_sp("pip install -q opennsfw2==0.10.2", realtime=False)
        try:
            import protobuf
        except ImportError as e:
            installer.set_details("...installing protobuf") #3.20.*
            run_sp("pip install -q protobuf==4.23.2", realtime=False)
            pass
        pass
    try:
        import gfpgan
    except ImportError as e:
        installer.set_details("...installing gfpgan")
        run_sp("pip install -q gfpgan", realtime=False)
        pass
    clear_pipes()

    from PIL import ImageOps
    if bool(roop_prefs['output_name']):
        fname = format_filename(roop_prefs['output_name'], force_underscore=True)
    elif bool(roop_prefs['batch_folder_name']):
        fname = format_filename(roop_prefs['batch_folder_name'], force_underscore=True)
    else:
        fname = "output"
    #TODO: Add prefix
    if bool(roop_prefs['batch_folder_name']):
        batch_output = os.path.join(stable_dir, roop_prefs['batch_folder_name'])
    else:
        batch_output = stable_dir
    if not os.path.exists(batch_output):
        os.makedirs(batch_output)
    #.rpartition(slash)[0], 'roop'
    output_path = os.path.join(prefs['image_output'], roop_prefs['batch_folder_name'])
    if not os.path.exists(output_path):
        os.makedirs(output_path)
    init_img = None
    if roop_prefs['source_image'].startswith('http'):
        installer.set_details("...downloading source image")
        init_img = PILImage.open(requests.get(roop_prefs['source_image'], stream=True).raw)
    else:
        if os.path.isfile(roop_prefs['source_image']):
            init_img = PILImage.open(roop_prefs['source_image'])
        else:
            alert_msg(page, f"ERROR: Couldn't find your source_image {roop_prefs['source_image']}")
            return
    source_name = roop_prefs['source_image'].rpartition("/")[2] if "/" in roop_prefs['source_image'] else roop_prefs['source_image'].rpartition(slash)[2]
    source_path = ""
    if init_img != None:
        installer.set_details("...resizing source image")
        width, height = init_img.size
        width, height = scale_dimensions(width, height, roop_prefs['max_size'])
        init_img = init_img.resize((width, height), resample=PILImage.Resampling.LANCZOS)
        init_img = ImageOps.exif_transpose(init_img).convert("RGB")
        source_path = os.path.join(batch_output, f'{source_name.rpartition(".")[0]}.png')
        init_img.save(source_path)
    is_video = roop_prefs['target_image'].endswith('mp4') or roop_prefs['target_image'].endswith('avi')

    target_img = None
    target_name = roop_prefs['target_image'].rpartition("/")[2] if "/" in roop_prefs['target_image'] else roop_prefs['target_image'].rpartition(slash)[2]
    target_path = ""
    if roop_prefs['target_image'].startswith('http'):
        if is_video:
            installer.set_details("...downloading target video")
            download_file(roop_prefs['target_image'], batch_output)
            target_path = os.path.join(batch_output, target_name)
        else:
            installer.set_details("...downloading target image")
            target_img = PILImage.open(requests.get(roop_prefs['target_image'], stream=True).raw)
    else:
        if os.path.isfile(roop_prefs['target_image']):
            if is_video:
                target_path = roop_prefs['target_image']
            else:
                target_img = PILImage.open(roop_prefs['target_image'])
        else:
            alert_msg(page, f"ERROR: Couldn't find your target_image {roop_prefs['target_image']}")
            return
    if target_img != None:
        installer.set_details("...resizing target image")
        width, height = target_img.size
        width, height = scale_dimensions(width, height, roop_prefs['max_size'])
        target_img = target_img.resize((width, height), resample=PILImage.Resampling.LANCZOS)
        target_img = ImageOps.exif_transpose(target_img).convert("RGB")
        target_path = os.path.join(batch_output, f'{target_name.rpartition(".")[0]}.png')
        target_img.save(target_path)
    clear_last()
    progress = ProgressBar(bar_height=8)
    prt(f"Generating your ROOP Face-Swap {'Video' if is_video else 'Image'}...")
    prt(progress)
    autoscroll(False)
    total_steps = 100 #?
    def callback_fnc(step: int) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}"
      progress.update()
    output_file = available_file(output_path, fname, 0, ext='mp4' if is_video else 'png')
    #output_file = os.path.join(output_path, f"{fname}{'.mp4' if is_video else '.png'}")
    cmd = f'python run.py -s "{source_path}" -t "{target_path}" -o "{output_file}"'
    cmd += f" --frame-processor {roop_prefs['frame_processor']}"
    cmd += f" --execution-provider {torch_device}"
    if roop_prefs['keep_fps'] and is_video: cmd += " --keep-fps"
    if roop_prefs['keep_audio'] and is_video: cmd += " --keep-audio"
    if roop_prefs['keep_frames'] and is_video: cmd += " --keep-frames"
    if roop_prefs['many_faces']: cmd += " --many-faces"
    if is_video:
        cmd += f" --video-encoder {roop_prefs['video_encoder']}"
        cmd += f" --video-quality {roop_prefs['video_quality']}"
    #--max-menory
    prt(f"Running {cmd}")
    try:
        run_process(cmd, cwd=roop_dir, page=page, realtime=True)
    except Exception as e:
        clear_last()
        clear_last()
        alert_msg(page, "Error running Python.", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))
        return
    clear_last()
    clear_last()
    clear_last()
    autoscroll(True)
    #TODO: Upscale Image
    if os.path.isfile(output_file):
        if is_video:
            prt(Row([VideoContainer(output_file)], alignment=MainAxisAlignment.CENTER))
            #prt(Row([VideoPlayer(video_file=output_file, width=width, height=height)], alignment=MainAxisAlignment.CENTER))
        else:
            prt(Row([ImageButton(src=output_file, data=output_file, width=width, height=height, page=page)], alignment=MainAxisAlignment.CENTER))
    else:
        prt("Error Generating Output File! A NSFW Image may have been detected.")
    prt(Row([Text(output_file)], alignment=MainAxisAlignment.CENTER))
    autoscroll(False)
    if prefs['enable_sounds']: page.snd_alert.play()

def run_animate_diff(page):
    global animate_diff_prefs, prefs, status, pipe_animate_diff, model_path
    #if not status['installed_diffusers']:
    #  alert_msg(page, "You need to Install HuggingFace Diffusers before using...")
    #  return
    def prt(line):
      if type(line) == str:
        line = Text(line, size=17)
      page.AnimateDiff.controls.append(line)
      page.AnimateDiff.update()
    def clear_last():
      del page.AnimateDiff.controls[-1]
      page.AnimateDiff.update()
    def autoscroll(scroll=True):
      page.AnimateDiff.auto_scroll = scroll
      page.AnimateDiff.update()
    def clear_list():
      page.AnimateDiff.controls = page.AnimateDiff.controls[:1]
    progress = ProgressBar(bar_height=8)
    total_steps = animate_diff_prefs['steps']
    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep}"
      progress.update()
      #print(f'{type(latents)} {len(latents)}- {str(latents)}')
    clear_list()
    autoscroll(True)
    installer = Installing("Installing AnimateDiff Requirements...")
    prt(installer)
    animatediff_dir = os.path.join(root_dir, 'animatediff-cli')
    if not os.path.exists(animatediff_dir):
        installer.set_details("...clone guoyww/animatediff")
        run_sp("git clone https://github.com/Skquark/animatediff-cli", realtime=False, cwd=root_dir) #/neggles
        run_sp("git lfs install", cwd=animatediff_dir, realtime=False)
    os.chdir(animatediff_dir)
    try:
        import diffusers
    except ModuleNotFoundError:
        installer.set_details("...installing diffusers")
        run_process("pip install --upgrade git+https://github.com/Skquark/diffusers.git@main#egg=diffusers[torch]", page=page)
        pass
    try:
        import transformers
    except ModuleNotFoundError:
        installer.set_details("...installing transformers")
        run_sp("pip install --upgrade transformers==4.30.2", realtime=False) #4.28
        pass
    try:
        import omegaconf
    except Exception:
        installer.set_details("...installing omegaconf & einops")
        run_sp("pip install omegaconf einops cmake", realtime=False)
        pass
    try:
        import colorama
    except Exception:
        installer.set_details("...installing colorama, rich, ninja")
        run_sp("pip install colorama rich ninja pydantic shellingham typer gdown", realtime=False) #=11.3
        pass
    try:
        import black
    except Exception:
        installer.set_details("...installing black, ruff, setuptools-scm")
        run_sp("pip install black ruff setuptools-scm", realtime=False)
        pass
    #if prefs['memory_optimization'] == 'Xformers Mem Efficient Attention':
    try:
        import xformers
    except ModuleNotFoundError:
        installer.set_details("...installing FaceBook's Xformers")
        #run_sp("pip install --pre -U triton", realtime=False)
        run_sp("pip install -U xformers", realtime=False)
        status['installed_xformers'] = True
        pass
    try:
        import imageio
    except Exception:
        installer.set_details("...installing imageio")
        run_sp("pip install imageio==2.27.0", realtime=False)
        pass
    try:
        import ffmpeg
    except ImportError as e:
        installer.set_details("...installing ffmpeg")
        run_sp("pip install -q ffmpeg-python", realtime=False)
        pass
    try:
        import sentencepiece
    except Exception:
        installer.set_details("...installing sentencepiece")
        run_sp("pip install sentencepiece>=0.1.99", realtime=False)
        pass
    try:
        import safetensors
    except ModuleNotFoundError:
        installer.set_details("...installing safetensors")
        run_process("pip install --upgrade safetensors", page=page)
        import safetensors
        from safetensors import safe_open
        pass
    try:
        from animatediff.cli import generate
    except ModuleNotFoundError:
        try:
            installer.set_details("...installing AnimateDiff Requirements")
            run_sp("pip install -e .[dev]", cwd=animatediff_dir, realtime=False) #'.[dev]'
        except Exception as e:
            clear_last()
            alert_msg(page, f"ERROR: Couldn't Install AnimateDiff Requirements for some reason...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            return
    try:
        import watchdog
    except ImportError as e:
        installer.set_details("...installing watchdog")
        run_sp("pip install -q watchdog", realtime=False)
        pass
    if animate_diff_prefs['save_video']:
        import platform
        rife_dir = os.path.join(animatediff_dir, 'data', 'rife')
        if len(os.listdir(rife_dir)) <= 1:
            installer.set_details("...downloading RiFE")
            if platform.system() == 'Linux':
                rife_zip = download_file("https://github.com/nihui/rife-ncnn-vulkan/releases/download/20221029/rife-ncnn-vulkan-20221029-ubuntu.zip")
            elif platform.system() == 'Windows':
                rife_zip = download_file("https://github.com/nihui/rife-ncnn-vulkan/releases/download/20221029/rife-ncnn-vulkan-20221029-windows.zip")
            elif platform.system() == 'Darwin':
                rife_zip = download_file("https://github.com/nihui/rife-ncnn-vulkan/releases/download/20221029/rife-ncnn-vulkan-20221029-macos.zip")
            installer.set_details("...extracting RiFE")
            shutil.unpack_archive(rife_zip, rife_dir)
            os.remove(rife_zip)
            for folder_name in os.listdir(rife_dir):
                if os.path.isdir(os.path.join(rife_dir, folder_name)):
                    rife_folder = os.path.join(rife_dir, folder_name)
                    break
            for file_name in os.listdir(rife_folder):
                shutil.move(os.path.join(rife_folder, file_name), rife_dir)
            run_sp(f"chmod 755 {rife_dir}", realtime=False)
            run_sp(f"chmod 755 {os.path.join(rife_dir, 'rife-ncnn-vulkan')}", realtime=False)
            os.chmod(os.path.join(rife_dir, 'rife-ncnn-vulkan'), 0o777)
            run_sp("apt-get install libvulkan-dev", realtime=False)
    from pathlib import Path
    output_path = os.path.join(prefs['image_output'], animate_diff_prefs['batch_folder_name'])
    if not os.path.exists(output_path):
        os.makedirs(output_path)
    #run_sp("apt -y install -qq aria2", realtime=False)
    sd_models = os.path.join(animatediff_dir, 'data', 'models', 'StableDiffusion')
    motion_module = os.path.join(animatediff_dir, 'data', 'models', 'motion-module')
    if not os.path.isdir(motion_module):
        os.makedirs(motion_module)
    run_sp(f"rm -rf {sd_models}", realtime=False)
    installer.set_details("...downloading stable-diffusion-v1-5")
    run_sp(f"git clone -b fp16 https://huggingface.co/runwayml/stable-diffusion-v1-5 {sd_models}", realtime=False, cwd=root_dir)
    #if animate_diff_prefs['motion_module'] == 'mm_sd_v14':
    #installer.set_details("...downloading motion_module-v1-4")
    #download_file("https://huggingface.co/guoyww/AnimateDiff/resolve/main/mm_sd_v14.ckpt", to=motion_module)
    #if animate_diff_prefs['motion_module'] == 'mm_sd_v15':
    #installer.set_details("...downloading motion_module-v1-5")
    #download_file("https://huggingface.co/guoyww/AnimateDiff/resolve/main/mm_sd_v15.ckpt", to=motion_module)
    #sd_models = "runwayml/stable-diffusion-v1-5"
    lora_model = {'name': 'None', 'file': '', 'path': '', 'weight_name': None}
    lora_dir = os.path.join(animatediff_dir, 'data', 'models', 'sd')
    #lora_dir = os.path.join(animatediff_dir, 'models', 'DreamBooth_LoRA')
    if not os.path.isdir(lora_dir):
        os.makedirs(lora_dir)
    lora_path = ""
    if animate_diff_prefs['dreambooth_lora'] == "Custom":
        lora = animate_diff_prefs['custom_lora']
        if lora.startswith("http"):
            installer.set_details(f"...downloading Custom LoRA")
            lora_file = download_file(lora_model['path'], to=lora_dir)
            if os.path.isfile(lora_file):
                lora_path = lora_file
        else:
            if os.path.isfile(lora):
                lora_path = lora
    elif animate_diff_prefs['dreambooth_lora'] != "None":
        for lora in animate_diff_loras:
            if lora['name'] == animate_diff_prefs['dreambooth_lora']:
                lora_model = lora
                break
        lora_path = os.path.join(lora_dir, lora_model['file'])
        if not os.path.isfile(lora_path):
            installer.set_details(f"...downloading {lora_model['name']}")
            download_file(lora_model['path'], to=lora_dir)
            #run_sp(f"aria2c --console-log-level=error -c -x 16 -s 16 -k 1M {lora_model['path']} -d {lora_dir} -o {lora_model['file']}", realtime=False)
    if bool(lora_path):
        fname = lora_path.rpartition(slash)[2]
        lora_path = f"models{slash}sd{slash}{fname}"
    clear_pipes()
    clear_last()
    prt("Generating AnimateDiff of your Prompts... See console for progress.")
    prt(progress)
    time.sleep(0.5)
    autoscroll(False)
    samples_dir = os.path.join(animatediff_dir, 'samples')
    #batch_output = os.path.join(stable_dir, animate_diff_prefs['batch_folder_name'])
    #if not os.path.isdir(batch_output):
    #  os.makedirs(batch_output)
    batch_output = os.path.join(prefs['image_output'], animate_diff_prefs['batch_folder_name'])
    if not os.path.isdir(batch_output):
      os.makedirs(batch_output)
    editing_prompts = []
    negative_prompts = []
    seeds = []
    if len(animate_diff_prefs['editing_prompts']) == 0:
        if not bool(animate_diff_prefs['prompt']):
            alert_msg(page, "Error: You must provide at least one Prompt to render...")
            return
        else:
            editing_prompts.append(animate_diff_prefs['prompt'])
            negative_prompts.append(animate_diff_prefs['negative_prompt'])
            random_seed = int(animate_diff_prefs['seed']) if int(animate_diff_prefs['seed']) > 0 else rnd.randint(0,4294967295)
            seeds.append(random_seed)
    else:
        for ep in animate_diff_prefs['editing_prompts']:
            editing_prompts.append(ep['prompt'])
            negative_prompts.append(ep['negative_prompt'])
            random_seed = int(ep['seed']) if int(ep['seed']) > 0 else rnd.randint(0,4294967295)
            seeds.append(random_seed)
    prompts_json = {
        'name': 'SDD',
        'base': "",
        'path': lora_path,
        'motion_module': f"models{slash}motion-module{slash}{animate_diff_prefs['motion_module']}.ckpt",
        #'motion_module': os.path.join(motion_module, f"{animate_diff_prefs['motion_module']}.ckpt"),
        'seed': seeds,
        'scheduler': animate_diff_prefs['scheduler'],
        'steps': int(animate_diff_prefs['steps']),
        'guidance_scale': float(animate_diff_prefs['guidance_scale']),
        #'lora_alpha': float(animate_diff_prefs['lora_alpha']),
        'prompt': editing_prompts,
        'n_prompt': negative_prompts,
    }
    #if bool(lora_path):
    #    prompts_json['lora_scale'] = animate_diff_prefs['lora_alpha']
    prompts_yaml = f'''
NewModel:
  path: "{lora_path}"
  base: ""

  motion_module:
    - "{os.path.join(motion_module, "mm_sd_v14.ckpt")}
    - "{os.path.join(motion_module, "mm_sd_v15.ckpt")}

  seeds:          [{", ".join(map(str, seeds))}]
  steps:          {int(animate_diff_prefs['steps'])}
  guidance_scale: {float(animate_diff_prefs['guidance_scale'])}

  prompt:
'''
    for p in editing_prompts:
        prompts_yaml += f'''
    - "{p}"'''
    prompts_yaml += f'''
  n_prompt:'''
    for n in negative_prompts:
        prompts_yaml += f'''
    - "{n}"'''
    yaml_file = os.path.join(animatediff_dir, "config", "prompts", "prompt.yaml")
    json_file = os.path.join(animatediff_dir, "config", "prompts", "sdd_prompt.json")
    cli_file = os.path.join(animatediff_dir, "src", "animatediff")
    out_dir = os.path.join(animatediff_dir, "output")
    prompts_file = open(yaml_file, "w")
    prompts_file.write(prompts_yaml)#(readme_text)
    prompts_file.close()
    with open(json_file, "w") as outfile:
        json.dump(prompts_json, outfile, indent=4)
    context = min(animate_diff_prefs['video_length'], animate_diff_prefs['context'])
    #cmd = f"python -m scripts.animate --config {yaml_file} --pretrained_model_path {os.path.join(sd_models, 'stable-diffusion-v1-5')}"
    cmd2 = f"python -m cli --config-path {json_file} --pretrained_model_path {os.path.join(sd_models, 'stable-diffusion-v1-5')}"
    cmd2 += f" --L {animate_diff_prefs['video_length']} --W {animate_diff_prefs['width']} --H {animate_diff_prefs['height']}"
    cmd = f"animatediff generate --config-path {json_file} --model-path {sd_models}"
    cmd += f" -L {animate_diff_prefs['video_length']} -W {animate_diff_prefs['width']} -H {animate_diff_prefs['height']} -C {context} -S {animate_diff_prefs['stride']}"
    if animate_diff_prefs['save_gif']:
        cmd += " --save-merged"
    #cmd += f" -O {animate_diff_prefs['overlap']}"
    w = 0
    h = 0
    output_dir = ""
    output_dirs = []
    img_idx = 0
    from watchdog.observers import Observer
    from watchdog.events import LoggingEventHandler, FileSystemEventHandler
    class Handler(FileSystemEventHandler):
      def __init__(self):
        super().__init__()
      def on_created(self, event):
        nonlocal img_idx, w, h, output_dir
        if event.is_directory:
          output_dir = event.src_path
          output_dirs.append(event.src_path)
          return None
        elif event.event_type == 'created' and (event.src_path.endswith("png") or event.src_path.endswith("gif")):
          autoscroll(True)
          if w == 0:
            time.sleep(0.8)
            try:
              frame = PILImage.open(event.src_path)
              w, h = frame.size
              clear_last()
            except Exception:
              pass
          clear_last()
          if animate_diff_prefs['save_frames']:
            fpath = os.path.join(output_path, event.src_path.rpartition(slash)[2])
          else:
            fpath = event.src_path
          #prt(Divider(height=6, thickness=2))
          prt(Row([ImageButton(src=event.src_path, data=fpath, width=w, height=h, subtitle=f"Frame {img_idx} - {event.src_path}", center=True, page=page)], alignment=MainAxisAlignment.CENTER))
          prt(Row([Text(f'{event.src_path}')], alignment=MainAxisAlignment.CENTER))
          page.update()
          prt(progress)
          if animate_diff_prefs['save_frames']:
            fpath = os.path.join(output_path, event.src_path.rpartition(slash)[2])
            shutil.copy(event.src_path, fpath)
          time.sleep(0.2)
          autoscroll(False)
          img_idx += 1
    image_handler = Handler()
    observer = Observer()
    observer.schedule(image_handler, out_dir, recursive=True)
    observer.start()
    #prt(f"Running {cmd}")
    console = RunConsole(show_progress=False)
    prt(console)
    '''try:
        #from animatediff import get_dir
        from animatediff.cli import generate, logger
    except ModuleNotFoundError:
        prt(f"Running {cmd}")
        pass'''
    try:
      '''try:
          from animatediff.cli import generate, logger
          from animatediff import get_dir
          os.chdir(animatediff_dir)
          config_dir = get_dir("config")
          config_path = config_dir.joinpath("prompts/sdd_prompt.json")
          out_dir = generate(
              config_path=config_path,#json_file),
              #model_name_or_path=Path("models/StableDiffusion"),#sd_models),
              width=animate_diff_prefs['width'],
              height=animate_diff_prefs['height'],
              length=animate_diff_prefs['video_length'],
              context=context,
              stride=animate_diff_prefs['stride'],
              save_merged=animate_diff_prefs['save_gif'],
              use_xformers=status['installed_xformers'],
          )
      except ModuleNotFoundError:'''
      print(f"Running {cmd}")
      #run_sp(cmd, cwd=animatediff_dir, realtime=True)
      #    pass
      console.run_process(cmd, cwd=animatediff_dir)
      #run_sp(cmd, cwd=animatediff_dir, realtime=True)
      #print(f"prompt={animate_diff_prefs['prompt']}, negative_prompt={animate_diff_prefs['negative_prompt']}, editing_prompt={editing_prompt}, edit_warmup_steps={edit_warmup_steps}, edit_guidance_scale={edit_guidance_scale}, edit_threshold={edit_threshold}, edit_weights={edit_weights}, reverse_editing_direction={reverse_editing_direction}, edit_momentum_scale={animate_diff_prefs['edit_momentum_scale']}, edit_mom_beta={animate_diff_prefs['edit_mom_beta']}, steps={animate_diff_prefs['steps']}, eta={animate_diff_prefs['eta']}, guidance_scale={animate_diff_prefs['guidance_scale']}")
      #images = pipe_animate_diff(prompt=animate_diff_prefs['prompt'], negative_prompt=animate_diff_prefs['negative_prompt'], editing_prompt=editing_prompts, edit_warmup_steps=edit_warmup_steps, edit_guidance_scale=edit_guidance_scale, edit_threshold=edit_threshold, edit_weights=edit_weights, reverse_editing_direction=reverse_editing_direction, edit_momentum_scale=animate_diff_prefs['edit_momentum_scale'], edit_mom_beta=animate_diff_prefs['edit_mom_beta'], steps=animate_diff_prefs['steps'], eta=animate_diff_prefs['eta'], guidance_scale=animate_diff_prefs['guidance_scale'], width=width, height=height, num_images_per_prompt=animate_diff_prefs['num_images'], generator=generator, callback=callback_fnc, callback_steps=1).images
    except Exception as e:
      clear_last()
      observer.stop()
      alert_msg(page, f"ERROR: Couldn't run AnimateDiff for some reason.  Possibly out of memory or something wrong with my code...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
      return
    clear_last()
    #clear_last()
    time.sleep(3)
    observer.stop()
    #filename = f"{format_filename(editing_prompts[0]['prompt'])}"
    #filename = filename[:int(prefs['file_max_length'])]
    #if prefs['file_suffix_seed']: filename += f"-{random_seed}"
    autoscroll(True)
    if animate_diff_prefs['save_video']:
        class VidHandler(FileSystemEventHandler):
          def __init__(self):
            super().__init__()
          def on_created(self, event):
            nonlocal w, h, output_dir
            if event.event_type == 'created' and (event.src_path.endswith("mp4") or event.src_path.endswith("avi")):
              autoscroll(True)
              #clear_last()
              prt(Divider(height=6, thickness=2))
              fpath = os.path.join(output_path, event.src_path.rpartition(slash)[2])
              time.sleep(1)
              shutil.copy(event.src_path, fpath)
              prt(Row([VideoContainer(event.src_path)], alignment=MainAxisAlignment.CENTER))
              #prt(Row([VideoPlayer(video_file=event.src_path, width=w, height=h)], alignment=MainAxisAlignment.CENTER))
              #prt(Row([ImageButton(src=event.src_path, data=fpath, width=w, height=h, subtitle=f"Frame {img_idx} - {event.src_path}", center=True, page=page)], alignment=MainAxisAlignment.CENTER))
              #prt(Row([Text(f'{event.src_path}')], alignment=MainAxisAlignment.CENTER))
              page.update()
              #prt(progress)
              time.sleep(0.2)
              autoscroll(False)
        video_handler = VidHandler()
        vidobserver = Observer()
        vidobserver.schedule(video_handler, out_dir, recursive=True)
        vidobserver.start()
        for dir in output_dirs:
            if len(os.listdir(dir)) < 4:
                continue
            interpolate_cmd = f"animatediff rife interpolate --temporal-tta --uhd {Path(dir)}" #--out_file --codec VideoCodec.h264
            try:
              run_sp(interpolate_cmd, cwd=animatediff_dir, realtime=True)
              #print(f"prompt={animate_diff_prefs['prompt']}, negative_prompt={animate_diff_prefs['negative_prompt']}, editing_prompt={editing_prompt}, edit_warmup_steps={edit_warmup_steps}, edit_guidance_scale={edit_guidance_scale}, edit_threshold={edit_threshold}, edit_weights={edit_weights}, reverse_editing_direction={reverse_editing_direction}, edit_momentum_scale={animate_diff_prefs['edit_momentum_scale']}, edit_mom_beta={animate_diff_prefs['edit_mom_beta']}, steps={animate_diff_prefs['steps']}, eta={animate_diff_prefs['eta']}, guidance_scale={animate_diff_prefs['guidance_scale']}")
              #images = pipe_animate_diff(prompt=animate_diff_prefs['prompt'], negative_prompt=animate_diff_prefs['negative_prompt'], editing_prompt=editing_prompts, edit_warmup_steps=edit_warmup_steps, edit_guidance_scale=edit_guidance_scale, edit_threshold=edit_threshold, edit_weights=edit_weights, reverse_editing_direction=reverse_editing_direction, edit_momentum_scale=animate_diff_prefs['edit_momentum_scale'], edit_mom_beta=animate_diff_prefs['edit_mom_beta'], steps=animate_diff_prefs['steps'], eta=animate_diff_prefs['eta'], guidance_scale=animate_diff_prefs['guidance_scale'], width=width, height=height, num_images_per_prompt=animate_diff_prefs['num_images'], generator=generator, callback=callback_fnc, callback_steps=1).images
            except Exception as e:
              #clear_last()
              alert_msg(page, f"ERROR: Couldn't interpolate video from AnimateDiff for some reason...", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
              return
        time.sleep(2)
        vidobserver.stop()
    '''
    num = 0
    for image in images:
        random_seed += num
        fname = filename + (f"-{random_seed}" if prefs['file_suffix_seed'] else "")
        image_path = available_file(os.path.join(stable_dir, animate_diff_prefs['batch_folder_name']), fname, num)
        unscaled_path = image_path
        output_file = image_path.rpartition(slash)[2]
        image.save(image_path)
        out_path = image_path.rpartition(slash)[0]
        upscaled_path = os.path.join(out_path, output_file)
        if not animate_diff_prefs['display_upscaled_image'] or not animate_diff_prefs['apply_ESRGAN_upscale']:
            prt(Row([ImageButton(src=unscaled_path, data=upscaled_path, width=width, height=height, page=page)], alignment=MainAxisAlignment.CENTER))
        if animate_diff_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
            os.chdir(os.path.join(dist_dir, 'Real-ESRGAN'))
            upload_folder = 'upload'
            result_folder = 'results'
            if os.path.isdir(upload_folder):
                shutil.rmtree(upload_folder)
            if os.path.isdir(result_folder):
                shutil.rmtree(result_folder)
            os.mkdir(upload_folder)
            os.mkdir(result_folder)
            short_name = f'{fname[:80]}-{num}.png'
            dst_path = os.path.join(dist_dir, 'Real-ESRGAN', upload_folder, short_name)
            #print(f'Moving {fpath} to {dst_path}')
            #shutil.move(fpath, dst_path)
            shutil.copy(image_path, dst_path)
            #faceenhance = ' --face_enhance' if animate_diff_prefs["face_enhance"] else ''
            faceenhance = ''
            run_sp(f'python inference_realesrgan.py -n realesr-general-x4v3 -i upload --outscale {animate_diff_prefs["enlarge_scale"]}{faceenhance}', cwd=os.path.join(dist_dir, 'Real-ESRGAN'), realtime=False)
            out_file = short_name.rpartition('.')[0] + '_out.png'
            shutil.move(os.path.join(dist_dir, 'Real-ESRGAN', result_folder, out_file), upscaled_path)
            image_path = upscaled_path
            os.chdir(stable_dir)
            if animate_diff_prefs['display_upscaled_image']:
                time.sleep(0.6)
                prt(Row([ImageButton(src=upscaled_path, data=upscaled_path, width=width * float(animate_diff_prefs["enlarge_scale"]), height=height * float(animate_diff_prefs["enlarge_scale"]), page=page)], alignment=MainAxisAlignment.CENTER))
                #prt(Row([Img(src=upscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
        if prefs['save_image_metadata']:
            img = PILImage.open(image_path)
            metadata = PngInfo()
            metadata.add_text("artist", prefs['meta_ArtistName'])
            metadata.add_text("copyright", prefs['meta_Copyright'])
            metadata.add_text("software", "Stable Diffusion Deluxe" + f", upscaled {animate_diff_prefs['enlarge_scale']}x with ESRGAN" if animate_diff_prefs['apply_ESRGAN_upscale'] else "")
            metadata.add_text("pipeline", "AnimateDiff")
            if prefs['save_config_in_metadata']:
              config_json = animate_diff_prefs.copy()
              config_json['model_path'] = model_path
              config_json['scheduler_mode'] = prefs['scheduler_mode']
              config_json['seed'] = random_seed
              del config_json['num_images']
              del config_json['width']
              del config_json['height']
              del config_json['display_upscaled_image']
              del config_json['batch_folder_name']
              if not config_json['apply_ESRGAN_upscale']:
                del config_json['enlarge_scale']
                del config_json['apply_ESRGAN_upscale']
              metadata.add_text("config_json", json.dumps(config_json, ensure_ascii=True, indent=4))
            img.save(image_path, pnginfo=metadata)
        #TODO: PyDrive
        if storage_type == "Colab Google Drive":
            new_file = available_file(os.path.join(prefs['image_output'], animate_diff_prefs['batch_folder_name']), fname, num)
            out_path = new_file
            shutil.copy(image_path, new_file)
        elif bool(prefs['image_output']):
            new_file = available_file(os.path.join(prefs['image_output'], animate_diff_prefs['batch_folder_name']), fname, num)
            out_path = new_file
            shutil.copy(image_path, new_file)
        prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))
        num += 1
        '''
    os.chdir(root_dir)
    autoscroll(False)
    if prefs['enable_sounds']: page.snd_alert.play()

def run_materialdiffusion(page):
    global materialdiffusion_prefs, prefs
    if not bool(materialdiffusion_prefs['material_prompt']):
      alert_msg(page, "You must provide a text prompt to process your material...")
      return
    if not bool(prefs['Replicate_api_key']):
      alert_msg(page, "You must provide your Replicate API Token in Settings to process your material...")
      return
    def prt(line):
      if type(line) == str:
        line = Text(line, size=17)
      page.MaterialDiffusion.controls.append(line)
      page.MaterialDiffusion.update()
    def clear_last():
      del page.MaterialDiffusion.controls[-1]
      page.MaterialDiffusion.update()
    def clear_list():
      page.MaterialDiffusion.controls = page.MaterialDiffusion.controls[:1]
    def autoscroll(scroll=True):
      page.MaterialDiffusion.auto_scroll = scroll
      page.MaterialDiffusion.update()
    progress = ProgressBar(bar_height=8)
    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress
      total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep}"
      progress.update()
      #print(f'{type(latents)} {len(latents)}- {str(latents)}')
    clear_list()
    autoscroll(True)
    '''try:
      run_sp("pip install git+https://github.com/TomMoore515/material_stable_diffusion.git@main#egg=predict", realtime=False)
    except Exception as e:
        print(f"Error: {e}")
        #alert_msg(page, f"Error installing Material Diffusion from TomMoore515...", content=Text(str(e)))
        pass'''
    prt(Installing("Installing Replicate Material Diffusion Pipeline..."))
    try:
        import replicate
    except ModuleNotFoundError as e:
        run_process("pip install replicate -qq", realtime=False)
        import replicate
        pass
    os.environ["REPLICATE_API_TOKEN"] = prefs['Replicate_api_key']
    #export REPLICATE_API_TOKEN=
    try:
        rep_model = replicate.models.get("tommoore515/material_stable_diffusion")
        rep_version = rep_model.versions.get("3b5c0242f8925a4ab6c79b4c51e9b4ce6374e9b07b5e8461d89e692fd0faa449")
    except Exception as e:
        alert_msg(page, f"Seems like your Replicate API Token is Invalid. Check it again...", content=Text(str(e)))
        return
    import requests
    from io import BytesIO
    from PIL import ImageOps
    init_img = None
    if bool(materialdiffusion_prefs['init_image']):
        if materialdiffusion_prefs['init_image'].startswith('http'):
            init_img = PILImage.open(requests.get(materialdiffusion_prefs['init_image'], stream=True).raw)
        else:
            if os.path.isfile(materialdiffusion_prefs['init_image']):
                init_img = PILImage.open(materialdiffusion_prefs['init_image'])
            else:
                alert_msg(page, f"ERROR: Couldn't find your init_image {materialdiffusion_prefs['init_image']}")
                return
        #width, height = init_img.size
        #width, height = scale_dimensions(materialdiffusion_prefs['width'], materialdiffusion_prefs['height'])
        init_img = init_img.resize((materialdiffusion_prefs['width'], materialdiffusion_prefs['height']), resample=PILImage.Resampling.LANCZOS)
        init_img = ImageOps.exif_transpose(init_img).convert("RGB")
    mask_img = None
    if bool(materialdiffusion_prefs['mask_image']):
        if materialdiffusion_prefs['mask_image'].startswith('http'):
            mask_img = PILImage.open(requests.get(materialdiffusion_prefs['mask_image'], stream=True).raw)
        else:
            if os.path.isfile(materialdiffusion_prefs['mask_image']):
                mask_img = PILImage.open(materialdiffusion_prefs['mask_image'])
            else:
                alert_msg(page, f"ERROR: Couldn't find your mask_image {materialdiffusion_prefs['mask_image']}")
                return
            if materialdiffusion_prefs['invert_mask']:
                mask_img = ImageOps.invert(mask_img.convert('RGB'))
                mask_img = mask_img.resize((materialdiffusion_prefs['width'], materialdiffusion_prefs['height']), resample=PILImage.NEAREST)
                mask_img = ImageOps.exif_transpose(mask_img).convert("RGB")
    #print(f'Resize to {width}x{height}')
    clear_pipes()
    clear_last()
    prt("Generating your Material Diffusion Image...")
    prt(progress)
    autoscroll(False)
    random_seed = int(materialdiffusion_prefs['seed']) if int(materialdiffusion_prefs['seed']) > 0 else rnd.randint(0,4294967295)
    #input = {'prompt':materialdiffusion_prefs['material_prompt'], 'width':materialdiffusion_prefs['width'], 'height':materialdiffusion_prefs['height'], 'init_image':init_img, 'mask':mask_img, 'prompt_strength':materialdiffusion_prefs['prompt_strength'], 'num_outputs':materialdiffusion_prefs['num_outputs'], 'num_inference_steps':materialdiffusion_prefs['steps'], 'guidance_scale':materialdiffusion_prefs['guidance_scale'], 'seed':random_seed}
    try:
        if bool(init_img) and bool(mask_img):
            images = rep_version.predict(prompt=materialdiffusion_prefs['material_prompt'], width=materialdiffusion_prefs['width'], height=materialdiffusion_prefs['height'], init_image=init_img, mask=mask_img, prompt_strength=materialdiffusion_prefs['prompt_strength'], num_outputs=materialdiffusion_prefs['num_outputs'], num_inference_steps=materialdiffusion_prefs['steps'], guidance_scale=materialdiffusion_prefs['guidance_scale'], seed=random_seed)
        elif bool(init_img):
            images = rep_version.predict(prompt=materialdiffusion_prefs['material_prompt'], width=materialdiffusion_prefs['width'], height=materialdiffusion_prefs['height'], init_image=init_img, prompt_strength=materialdiffusion_prefs['prompt_strength'], num_outputs=materialdiffusion_prefs['num_outputs'], num_inference_steps=materialdiffusion_prefs['steps'], guidance_scale=materialdiffusion_prefs['guidance_scale'], seed=random_seed)
            #images = version.predict(prompt=materialdiffusion_prefs['material_prompt'], width=materialdiffusion_prefs['width'], height=materialdiffusion_prefs['height'], init_image=init_img if bool(init_img) else "", mask=mask_img if bool(mask_img) else "", prompt_strength=materialdiffusion_prefs['prompt_strength'], num_outputs=materialdiffusion_prefs['num_outputs'], num_inference_steps=materialdiffusion_prefs['steps'], guidance_scale=materialdiffusion_prefs['guidance_scale'], seed=random_seed)
        else:
            images = rep_version.predict(prompt=materialdiffusion_prefs['material_prompt'], width=materialdiffusion_prefs['width'], height=materialdiffusion_prefs['height'], prompt_strength=materialdiffusion_prefs['prompt_strength'], num_outputs=materialdiffusion_prefs['num_outputs'], num_inference_steps=materialdiffusion_prefs['steps'], guidance_scale=materialdiffusion_prefs['guidance_scale'], seed=random_seed)
    except Exception as e:
        clear_last()
        clear_last()
        alert_msg(page, f"ERROR: Couldn't create your image for some reason.  Possibly out of memory or something wrong with my code...", content=Text(str(e)))
        return
    autoscroll(True)
    clear_last()
    clear_last()
    txt2img_output = stable_dir
    batch_output = prefs['image_output']
    #print(str(images))
    if images is None:
        prt(f"ERROR: Problem generating images, check your settings and run above blocks again, or report the error to Skquark if it really seems broken.")
        return
    idx = 0
    for image in images:
        random_seed += idx
        fname = format_filename(materialdiffusion_prefs['material_prompt'])
        seed_suffix = f"-{random_seed}" if bool(prefs['file_suffix_seed']) else ''
        fname = f'{materialdiffusion_prefs["file_prefix"]}{fname}{seed_suffix}'
        txt2img_output = stable_dir
        if bool(materialdiffusion_prefs['batch_folder_name']):
            txt2img_output = os.path.join(stable_dir, materialdiffusion_prefs['batch_folder_name'])
        if not os.path.exists(txt2img_output):
            os.makedirs(txt2img_output)
        image_path = available_file(txt2img_output, fname, 1)
        #image.save(image_path)
        response = requests.get(image, stream=True)
        with open(image_path, "wb") as f:
          f.write(response.content)
        new_file = image_path.rpartition(slash)[2]
        if not materialdiffusion_prefs['display_upscaled_image'] or not materialdiffusion_prefs['apply_ESRGAN_upscale']:
            #prt(Row([Img(src=image_path, width=materialdiffusion_prefs['width'], height=materialdiffusion_prefs['height'], fit=ImageFit.FILL, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
            prt(Row([ImageButton(src=image_path, width=materialdiffusion_prefs['width'], height=materialdiffusion_prefs['height'], page=page)], alignment=MainAxisAlignment.CENTER))

        #if save_to_GDrive:
        batch_output = os.path.join(prefs['image_output'], materialdiffusion_prefs['batch_folder_name'])
        if not os.path.exists(batch_output):
            os.makedirs(batch_output)
        if storage_type == "PyDrive Google Drive":
            newFolder = gdrive.CreateFile({'title': materialdiffusion_prefs['batch_folder_name'], "parents": [{"kind": "drive#fileLink", "id": prefs['image_output']}],"mimeType": "application/vnd.google-apps.folder"})
            newFolder.Upload()
            batch_output = newFolder
        out_path = batch_output# if save_to_GDrive else txt2img_output

        if materialdiffusion_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
            os.chdir(os.path.join(dist_dir, 'Real-ESRGAN'))
            upload_folder = 'upload'
            result_folder = 'results'
            if os.path.isdir(upload_folder):
                shutil.rmtree(upload_folder)
            if os.path.isdir(result_folder):
                shutil.rmtree(result_folder)
            os.mkdir(upload_folder)
            os.mkdir(result_folder)
            short_name = f'{fname[:80]}-{idx}.png'
            dst_path = os.path.join(dist_dir, 'Real-ESRGAN', upload_folder, short_name)
            #print(f'Moving {fpath} to {dst_path}')
            #shutil.move(fpath, dst_path)
            shutil.copy(image_path, dst_path)
            #faceenhance = ' --face_enhance' if materialdiffusion_prefs["face_enhance"] else ''
            faceenhance = ''
            #python inference_realesrgan.py -n RealESRGAN_x4plus -i upload --outscale {enlarge_scale}{faceenhance}
            run_sp(f'python inference_realesrgan.py -n realesr-general-x4v3 -i upload --outscale {materialdiffusion_prefs["enlarge_scale"]}{faceenhance}', cwd=os.path.join(dist_dir, 'Real-ESRGAN'), realtime=False)
            out_file = short_name.rpartition('.')[0] + '_out.png'
            #print(f'move {root_dir}Real-ESRGAN/{result_folder}/{out_file} to {fpath}')
            #shutil.move(f'{root_dir}Real-ESRGAN/{result_folder}/{out_file}', fpath)
            upscaled_path = os.path.join(out_path, new_file)
            shutil.move(os.path.join(dist_dir, 'Real-ESRGAN', result_folder, out_file), upscaled_path)
            # !python inference_realesrgan.py --model_path experiments/pretrained_models/RealESRGAN_x4plus.pth --input upload --netscale 4 --outscale 3.5 --half --face_enhance
            os.chdir(stable_dir)
            if materialdiffusion_prefs['display_upscaled_image']:
                time.sleep(0.6)
                prt(Row([Img(src=upscaled_path, width=materialdiffusion_prefs['width'] * float(materialdiffusion_prefs["enlarge_scale"]), height=materialdiffusion_prefs['height'] * float(materialdiffusion_prefs["enlarge_scale"]), fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
        else:
            try:
              shutil.copy(image_path, os.path.join(out_path, new_file))
            except shutil.SameFileError: pass
        # TODO: Add Metadata
        prt(Row([Text(new_file)], alignment=MainAxisAlignment.CENTER))
    del rep_model, rep_version
    autoscroll(False)
    if prefs['enable_sounds']: page.snd_alert.play()

def run_DiT(page, from_list=False):
    global DiT_prefs, pipe_DiT
    if not status['installed_diffusers']:
      alert_msg(page, "You must Install the HuggingFace Diffusers Library first... ")
      return
    def prt(line, update=True):
      if type(line) == str:
        line = Text(line)
      page.DiT.controls.append(line)
      if update:
        page.DiT.update()
    def clear_last():
      del page.DiT.controls[-1]
      page.DiT.update()
    def clear_list():
      page.DiT.controls = page.DiT.controls[:1]
    def autoscroll(scroll=True):
      page.DiT.auto_scroll = scroll
      page.DiT.update()
    progress = ProgressBar(bar_height=8)
    total_steps = DiT_prefs['num_inference_steps']
    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep}"
      progress.update()
    DiT_prompts = []
    if from_list:
      if len(prompts) < 1:
        alert_msg(page, "You need to add Prompts to your List first... ")
        return
      for p in prompts:
        DiT_prompts.append(p.prompt)
    else:
      if not bool(DiT_prefs['prompt']):
        alert_msg(page, "You need to add a Text Prompt first... ")
        return
      DiT_prompts.append(DiT_prefs['prompt'])
    clear_list()
    autoscroll(True)
    from PIL.PngImagePlugin import PngInfo
    clear_pipes('DiT')
    torch.cuda.empty_cache()
    torch.cuda.reset_max_memory_allocated()
    torch.cuda.reset_peak_memory_stats()
    if pipe_DiT == None:
        from diffusers import DiTPipeline
        prt(Installing("Downloading DiT Pipeline..."))
        try:
            pipe_DiT = DiTPipeline.from_pretrained("facebook/DiT-XL-2-512", torch_dtype=torch.float16 if not prefs['higher_vram_mode'] else torch.float32, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
            pipe_DiT.to(torch_device)
        except Exception as e:
            clear_last()
            alert_msg(page, "Error Downloading DiT Pipeline", content=Text(str(e)))
            return
        pipe_DiT.set_progress_bar_config(disable=True)
        clear_last()
    s = "s" if DiT_prefs['num_images'] > 1 else ""
    prt(f"Generating DiT{s} of your Image...")
    batch_output = os.path.join(stable_dir, DiT_prefs['batch_folder_name'])
    if not os.path.isdir(batch_output):
      os.makedirs(batch_output)
    batch_output = os.path.join(prefs['image_output'], DiT_prefs['batch_folder_name'])
    if not os.path.isdir(batch_output):
      os.makedirs(batch_output)
    for pr in DiT_prompts:
        for num in range(DiT_prefs['num_images']):
            prt(progress)
            autoscroll(False)
            random_seed = (int(DiT_prefs['seed']) + num) if int(DiT_prefs['seed']) > 0 else rnd.randint(0,4294967295)
            generator = torch.Generator(device=torch_device).manual_seed(random_seed)
            words = [w.strip() for w in pr.split(',')]
            try:
                ids = pipe_DiT.get_label_ids(words)
            except ValueError as e:
                clear_last()
                alert_msg(page, "ImageNet Token not found...", content=Text(str(e)))
                pass
            if len(ids) == 0:
                clear_last()
                alert_msg("No ImageNet class phases found in your prompt list. See full 1000 list for classifications.", page)
                return
            try:
                images = pipe_DiT(ids, num_inference_steps=DiT_prefs['num_inference_steps'], guidance_scale=DiT_prefs['guidance_scale'], generator=generator).images #, callback=callback_fnc, callback_steps=1
            except Exception as e:
                clear_last()
                alert_msg(page, "Error running DiT Pipeline", content=Text(str(e)))
                return
            clear_last()
            autoscroll(True)
            fname = format_filename(pr)

            if prefs['file_suffix_seed']: fname += f"-{random_seed}"
            for image in images:
                image_path = available_file(os.path.join(stable_dir, DiT_prefs['batch_folder_name']), fname, num)
                unscaled_path = image_path
                output_file = image_path.rpartition(slash)[2]
                image.save(image_path)
                out_path = image_path.rpartition(slash)[0]
                upscaled_path = os.path.join(out_path, output_file)
                if not DiT_prefs['display_upscaled_image'] or not DiT_prefs['apply_ESRGAN_upscale']:
                    prt(Row([ImageButton(src=unscaled_path, data=upscaled_path, width=512, height=512, page=page)], alignment=MainAxisAlignment.CENTER))
                    #prt(Row([Img(src=unscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
                if DiT_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
                    os.chdir(os.path.join(dist_dir, 'Real-ESRGAN'))
                    upload_folder = 'upload'
                    result_folder = 'results'
                    if os.path.isdir(upload_folder):
                        shutil.rmtree(upload_folder)
                    if os.path.isdir(result_folder):
                        shutil.rmtree(result_folder)
                    os.mkdir(upload_folder)
                    os.mkdir(result_folder)
                    short_name = f'{fname[:80]}-{num}.png'
                    dst_path = os.path.join(dist_dir, 'Real-ESRGAN', upload_folder, short_name)
                    #print(f'Moving {fpath} to {dst_path}')
                    #shutil.move(fpath, dst_path)
                    shutil.copy(image_path, dst_path)
                    #faceenhance = ' --face_enhance' if DiT_prefs["face_enhance"] else ''
                    faceenhance = ''
                    run_sp(f'python inference_realesrgan.py -n realesr-general-x4v3 -i upload --outscale {DiT_prefs["enlarge_scale"]}{faceenhance}', cwd=os.path.join(dist_dir, 'Real-ESRGAN'), realtime=False)
                    out_file = short_name.rpartition('.')[0] + '_out.png'
                    shutil.move(os.path.join(dist_dir, 'Real-ESRGAN', result_folder, out_file), upscaled_path)
                    image_path = upscaled_path
                    os.chdir(stable_dir)
                    if DiT_prefs['display_upscaled_image']:
                        time.sleep(0.6)
                        prt(Row([ImageButton(src=upscaled_path, data=upscaled_path, width=512 * float(DiT_prefs["enlarge_scale"]), height=512 * float(DiT_prefs["enlarge_scale"]), page=page)], alignment=MainAxisAlignment.CENTER))
                        #prt(Row([Img(src=upscaled_path, fit=ImageFit.FIT_WIDTH, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
                if prefs['save_image_metadata']:
                    img = PILImage.open(image_path)
                    metadata = PngInfo()
                    metadata.add_text("artist", prefs['meta_ArtistName'])
                    metadata.add_text("copyright", prefs['meta_Copyright'])
                    metadata.add_text("software", "Stable Diffusion Deluxe" + f", upscaled {DiT_prefs['enlarge_scale']}x with ESRGAN" if DiT_prefs['apply_ESRGAN_upscale'] else "")
                    metadata.add_text("pipeline", "DiT")
                    if prefs['save_config_in_metadata']:
                      metadata.add_text("title", pr)
                      config_json = DiT_prefs.copy()
                      config_json['model_path'] = "facebook/DiT-XL-2-512"
                      config_json['seed'] = random_seed
                      del config_json['num_images']
                      del config_json['display_upscaled_image']
                      del config_json['batch_folder_name']
                      if not config_json['apply_ESRGAN_upscale']:
                        del config_json['enlarge_scale']
                        del config_json['apply_ESRGAN_upscale']
                      metadata.add_text("config_json", json.dumps(config_json, ensure_ascii=True, indent=4))
                    img.save(image_path, pnginfo=metadata)
                #TODO: PyDrive
                if storage_type == "Colab Google Drive":
                    new_file = available_file(os.path.join(prefs['image_output'], DiT_prefs['batch_folder_name']), fname, num)
                    out_path = new_file
                    shutil.copy(image_path, new_file)
                elif bool(prefs['image_output']):
                    new_file = available_file(os.path.join(prefs['image_output'], DiT_prefs['batch_folder_name']), fname, num)
                    out_path = new_file
                    shutil.copy(image_path, new_file)
                time.sleep(0.2)
                prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))
    autoscroll(False)
    if prefs['enable_sounds']: page.snd_alert.play()

def get_dreamfusion(page):
    os.chdir(root_dir)
    run_process("git clone https://github.com/ashawkey/stable-dreamfusion.git -q", page=page)
    os.chdir(os.path.join(root_dir, "stable-dreamfusion"))
    run_process("pip install -r requirements.txt -q", page=page)
    run_process("pip install git+https://github.com/NVlabs/nvdiffrast/ -q", page=page)
    os.chdir(root_dir)

def run_dreamfusion(page):
    global dreamfusion_prefs, status
    def add_to_dreamfusion_output(o):
      page.dreamfusion_output.controls.append(o)
      page.dreamfusion_output.update()
    def clear_last():
      #page.dreamfusion_output.controls = []
      del page.dreamfusion_output.controls[-1]
      page.dreamfusion_output.update()
    if not status['installed_diffusers']:
      alert_msg(page, "You must Install HuggingFace Diffusers Pipeline before running...")
      return
    if not bool(dreamfusion_prefs["prompt_text"].strip()):
      alert_msg(page, "You must enter a simple prompt to generate 3D model from...")
      return
    page.dreamfusion_output.controls = []
    page.dreamfusion_output.update()
    if not status['installed_dreamfusion']:
      add_to_dreamfusion_output(Installing("Installing Stable DreamFusion 3D Pipeline..."))
      get_dreamfusion(page)
      status['installed_dreamfusion'] = True
      clear_last()
    def convert(seconds):
      seconds = seconds % (24 * 3600)
      hour = seconds // 3600
      seconds %= 3600
      minutes = seconds // 60
      seconds %= 60
      return "%d:%02d" % (hour, minutes)
    estimate = convert(int(dreamfusion_prefs["training_iters"] * 0.7))
    add_to_dreamfusion_output(Text("Generating your 3D model, this'll take a while...  Estimating " + estimate))
    add_to_dreamfusion_output(ProgressBar())
    df_path = os.path.join(root_dir, "stable-dreamfusion")
    os.chdir(df_path)
    run_str = f'python main.py -O --text "{dreamfusion_prefs["prompt_text"]}" --workspace {dreamfusion_prefs["workspace"]} --iters {dreamfusion_prefs["training_iters"]} --lr {dreamfusion_prefs["learning_rate"]} --w {dreamfusion_prefs["training_nerf_resolution"]} --h {dreamfusion_prefs["training_nerf_resolution"]} --seed {dreamfusion_prefs["seed"]} --lambda_entropy {dreamfusion_prefs["lambda_entropy"]} --ckpt {dreamfusion_prefs["checkpoint"]} --save_mesh --max_steps {dreamfusion_prefs["max_steps"]}'
    print(run_str)
    torch.cuda.empty_cache()
    try:
      run_process(run_str, page=page)
    except:
      clear_last()
      alert_msg(page, "Error running DreamFusion, probably Out of Memory. Adjust settings & try again.")
      return
    clear_last()
    add_to_dreamfusion_output(Text("Finished generating obj model, texture and video... Hope it's good."))
    df_out = os.path.join(df_path, dreamfusion_prefs["workspace"])
    if storage_type == "Colab Google Drive":
      dreamfusion_out = os.path.join(prefs['image_output'].rpartition(slash)[0], 'dreamfusion_out', dreamfusion_prefs["workspace"])
      #os.makedirs(dreamfusion_out, exist_ok=True)
      if os.path.exists(dreamfusion_out):
        dreamfusion_out = available_folder(os.path.join(prefs['image_output'].rpartition(slash)[0], 'dreamfusion_out'), dreamfusion_prefs["workspace"], 1)
      shutil.copytree(df_out, dreamfusion_out)
      add_to_dreamfusion_output(Text(f"Saved to {dreamfusion_out}"))
    else:
      add_to_dreamfusion_output(Text(f"Saved to {df_out}"))
    # TODO: PyDrive2
    if prefs['enable_sounds']: page.snd_alert.play()
    os.chdir(root_dir)

base_diffusion = upsampler_diffusion = point_sampler = None

def run_point_e(page):
    global point_e_prefs, status, base_diffusion, upsampler_diffusion, point_sampler
    def add_to_point_e_output(o):
      page.point_e_output.controls.append(o)
      page.point_e_output.update()
    def prt(line):
      if type(line) == str:
        line = Text(line)
      page.point_e_output.controls.append(line)
      page.point_e_output.update()
    def prt_status(text):
        nonlocal status_txt
        status_txt.value = text
        status_txt.update()
    def clear_last():
      #page.point_e_output.controls = []
      del page.point_e_output.controls[-1]
      page.point_e_output.update()
    if not status['installed_diffusers']:
      alert_msg(page, "You must Install HuggingFace Diffusers Pipeline before running...")
      return
    if not bool(point_e_prefs["prompt_text"].strip()):
      alert_msg(page, "You must enter a simple prompt to generate 3D model from...")
      return
    page.point_e_output.controls = []
    page.point_e_output.update()
    point_e_dir = os.path.join(root_dir, "point-e")
    if not os.path.exists(point_e_dir):
        add_to_point_e_output(Installing("Installing OpenAI Point-E 3D Library..."))
        try:
            run_process("pip install -U scikit-image")
            run_process("git clone https://github.com/openai/point-e.git", cwd=root_dir)
            run_process("pip install .", cwd=point_e_dir)
        except Exception as e:
            clear_last()
            alert_msg(page, "Error Installing Point-E Requirements", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            return
        clear_last()
    clear_pipes()
    from tqdm.auto import tqdm
    from point_e.diffusion.configs import DIFFUSION_CONFIGS, diffusion_from_config
    from point_e.util.pc_to_mesh import marching_cubes_mesh
    from point_e.diffusion.sampler import PointCloudSampler
    from point_e.models.download import load_checkpoint
    from point_e.models.configs import MODEL_CONFIGS, model_from_config
    from point_e.util.plotting import plot_point_cloud
    from point_e.util.point_cloud import PointCloud
    import skimage.measure
    from PIL import ImageOps

    if bool(point_e_prefs['batch_folder_name']):
        fname = format_filename(point_e_prefs['batch_folder_name'], force_underscore=True)
    else:
        if bool(point_e_prefs['prompt_text']):
            fname = format_filename(point_e_prefs['prompt_text'])
        else:
            alert_msg(page, "If you're not using Prompt Text, provide a name for your 3D Model.")
            return
    filename = format_filename(point_e_prefs['prompt_text'])
    #fname = f"{point_e_prefs['file_prefix']}{fname}"
    if bool(point_e_prefs['batch_folder_name']):
        point_e_out = os.path.join(point_e_dir, point_e_prefs['batch_folder_name'])
    else:
        point_e_out = point_e_dir
    if not os.path.exists(point_e_out):
        os.makedirs(point_e_out)
    #point_e_out = os.path.join(point_e_out, fname)
    #estimate = convert(int(point_e_prefs["training_iters"] * 0.7))
    init_img = None
    if bool(point_e_prefs['init_image']):
        if point_e_prefs['init_image'].startswith('http'):
            init_img = PILImage.open(requests.get(point_e_prefs['init_image'], stream=True).raw)
        else:
            if os.path.isfile(point_e_prefs['init_image']):
                init_img = PILImage.open(point_e_prefs['init_image'])
            else:
                alert_msg(page, f"ERROR: Couldn't find your init_image {point_e_prefs['init_image']}")
                if not bool(point_e_prefs['prompt_text']):
                    return
        if init_img != None:
            width, height = init_img.size
            width, height = scale_dimensions(width, height, 960)
            init_img = init_img.resize((width, height), resample=PILImage.Resampling.LANCZOS)
            init_img = ImageOps.exif_transpose(init_img).convert("RGB")

    status_txt = Text("Generating your 3D model, may take a while...")
    progress = ProgressBar(bar_height=8)
    total_steps = 130
    def callback_fnc(step: int) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}"
      progress.update()
    add_to_point_e_output(status_txt)
    add_to_point_e_output(progress)
    base_name = point_e_prefs['base_model']
    base_model = model_from_config(MODEL_CONFIGS[base_name], torch_device)
    base_model.eval()
    base_diffusion = diffusion_from_config(DIFFUSION_CONFIGS[base_name])

    prt_status('Creating Upsample model...')
    upsampler_model = model_from_config(MODEL_CONFIGS['upsample'], torch_device)
    upsampler_model.eval()
    upsampler_diffusion = diffusion_from_config(DIFFUSION_CONFIGS['upsample'])

    prt_status('Downloading Base Checkpoint...')
    base_model.load_state_dict(load_checkpoint(base_name, torch_device))

    prt_status('Downloading Upsampler Checkpoint...')
    upsampler_model.load_state_dict(load_checkpoint('upsample', torch_device))
    point_sampler = PointCloudSampler(
        device=torch_device,
        models=[base_model, upsampler_model],
        diffusions=[base_diffusion, upsampler_diffusion],
        num_points=[1024, 4096 - 1024],
        aux_channels=['R', 'G', 'B'],
        guidance_scale=[point_e_prefs['guidance_scale'], 0.0],
        model_kwargs_key_filter=('texts', ''), # Do not condition the upsampler at all
    )
    samples = None
    prt_status("Generating Point-E Samples...") #images=[img]
    step = 0
    if init_img != None:
        for x in tqdm(point_sampler.sample_batch_progressive(batch_size=point_e_prefs['batch_size'], model_kwargs=dict(images=[init_img]))):
            samples = x
            step += 1
            callback_fnc(step)
    else:
        for x in tqdm(point_sampler.sample_batch_progressive(batch_size=point_e_prefs['batch_size'], model_kwargs=dict(texts=[point_e_prefs['prompt_text']]))):
            samples = x
            step += 1
            callback_fnc(step)
    #print(f"Total steps: {step}")
    pc = point_sampler.output_to_point_clouds(samples)[0]
    fig = plot_point_cloud(pc, grid_size=3, fixed_bounds=((-0.75, -0.75, -0.75),(0.75, 0.75, 0.75)))

    prt_status('Saving PointCloud NPZ file...')
    pc_file = os.path.join(point_e_out, f'{filename}.npz')
    PointCloud.save(pc, pc_file)
    sdf = 'sdf'
    model = model_from_config(MODEL_CONFIGS[sdf], torch_device)
    model.eval()

    prt_status('Loading SDF model...')
    model.load_state_dict(load_checkpoint(sdf, torch_device))
    torch.cuda.empty_cache()
    #pc = PointCloud.load('example_data/pc_corgi.npz')
    # Plot the point cloud as a sanity check.
    #fig = plot_point_cloud(pc, grid_size=2)
    #fig = plot_point_cloud(pc, grid_size=3, fixed_bounds=((-0.75, -0.75, -0.75),(0.75, 0.75, 0.75)))
    # Produce a mesh (with vertex colors)
    try:
        mesh = marching_cubes_mesh(
            pc=pc,
            model=model,
            batch_size=4096,
            grid_size=32, # increase to 128 for resolution used in evals
            progress=True,
        )
    except Exception as e:
      clear_last()
      alert_msg(page, "Error running Point-E marching_cubes_mesh.", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))
      return
    # Write the mesh to a PLY file to import into some other program.
    ply_file = os.path.join(point_e_out, f'{filename}.ply')
    with open(ply_file, 'wb') as f:
        mesh.write_ply(f)

    #with open("mesh.ply", 'r') as file:
    #    print(file.name)
    #https://colab.research.google.com/drive/1Ok3ye2xWsuYOcmbAU3INN7AHy5gvvq5m
    del point_sampler
    flush()
    point_sampler = None
    clear_last()
    clear_last()
    prt("Finished generating Point Cloud and Mesh... Hope it's good.")
    if storage_type == "Colab Google Drive":
      point_e_save = os.path.join(prefs['image_output'].rpartition(slash)[0], 'point_e', point_e_prefs['batch_folder_name'])
      #os.makedirs(point_e_out, exist_ok=True)
      #if os.path.exists(point_e_save):
      #  point_e_save = available_folder(os.path.join(prefs['image_output'].rpartition(slash)[0], 'point_e'), fname, 1)
      shutil.copytree(point_e_out, point_e_save)
      prt(Text(f"Saved npz & ply files to {point_e_save}"))
    else:
      prt(Text(f"Saved npz & ply files to {point_e_out}"))
    # TODO: PyDrive2
    if prefs['enable_sounds']: page.snd_alert.play()
    os.chdir(root_dir)

def run_shap_e(page):
    global shap_e_prefs, status
    def prt(line):
      if type(line) == str:
        line = Text(line)
      page.shap_e_output.controls.append(line)
      page.shap_e_output.update()
    def prt_status(text):
        nonlocal status_txt
        status_txt.value = text
        status_txt.update()
    def clear_last(update=True):
      #page.shap_e_output.controls = []
      del page.shap_e_output.controls[-1]
      if update: page.shap_e_output.update()
    if not bool(shap_e_prefs["prompt_text"].strip()):
      alert_msg(page, "You must enter a simple prompt to generate 3D model from...")
      return
    page.shap_e_output.controls = []
    page.shap_e_output.update()
    shap_e_dir = os.path.join(root_dir, "shap-e")
    prt(Installing("Installing OpenAI Shap-E 3D Libraries..."))
    if not os.path.exists(shap_e_dir):
        try:
            #run_process("pip install -U scikit-image")
            run_process("git clone https://github.com/openai/shap-e.git", cwd=root_dir)
            run_process("pip install .", cwd=shap_e_dir)
        except Exception as e:
            clear_last()
            alert_msg(page, "Error Installing Shap-E Requirements", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
            return
    clear_pipes()
    from shap_e.diffusion.sample import sample_latents
    from shap_e.diffusion.gaussian_diffusion import diffusion_from_config
    from shap_e.models.download import load_model, load_config
    from shap_e.util.notebooks import create_pan_cameras, decode_latent_images, gif_widget
    from shap_e.util.image_util import load_image
    from PIL import ImageOps
    try:
      import imageio
    except Exception:
      run_sp("pip install imageio", realtime=False)
      import imageio
      pass
    try:
        xm = load_model('transmitter', device=torch_device)
        shap_e_model = load_model('image300M' if bool(shap_e_prefs['init_image']) else 'text300M' , device=torch_device)
        diffusion = diffusion_from_config(load_config('diffusion'))
    except Exception as e:
        clear_last()
        alert_msg(page, "Error downloading Shap-E models.", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
        return
    if bool(shap_e_prefs['prompt_text']):
        fname = format_filename(shap_e_prefs['prompt_text'])
    elif bool(shap_e_prefs['init_image']):
        fname = format_filename(shap_e_prefs['init_image'].rpartition(slash)[1].rparition('.')[0])
    elif bool(shap_e_prefs['batch_folder_name']):
        fname = format_filename(shap_e_prefs['batch_folder_name'], force_underscore=True)
    else:
        alert_msg(page, "If you're not using Prompt Text, provide a name for your 3D Model.")
        return

    #filename = format_filename(shap_e_prefs['prompt_text'])
    #fname = f"{shap_e_prefs['file_prefix']}{fname}"
    shap_e_out = os.path.join(prefs['image_output'].rpartition(slash)[0], 'shap_e', shap_e_prefs['batch_folder_name'])
    #if bool(shap_e_prefs['batch_folder_name']):
    #    shap_e_out = os.path.join(shap_e_dir, shap_e_prefs['batch_folder_name'])
    #else:
    #    shap_e_out = shap_e_dir
    if not os.path.exists(shap_e_out):
        os.makedirs(shap_e_out)
    #shap_e_out = os.path.join(shap_e_out, fname)
    #estimate = convert(int(shap_e_prefs["training_iters"] * 0.7))
    init_img = None
    if bool(shap_e_prefs['init_image']):
        if shap_e_prefs['init_image'].startswith('http'):
            init_img = PILImage.open(requests.get(shap_e_prefs['init_image'], stream=True).raw)
        else:
            if os.path.isfile(shap_e_prefs['init_image']):
                init_img = PILImage.open(shap_e_prefs['init_image'])
            else:
                alert_msg(page, f"ERROR: Couldn't find your init_image {shap_e_prefs['init_image']}")
                if not bool(shap_e_prefs['prompt_text']):
                    return
        if init_img != None:
            width, height = init_img.size
            width, height = scale_dimensions(width, height, 512)
            init_img = init_img.resize((width, height), resample=PILImage.Resampling.LANCZOS)
            init_img = ImageOps.exif_transpose(init_img).convert("RGB")

    status_txt = Text("Generating your 3D model... See console for progress.")
    progress = ProgressBar(bar_height=8)
    total_steps = shap_e_prefs['karras_steps']
    def callback_fnc(step: int) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}"
      progress.update()
    clear_last(update=False)
    prt(status_txt)
    prt(progress)
    if init_img == None:
        model_kwargs = dict(texts=[shap_e_prefs['prompt_text']] * shap_e_prefs['batch_size'])
    else:
        model_kwargs = dict(images=[init_img] * shap_e_prefs['batch_size'])
    try:
        latents = sample_latents(
            batch_size=shap_e_prefs['batch_size'],
            model=shap_e_model,
            diffusion=diffusion,
            guidance_scale=shap_e_prefs['guidance_scale'],
            model_kwargs=model_kwargs,
            progress=True,
            clip_denoised=True,
            use_fp16=True,
            use_karras=shap_e_prefs['use_karras'],
            karras_steps=shap_e_prefs['karras_steps'],
            sigma_min=1e-3,
            sigma_max=160,
            s_churn=0,
        )
    except Exception as e:
        clear_last()
        alert_msg(page, "Error running Shap-E sample_latents.", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
        return
    prt_status("Generating Shap-E 3D Models...") #images=[img]
    step = 0
    try:
        cameras = create_pan_cameras(shap_e_prefs['size'], torch_device)
        for i, latent in enumerate(latents):
            img_file = os.path.join(shap_e_out, f'{fname}_{i}.png')
            images = decode_latent_images(xm, latent, cameras, rendering_mode=shap_e_prefs['render_mode'].lower())
            #images.save(img_file)
            display(gif_widget(images))
            #callback_fnc(i)
    except Exception as e:
        clear_last()
        alert_msg(page, "Error running Shap-E decode_latent_images.", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))
        return
    if shap_e_prefs['save_frames']:
        imgs = []
        for i, img in enumerate(images):
            img_file = os.path.join(shap_e_out, f'{fname}_{i}.png')
            img.save(img_file)
            #imgs.append(imageio.imread(np.asarray(img)))
    gif_file = os.path.join(shap_e_out, f'{fname}.gif')
    #imageio.mimsave(gif_file, imgs, 'GIF', duration=100, loop=0)
    images[0].save(gif_file, save_all=True, append_images=images[1:], duration=100, loop=0)

    prt_status('Saving PLY mesh file...')
    from shap_e.util.notebooks import decode_latent_mesh
    try:
        for i, latent in enumerate(latents):
            pc_file = os.path.join(shap_e_out, f'{fname}_{i}.ply')
            with open(pc_file, 'wb') as f:
                decode_latent_mesh(xm, latent).tri_mesh().write_ply(f)
    except Exception as e:
      clear_last()
      alert_msg(page, "Error running Shap-E decode_latent_mesh.", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))
      return
    del xm
    del shap_e_model
    del diffusion
    del latents
    flush()
    clear_last(update=False)
    clear_last(update=False)
    prt(ImageButton(src=gif_file, width=shap_e_prefs['size'], height=shap_e_prefs['size'], data=gif_file, subtitle=pc_file, page=page))
    prt("Finished generating Shap-E Mesh... Hope it's good.")
    if prefs['enable_sounds']: page.snd_alert.play()
    os.chdir(root_dir)

pipe_shap_e = None
def run_shap_e2(page):
    global shap_e_prefs, pipe_shap_e, status
    def prt(line):
      if type(line) == str:
        line = Text(line)
      page.shap_e_output.controls.append(line)
      page.shap_e_output.update()
    def prt_status(text):
        nonlocal status_txt
        status_txt.value = text
        status_txt.update()
    def clear_last(update=True):
      #page.shap_e_output.controls = []
      del page.shap_e_output.controls[-1]
      if update: page.shap_e_output.update()
    if not bool(shap_e_prefs["prompt_text"].strip()):
      alert_msg(page, "You must enter a simple prompt to generate 3D model from...")
      return
    page.shap_e_output.controls = []
    page.shap_e_output.update()
    shap_e_dir = os.path.join(root_dir, "shap-e")
    installer = Installing("Installing OpenAI Shap-E 3D Libraries...")
    prt(installer)
    from diffusers import DiffusionPipeline
    from diffusers.utils import export_to_ply
    repo = "openai/shap-e"
    clear_pipes()
    try:
        pipe_shap_e = DiffusionPipeline.from_pretrained(repo, torch_dtype=torch.float16, variant="fp16")
        pipe_shap_e = pipe_shap_e.to(torch_device)
    except Exception as e:
        clear_last()
        alert_msg(page, "Error Installing Shap-E Pipeline", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
        return
    from PIL import ImageOps
    try:
      import trimesh
    except Exception:
      run_sp("pip install trimesh", realtime=False)
      import trimesh
      pass
    if bool(shap_e_prefs['prompt_text']):
        fname = format_filename(shap_e_prefs['prompt_text'])
    elif bool(shap_e_prefs['init_image']):
        fname = format_filename(shap_e_prefs['init_image'].rpartition(slash)[1].rparition('.')[0])
    elif bool(shap_e_prefs['batch_folder_name']):
        fname = format_filename(shap_e_prefs['batch_folder_name'], force_underscore=True)
    else:
        alert_msg(page, "If you're not using Prompt Text, provide a name for your 3D Model.")
        return

    #filename = format_filename(shap_e_prefs['prompt_text'])
    #fname = f"{shap_e_prefs['file_prefix']}{fname}"
    shap_e_out = os.path.join(prefs['image_output'].rpartition(slash)[0], 'shap_e', shap_e_prefs['batch_folder_name'])
    #if bool(shap_e_prefs['batch_folder_name']):
    #    shap_e_out = os.path.join(shap_e_dir, shap_e_prefs['batch_folder_name'])
    #else:
    #    shap_e_out = shap_e_dir
    if not os.path.exists(shap_e_out):
        os.makedirs(shap_e_out)
    #shap_e_out = os.path.join(shap_e_out, fname)
    #estimate = convert(int(shap_e_prefs["training_iters"] * 0.7))
    init_img = None
    if bool(shap_e_prefs['init_image']):
        if shap_e_prefs['init_image'].startswith('http'):
            init_img = PILImage.open(requests.get(shap_e_prefs['init_image'], stream=True).raw)
        else:
            if os.path.isfile(shap_e_prefs['init_image']):
                init_img = PILImage.open(shap_e_prefs['init_image'])
            else:
                alert_msg(page, f"ERROR: Couldn't find your init_image {shap_e_prefs['init_image']}")
                if not bool(shap_e_prefs['prompt_text']):
                    return
        if init_img != None:
            width, height = init_img.size
            width, height = scale_dimensions(width, height, 512)
            init_img = init_img.resize((width, height), resample=PILImage.Resampling.LANCZOS)
            init_img = ImageOps.exif_transpose(init_img).convert("RGB")

    status_txt = Text("Generating your 3D model... See console for progress.")
    progress = ProgressBar(bar_height=8)
    total_steps = shap_e_prefs['karras_steps']
    def callback_fnc(step: int) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}"
      progress.update()
    clear_last(update=False)
    prt(status_txt)
    prt(progress)
    if init_img == None:
        model_kwargs = dict(texts=[shap_e_prefs['prompt_text']] * shap_e_prefs['batch_size'])
    else:
        model_kwargs = dict(images=[init_img] * shap_e_prefs['batch_size'])
    try:
        images = pipe_shap_e(shap_e_prefs['prompt_text'], num_images_per_prompt=shap_e_prefs['batch_size'], guidance_scale=shap_e_prefs['guidance_scale'], num_inference_steps=shap_e_prefs['karras_steps'], frame_size=shap_e_prefs['size'], output_type="mesh").images
    except Exception as e:
        clear_last()
        alert_msg(page, "Error running Shap-E sample_latents.", content=Column([Text(str(e)), Text(str(traceback.format_exc()), selectable=True)]))
        return
    prt_status("Generating Shap-E 3D Models...") #images=[img]
    step = 0
    if shap_e_prefs['save_frames']:
        imgs = []
        for i, img in enumerate(images):
            img_file = os.path.join(shap_e_out, f'{fname}_{i}.png')
            img.save(img_file)
            #imgs.append(imageio.imread(np.asarray(img)))
    gif_file = os.path.join(shap_e_out, f'{fname}.gif')
    ply_file = os.path.join(shap_e_out, f'{fname}.ply')
    glb_file = os.path.join(shap_e_out, f'{fname}.glb')
    #imageio.mimsave(gif_file, imgs, 'GIF', duration=100, loop=0)
    images[0].save(gif_file, save_all=True, append_images=images[1:], duration=100, loop=0)

    prt_status('Saving PLY mesh file...')
    ply_path = export_to_ply(images[0], ply_file)
    print(f"saved to folder: {ply_path}")
    mesh = trimesh.load(ply_path)
    mesh.export(glb_file, file_type="glb")

    flush()
    clear_last(update=False)
    clear_last(update=False)
    prt(ImageButton(src=gif_file, width=shap_e_prefs['size'], height=shap_e_prefs['size'], data=gif_file, subtitle=pc_file, page=page))
    prt("Finished generating Shap-E Mesh... Hope it's good.")
    if prefs['enable_sounds']: page.snd_alert.play()
    os.chdir(root_dir)

def run_instant_ngp(page):
    global instant_ngp_prefs, prefs
    def prt(line):
      if type(line) == str:
        line = Text(line)
      page.instant_ngp_output.controls.append(line)
      page.instant_ngp_output.update()
    def clear_last():
      del page.instant_ngp_output.controls[-1]
      page.instant_ngp_output.update()
    if not status['installed_diffusers']:
      alert_msg(page, "You must Install the HuggingFace Diffusers Library first... ")
      return
    save_path = os.path.join(root_dir, "my_ngp")
    error = False
    if not os.path.exists(save_path):
      error = True
    elif len(os.listdir(save_path)) == 0:
      error = True
    if len(page.instant_ngp_file_list.controls) == 0:
      error = True
    if error:
      alert_msg(page, "Couldn't find a list of images to train model. Add image files to the list...")
      return
    page.instant_ngp_output.controls.clear()
    page.instant_ngp_output.update()
    prt(Installing("Downloading Instant-NGP Packages..."))
    instant_ngp_dir = os.path.join(root_dir, 'instant-ngp')
    '''if not os.path.exists(diffusers_dir):
      os.chdir(root_dir)
      run_process("git clone https://github.com/Skquark/diffusers.git", realtime=False, cwd=root_dir)
    run_process('pip install git+https://github.com/Skquark/diffusers.git#egg=diffusers[training]', cwd=root_dir, realtime=False)
    os.chdir(diffusers_dir)
    run_sp('pip install -e ".[training]"', cwd=diffusers_dir, realtime=False)
    '''
    try:
        run_sp("apt-get install \
            cmake \
            libgoogle-glog-dev \
            libgflags-dev \
            libatlas-base-dev \
            libeigen3-dev \
            libsuitesparse-dev \
            libboost-program-options-dev \
            libboost-filesystem-dev \
            libboost-graph-dev \
            libboost-system-dev \
            libboost-test-dev \
            libfreeimage-dev \
            libmetis-dev \
            libglew-dev \
            qtbase5-dev \
            libqt5opengl5-dev \
            libcgal-dev")
    except Exception as e:
        clear_last()
        alert_msg(page, f"ERROR Installing Instant-NGP Packages...", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip())]))
        return
    if not os.path.exists(root_dir, 'ceres-solver'):
      run_sp("wget https://github.com/camenduru/instant-ngp-colab/releases/download/v1.0/ceres-solver.zip", realtime=False)
      run_sp(f"unzip {os.path.join(root_dir, 'ceres-solver.zip')} -d ceres-solver", realtime=False)
      os.remove(os.path.join(root_dir, 'ceres-solver.zip'))
    run_sp(f"yes | cp -r {os.path.join(root_dir, 'ceres-solver', 'lib', '.')} {os.path.join('/usr', 'local', 'lib')}", realtime=False)
    run_sp(f"chmod 755 {os.path.join(root_dir, 'ceres-solver', 'bin', 'colmap')}", realtime=False)
    run_sp(f"yes | cp -r {os.path.join(root_dir, 'ceres-solver', 'bin', '.')} {os.path.join('/usr', 'local', 'bin')}", realtime=False)
    #run_sp(f"cp -r /content/ceres-solver/bin/. /usr/local/bin")
    if not os.path.exists(root_dir, 'instant-ngp'):
      run_sp("wget https://github.com/camenduru/instant-ngp-colab/releases/download/v1.0/instant-ngp.zip", realtime=False)
      #run_sp("unzip /content/instant-ngp.zip -d instant-ngp")
      run_sp(f"unzip {os.path.join(root_dir, 'instant-ngp.zip')} -d instant-ngp", realtime=False)
      os.remove(os.path.join(root_dir, 'instant-ngp.zip'))
    #run_sp("pip install commentjson huggingface-hub", realtime=False)
    os.chdir(instant_ngp_dir)
    #from huggingface_hub import create_repo, upload_folder
    #scene_path = "/content/drive/MyDrive/fox"
    train_path = os.path.join(root_dir, 'my_ngp')
    #train_path = "/content/train"
    #if not os.path.isdir(scene_path):
    #    raise NotADirectoryError(scene_path)
    '''
    rm -rf {train_path}
    mkdir {train_path}
    cp -r {scene_path}/. /content/train
    '''
    clear_pipes()
    clear_last()
    prt(Text("Running training on Images... This'll take a while, see console...", weight=FontWeight.BOLD))
    progress = ProgressBar(bar_height=8)
    prt(progress)
    name_of_your_model = instant_ngp_prefs['name_of_your_model']
    transforms_path = os.path.join(train_path, f"transforms.json")
    train_steps = instant_ngp_prefs['train_steps']
    snapshot_path = os.path.join(train_path, f"{train_steps}.msgpack")
    mesh_path = os.path.join(train_path, f"{name_of_your_model or train_steps}.ply")
    os.chdir(instant_ngp_dir)
    run_args = f"--scene {train_path} --mode nerf --n_steps {train_steps} --save_snapshot {snapshot_path} --save_mesh {mesh_path} --screenshot_dir {train_path}"
    if instant_ngp_prefs["vr_mode"]: run_args += " --vr"
    if instant_ngp_prefs["sharpen"] != 0.0: run_args += f" --sharpen {instant_ngp_prefs['sharpen']}"
    if instant_ngp_prefs["exposure"] != 0.0: run_args += f" --exposure {instant_ngp_prefs['exposure']}"
    try:
        run_sp(f"python ./scripts/colmap2nerf.py --colmap_matcher exhaustive --run_colmap --aabb_scale 4 --images {train_path} --out {transforms_path}", cwd=instant_ngp_dir)
        run_sp(f"python ./scripts/run.py {run_args}", cwd=instant_ngp_dir)
    except Exception as e:
        clear_last()
        alert_msg(page, f"ERROR Running InstantNGP Training...", content=Column([Text(str(e)), Text(str(traceback.format_exc()).strip())]))
        return
    #instant_ngp_dir = os.path.join(diffusers_dir, "examples", "dreambooth")
    #instant_ngp_dir = os.path.join(diffusers_dir, "examples", "text_to_image")
    #save_path = "./my_model"
    #if not os.path.exists(save_path):
    #  os.mkdir(save_path)
    output_dir = prefs['image_output'].rpartition(slash)[0] + slash + '3D_out'
    if bool(name_of_your_model):
      output_dir = os.path.join(output_dir, name_of_your_model)
    if not os.path.exists(output_dir): os.makedirs(output_dir)
    shutil.copytree(train_path, output_dir, dirs_exist_ok=True)
    '''video_camera_path = os.path.join(scene_path, "base_cam.json")
    if not os.path.isfile(video_camera_path):
      raise FileNotFoundError(video_camera_path)
    video_n_seconds = 5
    video_fps = 25
    width = 720
    height = 720
    output_video_path = os.path.join(scene_path, "output_video.mp4")
    python scripts/run.py {snapshot_path} --video_camera_path {video_camera_path} --video_n_seconds 2 --video_fps 25 --width 720 --height 720 --video_output {output_video_path}
    print(f"Generated video saved to:\n{output_video_path}")'''
    clear_last()
    clear_last()
    prt(Markdown(f"## Your model was saved successfully to _{output_dir}_.\nNow take those files and load then locally on Windows following [instant-ngp gui instructions](https://github.com/NVlabs/instant-ngp#testbed-controls) to export videos and meshes or try MeshLab... (wish we can do that for ya here)", on_tap_link=lambda e: e.page.launch_url(e.data)))
    if prefs['enable_sounds']: page.snd_alert.play()

def run_dall_e(page, from_list=False):
    global dall_e_prefs, prefs, prompts
    if (not bool(dall_e_prefs['prompt']) and not from_list) or (from_list and (len(prompts) == 0)):
      alert_msg(page, "You must provide a text prompt to process your image generation...")
      return
    if not bool(prefs['OpenAI_api_key']):
      alert_msg(page, "You must provide your OpenAI API Key in Settings to process your Dall-e 2 Creation...")
      return
    def prt(line):
      if type(line) == str:
        line = Text(line, size=17)
      page.DallE2.controls.append(line)
      page.DallE2.update()
    def clear_last():
      del page.DallE2.controls[-1]
      page.DallE2.update()
    def autoscroll(scroll=True):
      page.DallE2.auto_scroll = scroll
      page.DallE2.update()
    def clear_list():
      page.DallE2.controls = page.DallE2.controls[:1]
    progress = ProgressBar(bar_height=8)
    try:
        import openai
    except:
        prt(Installing("Installing OpenAi Dall-E 2 API..."))
        run_process("pip install -q openai", realtime=False)
        clear_last()
        import openai
        pass
    try:
        openai.api_key = prefs['OpenAI_api_key']
    except Exception as e:
        alert_msg(page, f"Seems like your OpenAI API Key is Invalid. Check it again...", content=Text(str(e)))
        return
    import requests
    from io import BytesIO
    from PIL import ImageOps

    save_dir = os.path.join(root_dir, 'dalle_inputs')
    init_img = None
    dall_e_list = []
    if from_list:
        if len(prompts) > 0:
            for p in prompts:
                dall_e_list.append({'prompt': p.prompt, 'init_image': p.arg['init_image'], 'mask_image': p.arg['mask_image']})
        else:
            alert_msg(page, f"Your Prompts List is empty. Add to your batch list to use feature.")
            return
    else:
        dall_e_list.append({'prompt': dall_e_prefs['prompt'], 'init_image': dall_e_prefs['init_image'], 'mask_image': dall_e_prefs['mask_image']})
    clear_list()
    autoscroll(True)
    for p in dall_e_list:
        init_image = p['init_image']
        mask_image = p['mask_image']
        if bool(init_image):
            fname = init_image.rpartition(slash)[2]
            init_file = os.path.join(save_dir, fname)
            if init_image.startswith('http'):
                init_img = PILImage.open(requests.get(init_image, stream=True).raw)
            else:
                if os.path.isfile(init_image):
                    init_img = PILImage.open(init_image)
                else:
                    alert_msg(page, f"ERROR: Couldn't find your init_image {init_image}")
                    return
            init_img = init_img.resize((dall_e_prefs['size'], dall_e_prefs['size']), resample=PILImage.Resampling.LANCZOS)
            init_img = ImageOps.exif_transpose(init_img).convert("RGB")
            init_img.save(init_file)
        mask_img = None
        if bool(mask_image):
            fname = init_image.rpartition(slash)[2]
            mask_file = os.path.join(save_dir, fname)
            if mask_image.startswith('http'):
                mask_img = PILImage.open(requests.get(mask_image, stream=True).raw)
            else:
                if os.path.isfile(mask_image):
                    mask_img = PILImage.open(mask_image)
                else:
                    alert_msg(page, f"ERROR: Couldn't find your mask_image {mask_image}")
                    return
                if dall_e_prefs['invert_mask']:
                    mask_img = ImageOps.invert(mask_img.convert('RGB'))
            mask_img = mask_img.resize((dall_e_prefs['size'], dall_e_prefs['size']), resample=PILImage.NEAREST)
            mask_img = ImageOps.exif_transpose(init_img).convert("RGB")
            mask_img.save(mask_file)
        #print(f'Resize to {width}x{height}')
        #clear_pipes()
        prt("Generating your Dall-E 2 Image...")
        prt(progress)
        autoscroll(False)

        try:
            if bool(init_image) and bool(dall_e_prefs['variation']):
                response = openai.Image.create_variation(image=open(init_file, 'rb'), size=dall_e_prefs['size'], n=dall_e_prefs['num_images'])
            elif bool(init_image) and not bool(mask_image):
                response = openai.Image.create_edit(prompt=p['prompt'], size=dall_e_prefs['size'], n=dall_e_prefs['num_images'], image=open(init_file, 'rb'))
            elif bool(init_image) and bool(mask_image):
                response = openai.Image.create_edit(prompt=p['prompt'], size=dall_e_prefs['size'], n=dall_e_prefs['num_images'], image=open(init_file, 'rb'), mask=open(mask_file, 'rb'))
            else:
                response = openai.Image.create(prompt=p['prompt'], size=dall_e_prefs['size'], n=dall_e_prefs['num_images'])
        except Exception as e:
            clear_last()
            clear_last()
            alert_msg(page, f"ERROR: Something went wrong generating image form API...", content=Text(str(e)))
            return
        clear_last()
        clear_last()
        autoscroll(True)
        txt2img_output = stable_dir
        batch_output = prefs['image_output']
        #print(str(images))
        if response is None:
            prt(f"ERROR: Problem generating images, check your settings and run above blocks again, or report the error to Skquark if it really seems broken.")
            return
        #print(str(response))
        idx = 0
        for i in response['data']:
            image = i['url']
            #random_seed += idx
            fname = format_filename(p['prompt'])
            #seed_suffix = f"-{random_seed}" if bool(prefs['file_suffix_seed']) else ''
            fname = f'{dall_e_prefs["file_prefix"]}{fname}'
            txt2img_output = stable_dir
            if bool(dall_e_prefs['batch_folder_name']):
                txt2img_output = os.path.join(stable_dir, dall_e_prefs['batch_folder_name'])
            if not os.path.exists(txt2img_output):
                os.makedirs(txt2img_output)
            image_path = available_file(txt2img_output, fname, 1)
            #image.save(image_path)
            response = requests.get(image, stream=True)
            with open(image_path, "wb") as f:
                f.write(response.content)
            #img = i['url']
            new_file = image_path.rpartition(slash)[2].rpartition('-')[0]
            size = int(dall_e_prefs['size'].rpartition('x')[0])
            out_path = batch_output if save_to_GDrive else txt2img_output
            new_path = available_file(out_path, new_file, idx)
            if not dall_e_prefs['display_upscaled_image'] or not dall_e_prefs['apply_ESRGAN_upscale']:
                prt(Row([ImageButton(src=image_path, data=new_file, width=size, height=size, page=page)], alignment=MainAxisAlignment.CENTER))
                #prt(Row([Img(src=image_path, width=size, height=size, fit=ImageFit.FILL, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))

            #if save_to_GDrive:
            batch_output = os.path.join(prefs['image_output'], dall_e_prefs['batch_folder_name'])
            if not os.path.exists(batch_output):
                os.makedirs(batch_output)
            if storage_type == "PyDrive Google Drive":
                newFolder = gdrive.CreateFile({'title': dall_e_prefs['batch_folder_name'], "parents": [{"kind": "drive#fileLink", "id": prefs['image_output']}],"mimeType": "application/vnd.google-apps.folder"})
                newFolder.Upload()
                batch_output = newFolder

            if dall_e_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
                os.chdir(os.path.join(dist_dir, 'Real-ESRGAN'))
                upload_folder = 'upload'
                result_folder = 'results'
                if os.path.isdir(upload_folder):
                    shutil.rmtree(upload_folder)
                if os.path.isdir(result_folder):
                    shutil.rmtree(result_folder)
                os.mkdir(upload_folder)
                os.mkdir(result_folder)
                short_name = f'{fname[:80]}-{idx}.png'
                dst_path = os.path.join(dist_dir, 'Real-ESRGAN', upload_folder, short_name)
                #print(f'Moving {fpath} to {dst_path}')
                #shutil.move(fpath, dst_path)
                shutil.copy(image_path, dst_path)
                faceenhance = ' --face_enhance' if dall_e_prefs["face_enhance"] else ''
                run_sp(f'python inference_realesrgan.py -n realesr-general-x4v3 -i upload --outscale {dall_e_prefs["enlarge_scale"]}{faceenhance}', cwd=os.path.join(dist_dir, 'Real-ESRGAN'), realtime=False)
                out_file = short_name.rpartition('.')[0] + '_out.png'
                upscaled_path = new_path
                shutil.move(os.path.join(dist_dir, 'Real-ESRGAN', result_folder, out_file), upscaled_path)
                # python inference_realesrgan.py --model_path experiments/pretrained_models/RealESRGAN_x4plus.pth --input upload --netscale 4 --outscale 3.5 --half --face_enhance
                os.chdir(stable_dir)
                if dall_e_prefs['display_upscaled_image']:
                    time.sleep(0.6)
                    prt(Row([ImageButton(src=upscaled_path, data=upscaled_path, width=size * float(dall_e_prefs["enlarge_scale"]), height=size * float(dall_e_prefs["enlarge_scale"]), page=page)], alignment=MainAxisAlignment.CENTER))
                    #prt(Row([Img(src=upscaled_path,fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
            else:
                shutil.copy(image_path, new_path)#os.path.join(out_path, new_file))
            # TODO: Add Metadata
            prt(Row([Text(new_path)], alignment=MainAxisAlignment.CENTER))
    autoscroll(False)
    if prefs['enable_sounds']: page.snd_alert.play()

loaded_kandinsky_task = ""
def run_kandinsky(page, from_list=False, with_params=False):
    global kandinsky_prefs, pipe_kandinsky, pipe_kandinsky_prior, prefs, loaded_kandinsky_task
    if not status['installed_diffusers']:
      alert_msg(page, "You need to Install HuggingFace Diffusers before using...")
      return
    if int(status['cpu_memory']) <= 10:
      alert_msg(page, f"Sorry, you only have {int(status['cpu_memory'])}GB RAM which is not quite enough to run Kandinsky 2.2 right now. Either Change runtime type to High-RAM mode and restart or use other Kandinsky 2.1 in Extras.")
      return
    kandinsky_prompts = []
    if from_list:
      if len(prompts) < 1:
        alert_msg(page, "You need to add Prompts to your List first... ")
        return
      for p in prompts:
        if with_params:
            kandinsky_prompts.append({'prompt': p.prompt, 'negative_prompt':p['negative_prompt'], 'init_image':kandinsky_prefs['init_image'], 'mask_image':kandinsky_prefs['mask_image'], 'guidance_scale':kandinsky_prefs['guidance_scale'], 'steps':kandinsky_prefs['steps'], 'width':kandinsky_prefs['width'], 'height':kandinsky_prefs['height'], 'strength':kandinsky_prefs['strength'], 'num_images':kandinsky_prefs['num_images'], 'seed':kandinsky_prefs['seed']})
        else:
            kandinsky_prompts.append({'prompt': p.prompt, 'negative_prompt':p['negative_prompt'], 'init_image':p['init_image'], 'mask_image':p['mask_image'], 'guidance_scale':p['guidance_scale'], 'steps':p['steps'], 'width':p['width'], 'height':p['height'], 'strength':p['init_image_strength'], 'num_images':p['batch_size'], 'seed':p['seed']})
    else:
      if not bool(kandinsky_prefs['prompt']):
        alert_msg(page, "You must provide a text prompt to process your image generation...")
        return
      kandinsky_prompts.append({'prompt': kandinsky_prefs['prompt'], 'negative_prompt':kandinsky_prefs['negative_prompt'], 'init_image':kandinsky_prefs['init_image'], 'mask_image':kandinsky_prefs['mask_image'], 'guidance_scale':kandinsky_prefs['guidance_scale'], 'steps':kandinsky_prefs['steps'], 'width':kandinsky_prefs['width'], 'height':kandinsky_prefs['height'], 'strength':kandinsky_prefs['strength'], 'num_images':kandinsky_prefs['num_images'], 'seed':kandinsky_prefs['seed']})
    def prt(line, update=True):
      if type(line) == str:
        line = Text(line, size=17)
      if from_list:
        page.imageColumn.controls.append(line)
        if update:
          page.imageColumn.update()
      else:
        page.Kandinsky.controls.append(line)
        if update:
          page.Kandinsky.update()
    def clear_last():
      if from_list:
        del page.imageColumn.controls[-1]
        page.imageColumn.update()
      else:
        del page.Kandinsky.controls[-1]
        page.Kandinsky.update()
    def autoscroll(scroll=True):
      if from_list:
        page.imageColumn.auto_scroll = scroll
        page.imageColumn.update()
        page.Kandinsky.auto_scroll = scroll
        page.Kandinsky.update()
      else:
        page.Kandinsky.auto_scroll = scroll
        page.Kandinsky.update()
    def clear_list():
      if from_list:
        page.imageColumn.controls.clear()
      else:
        page.Kandinsky.controls = page.Kandinsky.controls[:1]
    progress = ProgressBar(bar_height=8)
    total_steps = kandinsky_prefs['steps']
    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep}"
      progress.update()
    if from_list:
      page.tabs.selected_index = 4
      page.tabs.update()
    clear_list()
    autoscroll(True)
    installer = Installing("Installing Kandinsky 2.2 Engine & Models... See console log for progress.")
    clear_pipes("kandinsky")
    import requests
    from io import BytesIO
    from PIL.PngImagePlugin import PngInfo
    from PIL import ImageOps
    #from diffusers import KandinskyV22PriorPipeline
    #installer.set_details("...kandinsky-2-2-prior Pipeline")
    #if pipe_kandinsky_prior == None:
    #    pipe_kandinsky_prior = KandinskyV22PriorPipeline.from_pretrained("kandinsky-community/kandinsky-2-2-prior", torch_dtype=torch.float16)
    #    pipe_kandinsky_prior.to("cuda")
    #save_dir = os.path.join(root_dir, 'kandinsky_inputs')
    cpu_offload = False
    for pr in kandinsky_prompts:
        prt(installer)
        init_img = None
        if bool(pr['init_image']):
            fname = pr['init_image'].rpartition(slash)[2]
            #init_file = os.path.join(save_dir, fname)
            if pr['init_image'].startswith('http'):
                init_img = PILImage.open(requests.get(pr['init_image'], stream=True).raw)
            else:
                if os.path.isfile(pr['init_image']):
                    init_img = PILImage.open(pr['init_image'])
                else:
                    alert_msg(page, f"ERROR: Couldn't find your init_image {pr['init_image']}")
                    return
            init_img = init_img.resize((pr['width'], pr['height']), resample=PILImage.Resampling.LANCZOS)
            init_img = ImageOps.exif_transpose(init_img).convert("RGB")
            #init_img.save(init_file)
        mask_img = None
        if bool(pr['mask_image']):
            fname = pr['init_image'].rpartition(slash)[2]
            #mask_file = os.path.join(save_dir, fname)
            if pr['mask_image'].startswith('http'):
                mask_img = PILImage.open(requests.get(pr['mask_image'], stream=True).raw)
            else:
                if os.path.isfile(pr['mask_image']):
                    mask_img = PILImage.open(pr['mask_image'])
                else:
                    alert_msg(page, f"ERROR: Couldn't find your mask_image {pr['mask_image']}")
                    return
                if kandinsky_prefs['invert_mask']:
                    mask_img = ImageOps.invert(mask_img.convert('RGB'))
            mask_img = mask_img.resize((pr['width'], pr['height']), resample=PILImage.NEAREST)
            mask_img = ImageOps.exif_transpose(mask_img).convert("RGB")
            mask_img = np.asarray(mask_img)
            #mask_img.save(mask_file)
        #print(f'Resize to {width}x{height}')
        task_type = "inpainting" if bool(pr['init_image']) and bool(pr['mask_image']) else "img2img" if bool(pr['init_image']) and not bool(pr['mask_image']) else "text2img"
        installer.set_details(f"...kandinsky-2-2 {task_type} Pipeline")
        if pipe_kandinsky == None or loaded_kandinsky_task != task_type:
            clear_pipes('kandinsky_prior')
            try:
                #if bool(kandinsky_prefs['init_image']) and not bool(kandinsky_prefs['mask_image']):
                #    pipe_kandinsky = get_kandinsky2('cuda', task_type='img2img', model_version='2.1', use_flash_attention=False, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
                #pipe_kandinsky = get_kandinsky2('cuda', task_type=task_type, model_version='2.1', use_flash_attention=False, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
                '''if task_type == "text2img":
                    from diffusers import KandinskyV22Pipeline
                    pipe_kandinsky = KandinskyV22Pipeline.from_pretrained("kandinsky-community/kandinsky-2-2-decoder", torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
                elif task_type == "img2img":
                    from diffusers import KandinskyV22Img2ImgPipeline
                    pipe_kandinsky = KandinskyV22Img2ImgPipeline.from_pretrained("kandinsky-community/kandinsky-2-2-decoder", torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
                elif task_type == "inpainting":
                    from diffusers import KandinskyV22InpaintPipeline
                    pipe_kandinsky = KandinskyV22InpaintPipeline.from_pretrained("kandinsky-community/kandinsky-2-2-decoder-inpaint", torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
                '''
                if task_type == "text2img":
                    from diffusers import AutoPipelineForText2Image
                    pipe_kandinsky = AutoPipelineForText2Image.from_pretrained("kandinsky-community/kandinsky-2-2-decoder", torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
                elif task_type == "img2img":
                    from diffusers import AutoPipelineForImage2Image
                    pipe_kandinsky = AutoPipelineForImage2Image.from_pretrained("kandinsky-community/kandinsky-2-2-decoder", torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
                elif task_type == "inpainting":
                    from diffusers import AutoPipelineForInpainting
                    pipe_kandinsky = AutoPipelineForInpainting.from_pretrained("kandinsky-community/kandinsky-2-2-decoder-inpaint", torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
                if prefs['enable_torch_compile']:
                    installer.set_details(f"...Torch compiling unet")
                    pipe_kandinsky.unet.to(memory_format=torch.channels_last)
                    pipe_kandinsky.unet = torch.compile(pipe_kandinsky.unet, mode="reduce-overhead", fullgraph=True)
                    pipe_kandinsky = pipe_kandinsky.to("cuda")
                elif cpu_offload:
                    pipe_kandinsky.enable_model_cpu_offload()
                else:
                    pipe_kandinsky.to("cuda")
                loaded_kandinsky_task = task_type
            except Exception as e:
                clear_last()
                alert_msg(page, f"ERROR Initializing Kandinsky, try running without installing Diffusers first...", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))
                return
        else:
            clear_pipes('kandinsky')
        clear_last()
        prt("Generating your Kandinsky 2.2 Image...")
        prt(progress)
        autoscroll(False)
        total_steps = pr['steps']
        random_seed = int(pr['seed']) if int(pr['seed']) > 0 else rnd.randint(0,4294967295)
        generator = torch.Generator(device="cuda").manual_seed(random_seed)
        try:
            #image_embeds, negative_image_embeds = pipe_kandinsky_prior(pr['prompt'], negative_prompt=pr['negative_prompt'], num_inference_steps=5, guidance_scale=1.0, generator=generator).to_tuple()
            #negative_image_embeds = pipe_kandinsky_prior(pr['negative_prompt'], guidance_scale=2.0, num_inference_steps=25, generator=generator, negative_prompt=pr['negative_prompt']).images
            #guidance_scale=pr['guidance_scale'], num_inference_steps=pr['steps']
            if task_type == "text2img":
                #images_texts = [kandinsky_prefs['prompt'], init_img]
                #weights = [0.5, kandinsky_prefs['strength']]
                #images = pipe_kandinsky.generate_img2img(kandinsky_prefs['prompt'], init_img, strength=kandinsky_prefs['strength'], batch_size=kandinsky_prefs['num_images'], w=kandinsky_prefs['width'], h=kandinsky_prefs['height'], num_steps=kandinsky_prefs['steps'], denoised_type=kandinsky_prefs['denoised_type'], dynamic_threshold_v=kandinsky_prefs['dynamic_threshold_v'], sampler=kandinsky_prefs['sampler'], ddim_eta=kandinsky_prefs['ddim_eta'], guidance_scale=kandinsky_prefs['guidance_scale'])
                #images = pipe_kandinsky.generate_img2img(kandinsky_prefs['prompt'], init_img, strength=kandinsky_prefs['strength'], batch_size=kandinsky_prefs['num_images'], w=kandinsky_prefs['width'], h=kandinsky_prefs['height'], num_steps=kandinsky_prefs['steps'], prior_cf_scale=kandinsky_prefs['prior_cf_scale'], prior_steps=str(kandinsky_prefs['prior_steps']), sampler=kandinsky_prefs['sampler'], guidance_scale=kandinsky_prefs['guidance_scale'])
                #images = pipe_kandinsky.mix_images(images_texts, weights, batch_size=kandinsky_prefs['num_images'], w=kandinsky_prefs['width'], h=kandinsky_prefs['height'], num_steps=kandinsky_prefs['steps'], prior_cf_scale=kandinsky_prefs['prior_cf_scale'], prior_steps=str(kandinsky_prefs['prior_steps']), sampler=kandinsky_prefs['sampler'], guidance_scale=kandinsky_prefs['guidance_scale'])
                images = pipe_kandinsky(
                    prompt=pr['prompt'], negative_prompt=pr['negative_prompt'],
                    prior_guidance_scale=kandinsky_prefs['prior_guidance_scale'], prior_num_inference_steps=kandinsky_prefs['prior_steps'],
                    #pr['prompt'],
                    #image_embeds=image_embeds,
                    #negative_image_embeds=negative_image_embeds,
                    num_images_per_prompt=pr['num_images'],
                    height=pr['height'],
                    width=pr['width'],
                    num_inference_steps=pr['steps'],
                    guidance_scale=pr['guidance_scale'],
                    generator=generator,
                    callback=callback_fnc,
                ).images
            elif task_type == "img2img":
                #images = pipe_kandinsky.generate_inpainting(kandinsky_prefs['prompt'], init_img, mask_img, batch_size=kandinsky_prefs['num_images'], w=kandinsky_prefs['width'], h=kandinsky_prefs['height'], num_steps=kandinsky_prefs['steps'], denoised_type=kandinsky_prefs['denoised_type'], dynamic_threshold_v=kandinsky_prefs['dynamic_threshold_v'], sampler=kandinsky_prefs['sampler'], ddim_eta=kandinsky_prefs['ddim_eta'], guidance_scale=kandinsky_prefs['guidance_scale'])
                #images = pipe_kandinsky.generate_inpainting(kandinsky_prefs['prompt'], init_img, mask_img, batch_size=kandinsky_prefs['num_images'], w=kandinsky_prefs['width'], h=kandinsky_prefs['height'], num_steps=kandinsky_prefs['steps'], prior_cf_scale=kandinsky_prefs['prior_cf_scale'], prior_steps=str(kandinsky_prefs['prior_steps']), sampler=kandinsky_prefs['sampler'], guidance_scale=kandinsky_prefs['guidance_scale'])
                images = pipe_kandinsky(
                    prompt=pr['prompt'], negative_prompt=pr['negative_prompt'],
                    prior_guidance_scale=kandinsky_prefs['prior_guidance_scale'], prior_num_inference_steps=kandinsky_prefs['prior_steps'],
                    #pr['prompt'],
                    image=init_img,
                    strength=pr['strength'],
                    #image_embeds=image_embeds,
                    #negative_image_embeds=negative_image_embeds,
                    num_images_per_prompt=pr['num_images'],
                    height=pr['height'],
                    width=pr['width'],
                    num_inference_steps=pr['steps'],
                    guidance_scale=pr['guidance_scale'],
                    generator=generator,
                    callback=callback_fnc,
                ).images
            elif task_type == "inpainting":
                #images = pipe_kandinsky.generate_text2img(kandinsky_prefs['prompt'], batch_size=kandinsky_prefs['num_images'], w=kandinsky_prefs['width'], h=kandinsky_prefs['height'], num_steps=kandinsky_prefs['steps'], denoised_type=kandinsky_prefs['denoised_type'], dynamic_threshold_v=kandinsky_prefs['dynamic_threshold_v'], sampler=kandinsky_prefs['sampler'], ddim_eta=kandinsky_prefs['ddim_eta'], guidance_scale=kandinsky_prefs['guidance_scale'])
                #images = pipe_kandinsky.generate_text2img(kandinsky_prefs['prompt'], batch_size=kandinsky_prefs['num_images'], w=kandinsky_prefs['width'], h=kandinsky_prefs['height'], num_steps=kandinsky_prefs['steps'], prior_cf_scale=kandinsky_prefs['prior_cf_scale'], prior_steps=str(kandinsky_prefs['prior_steps']), sampler=kandinsky_prefs['sampler'], guidance_scale=kandinsky_prefs['guidance_scale'])
                images = pipe_kandinsky(
                    prompt=pr['prompt'], negative_prompt=pr['negative_prompt'],
                    prior_guidance_scale=kandinsky_prefs['prior_guidance_scale'], prior_num_inference_steps=kandinsky_prefs['prior_steps'],
                    #pr['prompt'],
                    image=init_img,
                    mask_image=mask_img,
                    strength=pr['strength'],
                    #image_embeds=image_embeds,
                    #negative_image_embeds=negative_image_embeds,
                    num_images_per_prompt=pr['num_images'],
                    height=pr['height'],
                    width=pr['width'],
                    num_inference_steps=pr['steps'],
                    guidance_scale=pr['guidance_scale'],
                    generator=generator,
                    callback=callback_fnc,
                ).images
        except Exception as e:
            clear_last()
            clear_last()
            alert_msg(page, f"ERROR: Something went wrong generating {task_type} images...", content=Text(str(e)))
            return
        clear_last()
        clear_last()
        autoscroll(True)
        txt2img_output = stable_dir
        batch_output = prefs['image_output']
        txt2img_output = stable_dir
        if bool(kandinsky_prefs['batch_folder_name']):
            txt2img_output = os.path.join(stable_dir, kandinsky_prefs['batch_folder_name'])
        if not os.path.exists(txt2img_output):
            os.makedirs(txt2img_output)
        #print(str(images))
        if images is None:
            prt(f"ERROR: Problem generating images, check your settings and run again, or report the error to Skquark if it really seems broken.")
            return
        idx = 0
        for image in images:
            fname = format_filename(pr['prompt'])
            #seed_suffix = f"-{random_seed}" if bool(prefs['file_suffix_seed']) else ''
            fname = f'{kandinsky_prefs["file_prefix"]}{fname}'
            image_path = available_file(txt2img_output, fname, 1)
            image.save(image_path)
            output_file = image_path.rpartition(slash)[2]
            if not kandinsky_prefs['display_upscaled_image'] or not kandinsky_prefs['apply_ESRGAN_upscale']:
                #prt(Row([Img(src=image_path, width=kandinsky_prefs['width'], height=kandinsky_prefs['height'], fit=ImageFit.FILL, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
                prt(Row([ImageButton(src=image_path, width=pr['width'], height=pr['height'], data=image_path, page=page)], alignment=MainAxisAlignment.CENTER))
            #if save_to_GDrive:
            batch_output = os.path.join(prefs['image_output'], kandinsky_prefs['batch_folder_name'])
            if not os.path.exists(batch_output):
                os.makedirs(batch_output)
            if storage_type == "PyDrive Google Drive":
                newFolder = gdrive.CreateFile({'title': kandinsky_prefs['batch_folder_name'], "parents": [{"kind": "drive#fileLink", "id": prefs['image_output']}],"mimeType": "application/vnd.google-apps.folder"})
                newFolder.Upload()
                batch_output = newFolder
            #out_path = batch_output# if save_to_GDrive else txt2img_output
            out_path = image_path.rpartition(slash)[0]
            upscaled_path = os.path.join(out_path, output_file)

            if kandinsky_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
                os.chdir(os.path.join(dist_dir, 'Real-ESRGAN'))
                upload_folder = 'upload'
                result_folder = 'results'
                if os.path.isdir(upload_folder):
                    shutil.rmtree(upload_folder)
                if os.path.isdir(result_folder):
                    shutil.rmtree(result_folder)
                os.mkdir(upload_folder)
                os.mkdir(result_folder)
                short_name = f'{fname[:80]}-{idx}.png'
                dst_path = os.path.join(dist_dir, 'Real-ESRGAN', upload_folder, short_name)
                #print(f'Moving {fpath} to {dst_path}')
                #shutil.move(fpath, dst_path)
                shutil.copy(image_path, dst_path)
                faceenhance = ' --face_enhance' if kandinsky_prefs["face_enhance"] else ''
                run_sp(f'python inference_realesrgan.py -n realesr-general-x4v3 -i upload --outscale {kandinsky_prefs["enlarge_scale"]}{faceenhance}', cwd=os.path.join(dist_dir, 'Real-ESRGAN'), realtime=False)
                out_file = short_name.rpartition('.')[0] + '_out.png'
                shutil.move(os.path.join(dist_dir, 'Real-ESRGAN', result_folder, out_file), upscaled_path)
                # python inference_realesrgan.py --model_path experiments/pretrained_models/RealESRGAN_x4plus.pth --input upload --netscale 4 --outscale 3.5 --half --face_enhance
                image_path = upscaled_path
                os.chdir(stable_dir)
                if kandinsky_prefs['display_upscaled_image']:
                    time.sleep(0.6)
                    prt(Row([Img(src=upscaled_path, width=pr['width'] * float(kandinsky_prefs["enlarge_scale"]), height=pr['height'] * float(kandinsky_prefs["enlarge_scale"]), fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
            #else:
            #    time.sleep(1.2)
            #    shutil.copy(image_path, os.path.join(out_path, output_file))
            if prefs['save_image_metadata']:
                img = PILImage.open(image_path)
                metadata = PngInfo()
                metadata.add_text("artist", prefs['meta_ArtistName'])
                metadata.add_text("copyright", prefs['meta_Copyright'])
                metadata.add_text("software", "Stable Diffusion Deluxe" + f", upscaled {kandinsky_prefs['enlarge_scale']}x with ESRGAN" if unCLIP_image_interpolation_prefs['apply_ESRGAN_upscale'] else "")
                metadata.add_text("pipeline", f"Kandinsky 2.1 {task_type}")
                if prefs['save_config_in_metadata']:
                    #metadata.add_text("title", kandinsky_prefs['file_name'])
                    # TODO: Merge Metadata with pr[]
                    config_json = kandinsky_prefs.copy()
                    config_json['model_path'] = "kandinsky-community/kandinsky-2-2-decoder"
                    config_json['seed'] = random_seed
                    del config_json['num_images']
                    del config_json['display_upscaled_image']
                    del config_json['batch_folder_name']
                    if not config_json['apply_ESRGAN_upscale']:
                        del config_json['enlarge_scale']
                        del config_json['apply_ESRGAN_upscale']
                    metadata.add_text("config_json", json.dumps(config_json, ensure_ascii=True, indent=4))
                img.save(image_path, pnginfo=metadata)
            if storage_type == "Colab Google Drive":
                new_file = available_file(os.path.join(prefs['image_output'], kandinsky_prefs['batch_folder_name']), fname, 0)
                out_path = new_file
                shutil.copy(image_path, new_file)
            elif bool(prefs['image_output']):
                new_file = available_file(os.path.join(prefs['image_output'], kandinsky_prefs['batch_folder_name']), fname, 0)
                out_path = new_file
                shutil.copy(image_path, new_file)
            prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))
    autoscroll(False)
    if prefs['enable_sounds']: page.snd_alert.play()


loaded_kandinsky2_task = ""
def run_kandinsky2(page):
    global kandinsky2_prefs, pipe_kandinsky, prefs, loaded_kandinsky2_task
    #if status['installed_diffusers']:
    #  alert_msg(page, "Sorry, currently incompatible with Diffusers installed...", content=Text("To run Kandinsky, restart runtime fresh and DO NOT install HuggingFace Diffusers library first, but you can install ESRGAN to use. Kandinsky is currently using an older version of Transformers and we haven't figured out how to easily downgrade version yet to run models together.. Sorry, trying to fix."))
    #  return
    if not bool(kandinsky2_prefs['prompt']):
      alert_msg(page, "You must provide a text prompt to process your image generation...")
      return
    def prt(line):
      if type(line) == str:
        line = Text(line, size=17)
      page.Kandinsky2.controls.append(line)
      page.Kandinsky2.update()
    def clear_last():
      del page.Kandinsky2.controls[-1]
      page.Kandinsky2.update()
    def autoscroll(scroll=True):
      page.Kandinsky2.auto_scroll = scroll
      page.Kandinsky2.update()
    def clear_list():
      page.Kandinsky2.controls = page.Kandinsky2.controls[:1]
    progress = ProgressBar(bar_height=8)
    clear_list()
    autoscroll(True)
    prt(Installing("Installing Kandinsky 2.1 Engine & Models... See console log for progress."))
    clear_pipes("kandinsky")
    '''try:
        if transformers.__version__ != "4.23.1": # Kandinsky conflict
          run_sp("pip uninstall -y transformers", realtime=True)
          run_process("pip uninstall -y git+https://github.com/huggingface/transformers", realtime=False)
    except Exception:
        pass
    finally:
        run_sp("pip install --target lib --upgrade transformers==4.23.1 -q", realtime=True)
        #print(f"Installed transformers v{transformers.__version__}")
    run_process("pip install -q sentencepiece", realtime=False)'''
    try:
        import accelerate
    except ModuleNotFoundError:
        run_sp("pip install -q --upgrade git+https://github.com/huggingface/accelerate.git", realtime=True)
        pass
    try:
        import clip
    except ModuleNotFoundError:
        run_sp('pip install git+https://github.com/openai/CLIP.git', realtime=True)
        pass
    try:
        from kandinsky2 import get_kandinsky2
    except ModuleNotFoundError:
        #run_process("pip install transformers==4.23.1 --upgrade --force-reinstall -q", realtime=False)
        #run_process("pip install -q git+https://github.com/ai-forever/Kandinsky-2.0.git", realtime=False)
        #run_sp('pip install -q "git+https://github.com/ai-forever/Kandinsky-2.0.git"', realtime=True)
        run_sp('pip install "git+https://github.com/Skquark/Kandinsky-2.git"', realtime=True)
        from kandinsky2 import get_kandinsky2
        pass
    import requests
    from io import BytesIO
    from PIL import ImageOps
    #save_dir = os.path.join(root_dir, 'kandinsky2_inputs')
    init_img = None
    if bool(kandinsky2_prefs['init_image']):
        fname = kandinsky2_prefs['init_image'].rpartition(slash)[2]
        #init_file = os.path.join(save_dir, fname)
        if kandinsky2_prefs['init_image'].startswith('http'):
            init_img = PILImage.open(requests.get(kandinsky2_prefs['init_image'], stream=True).raw)
        else:
            if os.path.isfile(kandinsky2_prefs['init_image']):
                init_img = PILImage.open(kandinsky2_prefs['init_image'])
            else:
                alert_msg(page, f"ERROR: Couldn't find your init_image {kandinsky2_prefs['init_image']}")
                return
        init_img = init_img.resize((kandinsky2_prefs['width'], kandinsky2_prefs['height']), resample=PILImage.Resampling.LANCZOS)
        init_img = ImageOps.exif_transpose(init_img).convert("RGB")
        #init_img.save(init_file)
    mask_img = None
    if bool(kandinsky2_prefs['mask_image']):
        fname = kandinsky2_prefs['init_image'].rpartition(slash)[2]
        #mask_file = os.path.join(save_dir, fname)
        if kandinsky2_prefs['mask_image'].startswith('http'):
            mask_img = PILImage.open(requests.get(kandinsky2_prefs['mask_image'], stream=True).raw)
        else:
            if os.path.isfile(kandinsky2_prefs['mask_image']):
                mask_img = PILImage.open(kandinsky2_prefs['mask_image'])
            else:
                alert_msg(page, f"ERROR: Couldn't find your mask_image {kandinsky2_prefs['mask_image']}")
                return
            if kandinsky2_prefs['invert_mask']:
                mask_img = ImageOps.invert(mask_img.convert('RGB'))
        mask_img = mask_img.resize((kandinsky2_prefs['width'], kandinsky2_prefs['height']), resample=PILImage.NEAREST)
        mask_img = ImageOps.exif_transpose(mask_img).convert("RGB")
        mask_img = numpy.asarray(mask_img)
        #mask_img.save(mask_file)
    #print(f'Resize to {width}x{height}')
    task_type = "inpainting" if bool(kandinsky2_prefs['init_image']) and bool(kandinsky2_prefs['mask_image']) else "text2img"
    if pipe_kandinsky == None or loaded_kandinsky2_task != task_type:
        clear_pipes()
        try:
            #if bool(kandinsky2_prefs['init_image']) and not bool(kandinsky2_prefs['mask_image']):
            #    pipe_kandinsky = get_kandinsky2('cuda', task_type='img2img', model_version='2.1', use_flash_attention=False, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
            pipe_kandinsky = get_kandinsky2('cuda', task_type=task_type, model_version='2.1', use_flash_attention=False, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
            loaded_kandinsky2_task = task_type
        except Exception as e:
            clear_last()
            alert_msg(page, f"ERROR Initializing Kandinsky, try running without installing Diffusers first...", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))
            return
    else:
        clear_pipes('kandinsky')
    clear_last()
    prt("Generating your Kandinsky 2.1 Image...")
    prt(progress)
    autoscroll(False)

    try:
        if bool(kandinsky2_prefs['init_image']) and not bool(kandinsky2_prefs['mask_image']):
            images_texts = [kandinsky2_prefs['prompt'], init_img]
            weights = [0.5, kandinsky2_prefs['strength']]
            #images = pipe_kandinsky.generate_img2img(kandinsky2_prefs['prompt'], init_img, strength=kandinsky2_prefs['strength'], batch_size=kandinsky2_prefs['num_images'], w=kandinsky2_prefs['width'], h=kandinsky2_prefs['height'], num_steps=kandinsky2_prefs['steps'], denoised_type=kandinsky2_prefs['denoised_type'], dynamic_threshold_v=kandinsky2_prefs['dynamic_threshold_v'], sampler=kandinsky2_prefs['sampler'], ddim_eta=kandinsky2_prefs['ddim_eta'], guidance_scale=kandinsky2_prefs['guidance_scale'])
            #images = pipe_kandinsky.generate_img2img(kandinsky2_prefs['prompt'], init_img, strength=kandinsky2_prefs['strength'], batch_size=kandinsky2_prefs['num_images'], w=kandinsky2_prefs['width'], h=kandinsky2_prefs['height'], num_steps=kandinsky2_prefs['steps'], prior_cf_scale=kandinsky2_prefs['prior_cf_scale'], prior_steps=str(kandinsky2_prefs['prior_steps']), sampler=kandinsky2_prefs['sampler'], guidance_scale=kandinsky2_prefs['guidance_scale'])
            images = pipe_kandinsky.mix_images(images_texts, weights, batch_size=kandinsky2_prefs['num_images'], w=kandinsky2_prefs['width'], h=kandinsky2_prefs['height'], num_steps=kandinsky2_prefs['steps'], prior_cf_scale=kandinsky2_prefs['prior_cf_scale'], prior_steps=str(kandinsky2_prefs['prior_steps']), sampler=kandinsky2_prefs['sampler'], guidance_scale=kandinsky2_prefs['guidance_scale'])
        elif bool(kandinsky2_prefs['init_image']) and bool(kandinsky2_prefs['mask_image']):
            #images = pipe_kandinsky.generate_inpainting(kandinsky2_prefs['prompt'], init_img, mask_img, batch_size=kandinsky2_prefs['num_images'], w=kandinsky2_prefs['width'], h=kandinsky2_prefs['height'], num_steps=kandinsky2_prefs['steps'], denoised_type=kandinsky2_prefs['denoised_type'], dynamic_threshold_v=kandinsky2_prefs['dynamic_threshold_v'], sampler=kandinsky2_prefs['sampler'], ddim_eta=kandinsky2_prefs['ddim_eta'], guidance_scale=kandinsky2_prefs['guidance_scale'])
            images = pipe_kandinsky.generate_inpainting(kandinsky2_prefs['prompt'], init_img, mask_img, batch_size=kandinsky2_prefs['num_images'], w=kandinsky2_prefs['width'], h=kandinsky2_prefs['height'], num_steps=kandinsky2_prefs['steps'], prior_cf_scale=kandinsky2_prefs['prior_cf_scale'], prior_steps=str(kandinsky2_prefs['prior_steps']), sampler=kandinsky2_prefs['sampler'], guidance_scale=kandinsky2_prefs['guidance_scale'])
        else:
            #images = pipe_kandinsky.generate_text2img(kandinsky2_prefs['prompt'], batch_size=kandinsky2_prefs['num_images'], w=kandinsky2_prefs['width'], h=kandinsky2_prefs['height'], num_steps=kandinsky2_prefs['steps'], denoised_type=kandinsky2_prefs['denoised_type'], dynamic_threshold_v=kandinsky2_prefs['dynamic_threshold_v'], sampler=kandinsky2_prefs['sampler'], ddim_eta=kandinsky2_prefs['ddim_eta'], guidance_scale=kandinsky2_prefs['guidance_scale'])
            images = pipe_kandinsky.generate_text2img(kandinsky2_prefs['prompt'], batch_size=kandinsky2_prefs['num_images'], w=kandinsky2_prefs['width'], h=kandinsky2_prefs['height'], num_steps=kandinsky2_prefs['steps'], prior_cf_scale=kandinsky2_prefs['prior_cf_scale'], prior_steps=str(kandinsky2_prefs['prior_steps']), sampler=kandinsky2_prefs['sampler'], guidance_scale=kandinsky2_prefs['guidance_scale'])
    except Exception as e:
        clear_last()
        clear_last()
        alert_msg(page, f"ERROR: Something went wrong generating images...", content=Text(str(e)))
        return
    clear_last()
    clear_last()
    autoscroll(True)
    txt2img_output = stable_dir
    batch_output = prefs['image_output']
    #print(str(images))
    if images is None:
        prt(f"ERROR: Problem generating images, check your settings and run again, or report the error to Skquark if it really seems broken.")
        return
    idx = 0
    for image in images:
        fname = format_filename(kandinsky2_prefs['prompt'])
        #seed_suffix = f"-{random_seed}" if bool(prefs['file_suffix_seed']) else ''
        fname = f'{kandinsky2_prefs["file_prefix"]}{fname}'
        txt2img_output = stable_dir
        if bool(kandinsky2_prefs['batch_folder_name']):
            txt2img_output = os.path.join(stable_dir, kandinsky2_prefs['batch_folder_name'])
        if not os.path.exists(txt2img_output):
            os.makedirs(txt2img_output)
        image_path = available_file(txt2img_output, fname, 1)
        image.save(image_path)
        new_file = image_path.rpartition(slash)[2]
        if not kandinsky2_prefs['display_upscaled_image'] or not kandinsky2_prefs['apply_ESRGAN_upscale']:
            #prt(Row([Img(src=image_path, width=kandinsky2_prefs['width'], height=kandinsky2_prefs['height'], fit=ImageFit.FILL, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
            prt(Row([ImageButton(src=image_path, width=kandinsky2_prefs['width'], height=kandinsky2_prefs['height'], data=image_path, page=page)], alignment=MainAxisAlignment.CENTER))
        #if save_to_GDrive:
        batch_output = os.path.join(prefs['image_output'], kandinsky2_prefs['batch_folder_name'])
        if not os.path.exists(batch_output):
            os.makedirs(batch_output)
        if storage_type == "PyDrive Google Drive":
            newFolder = gdrive.CreateFile({'title': kandinsky2_prefs['batch_folder_name'], "parents": [{"kind": "drive#fileLink", "id": prefs['image_output']}],"mimeType": "application/vnd.google-apps.folder"})
            newFolder.Upload()
            batch_output = newFolder
        out_path = batch_output if save_to_GDrive else txt2img_output

        if kandinsky2_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
            os.chdir(os.path.join(dist_dir, 'Real-ESRGAN'))
            upload_folder = 'upload'
            result_folder = 'results'
            if os.path.isdir(upload_folder):
                shutil.rmtree(upload_folder)
            if os.path.isdir(result_folder):
                shutil.rmtree(result_folder)
            os.mkdir(upload_folder)
            os.mkdir(result_folder)
            short_name = f'{fname[:80]}-{idx}.png'
            dst_path = os.path.join(dist_dir, 'Real-ESRGAN', upload_folder, short_name)
            #print(f'Moving {fpath} to {dst_path}')
            #shutil.move(fpath, dst_path)
            shutil.copy(image_path, dst_path)
            faceenhance = ' --face_enhance' if kandinsky2_prefs["face_enhance"] else ''
            run_sp(f'python inference_realesrgan.py -n realesr-general-x4v3 -i upload --outscale {kandinsky2_prefs["enlarge_scale"]}{faceenhance}', cwd=os.path.join(dist_dir, 'Real-ESRGAN'), realtime=False)
            out_file = short_name.rpartition('.')[0] + '_out.png'
            upscaled_path = os.path.join(out_path, new_file)
            shutil.move(os.path.join(dist_dir, 'Real-ESRGAN', result_folder, out_file), upscaled_path)
            # python inference_realesrgan.py --model_path experiments/pretrained_models/RealESRGAN_x4plus.pth --input upload --netscale 4 --outscale 3.5 --half --face_enhance
            os.chdir(stable_dir)
            if kandinsky2_prefs['display_upscaled_image']:
                time.sleep(0.6)
                prt(Row([Img(src=upscaled_path, width=kandinsky2_prefs['width'] * float(kandinsky2_prefs["enlarge_scale"]), height=kandinsky2_prefs['height'] * float(kandinsky2_prefs["enlarge_scale"]), fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
        else:
            shutil.copy(image_path, os.path.join(out_path, new_file))
        # TODO: Add Metadata
        prt(Row([Text(new_file)], alignment=MainAxisAlignment.CENTER))
    autoscroll(False)
    if prefs['enable_sounds']: page.snd_alert.play()


def run_kandinsky_fuse(page):
    global kandinsky_fuse_prefs, pipe_kandinsky, pipe_kandinsky_prior, prefs, loaded_kandinsky_task
    if not status['installed_diffusers']:
      alert_msg(page, "You need to Install HuggingFace Diffusers before using...")
      return
    if int(status['cpu_memory']) <= 12:
      alert_msg(page, f"Sorry, you only have {int(status['cpu_memory'])}GB RAM which is not quite enough to run Kandinsky 2.2 right now. Either Change runtime type to High-RAM mode and restart or use other Kandinsky 2.1 in Extras.")
      return
    if len(kandinsky_fuse_prefs['mixes']) < 1:
      alert_msg(page, "You must provide layers to fuse to process your image generation...")
      return
    def prt(line):
      if type(line) == str:
        line = Text(line, size=17)
      page.KandinskyFuse.controls.append(line)
      page.KandinskyFuse.update()
    def clear_last():
      del page.KandinskyFuse.controls[-1]
      page.KandinskyFuse.update()
    def autoscroll(scroll=True):
      page.KandinskyFuse.auto_scroll = scroll
      page.KandinskyFuse.update()
    def clear_list():
      page.KandinskyFuse.controls = page.KandinskyFuse.controls[:1]
    progress = ProgressBar(bar_height=8)
    total_steps = kandinsky_fuse_prefs['steps']
    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep}"
      progress.update()
    clear_list()
    autoscroll(True)
    prt(Installing("Installing Kandinsky 2.2 Engine & Models... See console log for progress."))
    clear_pipes("kandinsky")
    import requests
    from io import BytesIO
    from PIL import ImageOps
    from diffusers import KandinskyV22PriorPipeline, KandinskyV22Pipeline
    if pipe_kandinsky_prior == None:
        pipe_kandinsky_prior = KandinskyV22PriorPipeline.from_pretrained(
            "kandinsky-community/kandinsky-2-2-prior", torch_dtype=torch.float16
        )
        pipe_kandinsky_prior.to("cuda")
    #save_dir = os.path.join(root_dir, 'kandinsky_fuse_inputs')
    images_texts = []
    weights = []
    mix_names = []
    for mix in kandinsky_fuse_prefs['mixes']:
        if 'prompt' in mix:
            images_texts.append(mix['prompt'])
            mix_names.append(mix['prompt'])
        else:
            init_img = None
            fname = mix['init_image'].rpartition(slash)[2]
            #init_file = os.path.join(save_dir, fname)
            if mix['init_image'].startswith('http'):
                init_img = PILImage.open(requests.get(mix['init_image'], stream=True).raw)
            else:
                if os.path.isfile(mix['init_image']):
                    init_img = PILImage.open(mix['init_image'])
                else:
                    alert_msg(page, f"ERROR: Couldn't find your init_image {mix['init_image']}")
                    return
            init_img = init_img.resize((kandinsky_fuse_prefs['width'], kandinsky_fuse_prefs['height']), resample=PILImage.Resampling.LANCZOS)
            init_img = ImageOps.exif_transpose(init_img).convert("RGB")
            images_texts.append(init_img)
        weights.append(mix['weight'])
    mix_name = " - ".join(mix_names)
    #print(f'Resize to {width}x{height}')
    task_type = "text2img"
    if pipe_kandinsky == None or loaded_kandinsky_task != task_type:
        clear_pipes('kandinsky_prior')
        try:
            #pipe_kandinsky = get_kandinsky2('cuda', task_type='text2img', model_version='2.1', use_flash_attention=False, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
            pipe_kandinsky = KandinskyV22Pipeline.from_pretrained("kandinsky-community/kandinsky-2-2-decoder", torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
            if prefs['enable_torch_compile']:
                pipe_kandinsky.unet.to(memory_format=torch.channels_last)
                pipe_kandinsky.unet = torch.compile(pipe_kandinsky.unet, mode="reduce-overhead", fullgraph=True)
            else:
                pipe_kandinsky.to("cuda")
            loaded_kandinsky_task = "text2img"
        except Exception as e:
            clear_last()
            alert_msg(page, f"ERROR Initializing Kandinsky, try running without installing Diffusers first...", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))
            return
    else:
        clear_pipes("kandinsky")
    clear_last()
    prt("Generating your Kandinsky 2.2 Fused Image...")
    prt(progress)
    autoscroll(False)
    random_seed = int(kandinsky_fuse_prefs['seed']) if int(kandinsky_fuse_prefs['seed']) > 0 else rnd.randint(0,4294967295)
    generator = torch.Generator(device="cuda").manual_seed(random_seed)

    try:
        prior_out = pipe_kandinsky_prior.interpolate(images_texts, weights)
        #images = pipe_kandinsky.mix_images(images_texts, weights, batch_size=kandinsky_fuse_prefs['num_images'], w=kandinsky_fuse_prefs['width'], h=kandinsky_fuse_prefs['height'], num_steps=kandinsky_fuse_prefs['steps'], prior_cf_scale=kandinsky_fuse_prefs['prior_cf_scale'], prior_steps=str(kandinsky_fuse_prefs['prior_steps']), sampler=kandinsky_fuse_prefs['sampler'], guidance_scale=kandinsky_fuse_prefs['guidance_scale'])
        images = pipe_kandinsky(
            "",
            **prior_out,
            num_images_per_prompt=kandinsky_fuse_prefs['num_images'],
            height=kandinsky_fuse_prefs['height'],
            width=kandinsky_fuse_prefs['width'],
            num_inference_steps=kandinsky_fuse_prefs['steps'],
            guidance_scale=kandinsky_fuse_prefs['guidance_scale'],
            generator=generator,
            #callback=callback_fnc,
        ).images
    except Exception as e:
        clear_last()
        clear_last()
        alert_msg(page, f"ERROR: Something went wrong generating images...", content=Text(str(e)))
        return
    clear_last()
    clear_last()
    autoscroll(True)
    txt2img_output = stable_dir
    batch_output = prefs['image_output']
    #print(str(images))
    if images is None:
        prt(f"ERROR: Problem generating images, check your settings and run again, or report the error to Skquark if it really seems broken.")
        return
    idx = 0
    for image in images:
        fname = format_filename(mix_name)
        #seed_suffix = f"-{random_seed}" if bool(prefs['file_suffix_seed']) else ''
        fname = f'{kandinsky_fuse_prefs["file_prefix"]}{fname}'
        txt2img_output = stable_dir
        if bool(kandinsky_fuse_prefs['batch_folder_name']):
            txt2img_output = os.path.join(stable_dir, kandinsky_fuse_prefs['batch_folder_name'])
        if not os.path.exists(txt2img_output):
            os.makedirs(txt2img_output)
        image_path = available_file(txt2img_output, fname, 1)
        image.save(image_path)
        new_file = image_path.rpartition(slash)[2]
        if not kandinsky_fuse_prefs['display_upscaled_image'] or not kandinsky_fuse_prefs['apply_ESRGAN_upscale']:
            #prt(Row([Img(src=image_path, width=kandinsky_fuse_prefs['width'], height=kandinsky_fuse_prefs['height'], fit=ImageFit.FILL, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
            prt(Row([ImageButton(src=image_path, width=kandinsky_fuse_prefs['width'], height=kandinsky_fuse_prefs['height'], data=image_path, page=page)], alignment=MainAxisAlignment.CENTER))

        #if save_to_GDrive:
        batch_output = os.path.join(prefs['image_output'], kandinsky_fuse_prefs['batch_folder_name'])
        if not os.path.exists(batch_output):
            os.makedirs(batch_output)
        if storage_type == "PyDrive Google Drive":
            newFolder = gdrive.CreateFile({'title': kandinsky_fuse_prefs['batch_folder_name'], "parents": [{"kind": "drive#fileLink", "id": prefs['image_output']}],"mimeType": "application/vnd.google-apps.folder"})
            newFolder.Upload()
            batch_output = newFolder
        out_path = batch_output# if save_to_GDrive else txt2img_output

        if kandinsky_fuse_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
            os.chdir(os.path.join(dist_dir, 'Real-ESRGAN'))
            upload_folder = 'upload'
            result_folder = 'results'
            if os.path.isdir(upload_folder):
                shutil.rmtree(upload_folder)
            if os.path.isdir(result_folder):
                shutil.rmtree(result_folder)
            os.mkdir(upload_folder)
            os.mkdir(result_folder)
            short_name = f'{fname[:80]}-{idx}.png'
            dst_path = os.path.join(dist_dir, 'Real-ESRGAN', upload_folder, short_name)
            #print(f'Moving {fpath} to {dst_path}')
            #shutil.move(fpath, dst_path)
            shutil.copy(image_path, dst_path)
            faceenhance = ' --face_enhance' if kandinsky_fuse_prefs["face_enhance"] else ''
            run_sp(f'python inference_realesrgan.py -n realesr-general-x4v3 -i upload --outscale {kandinsky_fuse_prefs["enlarge_scale"]}{faceenhance}', cwd=os.path.join(dist_dir, 'Real-ESRGAN'), realtime=False)
            out_file = short_name.rpartition('.')[0] + '_out.png'
            upscaled_path = os.path.join(out_path, new_file)
            shutil.move(os.path.join(dist_dir, 'Real-ESRGAN', result_folder, out_file), upscaled_path)
            # python inference_realesrgan.py --model_path experiments/pretrained_models/RealESRGAN_x4plus.pth --input upload --netscale 4 --outscale 3.5 --half --face_enhance
            os.chdir(stable_dir)
            if kandinsky_fuse_prefs['display_upscaled_image']:
                time.sleep(0.6)
                prt(Row([Img(src=upscaled_path, width=kandinsky_fuse_prefs['width'] * float(kandinsky_fuse_prefs["enlarge_scale"]), height=kandinsky_fuse_prefs['height'] * float(kandinsky_fuse_prefs["enlarge_scale"]), fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
        else:
            time.sleep(1.2)
            shutil.copy(image_path, os.path.join(out_path, new_file))
        # TODO: Add Metadata
        prt(Row([Text(new_file)], alignment=MainAxisAlignment.CENTER))
    autoscroll(False)
    if prefs['enable_sounds']: page.snd_alert.play()

def run_kandinsky2_fuse(page):
    global kandinsky2_fuse_prefs, pipe_kandinsky, prefs, loaded_kandinsky2_task
    #if status['installed_diffusers']:
    #  alert_msg(page, "Sorry, currently incompatible with Diffusers installed...", content=Text("To run Kandinsky, restart runtime fresh and DO NOT install HuggingFace Diffusers library first, but you can install ESRGAN to use. Kandinsky is currently using an older version of Transformers and we haven't figured out how to easily downgrade version yet to run models together.. Sorry, trying to fix."))
    #  return
    if len(kandinsky2_fuse_prefs['mixes']) < 1:
      alert_msg(page, "You must provide layers to fuse to process your image generation...")
      return
    def prt(line):
      if type(line) == str:
        line = Text(line, size=17)
      page.Kandinsky2Fuse.controls.append(line)
      page.Kandinsky2Fuse.update()
    def clear_last():
      del page.Kandinsky2Fuse.controls[-1]
      page.Kandinsky2Fuse.update()
    def autoscroll(scroll=True):
      page.Kandinsky2Fuse.auto_scroll = scroll
      page.Kandinsky2Fuse.update()
    def clear_list():
      page.Kandinsky2Fuse.controls = page.Kandinsky2Fuse.controls[:1]
    progress = ProgressBar(bar_height=8)
    clear_list()
    autoscroll(True)
    prt(Installing("Installing Kandinsky 2.1 Engine & Models... See console log for progress."))
    clear_pipes("kandinsky")
    try:
        import clip
    except ModuleNotFoundError:
        run_sp('pip install git+https://github.com/openai/CLIP.git', realtime=False)
    try:
        from kandinsky2 import get_kandinsky2
    except ModuleNotFoundError:
        run_sp('pip install -q "git+https://github.com/Skquark/Kandinsky-2.git"', realtime=False)
        from kandinsky2 import get_kandinsky2
        pass
    import requests
    from io import BytesIO
    from PIL import ImageOps
    #save_dir = os.path.join(root_dir, 'kandinsky2_fuse_inputs')
    images_texts = []
    weights = []
    mix_names = []
    for mix in kandinsky2_fuse_prefs['mixes']:
        if 'prompt' in mix:
            images_texts.append(mix['prompt'])
            mix_names.append(mix['prompt'])
        else:
            init_img = None
            fname = mix['init_image'].rpartition(slash)[2]
            #init_file = os.path.join(save_dir, fname)
            if mix['init_image'].startswith('http'):
                init_img = PILImage.open(requests.get(mix['init_image'], stream=True).raw)
            else:
                if os.path.isfile(mix['init_image']):
                    init_img = PILImage.open(mix['init_image'])
                else:
                    alert_msg(page, f"ERROR: Couldn't find your init_image {mix['init_image']}")
                    return
            init_img = init_img.resize((kandinsky2_fuse_prefs['width'], kandinsky2_fuse_prefs['height']), resample=PILImage.Resampling.LANCZOS)
            init_img = ImageOps.exif_transpose(init_img).convert("RGB")
            images_texts.append(init_img)
        weights.append(mix['weight'])
    mix_name = " - ".join(mix_names)
    #print(f'Resize to {width}x{height}')
    task_type = "text2img"
    if pipe_kandinsky == None or loaded_kandinsky2_task != task_type:
        clear_pipes()
        try:
            pipe_kandinsky = get_kandinsky2('cuda', task_type='text2img', model_version='2.1', use_flash_attention=False, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
            loaded_kandinsky2_task = "text2img"
        except Exception as e:
            clear_last()
            alert_msg(page, f"ERROR Initializing Kandinsky, try running without installing Diffusers first...", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))
            return
    else:
        clear_pipes("kandinsky")
    clear_last()
    prt("Generating your Kandinsky 2.1 Fused Image...")
    prt(progress)
    autoscroll(False)

    try:
        images = pipe_kandinsky.mix_images(images_texts, weights, batch_size=kandinsky2_fuse_prefs['num_images'], w=kandinsky2_fuse_prefs['width'], h=kandinsky2_fuse_prefs['height'], num_steps=kandinsky2_fuse_prefs['steps'], prior_cf_scale=kandinsky2_fuse_prefs['prior_cf_scale'], prior_steps=str(kandinsky2_fuse_prefs['prior_steps']), sampler=kandinsky2_fuse_prefs['sampler'], guidance_scale=kandinsky2_fuse_prefs['guidance_scale'])
    except Exception as e:
        clear_last()
        clear_last()
        alert_msg(page, f"ERROR: Something went wrong generating images...", content=Text(str(e)))
        return
    clear_last()
    clear_last()
    autoscroll(True)
    txt2img_output = stable_dir
    batch_output = prefs['image_output']
    #print(str(images))
    if images is None:
        prt(f"ERROR: Problem generating images, check your settings and run again, or report the error to Skquark if it really seems broken.")
        return
    idx = 0
    for image in images:
        fname = format_filename(mix_name)
        #seed_suffix = f"-{random_seed}" if bool(prefs['file_suffix_seed']) else ''
        fname = f'{kandinsky2_fuse_prefs["file_prefix"]}{fname}'
        txt2img_output = stable_dir
        if bool(kandinsky2_fuse_prefs['batch_folder_name']):
            txt2img_output = os.path.join(stable_dir, kandinsky2_fuse_prefs['batch_folder_name'])
        if not os.path.exists(txt2img_output):
            os.makedirs(txt2img_output)
        image_path = available_file(txt2img_output, fname, 1)
        image.save(image_path)
        new_file = image_path.rpartition(slash)[2]
        if not kandinsky2_fuse_prefs['display_upscaled_image'] or not kandinsky2_fuse_prefs['apply_ESRGAN_upscale']:
            #prt(Row([Img(src=image_path, width=kandinsky2_fuse_prefs['width'], height=kandinsky2_fuse_prefs['height'], fit=ImageFit.FILL, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
            prt(Row([ImageButton(src=image_path, width=kandinsky2_fuse_prefs['width'], height=kandinsky2_fuse_prefs['height'], data=image_path, page=page)], alignment=MainAxisAlignment.CENTER))

        #if save_to_GDrive:
        batch_output = os.path.join(prefs['image_output'], kandinsky2_fuse_prefs['batch_folder_name'])
        if not os.path.exists(batch_output):
            os.makedirs(batch_output)
        if storage_type == "PyDrive Google Drive":
            newFolder = gdrive.CreateFile({'title': kandinsky2_fuse_prefs['batch_folder_name'], "parents": [{"kind": "drive#fileLink", "id": prefs['image_output']}],"mimeType": "application/vnd.google-apps.folder"})
            newFolder.Upload()
            batch_output = newFolder
        out_path = batch_output if save_to_GDrive else txt2img_output

        if kandinsky2_fuse_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
            os.chdir(os.path.join(dist_dir, 'Real-ESRGAN'))
            upload_folder = 'upload'
            result_folder = 'results'
            if os.path.isdir(upload_folder):
                shutil.rmtree(upload_folder)
            if os.path.isdir(result_folder):
                shutil.rmtree(result_folder)
            os.mkdir(upload_folder)
            os.mkdir(result_folder)
            short_name = f'{fname[:80]}-{idx}.png'
            dst_path = os.path.join(dist_dir, 'Real-ESRGAN', upload_folder, short_name)
            #print(f'Moving {fpath} to {dst_path}')
            #shutil.move(fpath, dst_path)
            shutil.copy(image_path, dst_path)
            faceenhance = ' --face_enhance' if kandinsky2_fuse_prefs["face_enhance"] else ''
            run_sp(f'python inference_realesrgan.py -n realesr-general-x4v3 -i upload --outscale {kandinsky2_fuse_prefs["enlarge_scale"]}{faceenhance}', cwd=os.path.join(dist_dir, 'Real-ESRGAN'), realtime=False)
            out_file = short_name.rpartition('.')[0] + '_out.png'
            upscaled_path = os.path.join(out_path, new_file)
            shutil.move(os.path.join(dist_dir, 'Real-ESRGAN', result_folder, out_file), upscaled_path)
            # python inference_realesrgan.py --model_path experiments/pretrained_models/RealESRGAN_x4plus.pth --input upload --netscale 4 --outscale 3.5 --half --face_enhance
            os.chdir(stable_dir)
            if kandinsky2_fuse_prefs['display_upscaled_image']:
                time.sleep(0.6)
                prt(Row([Img(src=upscaled_path, width=kandinsky2_fuse_prefs['width'] * float(kandinsky2_fuse_prefs["enlarge_scale"]), height=kandinsky2_fuse_prefs['height'] * float(kandinsky2_fuse_prefs["enlarge_scale"]), fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
        else:
            shutil.copy(image_path, os.path.join(out_path, new_file))
        # TODO: Add Metadata
        prt(Row([Text(new_file)], alignment=MainAxisAlignment.CENTER))
    autoscroll(False)
    if prefs['enable_sounds']: page.snd_alert.play()

def run_kandinsky_controlnet(page, from_list=False, with_params=False):
    global kandinsky_controlnet_prefs, pipe_kandinsky, pipe_kandinsky_controlnet_prior, prefs, loaded_kandinsky_task, depth_estimator
    if not status['installed_diffusers']:
      alert_msg(page, "You need to Install HuggingFace Diffusers before using...")
      return
    if int(status['cpu_memory']) <= 12:
      alert_msg(page, f"Sorry, you only have {int(status['cpu_memory'])}GB RAM which is not quite enough to run Kandinsky 2.2 right now. Either Change runtime type to High-RAM mode and restart or use other Kandinsky 2.1 in Extras.")
      return
    kandinsky_controlnet_prompts = []
    if from_list:
      if len(prompts) < 1:
        alert_msg(page, "You need to add Prompts to your List first... ")
        return
      for p in prompts:
        if with_params:
            kandinsky_controlnet_prompts.append({'prompt': p.prompt, 'negative_prompt':p['negative_prompt'], 'init_image':kandinsky_controlnet_prefs['init_image'], 'guidance_scale':kandinsky_controlnet_prefs['guidance_scale'], 'steps':kandinsky_controlnet_prefs['steps'], 'width':kandinsky_controlnet_prefs['width'], 'height':kandinsky_controlnet_prefs['height'], 'strength':kandinsky_controlnet_prefs['strength'], 'num_images':kandinsky_controlnet_prefs['num_images'], 'batch_size':kandinsky_controlnet_prefs['batch_size'], 'seed':kandinsky_controlnet_prefs['seed']})
        else:
            kandinsky_controlnet_prompts.append({'prompt': p.prompt, 'negative_prompt':p['negative_prompt'], 'init_image':p['init_image'], 'guidance_scale':p['guidance_scale'], 'steps':p['steps'], 'width':p['width'], 'height':p['height'], 'strength':p['init_image_strength'], 'num_images':p['n_iterations'], 'batch_size':p['batch_size'], 'seed':p['seed']})
    else:
      if not bool(kandinsky_controlnet_prefs['prompt']):
        alert_msg(page, "You must provide a text prompt to process your image generation...")
        return
      kandinsky_controlnet_prompts.append({'prompt': kandinsky_controlnet_prefs['prompt'], 'negative_prompt':kandinsky_controlnet_prefs['negative_prompt'], 'init_image':kandinsky_controlnet_prefs['init_image'], 'guidance_scale':kandinsky_controlnet_prefs['guidance_scale'], 'steps':kandinsky_controlnet_prefs['steps'], 'width':kandinsky_controlnet_prefs['width'], 'height':kandinsky_controlnet_prefs['height'], 'strength':kandinsky_controlnet_prefs['strength'], 'num_images':kandinsky_controlnet_prefs['num_images'], 'batch_size':kandinsky_controlnet_prefs['batch_size'], 'seed':kandinsky_controlnet_prefs['seed']})
    def prt(line, update=True):
      if type(line) == str:
        line = Text(line, size=17)
      if from_list:
        page.imageColumn.controls.append(line)
        if update:
          page.imageColumn.update()
      else:
        page.KandinskyControlNet.controls.append(line)
        if update:
          page.KandinskyControlNet.update()
    def clear_last():
      if from_list:
        del page.imageColumn.controls[-1]
        page.imageColumn.update()
      else:
        del page.KandinskyControlNet.controls[-1]
        page.KandinskyControlNet.update()
    def autoscroll(scroll=True):
      if from_list:
        page.imageColumn.auto_scroll = scroll
        page.imageColumn.update()
        page.KandinskyControlNet.auto_scroll = scroll
        page.KandinskyControlNet.update()
      else:
        page.KandinskyControlNet.auto_scroll = scroll
        page.KandinskyControlNet.update()
    def clear_list():
      if from_list:
        page.imageColumn.controls.clear()
      else:
        page.KandinskyControlNet.controls = page.KandinskyControlNet.controls[:1]
    progress = ProgressBar(bar_height=8)
    total_steps = kandinsky_controlnet_prefs['steps']
    def callback_fnc(step: int, timestep: int, latents: torch.FloatTensor) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}  Timestep: {timestep}"
      progress.update()
    if from_list:
      page.tabs.selected_index = 4
      page.tabs.update()
    clear_list()
    autoscroll(True)
    installer = Installing("Installing Kandinsky 2.2 Engine & Models... See console log for progress.")
    prt(installer)
    clear_pipes("kandinsky")
    import requests
    from io import BytesIO
    from PIL.PngImagePlugin import PngInfo
    from PIL import ImageOps
    from diffusers import KandinskyV22PriorEmb2EmbPipeline, KandinskyV22ControlnetImg2ImgPipeline
    from transformers import pipeline
    if pipe_kandinsky_controlnet_prior == None:
        installer.set_details("...kandinsky-2-2-prior Emb2Emb Pipeline")
        pipe_kandinsky_controlnet_prior = KandinskyV22PriorEmb2EmbPipeline.from_pretrained("kandinsky-community/kandinsky-2-2-prior",torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
        pipe_kandinsky_controlnet_prior.to("cuda")
    task_type = "controlnet"
    if pipe_kandinsky == None or loaded_kandinsky_task != task_type:
        installer.set_details(f"...kandinsky-2-2 {task_type} Pipeline")
        clear_pipes('kandinsky_controlnet_prior')
        try:
            if task_type == "controlnet":
                pipe_kandinsky = KandinskyV22ControlnetImg2ImgPipeline.from_pretrained("kandinsky-community/kandinsky-2-2-controlnet-depth", torch_dtype=torch.float16, cache_dir=prefs['cache_dir'] if bool(prefs['cache_dir']) else None)
            if prefs['enable_torch_compile']:
                installer.set_details(f"...run torch compile")
                pipe_kandinsky.unet.to(memory_format=torch.channels_last)
                pipe_kandinsky.unet = torch.compile(pipe_kandinsky.unet, mode="reduce-overhead", fullgraph=True)
                pipe_kandinsky = pipe_kandinsky.to("cuda")
            elif int(status['cpu_memory']) <= 12:
                installer.set_details(f"...enable_model_cpu_offload")
                pipe_kandinsky.enable_model_cpu_offload()
            else:
                pipe_kandinsky.to("cuda")
            loaded_kandinsky_task = task_type
        except Exception as e:
            clear_last()
            alert_msg(page, f"ERROR Initializing KandinskyV22ControlnetImg2ImgPipeline...", content=Column([Text(str(e)), Text(str(traceback.format_exc()))]))
            return
    else:
        clear_pipes('kandinsky')
    #save_dir = os.path.join(root_dir, 'kandinsky_controlnet_inputs')
    def make_hint(image, depth_estimator):
        image = depth_estimator(image)['depth']
        image = np.array(image)
        image = image[:, :, None]
        image = np.concatenate([image, image, image], axis=2)
        detected_map = torch.from_numpy(image).float() / 255.0
        hint = detected_map.permute(2, 0, 1)
        return hint
    # step1: create models and pipelines
    if depth_estimator == None:
        installer.set_details("...depth-estimation Pipeline")
        depth_estimator = pipeline('depth-estimation')
    clear_last()
    for pr in kandinsky_controlnet_prompts:
        init_img = None
        if bool(pr['init_image']):
            fname = pr['init_image'].rpartition(slash)[2]
            #init_file = os.path.join(save_dir, fname)
            if pr['init_image'].startswith('http'):
                init_img = PILImage.open(requests.get(pr['init_image'], stream=True).raw)
            else:
                if os.path.isfile(pr['init_image']):
                    init_img = PILImage.open(pr['init_image'])
                else:
                    alert_msg(page, f"ERROR: Couldn't find your init_image {pr['init_image']}")
                    return
            init_img = init_img.resize((pr['width'], pr['height']), resample=PILImage.Resampling.LANCZOS)
            init_img = ImageOps.exif_transpose(init_img).convert("RGB")
            #init_img.save(init_file)
        #clear_last()
        prt("Generating your Kandinsky 2.2 ControlNet Image...")
        prt(progress)
        autoscroll(False)
        hint = make_hint(init_img, depth_estimator).unsqueeze(0).half().to('cuda')
        for num in range(pr['num_images']):
            random_seed = int(pr['seed'] + num) if int(pr['seed']) > 0 else rnd.randint(0,4294967295)
            generator = torch.Generator(device="cuda").manual_seed(random_seed)
            try:
                image_embeds = pipe_kandinsky_controlnet_prior(prompt=pr['prompt'], image=init_img, strength=kandinsky_controlnet_prefs['prior_strength'], num_inference_steps = kandinsky_controlnet_prefs['prior_steps'], generator=generator)
                negative_image_embeds = pipe_kandinsky_controlnet_prior(prompt=pr['negative_prompt'], image=init_img, strength=1, num_inference_steps=kandinsky_controlnet_prefs['prior_steps'], generator=generator)
                #image_embeds, negative_image_embeds = pipe_kandinsky_controlnet_prior(pr['prompt'], negative_prompt=pr['negative_prompt'], num_inference_steps=5, guidance_scale=1.0, generator=generator).to_tuple()
                #if task_type == "controlnet":
                images = pipe_kandinsky(
                    #pr['prompt'],
                    image=init_img,
                    image_embeds=image_embeds.image_embeds,
                    negative_image_embeds=negative_image_embeds.image_embeds,
                    hint=hint,
                    strength=pr['strength'],
                    height=pr['height'],
                    width=pr['width'],
                    num_inference_steps=pr['steps'],
                    guidance_scale=pr['guidance_scale'],
                    num_images_per_prompt=pr['batch_size'],
                    generator=generator,
                    #callback=callback_fnc,
                ).images

            except Exception as e:
                clear_last()
                clear_last()
                alert_msg(page, f"ERROR: Something went wrong generating {task_type} images...", content=Text(str(e)))
                return
            clear_last()
            clear_last()
            autoscroll(True)
            txt2img_output = stable_dir
            batch_output = prefs['image_output']
            txt2img_output = stable_dir
            if bool(kandinsky_controlnet_prefs['batch_folder_name']):
                txt2img_output = os.path.join(stable_dir, kandinsky_controlnet_prefs['batch_folder_name'])
            if not os.path.exists(txt2img_output):
                os.makedirs(txt2img_output)
            #print(str(images))
            if images is None:
                prt(f"ERROR: Problem generating images, check your settings and run again, or report the error to Skquark if it really seems broken.")
                return
            idx = 0
            for image in images:
                fname = format_filename(pr['prompt'])
                #seed_suffix = f"-{random_seed}" if bool(prefs['file_suffix_seed']) else ''
                fname = f'{kandinsky_controlnet_prefs["file_prefix"]}{fname}'
                image_path = available_file(txt2img_output, fname, 1)
                image.save(image_path)
                output_file = image_path.rpartition(slash)[2]
                if not kandinsky_controlnet_prefs['display_upscaled_image'] or not kandinsky_controlnet_prefs['apply_ESRGAN_upscale']:
                    #prt(Row([Img(src=image_path, width=kandinsky_controlnet_prefs['width'], height=kandinsky_controlnet_prefs['height'], fit=ImageFit.FILL, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
                    prt(Row([ImageButton(src=image_path, width=pr['width'], height=pr['height'], data=image_path, page=page)], alignment=MainAxisAlignment.CENTER))
                #if save_to_GDrive:
                batch_output = os.path.join(prefs['image_output'], kandinsky_controlnet_prefs['batch_folder_name'])
                if not os.path.exists(batch_output):
                    os.makedirs(batch_output)
                if storage_type == "PyDrive Google Drive":
                    newFolder = gdrive.CreateFile({'title': kandinsky_controlnet_prefs['batch_folder_name'], "parents": [{"kind": "drive#fileLink", "id": prefs['image_output']}],"mimeType": "application/vnd.google-apps.folder"})
                    newFolder.Upload()
                    batch_output = newFolder
                #out_path = batch_output# if save_to_GDrive else txt2img_output
                out_path = image_path.rpartition(slash)[0]
                upscaled_path = os.path.join(out_path, output_file)

                if kandinsky_controlnet_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
                    os.chdir(os.path.join(dist_dir, 'Real-ESRGAN'))
                    upload_folder = 'upload'
                    result_folder = 'results'
                    if os.path.isdir(upload_folder):
                        shutil.rmtree(upload_folder)
                    if os.path.isdir(result_folder):
                        shutil.rmtree(result_folder)
                    os.mkdir(upload_folder)
                    os.mkdir(result_folder)
                    short_name = f'{fname[:80]}-{idx}.png'
                    dst_path = os.path.join(dist_dir, 'Real-ESRGAN', upload_folder, short_name)
                    #print(f'Moving {fpath} to {dst_path}')
                    #shutil.move(fpath, dst_path)
                    shutil.copy(image_path, dst_path)
                    faceenhance = ' --face_enhance' if kandinsky_controlnet_prefs["face_enhance"] else ''
                    run_sp(f'python inference_realesrgan.py -n realesr-general-x4v3 -i upload --outscale {kandinsky_controlnet_prefs["enlarge_scale"]}{faceenhance}', cwd=os.path.join(dist_dir, 'Real-ESRGAN'), realtime=False)
                    out_file = short_name.rpartition('.')[0] + '_out.png'
                    shutil.move(os.path.join(dist_dir, 'Real-ESRGAN', result_folder, out_file), upscaled_path)
                    # python inference_realesrgan.py --model_path experiments/pretrained_models/RealESRGAN_x4plus.pth --input upload --netscale 4 --outscale 3.5 --half --face_enhance
                    image_path = upscaled_path
                    os.chdir(stable_dir)
                    if kandinsky_controlnet_prefs['display_upscaled_image']:
                        time.sleep(0.6)
                        prt(Row([Img(src=upscaled_path, width=pr['width'] * float(kandinsky_controlnet_prefs["enlarge_scale"]), height=pr['height'] * float(kandinsky_controlnet_prefs["enlarge_scale"]), fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
                #else:
                #    time.sleep(1.2)
                #    shutil.copy(image_path, os.path.join(out_path, output_file))
                if prefs['save_image_metadata']:
                    img = PILImage.open(image_path)
                    metadata = PngInfo()
                    metadata.add_text("artist", prefs['meta_ArtistName'])
                    metadata.add_text("copyright", prefs['meta_Copyright'])
                    metadata.add_text("software", "Stable Diffusion Deluxe" + f", upscaled {kandinsky_controlnet_prefs['enlarge_scale']}x with ESRGAN" if unCLIP_image_interpolation_prefs['apply_ESRGAN_upscale'] else "")
                    metadata.add_text("pipeline", f"Kandinsky 2.1 {task_type}")
                    if prefs['save_config_in_metadata']:
                        #metadata.add_text("title", kandinsky_controlnet_prefs['file_name'])
                        # TODO: Merge Metadata with pr[]
                        config_json = kandinsky_controlnet_prefs.copy()
                        config_json['model_path'] = "kandinsky-community/kandinsky-2-2-controlnet-depth"
                        config_json['seed'] = random_seed
                        del config_json['num_images']
                        del config_json['display_upscaled_image']
                        del config_json['batch_folder_name']
                        if not config_json['apply_ESRGAN_upscale']:
                            del config_json['enlarge_scale']
                            del config_json['apply_ESRGAN_upscale']
                        metadata.add_text("config_json", json.dumps(config_json, ensure_ascii=True, indent=4))
                    img.save(image_path, pnginfo=metadata)
                if storage_type == "Colab Google Drive":
                    new_file = available_file(os.path.join(prefs['image_output'], kandinsky_controlnet_prefs['batch_folder_name']), fname, 0)
                    out_path = new_file
                    shutil.copy(image_path, new_file)
                elif bool(prefs['image_output']):
                    new_file = available_file(os.path.join(prefs['image_output'], kandinsky_controlnet_prefs['batch_folder_name']), fname, 0)
                    out_path = new_file
                    shutil.copy(image_path, new_file)
                prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))
    autoscroll(False)
    if prefs['enable_sounds']: page.snd_alert.play()


def run_deep_daze(page):
    global deep_daze_prefs, status
    #https://colab.research.google.com/drive/1_YOHdORb0Fg1Q7vWZ_KlrtFe9Ur3pmVj?usp=sharing#scrollTo=6IPQ_rdA2Sa7
    def add_to_deep_daze_output(o):
      page.DeepDaze.controls.append(o)
      page.DeepDaze.update()
    def prt(line):
      if type(line) == str:
        line = Text(line)
      page.DeepDaze.controls.append(line)
      page.DeepDaze.update()
    def clear_last():
      #page.deep_daze_output.controls = []
      del page.DeepDaze.controls[-1]
      page.DeepDaze.update()
    def autoscroll(scroll=True):
      page.DeepDaze.auto_scroll = scroll
      page.DeepDaze.update()
    def clear_list():
      page.DeepDaze.controls = page.DeepDaze.controls[:1]
    clear_list()
    autoscroll(True)
    progress = ProgressBar(bar_height=8, expand=True)
    total_steps = deep_daze_prefs['iterations']
    def callback_fnc(step: int) -> None:
      callback_fnc.has_been_called = True
      nonlocal progress, total_steps
      #total_steps = len(latents)
      percent = (step +1)/ total_steps
      progress.value = percent
      progress.tooltip = f"{step +1} / {total_steps}"
      progress.update()
    prt(Installing("Initializing Deep Daze Pipeline..."))
    from PIL.PngImagePlugin import PngInfo
    from tqdm.notebook import trange
    try:
        from deep_daze import Imagine
    except Exception:
        run_process("pip install deep-daze --upgrade", page=page)
        from deep_daze import Imagine
    output_dir = os.path.join(root_dir, "deep_daze")
    if bool(deep_daze_prefs['batch_folder_name']):
        output_dir = os.path.join(output_dir, deep_daze_prefs['batch_folder_name'])
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    os.chdir(output_dir)
    abort_run = False
    def abort_daze(e):
        nonlocal abort_run
        abort_run = True
        autoscroll(False)
        page.snd_error.play()
        page.snd_delete.play()
    pipe_deep_daze = Imagine(
        text = deep_daze_prefs['prompt'],
        num_layers = deep_daze_prefs['num_layers'],
        save_every = deep_daze_prefs['save_every'],
        image_width = deep_daze_prefs['max_size'],
        lr = deep_daze_prefs['learning_rate'],
        iterations = deep_daze_prefs['iterations'],
        save_progress = True,#deep_daze_prefs['save_progress']
    )
    clear_last()
    prt("    Dreaming up your DeepDaze Image...")
    autoscroll(False)
    prt(Row([progress, IconButton(icon=icons.CANCEL, tooltip="Abort Daze & Save Progress", on_click=abort_daze)], alignment=MainAxisAlignment.SPACE_BETWEEN))
    #filename = format_filename(deep_daze_prefs['prompt'])
    filename = deep_daze_prefs['prompt'].replace(' ', '_')
    img = None
    image_files = []
    s = 0
    for epoch in trange(20, desc = 'epochs'):
        for i in trange(deep_daze_prefs['iterations'], desc = 'iteration'):
            pipe_deep_daze.train_step(epoch, i)
            callback_fnc(i)
            if abort_run: break
            if i % pipe_deep_daze.save_every != 0:
                continue
            num_str = '' if s==0 else f'.{str(s).zfill(6)}'
            fname = f"{filename}{num_str}.jpg"
            output_file = os.path.join(output_dir, fname)
            image_files.append(output_file)
            if s == 0:
                img = Img(src=output_file, fit=ImageFit.FIT_WIDTH, gapless_playback=True)
                prt(Row([img], alignment=MainAxisAlignment.CENTER))
            else:
                img.src=output_file
                img.update()
            s += 1
        if abort_run: break
            #image = Image(f'./{filename}.jpg')
            #display(image)
    output_filename = deep_daze_prefs['file_prefix'] + format_filename(deep_daze_prefs['prompt'])
    unscaled_path = fname
    image_path = available_file(output_dir, output_filename, 0)
    unscaled_path = image_path
    output_file = image_path.rpartition(slash)[2]
    out_path = image_path.rpartition(slash)[0]
    upscaled_path = os.path.join(out_path, output_file)
    input = PILImage.open(fname)
    input.save(image_path)
    #shutil.copy(fname, image_path)
    clear_last()
    clear_last()
    clear_last()
    autoscroll(True)
    if not deep_daze_prefs['display_upscaled_image'] or not deep_daze_prefs['apply_ESRGAN_upscale']:
        prt(Row([ImageButton(src=unscaled_path, data=upscaled_path, width=512, height=512, page=page)], alignment=MainAxisAlignment.CENTER))
        #prt(Row([Img(src=unscaled_path, fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
    if deep_daze_prefs['apply_ESRGAN_upscale'] and status['installed_ESRGAN']:
        os.chdir(os.path.join(dist_dir, 'Real-ESRGAN'))
        upload_folder = 'upload'
        result_folder = 'results'
        if os.path.isdir(upload_folder):
            shutil.rmtree(upload_folder)
        if os.path.isdir(result_folder):
            shutil.rmtree(result_folder)
        os.mkdir(upload_folder)
        os.mkdir(result_folder)
        short_name = f'{fname[:80]}.png'
        dst_path = os.path.join(dist_dir, 'Real-ESRGAN', upload_folder, short_name)
        #print(f'Moving {fpath} to {dst_path}')
        #shutil.move(fpath, dst_path)
        shutil.copy(image_path, dst_path)
        #faceenhance = ' --face_enhance' if deep_daze_prefs["face_enhance"] else ''
        faceenhance = ''
        run_sp(f'python inference_realesrgan.py -n realesr-general-x4v3 -i upload --outscale {deep_daze_prefs["enlarge_scale"]}{faceenhance}', cwd=os.path.join(dist_dir, 'Real-ESRGAN'), realtime=False)
        out_file = short_name.rpartition('.')[0] + '_out.png'
        upscaled_path = os.path.join(out_path, output_file)
        shutil.move(os.path.join(dist_dir, 'Real-ESRGAN', result_folder, out_file), upscaled_path)
        image_path = upscaled_path
        os.chdir(stable_dir)
    if prefs['save_image_metadata']:
        img = PILImage.open(image_path)
        metadata = PngInfo()
        metadata.add_text("artist", prefs['meta_ArtistName'])
        metadata.add_text("copyright", prefs['meta_Copyright'])
        metadata.add_text("software", "Stable Diffusion Deluxe" + f", upscaled {deep_daze_prefs['enlarge_scale']}x with ESRGAN" if deep_daze_prefs['apply_ESRGAN_upscale'] else "")
        metadata.add_text("pipeline", f"Deep Daze")
        if prefs['save_config_in_metadata']:
            metadata.add_text("title", deep_daze_prefs['prompt'])
            config_json = deep_daze_prefs.copy()
            #config_json['model_path'] = model_id
            del config_json['num_images']
            del config_json['display_upscaled_image']
            del config_json['batch_folder_name']
            if not config_json['apply_ESRGAN_upscale']:
                del config_json['enlarge_scale']
            del config_json['apply_ESRGAN_upscale']
            metadata.add_text("config_json", json.dumps(config_json, ensure_ascii=True, indent=4))
        img.save(image_path, pnginfo=metadata)
    #TODO: PyDrive
    if storage_type == "Colab Google Drive":
        new_file = available_file(os.path.join(prefs['image_output'], deep_daze_prefs['batch_folder_name']), output_filename, 0)
        out_path = new_file
        shutil.copy(image_path, new_file)
    elif bool(prefs['image_output']):
        new_file = available_file(os.path.join(prefs['image_output'], deep_daze_prefs['batch_folder_name']), output_filename, 0)
        out_path = new_file
        shutil.copy(image_path, new_file)
    if deep_daze_prefs['save_progress']:
        for f in image_files:
            if os.path.exists(f):
                shutil.copy(f, os.path.join(prefs['image_output'], deep_daze_prefs['batch_folder_name'], f.rpartition(slash)[2]))
    else:
        for f in image_files[:-1]:
            if os.path.exists(f):
                os.remove(f)
    if deep_daze_prefs['display_upscaled_image']:
        prt(Row([ImageButton(src=upscaled_path, data=upscaled_path, width=512 * float(deep_daze_prefs["enlarge_scale"]), height=512 * float(deep_daze_prefs["enlarge_scale"]), page=page)], alignment=MainAxisAlignment.CENTER))
        #prt(Row([Img(src=upscaled_path, fit=ImageFit.CONTAIN, gapless_playback=True)], alignment=MainAxisAlignment.CENTER))
    prt(Row([Text(out_path)], alignment=MainAxisAlignment.CENTER))

    del pipe_deep_daze
    flush()
    autoscroll(False)
    if prefs['enable_sounds']: page.snd_alert.play()


def main(page: Page):
    global status
    page.title = "Stable Diffusion Deluxe - FletUI"
    #page.scroll=ScrollMode.AUTO
    #page.auto_scroll=True
    def open_url(e):
        page.launch_url(e.data)
    def exit_disconnect(e):
        save_settings_file(page)
        if is_Colab:
          #run_sp("install pyautogui", realtime=False)
          #import pyautogui
          #pyautogui.hotkey('ctrl', 'w')
          #import keyboard
          #keyboard.press_and_release('ctrl+w')
          #time.sleep(1.5)
          #close_tab()
          page.window_close()
          from google.colab import runtime
          runtime.unassign()
          #import time
        else:
          page.window_close()
    def minimize_window(e):
        page.window_minimized = True
        page.update()
    def maximize_window(e):
        if page.window_maximized:
          page.window_maximized = False
          appbar.actions[2].icon=icons.CHECK_BOX_OUTLINE_BLANK
          appbar.actions[2].tooltip = "Maximize Window"
        else:
          page.window_maximized = True
          appbar.actions[2].icon=icons.MAXIMIZE
          appbar.actions[2].tooltip = "Restore Window"
        page.update()
    def get_memory():
        from subprocess import getoutput
        s = getoutput('nvidia-smi -L')
        if 'T4' in s: gpu = 'T4'
        elif 'P100' in s: gpu = 'P100'
        elif 'V100' in s: gpu = 'V100'
        elif 'A100' in s: gpu = 'A100'
        else:
            import re
            gpu = re.sub(r"\([^()]*\)", "", s).strip()
        status['cpu_used'] = psutil.virtual_memory().used / (1024 * 1024 * 1024)
        #memory_stats = torch.cuda.memory_stats(device=torch.device("cuda"))
        #available_memory = memory_stats["reserved_bytes.all.allocated"] - memory_stats["allocated_bytes.all.current"]
        try:
            status['gpu_used'] = torch.cuda.max_memory_allocated(device=torch.device("cuda")) / (1024 * 1024 * 1024)
            gpu_mem = f"{status['gpu_used']:.1f}/{status['gpu_memory']:.0f}GB"
        except Exception:
            status['gpu_used'] = 0
            gpu_mem = "N/A"
            pass
        memory = f"GPU VRAM: {gpu_mem} - CPU RAM: {status['cpu_used']:.1f}/{status['cpu_memory']:.0f}GB | Runnning {gpu}"
        return memory
    help_dlg = None
    def open_help_dlg(e):
        nonlocal help_dlg
        def clear_the_pipes(e):
            nonlocal memory_text
            clear_pipes()
            time.sleep(0.8)
            memory_text.value = get_memory()
            memory_text.update()
        memory_text = Text(get_memory())
        clear_pipes_btn = ElevatedButton(content=Text("Clear Memory", size=16), on_click=clear_the_pipes)
        memory_row = Row([memory_text, Container(content=None, expand=True), clear_pipes_btn])
        help_dlg = AlertDialog(
            title=Text("💁   Help/Information - Stable Diffusion Deluxe " + SDD_version), content=Column([memory_row, Text("If you don't know what Stable Diffusion is, you're in for a pleasant surprise.. If you're already familiar, you're gonna love how easy it is to be an artist with the help of our AI Friends using our pretty interface."),
                  Text("Simply go through the self-explanitory tabs step-by-step and set your preferences to get started. The default values are good for most, but you can have some fun experimenting. Most values are automatically saved as you make changes and change tabs."),
                  Text("Each time you open the app, you should start in the Installers section, turn on all the components you plan on using in you session, then Run the Installers and let them download. You can multitask and work in other tabs while it's installing."),
                  Text("In the Prompts List, add as many text prompts as you can think of, and edit any prompt to override any default Image Parameter.  Once you're ready, Run Diffusion on your Prompts List and watch it fill your Drive with beauty.."),
                  Text("Try out any and all of our Prompt Helpers to use practical text AIs to make unique descriptive prompts fast, with our Prompt Generator, Remixer, Brainstormer and Advanced Writer.  You'll never run out of inspiration again..."),
                  Text("Use the other Stable Diffusers to get unique results with advanced AI tools to refine your art to the next level with powerful surpises.. There's a lot of specialized Diffusers that are pure magic when you challenge it. Have fun, and get creative..."),
            ], scroll=ScrollMode.AUTO),
            actions=[TextButton("👍  Thanks! ", on_click=close_help_dlg)], actions_alignment=MainAxisAlignment.END,
        )
        page.dialog = help_dlg
        help_dlg.open = True
        page.update()
    def close_help_dlg(e):
        nonlocal help_dlg
        help_dlg.open = False
        page.update()
    def open_credits_dlg(e):
        page.dialog = credits_dlg
        credits_dlg.open = True
        page.update()
    def close_credits_dlg(e):
        credits_dlg.open = False
        page.update()
    credits_markdown = '''This toolkit is an Open-Source side project by [Skquark, Inc.](https://Skquark.com), primarily created by Alan Bedian for fun and full-feature functionality.  Official website is [DiffusionDeluxe.com](https://DiffusionDeluxe.com) for more info.

The real credit goes to the team at [Stability.ai](https://Stability.ai) for making Stable Diffusion so great, and [HuggingFace](https://HuggingFace.co) for their work on the [Diffusers Pipelines](https://github.com/huggingface/diffusers). The HuggingFace Diffusers team includes Patrick von Platen, Suraj Patil, Anton Lozhkov, Pedro Cuenca, Nathan Lambert, Kashif Rasul, Mishig Davaadorj & Thomas Wolf.

For the great app UI framework, we thank [Flet](https://Flet.dev) with the amazing Flutter based Python library with a very functional dev platform that made this possible.

For the brains behind our Prompt Helpers, we thank our friends [OpenAI](https://beta.OpenAI.com), [Bloom-AI](https://huggingface.co/bigscience/bloom), [TextSynth](https://TextSynth.com) and others for making an AIs so fun to talk to and use.

Shoutouts to the Discord Community of [Disco Diffusion](https://discord.gg/d5ZVbAfm), [Stable Diffusion](https://discord.gg/stablediffusion), [HuggingFace](https://discord.gg/hugging-face-879548962464493619) and [Flet](https://discord.gg/nFqy742h) for their support and user contributions.'''
    credits_dlg = AlertDialog(
        title=Text("🙌   Credits/Acknowledgments"), content=Column([Markdown(credits_markdown, extension_set="gitHubWeb", on_tap_link=open_url)
        ], scroll=ScrollMode.AUTO),
        actions=[TextButton("👊   Good Stuff... ", on_click=close_credits_dlg)], actions_alignment=MainAxisAlignment.END,
    )
    def open_contact_dlg(e):
        page.dialog = contact_dlg
        contact_dlg.open = True
        page.update()
    def close_contact_dlg(e):
        contact_dlg.open = False
        page.update()
    contact_dlg = AlertDialog(
        title=Text("📨  Contact Us"), content=Column([
          Text("If you want to reach Alan Bedian/Skquark, Inc. for any reason, feel free to send me a message (as long as it's not spam) by Email or on Discord @Skquark#0394 where I'm usually available."),
          Text("If you're a developer and want to help with this project (or one of my other almost done apps) then you can Join my Discord Channel and get involved. It's fairly quiet in there though..."),
          Row([ft.FilledButton("Email Skquark", on_click=lambda _:page.launch_url("mailto:Alan@Skquark.com")), ft.FilledButton("Skquark Discord", on_click=lambda _:page.launch_url("https://discord.gg/fTraJ96Z"))], alignment=MainAxisAlignment.CENTER),
        ], scroll=ScrollMode.AUTO),
        actions=[TextButton("👁️‍🗨️  Maybe...", on_click=close_contact_dlg)], actions_alignment=MainAxisAlignment.END,
    )
    def open_donate_dlg(e):
        page.dialog = donate_dlg
        donate_dlg.open = True
        page.update()
    def close_donate_dlg(e):
        donate_dlg.open = False
        page.update()
    donate_dlg = AlertDialog(
        title=Text("🎁  Donate to the Cause"), content=Column([
          Text("This app has been a one-man labour of love with a lot of effort poured into it over several months. It's powerful enough to be a paid commercial application, however it's all open-source code behind it created by many contributers to make the tools as cool as they are."),
          Text("While we offer this software free of charge, your support would be greatly appreciated to continue the efforts to keep making it better. If you find value in this software and are able to show some love, any amount you can offer is welcomed..."),
          Text("If you're technical enough, you can also donate by making contributions to the code, offering suggestions for improvements, adding to the Community Fine-Tuned Models list, or whatever enhancements to the project you can provide..."),
          Row([ft.FilledButton("Donate with PayPal", on_click=lambda _:page.launch_url("https://paypal.me/StarmaTech")), ft.FilledButton("Donate with Venmo", on_click=lambda _:page.launch_url("https://venmo.com/u/Alan-Bedian"))], alignment=MainAxisAlignment.CENTER),
        ], scroll=ScrollMode.AUTO),
        actions=[TextButton("💸  Much appreciated", on_click=close_donate_dlg)], actions_alignment=MainAxisAlignment.END,
    )
    page.etas = []
    page.theme_mode = prefs['theme_mode'].lower()
    if prefs['theme_mode'] == 'Dark':
      page.dark_theme = theme.Theme(color_scheme_seed=prefs['theme_color'].lower())#, use_material3=True)
    else:
      page.theme = theme.Theme(color_scheme_seed=prefs['theme_color'].lower())
    app_icon_color = colors.AMBER_800
    space = " "  if (page.width if page.web else page.window_width) >= 1024 else ""
    appbar=AppBar(title=ft.WindowDragArea(Row([Container(Text(f"👨‍🎨️{space}  Stable Diffusion - Deluxe Edition  {space}🧰" if (page.width if page.web else page.window_width) >= 768 else "Stable Diffusion Deluxe  🖌️", weight=FontWeight.BOLD, color=colors.ON_SURFACE))], alignment=MainAxisAlignment.CENTER), expand=True), elevation=20,
      center_title=True,
      bgcolor=colors.SURFACE,
      leading=IconButton(icon=icons.LOCAL_FIRE_DEPARTMENT_OUTLINED, icon_color=app_icon_color, icon_size=32, tooltip="Save Settings File", on_click=lambda _: app_icon_save()),
      #leading_width=40,
      actions=[PopupMenuButton(items=[
          PopupMenuItem(text="🤔  Help/Info", on_click=open_help_dlg),
          PopupMenuItem(text="👏  Credits", on_click=open_credits_dlg),
          PopupMenuItem(text="🤧  Issues/Suggestions", on_click=lambda _:page.launch_url("https://github.com/Skquark/AI-Friends/issues")),
          PopupMenuItem(text="😋  Contact Skquark", on_click=open_contact_dlg),
          #PopupMenuItem(text="📨  Email Skquark", on_click=lambda _:page.launch_url("mailto:Alan@Skquark.com")),
          PopupMenuItem(text="🤑  Offer Donation", on_click=open_donate_dlg),
          #PopupMenuItem(text="❎  Exit/Disconnect Runtime", on_click=exit_disconnect) if is_Colab else PopupMenuItem(),
      ])])
    if is_Colab:
      appbar.actions[0].items.append(PopupMenuItem())
      appbar.actions[0].items.append(PopupMenuItem(text="❎  Exit/Disconnect Runtime", on_click=exit_disconnect))
    else:
      appbar.actions.append(IconButton(icon=icons.MINIMIZE, tooltip="Minimize Window", on_click=minimize_window))
      appbar.actions.append(IconButton(icon=icons.CHECK_BOX_OUTLINE_BLANK, tooltip="Maximize Window", on_click=maximize_window))
      appbar.actions.append(IconButton(icon=icons.CLOSE, tooltip="❎  Exit Application", on_click=exit_disconnect))
      page.window_title_bar_hidden = True
    page.appbar = appbar
    def app_icon_save():
      app_icon_color = colors.GREEN_800
      appbar.leading = IconButton(icon=icons.LOCAL_FIRE_DEPARTMENT_OUTLINED, icon_color=app_icon_color, icon_size=32, tooltip="Saving Settings File")
      appbar.update()
      time.sleep(0.6)
      app_icon_color = colors.AMBER_800
      appbar.leading = IconButton(icon=icons.LOCAL_FIRE_DEPARTMENT_OUTLINED, icon_color=app_icon_color, icon_size=32, tooltip="Save Settings File", on_click=lambda _: app_icon_save())
      appbar.update()
    page.app_icon_save = app_icon_save
    page.vertical_alignment = MainAxisAlignment.START
    page.horizontal_alignment = CrossAxisAlignment.START
    t = buildTabs(page)
    t.on_change = tab_on_change
    #(t,page)
    def close_banner(e):
        page.banner.open = False
        page.update()
    #leading=Icon(icons.DOWNLOADING, color=colors.AMBER, size=40),
    page.banner = Banner(bgcolor=colors.SECONDARY_CONTAINER, content=Column([]), actions=[TextButton("Close", on_click=close_banner)])
    def show_banner_click(e):
        page.banner.open = True
        page.update()

    page.add(t)
    initState(page)
    if not status['initialized']:
        status['initialized'] = True
    #page.add(ElevatedButton("Show Banner", on_click=show_banner_click))
    #page.add (Text ("Enhanced Stable Diffusion Deluxe by Skquark, Inc."))

class Header(UserControl):
    def __init__(self, title="", subtitle="", value="", actions=[], divider=True):
        super().__init__()
        self.title = title
        self.subtitle = subtitle
        self.value = value
        self.actions = actions
        self.divider = divider
        self.build()
    def build(self):
        #self.column = Column([Row([Text(self.title, style=TextThemeStyle.TITLE_LARGE, color=colors.SECONDARY, weight=FontWeight.BOLD), Row(self.actions) if bool(self.actions) else Container(content=None)], alignment=MainAxisAlignment.SPACE_BETWEEN, spacing=0, vertical_alignment=CrossAxisAlignment.END)], spacing=4)
        self.column = Column([Row([Column([Text(self.title, style=TextThemeStyle.TITLE_LARGE, color=colors.SECONDARY, weight=FontWeight.BOLD), Text(self.subtitle, style="titleSmall", color=colors.TERTIARY) if bool(self.subtitle) else Container(content=None)], spacing=4, expand=True), Row(self.actions) if bool(self.actions) else Container(content=None)], alignment=MainAxisAlignment.SPACE_BETWEEN, spacing=1, vertical_alignment=CrossAxisAlignment.END), Divider(thickness=3, height=5, color=colors.SURFACE_VARIANT) if self.divider else Container(content=None), Container(content=None, height=3)], spacing=2)
        return self.column

class FileInput(UserControl):
    def __init__(self, label="Initial Image", type="image", pref="", key="", expand=False, col=None, max_size=None, output_dir=None, page=None):
        super().__init__()
        self.label = label
        self.type = type
        self.pref = pref
        self.key = key
        self.expand = expand
        self.col = col
        self.max_size = max_size
        self.output_dir = output_dir
        self.page = page
        self.build()
    def build(self):
        def upload_files(e):
            uf = []
            if self.file_picker.result != None and self.file_picker.result.files != None:
                for f in self.file_picker.result.files:
                  if self.page.web:
                    uf.append(FilePickerUploadFile(f.name, upload_url=self.page.get_upload_url(f.name, 600)))
                  else:
                    on_upload_progress(FilePickerUploadEvent(f.path, 1, ""))
                self.file_picker.upload(uf)
        def file_picker_result(e: FilePickerResultEvent):
            if e.files != None:
              upload_files(e)
        def on_upload_progress(e: FilePickerUploadEvent):
            if e.progress == 1:
              if self.output_dir == None:
                save_dir = root_dir
              else:
                save_dir = self.output_dir
                if not os.path.exists(save_dir):
                  os.mkdir(save_dir)
              if not slash in e.file_name:
                fname = os.path.join(stable_dir, e.file_name)
                fpath = os.path.join(save_dir, e.file_name)
                self.pref[self.key] = e.file_name.rpartition('.')[0]
              else:
                fname = e.file_name
                self.pref[self.key] = e.file_name.rpartition(slash)[2].rpartition('.')[0]
              if self.max_size != None:
                original_img = PILImage.open(fname)
                width, height = original_img.size
                width, height = scale_dimensions(width, height, self.max_size)
                original_img = original_img.resize((width, height), resample=PILImage.Resampling.LANCZOS).convert("RGB")
                original_img.save(fpath)
              self.textfield.value = fname
              self.textfield.update()
              self.pref[self.key] = fname
              self.page.update()
            else:
              percent = int(e.progress * 10)
              self.textfield.value = f"  {percent}%"
              self.textfield.update()
        def pick_file(e):
            img_ext = ["png", "PNG", "jpg", "jpeg"]
            vid_ext = ["mp4", "avi", "MP4", "AVI"]
            aud_ext = ["mp3", "wav", "MP3", "WAV"]
            ext = img_ext if self.type == "image" else vid_ext if self.type == "video" else aud_ext if self.type == "audio" else vid_ext+aud_ext if self.type == "media" else vid_ext+img_ext if self.type == "picture" else img_ext
            name = self.key.replace("_", " ").title()
            self.file_picker.pick_files(allow_multiple=False, allowed_extensions=ext, dialog_title=f"Pick {name} File")
        def changed(e):
            self.pref[self.key] = e.control.value
        self.file_picker = FilePicker(on_result=file_picker_result, on_upload=on_upload_progress)
        self.page.overlay.append(self.file_picker)
        self.textfield = TextField(label=self.label, value=self.pref[self.key], expand=self.expand, on_change=changed, height=60, suffix=IconButton(icon=icons.DRIVE_FOLDER_UPLOAD, on_click=pick_file))
        return self.textfield

class ImageButton(UserControl):
    def __init__(self, src="", subtitle="", actions=[], center=True, width=None, height=None, data=None, fit=ImageFit.SCALE_DOWN, show_subtitle=False, page=None):
        super().__init__()
        self.src = src
        self.subtitle = subtitle
        self.actions = actions
        self.center = center
        self.width = width
        self.height = height
        self.data = data or src
        self.fit = fit
        self.show_subtitle = show_subtitle
        self.page = page
        self.build()
    def build(self):
        #self.column = Column([Row([Text(self.title, style=TextThemeStyle.TITLE_LARGE, color=colors.SECONDARY, weight=FontWeight.BOLD), Row(self.actions) if bool(self.actions) else Container(content=None)], alignment=MainAxisAlignment.SPACE_BETWEEN, spacing=0, vertical_alignment=CrossAxisAlignment.END)], spacing=4)
        #Img(src=self.src, width=self.width, height=self.height, fit=self.fit, gapless_playback=True),
        def download_image(e):
          if is_Colab:
            print(f"{type(self.data)} {self.data}")
            from google.colab import files
            if os.path.isfile(self.data):
              files.download(self.data)
            else:
              time.sleep(5)
              files.download(self.data)
          self.page.snack_bar = SnackBar(content=Text(f"📲  Downloading {self.data}... May have to Stop Script for Downloads to start in Colab."))
          self.page.snack_bar.open = True
          self.page.update()
        def copy_path(e):
            self.page.set_clipboard(self.data)
            self.page.snack_bar = SnackBar(content=Text(f"📋  {self.data} copied to clipboard... Paste as Init Image."))
            self.page.snack_bar.open = True
            self.page.update()
        def image_details(e):
          #TODO: Get size & meta
            alert_msg(self.page, "Image Details", content=Column([Text(self.subtitle or self.data, selectable=True), Img(src=self.data, gapless_playback=True)], horizontal_alignment=CrossAxisAlignment.CENTER), sound=False)
        def delete_image(e):
            #self.image = Container(content=None)
            if os.path.exists(self.src):
              os.remove(self.src)
            if self.data != self.src:
              if os.path.exists(self.data):
                os.remove(self.data)
            self.image.visible = False
            self.image.update()
            self.visible = False
            self.update()
            self.page.snack_bar = SnackBar(content=Text(f"🗑️  Deleted {self.data}..."))
            self.page.snack_bar.open = True
            self.page.update()
        def scale_width(width, height, max_width):
            if width > max_width: return max_width, int(height / (width / max_width))
            else: return width, height
        #self.image = Row([Img(src=self.src, width=self.width, height=self.height, fit=self.fit, gapless_playback=True)], alignment=MainAxisAlignment.CENTER)
        if self.width == None:
            self.image = Img(src=self.src, fit=self.fit, gapless_playback=True)
        else:
            self.width, self.height = scale_width(self.width, self.height, (self.page.width if self.page.web else self.page.window_width) - 28)
            self.image = Img(src=self.src, width=self.width, height=self.height, fit=self.fit, gapless_playback=True)
        self.column = Column([Row([PopupMenuButton(
            items = [
                PopupMenuItem(text="View Image", icon=icons.FULLSCREEN, on_click=image_details),
                PopupMenuItem(text="Copy Path", icon=icons.CONTENT_PASTE, on_click=copy_path),
                PopupMenuItem(text="Download Locally", icon=icons.DOWNLOAD, on_click=download_image),
                PopupMenuItem(text="Delete Image", icon=icons.DELETE, on_click=delete_image),
            ], tooltip="👁️", expand=True,
            content=Row([self.image], expand=True),
            #content=Row([Img(src=self.src, width=self.width, height=self.height, fit=self.fit, gapless_playback=True)], alignment=MainAxisAlignment.CENTER if self.center else MainAxisAlignment.START),
            data=self.src if self.data==None else self.data, width=self.width, height=self.height)],
            alignment=MainAxisAlignment.CENTER if self.center else MainAxisAlignment.START)], horizontal_alignment=CrossAxisAlignment.CENTER if self.center else CrossAxisAlignment.START)
        if self.show_subtitle:
            self.column.controls.append(Row([Text(self.subtitle)], alignment=MainAxisAlignment.CENTER if self.center else MainAxisAlignment.START))
        return self.column


class NumberPicker(UserControl):
    def __init__(self, label="", value=1, min=0, max=20, step=1, height=50, tooltip=None, on_change=None):
        super().__init__()
        self.value = value
        self.min = min
        self.max = max
        self.step = step
        self.label = label
        self.height = height
        self.tooltip = tooltip
        self.on_change = on_change
        self.build()
    def build(self):
        def changed(e):
            self.value = int(e.control.value)
            if self.value < self.min:
              self.value = self.min
              self.txt_number.value = self.value
              self.txt_number.update()
            if self.value > self.max:
              self.value = self.max
              self.txt_number.value = self.value
              self.txt_number.update()
            if self.on_change is not None:
              e.control = self
              self.on_change(e)
        def minus_click(e):
            self.value = int(self.value)
            if self.value > self.min:
              self.value -= self.step
              self.txt_number.value = self.value
              self.txt_number.update()
              e.control = self
              if self.on_change is not None:
                self.on_change(e)
        def plus_click(e):
            self.value = int(self.value)
            if self.value < self.max:
              self.value += self.step
              self.txt_number.value = self.value
              self.txt_number.update()
              e.control = self
              if self.on_change is not None:
                self.on_change(e)
        self.txt_number = TextField(value=str(self.value), text_align=TextAlign.CENTER, width=55, height=self.height, content_padding=padding.only(top=4), keyboard_type=KeyboardType.NUMBER, on_change=changed)
        label_text = Text(self.label) if not self.tooltip else Tooltip(message=self.tooltip, content=Text(self.label))
        return Row([label_text, IconButton(icons.REMOVE, on_click=minus_click), self.txt_number, IconButton(icons.ADD, on_click=plus_click)], spacing=1)

class SliderRow(UserControl):
    def __init__(self, label="", value=None, min=0, max=20, divisions=20, step=None, multiple=1, round=0, suffix="", left_text=None, right_text=None, visible=True, tooltip="", pref=None, key=None, expand=None, col=None, on_change=None):
        super().__init__()
        self.value = value or pref[key]
        self.min = min
        self.max = max
        if step:
            self.divisions = (max - min) / step
        else:
            self.divisions = divisions
        self.multiple = multiple
        self.label = label
        self.round = round
        self.suffix = suffix
        self.left_text = left_text
        self.right_text = right_text
        self._visible = visible
        self.slider_row = Container(content=None)
        self.tooltip = tooltip
        self.expand = expand
        self.pref = pref
        self.key = key
        self.col = col
        self.on_change = on_change
        self.build()
    def build(self):
        def change_slider(e):
            self.value = round(e.control.value, self.round)
            v = int(self.value) if self.round == 0 else float(self.value)
            # TODO: Figure out how to update label text on web
            #slider.label = f"{v}{self.suffix}"
            #slider.update()
            self.slider_value.value = f"{v}{self.suffix}"
            self.slider_value.update()
            self.slider_edit.value = f"{v}"
            self.slider_edit.update()
            if self.on_change is not None:
              e.control = self
              self.on_change(e)
            self.pref[self.key] = int(self.value) if self.round == 0 else float(self.value)
        def changed(e):
            try:
              self.value = int(e.control.value) if self.round == 0 else round(float(e.control.value), self.round)
            except ValueError:
              e.control.value = str(self.value)
              e.control.update()
              return
            if self.value < self.min:
              self.value = self.min
              self.slider_edit.value = self.value
              self.slider_value.value = self.value
              self.slider_value.update()
            if self.value > self.max:
              self.value = self.max
              self.slider_edit.value = self.value
              self.slider_value.value = self.value
              self.slider_value.update()
            if self.multiple != 1:
              v = int(self.multiple * round(self.value / self.multiple))
              if v != self.value:
                self.slider_edit.value = v
                self.slider_edit.update()
                self.value = v
            if self.on_change is not None:
              e.control = self
              self.on_change(e)
            self.slider.value = self.value
            self.slider.label = f"{self.value}{self.suffix}"
            self.slider.update()
            self.slider_value.value = f"{self.value}{self.suffix}"
            self.slider_value.update()
            self.pref[self.key] = int(self.value) if self.round == 0 else float(self.value)
        def blur(e):
            self.slider_edit.visible = False
            slider_text.visible = True
            self.slider_edit.update()
            slider_text.update()
            slider_label.value = f"{self.label}: "
            slider_label.update()
        def edit(e):
            self.slider_edit.visible = True
            slider_text.visible = False
            slider_text.update()
            self.slider_edit.update()
            slider_label.value = f"{self.label}"
            slider_label.update()
            #self.slider_number = TextField(value=str(self.value), on_blur=blur, text_align=TextAlign.CENTER, width=55, height=50, content_padding=padding.only(top=4), keyboard_type=KeyboardType.NUMBER, on_change=changed)
            #self.update()
            #e.control.update()
            #e.page.update()
        self.slider_edit = TextField(value=str(self.value), on_blur=blur, autofocus=True, visible=False, text_align=TextAlign.CENTER, width=51, height=45, content_padding=padding.only(top=6), keyboard_type=KeyboardType.NUMBER, on_change=changed)
        if newest_flet:
            self.slider = Slider(min=float(self.min), max=float(self.max), divisions=int(self.divisions), round=self.round, label="{value}" + self.suffix, value=float(self.pref[self.key]), tooltip=self.tooltip, expand=True, on_change=change_slider)
        else:
            self.slider = Slider(min=float(self.min), max=float(self.max), divisions=int(self.divisions), label="{value}" + self.suffix, value=float(self.pref[self.key]), tooltip=self.tooltip, expand=True, on_change=change_slider)
        self.slider_value = Text(f" {self.pref[self.key]}{self.suffix}", weight=FontWeight.BOLD)
        slider_text = GestureDetector(self.slider_value, on_tap=edit, mouse_cursor=ft.MouseCursor.PRECISE)
        slider_label = Text(f"{self.label}: ")
        left = Text("", visible=False)
        right = Text("", visible=False)
        if bool(self.left_text): left.value = self.left_text
        if bool(self.right_text): right.value = self.right_text
        self.slider_number = slider_text
        self.slider_row = Container(Row([slider_label, slider_text, self.slider_edit, left, self.slider, right]), expand=self.expand, visible=self._visible, col=self.col)
        return self.slider_row
    def set_value(self, value):
        self.value = value
        self.slider.value = value
        self.slider_edit.value = value
        self.pref[self.key] = value
        self.slider_value.value = f" {self.pref[self.key]}{self.suffix}"
        self.slider_value.update()
        self.slider.update()
    def set_min(self, value):
        self.min = value
        self.slider.min = value
    def set_max(self, value):
        self.max = value
        self.slider.max = value
    def set_divisions(self, value):
        self.divisions = value
        self.slider.divisions = value
    @property
    def show(self):
        return self._visible
    @show.setter
    def show(self, value):
        self._visible = value
        self.slider_row.visible = value
        self.slider_row.update()
    def update_slider(self):
        self.slider.update()

class Installing(UserControl):
    def __init__(self, message=""):
        super().__init__()
        self.message = message
        self.build()
    def build(self):
        self.message_txt = Text(self.message, style=ft.TextThemeStyle.BODY_LARGE, color=colors.SECONDARY, weight=FontWeight.BOLD, max_lines=3)
        self.details = Text("")
        self.progress = ProgressRing()
        return Container(content=Row([self.progress, Container(content=None, width=1), self.message_txt, Container(content=None, expand=True), self.details]), padding=padding.only(left=9, bottom=4))
    def set_message(self, msg):
        self.message_txt.value = msg
        self.message_txt.update()
    def set_details(self, msg):
        self.details.value = msg
        self.details.update()
    def show_progress(self, show):
        self.progress.visible = show
        self.progress.update()

class Switcher(UserControl):
    def __init__(self, label="", value=False, tooltip=None, disabled=False, on_change=None, active_color=None, active_track_color=None):
        super().__init__()
        self.label = label
        self.value = value
        self.tooltip = tooltip
        self.disabled = disabled
        self.on_change = on_change
        self.active_color = active_color if active_color != None else colors.PRIMARY_CONTAINER
        self.active_track_color = active_track_color if active_track_color != None else colors.PRIMARY
        self.build()
    def build(self):
        self.switch = Switch(label=f"  {self.label} ", value=self.value, disabled=self.disabled, active_color=self.active_color, active_track_color=self.active_track_color, on_change=self.on_change)
        if self.tooltip != None:
            return Tooltip(message=self.tooltip, content=self.switch)
        else:
            return self.switch

class RunConsole(UserControl):
    def __init__(self, title="", height=200, show_progress=True):
        super().__init__()
        self.title = title
        self.height = height
        self.show_progress = show_progress
        self.build()
    def add_text(self, text):
        try:
          self.column.controls.append(Text(text, style="titleSmall", color=colors.ON_SURFACE_VARIANT))
          self.column.update()
        except Exception:
          pass
    def clear_last(self):
        del self.column.controls[-1]
        self.column.update()
    def clear(self):
        self.main_column.controls.clear()
        self.main_column.update()
    def run_process(self, cmd_str, cwd=None):
        cmd_list = cmd_str if type(cmd_str) is list else cmd_str.split()
        if cwd is None:
          process = subprocess.Popen(cmd_str, shell = True, env=env, bufsize = 1, stdout=subprocess.PIPE, stderr = subprocess.STDOUT, encoding='utf-8', errors = 'replace' )
        else:
          process = subprocess.Popen(cmd_str, shell = True, cwd=cwd, env=env, bufsize = 1, stdout=subprocess.PIPE, stderr = subprocess.STDOUT, encoding='utf-8', errors = 'replace' )
        while True:
          realtime_output = process.stdout.readline()
          if realtime_output == '' and process.poll() is not None:
            break
          if "Downloading" in realtime_output:
              self.clear_last()
          self.add_text(realtime_output.strip())
          #self.column.controls.append(Text(realtime_output.strip(), style="titleSmall", color=colors.TERTIARY))
          #self.column.update()
          sys.stdout.flush()
        self.clear()
    def build(self):
        #self.column = Column([Row([Text(self.title, style=TextThemeStyle.TITLE_LARGE, color=colors.SECONDARY, weight=FontWeight.BOLD), Row(self.actions) if bool(self.actions) else Container(content=None)], alignment=MainAxisAlignment.SPACE_BETWEEN, spacing=0, vertical_alignment=CrossAxisAlignment.END)], spacing=4)
        self.column = Column([], spacing=1, scroll=ScrollMode.AUTO, auto_scroll=True) #, width=(page.width if page.web else page.window_width) - 20
        self.container = Container(content=self.column, border_radius=ft.border_radius.all(16), height=self.height, bgcolor=colors.SURFACE_VARIANT, padding=ft.padding.symmetric(10, 12))
        self.main_column = Column([])
        if bool(self.title):
            self.main_column.controls.append(Text(self.title, style=ft.TextThemeStyle.BODY_LARGE, color=colors.SECONDARY, weight=FontWeight.BOLD, max_lines=3))
        self.main_column.controls.append(self.container)
        if self.show_progress:
            self.main_column.controls.append(ProgressBar(bar_height=8))
        return self.main_column

class VideoPlayer(UserControl):
    def __init__(self, video_file="", width=500, height=500):
        super().__init__()
        self.video_file = video_file
        self.width = width
        self.height = height
        self.build()
    def build(self):
        import cv2
        import threading
        import time
        import base64
        cap = cv2.VideoCapture(self.video_file)
        b64_string = None
        blank_img = np.zeros((self.width, self.height, 1), dtype = "uint8")#cv2.imencode('.jpg', frame)
        b64_string = base64.b64encode(blank_img[1]).decode('utf-8')
        image_box = ft.Image(src_base64=b64_string, width=self.width, height=self.height)
        video_container = ft.Container(image_box, alignment=ft.alignment.center, expand=True)
        def update_images():
            while(cap.isOpened()):
                # Capture frame-by-frame
                ret, frame = cap.read()
                if ret == True:
                    # encode the resulting frame
                    jpg_img = cv2.imencode('.jpg', frame)
                    b64_string = base64.b64encode(jpg_img[1]).decode('utf-8')
                    image_box.src_base64 = b64_string
                    self.page.update()
                else:
                    break
                time.sleep(1/115)
            # when video is finished
            #self.video_container.content.clean()
            #self.video_container.content = ft.Text("Video Ended", size=20)
            self.page.update()
        update_image_thread = threading.Thread(target=update_images)
        update_image_thread.daemon = True
        update_image_thread.start()
        return video_container

class AudioPlayer(UserControl):
    def __init__(self, src="", display="", data=None, autoplay=False, page=None):
        super().__init__()
        self.src = src
        self.display = display
        self.data = data if data != None else src
        self.page = page
        self.state = "stopped"
        self.audio = Audio(src=self.src, autoplay=autoplay, on_state_changed=self.state_changed)
        self.page.overlay.append(self.audio)
        self.build()
    def play_audio(self, e):
        if self.state == "playing":
            self.audio.pause()
        elif self.state == "paused":
            self.audio.resume()
        else:
            self.audio.play()
    def state_changed(self, e):
        self.state = e.data
        #print(self.state)
        if e.data == "completed" or e.data == "paused":
            self.icon = icons.PLAY_CIRCLE_FILLED
        if e.data == "playing":
            self.icon = icons.PAUSE
        self.button.icon = self.icon
        self.button.update()
    def did_mount(self):
        self.page.update()
        try:
            duration = self.audio.get_duration()
        except Exception:
            duration = 0
            pass
        #print(duration)
        if duration > 0:
            dt = datetime.datetime.fromtimestamp(duration / 1000)
            dur = dt.strftime('%H:%M:%S.%f')[:-3]
            self.button.tooltip = f"Duration: {dur}"
            self.button.update()
    def build(self):
        self.icon = icons.PLAY_CIRCLE_FILLED
        dur = ""#dt.strftime('%H:%M:%S.%f')[:-3]
        self.button = IconButton(icon=self.icon, icon_size=48, tooltip=f"Duration: {dur}", on_click=self.play_audio, data=self.data)
        self.row = Row([self.button, Text(self.display)])
        return self.row


class VideoContainer(Container):
    """This will show a video you choose."""
    def __init__(
            self,
            video_path: str,
            fps: int = 0,
            play_after_loading=True,
            video_frame_fit_type: ft.ImageFit = None,
            video_progress_bar=True,
            video_play_button=True,
            exec_after_full_loaded=None,
            only_show_cover=False,
            content=None,
            ref=None,
            key=None,
            width=None,
            height=None,
            left=None,
            top=None,
            right=None,
            bottom=None,
            expand=None,
            col=None,
            opacity=None,
            rotate=None,
            scale=None,
            offset=None,
            aspect_ratio=None,
            animate_opacity=None,
            animate_size=None,
            animate_position=None,
            animate_rotation=None,
            animate_scale=None,
            animate_offset=None,
            on_animation_end=None,
            tooltip=None,
            visible=None,
            disabled=None,
            data=None,
            padding=None,
            margin=None,
            alignment=None,
            bgcolor=None,
            gradient=None,
            blend_mode=BlendMode.NONE,
            border=None,
            border_radius=None,
            image_src=None,
            image_src_base64=None,
            image_repeat=None,
            image_fit=None,
            image_opacity=1.0,#OptionalNumber = None,
            shape=None,
            clip_behavior=None,
            ink=None,
            animate=None,
            blur=None,
            shadow=None,
            url=None,
            url_target=None,
            theme=None,
            theme_mode=None,
            on_click=None,
            on_long_press=None,
            on_hover=None
    ):
        super().__init__(content, ref, key, width, height, left, top, right, bottom, expand, col, opacity, rotate,
                         scale, offset, aspect_ratio, animate_opacity, animate_size, animate_position, animate_rotation,
                         animate_scale, animate_offset, on_animation_end, tooltip, visible, disabled, data, padding,
                         margin, alignment, bgcolor, gradient, blend_mode, border, border_radius, image_src,
                         image_src_base64, image_repeat, image_fit, image_opacity, shape, clip_behavior, ink, animate,
                         blur, shadow, url, url_target, theme, theme_mode, on_click, on_long_press, on_hover)
        self.__cur_play_frame = 0
        self.__video_pause_button = None
        self.__video_play_button = None
        self.__video_is_play = False
        self.vid_duration = None
        self.fps = fps
        self.__video_is_full_loaded = None
        self.video_frames = None
        self.exec_after_full_loaded = exec_after_full_loaded
        if not os.path.isfile(video_path):
            raise FileNotFoundError("Cannot find the video at the path you set.")
        self.all_frames_of_video = []
        self.frame_length = 0
        self.__video_played = False
        self.video_progress_bar = video_progress_bar
        self.video_play_button = video_play_button
        if video_frame_fit_type is None:
            self.video_frame_fit_type = ft.ImageFit.CONTAIN
        self.__ui()
        if only_show_cover:
            self.read_video_cover(video_path)
            return
        if play_after_loading:
            print("Please wait the video is loading..\nThis will take a time based on your video size...")
            self.read_the_video(video_path)
        else:
            threading.Thread(target=self.read_the_video, args=[video_path], daemon=True).start()
        self.audio_path = None
        self.__audio_path = None
        self.get_video_duration(video_path)
        self.__frame_per_sleep = 1.0 / self.fps

    def show_play(self):
        self.__video_is_play = False
        self.__video_play_button.visible = True
        self.__video_pause_button.visible = False
        self.__video_play_button.update()
        self.__video_pause_button.update()

    def show_pause(self):
        self.__video_is_play = True
        self.__video_play_button.visible = False
        self.__video_pause_button.visible = True
        self.__video_play_button.update()
        self.__video_pause_button.update()

    def __ui(self):
        # the video tools control
        self.video_tool_stack = Stack(expand=False)
        self.content = self.video_tool_stack
        self.image_frames_viewer = Image(expand=True, visible=False, fit=self.video_frame_fit_type)
        self.video_tool_stack.controls.append(Row([self.image_frames_viewer], alignment=ft.MainAxisAlignment.CENTER))
        self.__video_progress_bar = Container(height=2, bgcolor=ft.colors.BLUE_200)
        self.video_tool_stack.controls.append(Row([self.__video_progress_bar], alignment=ft.MainAxisAlignment.START))

        def play_video(e):
            print(e)
            if self.__video_is_play:
                self.pause()
                self.show_play()
            else:
                self.show_pause()
                self.play()

        self.__video_play_button = IconButton(
            icon=icons.SMART_DISPLAY,
            icon_color=colors.WHITE54,
            icon_size=60,
            data=0,
            style=ButtonStyle(
                elevation=4,
            ),
            on_click=play_video,
            visible=True
        )
        self.__video_pause_button = IconButton(
            icon=icons.PAUSE_PRESENTATION,
            icon_color=colors.WHITE54,
            icon_size=60,
            data=0,
            style=ButtonStyle(
                elevation=4,
            ),
            on_click=play_video,
            visible=False
        )
        self.video_tool_stack.controls.append(
            Container(
                content=Row(
                    controls=[
                        self.__video_play_button,
                        self.__video_pause_button
                    ]
                ),
                padding=padding.only(25, 10, 10, 10),
                left=0,
                bottom=0,
            ),
        )
        if not self.video_progress_bar:
            self.__video_progress_bar.visible = False
        if not self.video_play_button:
            self.__video_play_button.visible = False

    def update_video_progress(self, frame_number):
        if not self.video_progress_bar:
            return
        percent_of_progress = frame_number / self.video_frames * 1
        if self.width:
            self.__video_progress_bar.width = percent_of_progress * 1 * self.width
        else:
            self.__video_progress_bar.width = percent_of_progress * 1 * self.page.width
        if self.__video_progress_bar.page is not None:
            try:
                self.__video_progress_bar.update()
            except Exception as e:
                pattern = r"control with ID '(.*)' not found"
                match = re.search(pattern, e.args[0])
                if not match:
                    print(e)
                return

    def update(self):
        self.image_frames_viewer.fit = self.video_frame_fit_type
        self.__video_progress_bar.visible = self.video_progress_bar
        return super().update()

    def play(self):
        """Play the video. (it's not blocking, because its on thread)."""
        if self.page is None:
            raise Exception("The control must be on page first.")
        self.__video_played = True
        threading.Thread(target=self.__play, daemon=True).start()

    def __play(self):
        self.image_frames_viewer.visible = True
        num = self.__cur_play_frame
        video_frames_len = len(self.all_frames_of_video)
        for index, i in enumerate(self.all_frames_of_video[self.__cur_play_frame:-1]):
            if not self.__video_played:
                self.__cur_play_frame = self.__cur_play_frame + index
                break
            if index + self.__cur_play_frame == video_frames_len - 2:
                self.__cur_play_frame = 0
            threading.Thread(target=self.update_video_progress, args=[num], daemon=True).start()
            self.image_frames_viewer.src_base64 = i
            try:
                self.image_frames_viewer.update()
            except Exception as e:
                pattern = r"control with ID '(.*)' not found"
                match = re.search(pattern, e.args[0])
                if not match:
                    print(e)
                return
            time.sleep(self.__frame_per_sleep)
            num += 1
        self.show_play()

    def pause(self):
        self.__video_played = False

    def read_video_cover(self, video_path):
        video = cv2.VideoCapture(video_path)
        frame_count = video.get(cv2.CAP_PROP_FRAME_COUNT)
        slice_frame_num = frame_count / 2
        video.set(cv2.CAP_PROP_POS_FRAMES, slice_frame_num)
        success, frame = video.read()
        _, buffer = cv2.imencode('.jpg', frame)
        encoded_frame = base64.b64encode(buffer).decode('utf-8')
        if self.image_frames_viewer.src_base64 is None:
            self.image_frames_viewer.src_base64 = encoded_frame
            self.image_frames_viewer.visible = True
            if self.image_frames_viewer.page is not None:
                self.image_frames_viewer.update()
        video.release()

    def read_the_video(self, video_path):
        video = cv2.VideoCapture(video_path)
        success, frame = video.read()
        while success:
            _, buffer = cv2.imencode('.jpg', frame)
            encoded_frame = base64.b64encode(buffer).decode('utf-8')
            self.all_frames_of_video.append(encoded_frame)
            if self.image_frames_viewer.src_base64 is None:
                self.image_frames_viewer.src_base64 = encoded_frame
                self.image_frames_viewer.visible = True
                if self.image_frames_viewer.page is not None:
                    self.image_frames_viewer.update()
            success, frame = video.read()
        video.release()
        self.__video_is_full_loaded = True
        if self.exec_after_full_loaded:
            self.exec_after_full_loaded()
        self.frame_length = len(self.all_frames_of_video)
        return self.all_frames_of_video

    def get_video_duration(self, video_path):
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            print("Error opening video file")
            return
        if self.fps == 0:
            fps = cap.get(cv2.CAP_PROP_FPS)
            self.fps = fps
        total_frames = cap.get(cv2.CAP_PROP_FRAME_COUNT)
        self.video_frames = total_frames
        duration = total_frames / fps
        self.vid_duration = duration
        cap.release()
''' Sample alt Object format
class Component(UserControl):
    def __init__(self):
        super().__init__()
        self.build()
    def search(self, e):
        pass
    def build(self):
        self.expand = True
        #self.table
        self.parameter = Ref[TextField]()
        return Column(controls=[])
class Main:
    def __init__(self):
        self.page = None
    def __call__(self, page: Page):
        self.page = page
        page.title = "Alternative Boot experiment"
        self.add_stuff()
    def add_stuff(self):
        self.page.add(Text("Some text", size=20), color=colors.ON_PRIMARY_CONTAINER, bgcolor=colors.PRIMARY_CONTAINER, height=45)
        self.page.update()
main = Main()'''

port = 80
if tunnel_type == "ngrok":
  #if bool(url):
  #  public_url = url
  #else:
    from pyngrok import conf, ngrok
    conf.get_default().ngrok_version = "v3"
    #public_url = ngrok.connect(port = str(port)).public_url
    public_url = ngrok.connect(port).public_url
elif tunnel_type == "localtunnel":
  if False:
  #if bool(url):
    public_url = url
  else:
    import re
    #print(run_sp('lt -p 80', realtime=False))
    localtunnel = subprocess.Popen(['lt', '--port', '8000', 'http'], stdout=subprocess.PIPE)
    url = str(localtunnel.stdout.readline())
    public_url = (re.search("(?P<url>https?:\/\/[^\s]+loca.lt)", url).group("url"))
else: public_url=""
from IPython.display import Javascript
if bool(public_url):
    import urllib
    if auto_launch_website:
        display(Javascript('window.open("{url}");'.format(url=public_url)))
        time.sleep(0.7)
        clear_output()
    print("\nOpen URL in browser to launch app in tab: " + str(public_url))
    if tunnel_type == "localtunnel":
        print("Copy/Paste the Password/Enpoint IP for localtunnel:",urllib.request.urlopen('https://ipv4.icanhazip.com').read().decode('utf8').strip("\n"))
def close_tab():
  display(Javascript("window.close('', '_parent', '');"))
#await google.colab.kernel.proxyPort(%s)
# Still not working to display app in Colab console, but tried.
def show_port(adr, height=500):
  display(Javascript("""
  (async ()=>{
    fm = document.createElement('iframe')
    fm.src = '%s'
    fm.width = '100%%'
    fm.height = '%d'
    fm.frameBorder = 0
    document.body.append(fm)
  })();
  """ % (adr, height) ))
#import requests
#r = requests.get('http://localhost:4040/api/tunnels')
#url = r.json()['tunnels'][0]['public_url']
#print(url)
#await google.colab.kernel.proxyPort(%s)
#get_ipython().system_raw('python3 -m http.server 8888 &')
#get_ipython().system_raw('./ngrok http 8501 &')
#show_port(public_url.public_url, port)0
#show_port(public_url.public_url)
#run_sp(f'python -m webbrowser -t "{public_url.public_url}"')
#webbrowser.open(public_url.public_url, new=0, autoraise=True)
#webbrowser.open_new_tab(public_url.public_url)
#ft.app(target=main, view=flet.WEB_BROWSER, port=port, host=socket_host)
#ft.app(target=main, view=flet.WEB_BROWSER, port=port, host=host_address)
#ft.app(target=main, view=flet.WEB_BROWSER, port=80, host=public_url.public_url)
#ft.app(target=main, view=flet.WEB_BROWSER, port=port, host="0.0.0.0")
#ft.app(target=main, view=ft.WEB_BROWSER, port=80, assets_dir=root_dir, upload_dir=root_dir, web_renderer="html")
#import logging
#logging.getLogger("flet_core").setLevel(logging.DEBUG)
#logging.basicConfig(level=logging.DEBUG)
if tunnel_type == "desktop":
  ft.app(target=main, assets_dir=root_dir, upload_dir=root_dir)
else:
  if newest_flet:
    ft.app(target=main, view=ft.WEB_BROWSER, port=8000, assets_dir=root_dir, upload_dir=root_dir, use_color_emoji=True)
  else:
    ft.app(target=main, view=ft.WEB_BROWSER, port=8000, assets_dir=root_dir, upload_dir=root_dir)
